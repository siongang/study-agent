{
  "file_id": "a4850aad-b76a-4fb8-946f-4fc91c415bbb",
  "path": "David H McIntyre_ Corinne A Manogue_ Janet Tate_ Oregon State Un - Quantum mechanics _ a paradigms approach (2012, Pearson ).pdf",
  "num_pages": 598,
  "pages": [
    "",
    "",
    "QUANTUM MECHANICS\nA Paradigms Approach\n",
    "This page intentionally left blank \n",
    "Boston   Columbus   Indianapolis   New York   San Francisco   Upper Saddle River  \nAmsterdam   Cape Town   Dubai   London   Madrid   Milan   Munich   Paris   Montréal   Toronto  \nDelhi   Mexico City   São Paulo   Sydney   Hong Kong   Seoul   Singapore   Taipei   Tokyo\nQUANTUM MECHANICS\nA Paradigms Approach\nDavid H. McIntyre\nOregon State University\nwith contributions from Corinne A. Manogue, Janet Tate  \nand the Paradigms in Physics group at Oregon State University\n",
    "Publisher: Jim Smith\nEditorial Manager: Laura Kenney\nSenior Project Editor: Katie Conley\nAssistant Editors: Peter Alston and Steven Le\nSenior Marketing Manager: Kerry McGinnis\nManaging Editor: Corinne Benson\nProduction Project Manager: Mary O’Connell\nProduction Management and Composition: \nElement LLC\nCover Design: Mark Ong\nManufacturing Buyer: Kathy Sleys\nManager, Rights and Permissions: Zina Arabia\nManager, Cover Visual Research & \nPermissions: Karen Sanatar\nPrinter and Binder: Courier, Westford\nCover Printer: Courier, Westford\nCover Images: David H. McIntyre\nCopyright © 2012 Pearson Education, Inc., publishing as Pearson Addison-Wesley, 1301 Sansome St., San Francisco, \nCA 94111. All rights reserved. Manufactured in the United States of America. This publication is protected by Copyright \nand permission should be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, \nor transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise. To obtain \npermission(s) to use material from this work, please submit a written request to Pearson Education, Inc., Permissions \nDepartment, 1900 E. Lake Ave., Glenview, IL 60025. For information regarding permissions, call (847) 486-2635.\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. Where \nthose designations appear in this book, and the publisher was aware of a trademark claim, the designations have been printed \nin initial caps or all caps.\nLibrary of Congress Cataloging-in-Publication Data\nMcIntyre, David H.\n Quantum mechanics : a paradigms approach / David H. McIntyre ; with  \ncontributions from Corinne A. Manogue, Janet Tate, and the Paradigms in \nPhysics group at Oregon State University.\n  p. cm.\n Includes bibliographical references and index.\n ISBN-13: 978-0-321-76579-6\n ISBN-10: 0-321-76579-6\n 1. Quantum theory. 2. Mechanics.  I. Manogue, Corinne A. II. Tate, Janet.\nIII. Oregon State University. IV. Title.\n QC174.12.M3785 2012\n 530.12--dc23\n                2011039322\nISBN 10: 0-321-76579-6  \nISBN 13: 978-0-321-76579-6\n1 2 3 4 5 6 7 8 9 10 —CRW—16 15 14 13 12 11\nwww.pearsonhighered.com\n",
    " \nv\n Brief Contents\n 1 Stern-Gerlach Experiments \n1\n 2 Operators and Measurement \n34\n 3 Schrödinger Time Evolution \n68\n 4 Quantum Spookiness \n97\n 5 Quantized Energies: Particle in a Box \n107\n 6 Unbound States \n161\n 7 Angular Momentum \n202\n 8 Hydrogen Atom \n250\n 9 Harmonic Oscillator \n275\n 10 Perturbation Theory \n312\n 11 Hyperﬁne Structure and the Addition of Angular Momenta \n355\n 12 Perturbation of Hydrogen \n382\n 13 Identical Particles \n410\n 14 Time-Dependent Perturbation Theory \n445\n 15 Periodic Systems \n469\n 16 Modern Applications of Quantum Mechanics \n502\nAppendices \n529\nIndex \n553\n",
    "This page intentionally left blank \n",
    " \nvii\nContents\nPreface \nxiii\nPrologue \nxix\n \n1 \u0002 Stern-Gerlach Experiments \n1\n1.1 \nStern-Gerlach Experiment 1\n1.1.1 \nExperiment 1 5\n1.1.2 \nExperiment 2 6\n1.1.3 \nExperiment 3 7\n1.1.4 \nExperiment 4 8\n1.2 \nQuantum State Vectors 10\n1.2.1 \nAnalysis of Experiment 1 16\n1.2.2 \nAnalysis of Experiment 2 16\n1.2.3 \nSuperposition States 19\n1.3 \nMatrix Notation 22\n1.4 \nGeneral Quantum Systems 25\n1.5 \nPostulates  27\nSummary 28\nProblems 29\nResources 32\nActivities 32\nFurther Reading 33\n \n2 \u0002 Operators and Measurement \n34\n2.1 \nOperators, Eigenvalues, and Eigenvectors 34\n2.1.1 \nMatrix Representation of Operators 37\n2.1.2 \nDiagonalization of Operators 38\n2.2 \nNew Operators 41\n2.2.1 \nSpin Component in a General Direction 41\n2.2.2 \nHermitian Operators 44\n2.2.3 \nProjection Operators 44\n2.2.4 \nAnalysis of Experiments 3 and 4 47\n2.3 \nMeasurement 50\n2.4 \nCommuting Observables 54\n2.5 \nUncertainty Principle 56\n2.6 \nS2 Operator 57\n2.7 \nSpin-1 System 59\n",
    "viii \nContents \n2.8 \nGeneral Quantum Systems 62\nSummary 63\nProblems 64\nResources 67\nActivities 67\n \n3 \u0002 Schrödinger Time Evolution \n68\n3.1 \nSchrödinger Equation 68\n3.2 \nSpin Precession 72\n3.2.1 \nMagnetic Field in the z-Direction 72\n3.2.2 \nMagnetic Field in a General Direction 78\n3.3 \nNeutrino Oscillations 84\n3.4 \nTime-Dependent Hamiltonians 87\n3.4.1 \nMagnetic Resonance 87\n3.4.2 \nLight-Matter Interactions 92\nSummary 93\nProblems 94\nResources 96\nActivities 96\nFurther Reading 96\n \n4 \u0002 Quantum Spookiness \n97\n4.1 \nEinstein-Podolsky-Rosen Paradox 97\n4.2 \nSchrödinger Cat Paradox 102\nProblems 105\nResources 106\nFurther Reading 106\n \n5 \u0002 Quantized Energies: Particle in a Box \n107\n5.1 \nSpectroscopy 107\n5.2 \nEnergy Eigenvalue Equation 110\n5.3 \nThe Wave Function 112\n5.4 \nInﬁnite Square Well 119\n5.5 \nFinite Square Well 128\n5.6 \nCompare and Contrast 133\n5.6.1 \nWave Function Curvature 133\n5.6.2 \nNodes 135\n5.6.3 \nBarrier Penetration 135\n5.6.4 \nInversion Symmetry and Parity 136\n5.6.5 \nOrthonormality 136\n5.6.6 \nCompleteness 137\n5.7 \nSuperposition States and Time Dependence 137\n5.8 \nModern Application: Quantum Wells and Dots 146\n5.9 \n Asymmetric Square Well: Sneak Peek at  \nPerturbations 147\n5.10  Fitting Energy Eigenstates by Eye or by  \nComputer 150\n5.10.1 Qualitative (Eyeball) Solutions 150\n",
    "Contents  \n \nix\n5.10.2 Numerical Solutions 151\n5.10.3 General Potential Wells 154\nSummary 154\nProblems 156\nResources 159\nActivities 159\nFurther Reading 160\n \n6 \u0002 Unbound States \n161\n6.1 \nFree Particle Eigenstates 161\n6.1.1 \nEnergy Eigenstates 161\n6.1.2 \nMomentum Eigenstates 163\n6.2 \nWave Packets 168\n6.2.1 \nDiscrete Superposition 168\n6.2.2 \nContinuous Superposition 171\n6.3 \nUncertainty Principle 176\n6.3.1 \nEnergy Estimation 180\n6.4 \nUnbound States and Scattering 181\n6.5 \nTunneling Through Barriers 188\n6.6 \nAtom Interferometry 192\nSummary 197\nProblems 197\nResources 201\nActivities 201\nFurther Reading 201\n \n7 \u0002 Angular Momentum \n202\n7.1 \n Separating Center-of-Mass and Relative  \nMotion 204\n7.2 \n Energy Eigenvalue Equation in Spherical  \nCoordinates 208\n7.3 \nAngular Momentum 210\n7.3.1 \nClassical Angular Momentum 210\n7.3.2 \n Quantum Mechanical Angular  \nMomentum 210\n7.4 \nSeparation of Variables: Spherical Coordinates 215\n7.5 \nMotion of a Particle on a Ring 218\n7.5.1 \nAzimuthal Solution 220\n7.5.2 \n Quantum Measurements on a Particle  \nConﬁned to a Ring 223\n7.5.3 \nSuperposition States 224\n7.6 \nMotion on a Sphere 227\n7.6.1 \nSeries Solution of Legendre’s Equation 228\n7.6.2 \nAssociated Legendre Functions 233\n7.6.3 \nEnergy Eigenvalues of a Rigid Rotor 236\n7.6.4 \nSpherical Harmonics 237\n7.6.5 \nVisualization of Spherical Harmonics 240\n",
    "x \nContents \nSummary 245\nProblems 245\nResources 249\nActivities 249\n \n8 \u0002 Hydrogen Atom \n250\n8.1 \nThe Radial Eigenvalue Equation 250\n8.2 \nSolving the Radial Equation 252\n8.2.1 \n Asymptotic Solutions to the Radial  \nEquation 252\n8.2.2 \nSeries Solution to the Radial Equation 253\n8.3 \nHydrogen Energies and Spectrum 256\n8.4 \nThe Radial Wave Functions 261\n8.5 \nThe Full Hydrogen Wave Functions 263\n8.6 \nSuperposition States 270\nSummary 272\nProblems 272\nResources 274\nActivities 274\nFurther Reading 274\n \n9 \u0002 Harmonic Oscillator \n275\n9.1 \nClassical Harmonic Oscillator 275\n9.2 \nQuantum Mechanical Harmonic Oscillator 277\n9.3 \nWave Functions 284\n9.4 \nDirac Notation 289\n9.5 \nMatrix Representations 293\n9.6 \nMomentum Space Wave Function 296\n9.7 \nThe Uncertainty Principle 298\n9.8 \nTime Dependence 300\n9.9 \nMolecular Vibrations 305\nSummary 307\nProblems 308\nResources 311\nActivities 311\nFurther Reading 311\n \n10 \u0002 Perturbation Theory \n312\n10.1 Spin-1/2 Example 313\n10.2 General Two-Level Example 317\n10.3 Nondegenerate Perturbation Theory 319\n10.3.1 First-Order Energy Correction 320\n10.3.2 First-Order State Vector Correction 324\n10.4  Second-Order Nondegenerate Perturbation  \nTheory 329\n10.5 Degenerate Perturbation Theory 336\n10.6 More Examples 343\n",
    "Contents  \n \nxi\n10.6.1 Harmonic Oscillator 343\n10.6.2 Stark Effect in Hydrogen 346\nSummary 351\nProblems 352\n \n11 \u0002  Hyperﬁne Structure and the Addition of \nAngular Momenta \n355\n11.1 Hyperﬁne Interaction 355\n11.2 Angular Momentum Review 357\n11.3 Angular Momentum Ladder Operators 359\n11.4 Diagonalization of the Hyperﬁne Perturbation 361\n11.5 The Coupled Basis 365\n11.6 Addition of Generalized Angular Momenta 370\n11.7  Angular Momentum in Atoms and Spectroscopic  \nNotation 377\nSummary 377\nProblems 379\nResources 381\nActivities 381\nFurther Reading 381\n \n12 \u0002 Perturbation of Hydrogen \n382\n12.1 Hydrogen Energy Levels 382\n12.2 Fine Structure of Hydrogen 386\n12.2.1 Relativistic Correction 386\n12.2.2 Spin-Orbit Coupling 388\n12.3 Zeeman Effect 393\n12.3.1 Zeeman Effect without Spin 394\n12.3.2 Zeeman Effect with Spin 396\n12.3.2.1 Weak magnetic ﬁeld 396\n12.3.2.2 Strong magnetic ﬁeld 402\n12.3.2.3 Intermediate magnetic ﬁeld 403\n12.3.3  Zeeman Perturbation of the 1s \nHyperﬁne Structure 405\nSummary 407\nProblems 407\nResources 409\nActivities 409\nFurther Reading 409\n \n13 \u0002 Identical Particles \n410\n13.1 Two Spin-1/2 Particles 410\n13.2 Two Identical Particles in One Dimension 414\n13.2.1 Two-Particle Ground State 415\n13.2.2 Two-Particle Excited State 416\n13.2.3 Visualization of States 417\n13.2.4 Exchange Interaction 420\n",
    "xii \nContents \n13.2.5  Consequences of the Symmetrization  \nPostulate 421\n13.3 Interacting Particles 423\n13.4 Example: The Helium Atom 427\n13.4.1 Helium Ground State 428\n13.4.2 Helium Excited States 431\n13.5 The Periodic Table 434\n13.6 Example: The Hydrogen Molecule 437\n13.6.1 The Hydrogen Molecular Ion H2\n+ 438\n13.6.2 The Hydrogen Molecule H2 440\nSummary 442\nProblems 442\nResources 444\nFurther Reading 444\n \n14 \u0002 Time-Dependent Perturbation Theory \n445\n14.1 Transition Probability 445\n14.2 Harmonic Perturbation 450\n14.3 Electric Dipole Interaction 454\n14.3.1 Einstein Model: Broadband Excitation 456\n14.3.2 Laser Excitation 460\n14.4 Selection Rules 462\nSummary 466\nProblems 467\nResources 468\nFurther Reading 468\n \n15 \u0002 Periodic Systems \n469\n15.1  The Energy Eigenvalues and Eigenstates of a  \nPeriodic Chain of Wells 471\n15.1.1 A Two-Well Chain 471\n15.1.2 N-Well Chain 473\n15.2  Boundary Conditions and the Allowed Values  \nof k 476\n15.3 The Brillouin Zones 478\n15.4 Multiple Bands from Multiple Atomic Levels 478\n15.5 Bloch’s Theorem and the Molecular States 480\n15.6 Molecular Wave Functions—a Gallery 482\n15.7 The Density of States 484\n15.8 Calculation of the Model Parameters 486\n15.8.1 LCAO Summary 488\n15.9 The Kronig-Penney Model 489\n15.10  Practical Applications: Metals, Insulators, and  \nSemiconductors 491\n15.11 Effective Mass 494\n15.12 Direct and Indirect Band Gaps 496\n15.13 New Directions—Low-Dimensional Carbon 497\n",
    "Contents  \n \nxiii\nSummary 498\nProblems 499\nResources 500\nActivities 500\nFurther Reading 500\n \n16 \u0002 Modern Applications of Quantum Mechanics \n502\n16.1  Manipulating Atoms with Quantum  \nMechanical Forces 502\n16.1.1 Magnetic Trapping 502\n16.1.2 Laser Cooling 506\n16.2 Quantum Information Processing 514\n16.2.1 Quantum Bits—Qubits 515\n16.2.2 Quantum Gates 518\n16.2.3 Quantum Teleportation 524\nSummary 526\nProblems 527\nResources 528\nFurther Reading 528\nAppendix A: Probability \n529\nAppendix B: Complex Numbers \n533\nAppendix C: Matrices \n537\nAppendix D: Waves and Fourier Analysis \n541\nAppendix E: Separation of Variables \n547\nAppendix F: Integrals \n549\nAppendix G: Physical Constants \n551\nIndex \n553\n",
    "This page intentionally left blank \n",
    " \nxv\nPreface\nThis text is designed to introduce undergraduates at the junior and senior levels to quantum mechan-\nics. The text is an outgrowth of the new physics major curriculum developed by the Paradigms in \nPhysics program at Oregon State University. This new curriculum distributes material from the sub-\ndisciplines throughout the two upper-division years and provides students with a more gradual tran-\nsition between introductory and advanced levels. We have also incorporated and developed modern \npedagogical strategies to help improve student learning. This text covers the quantum mechanical \naspects of our curriculum in a way that can also be used in traditional curricula, but that still pre-\nserves the advantages of the Paradigms approach to the ordering of materials and the use of student \nengagement activities.\nPARADIGMS PROGRAM\nThe Paradigms project began in 1997, when the Department of Physics at Oregon State University \nbegan an extensive revision of the upper-division physics major. In an effort to encourage students \nto draw connections between the subdisciplines of physics, the structure of the Paradigms has been \ncrafted to mimic the organization of expert physics knowledge. Students are presented with a model \nof how physicists organize their understanding of physical phenomena and problem solving. Each \nof the nine short junior-year Paradigms courses focuses on a speciﬁc paradigm or class of physics \nproblems that serves as the centerpiece of the course and on which different tools and skills are built. \nIn the senior year, students resume a more traditional curriculum, taking six capstone courses in \nthe traditional disciplines. This curriculum incorporates a diverse set of student activities that allow \nstudents to stay actively engaged in the classroom and to work together in constructing their under-\nstanding of physics. Computer resources are used frequently to help students visualize the systems \nthey are studying.\nCONTENT AND APPROACH\nQuantum mechanics is integrated into four of the junior-year Paradigms courses and one senior-year \ncapstone course at Oregon State University. This text includes all the quantum mechanics topics \ncovered in those ﬁve courses. We adopt a “spins-ﬁrst” approach by introducing quantum mechanics \nthrough the analysis of sequential Stern-Gerlach spin measurements. This approach is based upon \nprevious presentations of spin systems by Feynman, Leighton, and Sands; Cohen-Tannoudji, Diu, \nand Laloe; Sakurai; and Townsend. The aim of the spins-ﬁrst approach is twofold: (1) To imme-\ndiately immerse students in the inherently quantum mechanical aspects of physics by focusing on \nsimple measurements that have no classical explanation, and (2) To give students early and extensive \nexperience with the mechanics of quantum mechanics in the forms of Dirac and matrix notation. \n",
    "xvi \nPreface \nThe  simplicity of the spin-1/2 and spin-1 systems allows the students to focus on these new features, \nwhich run counter to classical mechanics.\nThe ﬁrst three chapters of this text deal exclusively with spin systems and extensions to general \ntwo- and three-state quantum mechanical systems. The basic postulates of quantum mechanics are \nillustrated through their manifestation in the Stern-Gerlach experiments. After these three chapters, \nstudents have the tools to tackle any quantum mechanical problem presented in Dirac or matrix \nnotation. After a brief interlude into quantum spookiness (the EPR Paradox and Schrödinger’s cat) \nin Chapter 4, we tackle the traditional wave function aspects of quantum mechanics. We present \nseveral quantum systems—a particle in a box, on a ring, on a sphere, the hydrogen atom, and the \nharmonic oscillator—and emphasize their common features and their connections to the basic pos-\ntulates. The differential equations of angular momentum and the hydrogen atom radial problem are \nsolved in detail to expose students to the rigor of series solutions, though we stress that these are \nagain eigenvalue equations, no different in principle from the spin eigenvalue equations. Whenever \npossible, we continue the use of Dirac notation and matrix notation learned in the spin chapters, \nemphasizing the importance of ﬂuency in multiple representations. We build upon the spins-ﬁrst \napproach by using the spin-1/2 example to introduce perturbation theory, the addition of angular \nmomentum, and identical particles.\nUSAGE\nAt Oregon State University, the content of this text is taught in ﬁve courses as shown below.\nJunior-Year Paradigms Courses\nSpin and Quantum \nMeasurement\nWaves\nCentral Forces\nPeriod Systems\n1.  Stern-Gerlach  \nExperiments\n2.  Operators and  \nMeasurement\n3.  Schrödinger Time \nEvolution\n4. Quantum Spookiness\nMechanical waves \nand EM waves\n5.  Quantized Energies:  \nParticle in a Box\n6. Unbound States\nPlanetary orbits\n7.  Angular  \nMomentum\n8. Hydrogen Atom\nCoupled \nOscillations\n15.  Periodic  \nSystems\nSenior-Year Quantum Mechanics Capstone Course\n 9. Harmonic Oscillator\n10. Perturbation Theory\n11.  Hyperﬁne Structure \nand the Addition of \nAngular Momentum\n12.  Perturbation of \nHydrogen\n13. Identical Particles\n14.  Time-Dependent \nPerturbation \nTheory\n16.  Modern \nApplications\nFor a traditional curriculum, the content of this text would cover a full-year course, either two \nsemesters or three quarters. A proposed weekly outline for two 15-week semesters or three 10-week \nquarters is shown below.\n",
    "Preface  \nxvii\nWeek\nChapter\nTopics\n1\n1\nStern-Gerlach experiment, Quantum State Vectors, Bra-ket notation \n2\n1\nMatrix notation, General Quantum Systems\n3\n2\nOperators, Measurement, Commuting Observables\n4\n2\nUncertainty Principle, S2 Operator, Spin-1 System\n5\n3\nSchrödinger Equation, Time Evolution\n6\n3\nSpin Precession, Neutrino Oscillations, Magnetic Resonance\n7\n4\nEPR Paradox, Bell’s Inequalities, Schrödinger’s Cat\n8\n5\nEnergy Eigenvalue Equation, Wave Function\n9\n5\nOne-Dimensional Potentials, Finite Well, Inﬁnite Well\n10\n6\nFree Particle, Wave Packets, Momentum Space\n11\n6\nUncertainty Principle, Barriers\n12\n7\nThree-Dimensional Energy Eigenvalue Equation, Separation of Variables\n13\n7\nAngular Momentum, Motion on a Ring and Sphere, Spherical Harmonics\n14\n8\nHydrogen Atom, Radial Equation, Energy Eigenvalues\n15\n8\nHydrogen Wave Functions, Spectroscopy\n16\n9\n1-D Harmonic Oscillator, Operator Approach, Energy Spectrum\n17\n9\nHarmonic Oscillator Wave Functions, Matrix Representation\n18\n9\nMomentum Space Wave Functions, Time Dependence, Molecular Vibrations\n19\n10\nTime-Independent Perturbation Theory: Nondegenerate, Degenerate\n20\n10\nPerturbation Examples: Harmonic Oscillator, Stark Effect in Hydrogen\n21\n11\nHyperﬁne Structure, Coupled Basis\n22\n11\nAddition of Angular Momenta, Clebsch-Gordan Coefﬁcients\n23\n12\nHydrogen Atom: Fine Structure, Spin-Orbit, Zeeman Effect\n24\n13\nIdentical Particles, Symmetrization, Helium Atom\n25\n14\nTime-Dependent Perturbation Theory, Harmonic Perturbation\n26\n14\nRadiation, Selection Rules\n27\n15\nPeriodic Potentials, Bloch’s Theorem\n28\n15\nDispersion Relation, Density of States, Semiconductors\n29\n16\nModern Applications of Quantum Mechanics, Laser Cooling and Trapping\n30\n16\nQuantum Information Processing\n",
    "xviii \nPreface \nAUDIENCE AND EXPECTED BACKGROUND\nThe intended audience is junior and senior physics majors, who are expected to have taken intermediate-\nlevel courses in modern physics and linear algebra. No other upper-level physics or mathematics courses \nare required. For our own students, we review matrix algebra in a seven contact hour “preface” course \nthat precedes the Paradigms courses that teach quantum mechanics. The material for that preface course \nis in Appendix C. The material in Appendix B summarizes an earlier Paradigms course on oscillations, \nand the material in Appendix D summarizes the classical wave part of the Paradigms course on waves.\nSTUDENT ACTIVITIES AND WEBSITE\nStudent engagement activities are an integral part of the Paradigms curriculum. All of the activities \nthat we have developed are freely available on our wiki website:\nhttp://physics.oregonstate.edu/portfolioswiki\nThe wiki contains a wealth of information about the Paradigms project, the courses we teach, and the \nmaterials we have developed. Details about individual activities include descriptions, student handouts, \ninstructor’s guides, advice about how to use active engagement strategies, videos of classroom prac-\ntice, narratives of classroom activities, and comments from users—both internal and external to Oregon \nState University. This is a dynamic website that is continually updated as we develop new activities and \nimprove existing ones. We encourage you to visit the website and join the community. E-mail us with \ncorrections, additions, and suggestions.\nEach of the quantum mechanics activities that we use in our ﬁve courses is referenced in the \nresource section at the end of the appropriate chapter in the text. The quantum mechanics activities are \ncollected within the wiki website with a direct link: \nwww.physics.oregonstate.edu/qmactivities \nThese activities include different types of activities such as computer-based activities, group activities, \nand class response activities. The most extensive activity is a computer simulation of Stern-Gerlach \nexperiments. This SPINS software is a full-featured, menu-driven application that allows students to \nsimulate successive Stern-Gerlach measurements and explore incompatible observables, eigenstate \nexpansions, interference, and quantum dynamics. The use of the SPINS software facilitates our spins-\nﬁrst approach. The beauty of the simulation is that students steeped in classical physics perform a foun-\ndational quantum experiment and learn the most fascinating and counterintuitive aspects of quantum \nmechanics at an early stage.\nACKNOWLEDGMENTS\nThis work is the product of a broad and energetic community of educators and students within the \nParadigms in Physics program. I thank all of our students for their hard work, insights, and innu-\nmerable suggestions. My colleagues Corinne Manogue and Janet Tate have developed some of the \ncourses upon which this text is based. They have worked with me throughout the writing of this text \nand I am indebted to them for their valuable contributions. I gratefully acknowedge my fellow faculty \nwho have developed and taught in the new curriculum: Dedra Demaree, Tevian Dray, Tomasz Gieb-\nultowicz, Elizabeth Gire, William Hetherington, Henri Jansen, Kenneth Krane, Yun-Shik Lee, Victor \nMadsen, Ethan Minot, Oksana Ostroverkhova, David Roundy, Philip Siemens, Albert Stetz, William \n",
    "Preface   \nxix\nWarren, and Allen Wasserman. I would also like to acknowledge the important contributions of early \nteaching assistants Kerry Browne, Jason Janesky, Cheryl Klipp, Katherine Meyer, Steve Sahyun, and \nEmily Townsend—their expertise, dedication, and enthusiasm were above and beyond the call of \nduty. The many subsequent teaching assistants have also been enthusiastic and valued contributors. \nI also thank those who have contributed in various ways to the development of activities: Mario Bel-\nloni, Tim Budd, Wolfgang Christian, Paco Esquembre, Lichun Jia, and Shannon Mayer. I particularly \nthank Daniel Schroeder for sharing his original SPINS software. I acknowledge useful and construc-\ntive feedback from Jeffrey Dunham, Joshua Folk, Rubin Landau, Edward (Joe) Redish, Joseph Roth-\nberg, Homeyra Sadaghiani, Daniel Schroeder, Chandralekha Singh, and Daniel Styer. The Paradigms \nadvisory committee has also provided valuable feedback and I acknowledge David Grifﬁths, Bruce \nMason, William McCallum, Harriett Platsek, and Michael Wittmann for their help. I am grateful to the \nsuccessive Physics Department chairs, Kenneth Krane and Henri Jansen, and Deans Fred Horne and \nSherman Bloomer at Oregon State University for their endorsement of the Paradigms project.\nThis material is based on work supported by the National Science Foundation under Grant Nos. \n9653250, 0231194, and 0618877. Any opinions, ﬁndings, and conclusions or recommendations \nexpressed in this material are those of the authors and do not necessarily reﬂect the views of the \nNational Science Foundation. I thank Duncan McBride and Jack Hehn for their encouragement and \nsupport of our endeavor.\nJim Smith at Addison Wesley has been enthusiastic about this project from the early stages. Peter \nAlston has navigated me through the editorial process with skill and patience. I am grateful to them \nand also to Katie Conley, Steven Le, and the rest of the staff at Addison Wesley for their work to pro-\nduce this text.\nDavid H. McIntyre\nCorvallis, Oregon  \nNovember 2011\n",
    "This page intentionally left blank \n",
    " \nxxi\nPrologue\nIt was a dark and stormy night. Erwin huddled under his covers as he had done numerous times that \nsummer. As the wind and rain lashed at the window, he feared having to retreat to the storm cellar \nonce again. The residents of Erwin’s apartment building sought shelter whenever there were threats of \ntornadoes in the area. While it was safe down there, Erwin feared the ridicule he would face once again \nfrom the other school boys. In the rush to the cellar, Erwin seemed to always end up with a random \npair of socks, and the other boys teased him about it mercilessly.\nNot that Erwin hadn’t tried hard to solve this problem. He had a very simple collection of \nsocks—black or white, for either school or play; short or long, for either trousers or lederhosen. \nAfter the ﬁrst few teasing episodes from the other boys, Erwin had sorted his socks into two sepa-\nrate drawers. He placed all the black socks in one drawer and all the white socks in another drawer. \nErwin ﬁgured he could determine an individual sock’s length in the dark of night simply by feel-\ning it, but he had to have them presorted into white and black because the apartment generally lost \npower before the call to the shelter.\nUnfortunately, Erwin found that this presorting of the socks by color was ineffective. Whenever \nhe reached into the white sock drawer and chose two long socks, or two short socks, there was a 50% \nprobability of any one sock being black or white. The results from the black sock drawer were the \nsame. The socks seemed to have “forgotten” the color that Erwin had determined previously.\nErwin also tried sorting the socks into two drawers based upon their length, without regard to \ncolor. When he chose black or white socks from these long and short drawers, the socks had also “for-\ngotten” whether they were long or short.\nAfter these fruitless attempts to solve his problem through experiments, Erwin decided to save \nhimself the fashion embarrassment, and he replaced his sock collection with a set of medium length \nbrown socks. However, he continued to ponder the mysteries of the socks throughout his childhood.\nAfter many years of daydreaming about the mystery socks, Erwin Schrödinger proposed his the-\nory of “Quantum Socks” and become famous. And that is the beginning of the story of the quantum \nsocks.\nThe End.\nFarfetched?? You bet. But Erwin’s adventure with his socks is the way quantum mechanics works. \nRead on.\n",
    "This page intentionally left blank \n",
    " \n1\nC H A P T E R \n1\nStern-Gerlach Experiments\nIt was not a dark and stormy night when Otto Stern and Walther  Gerlach performed their now famous \nexperiment in 1922. The Stern-Gerlach experiment demonstrated that measurements on microscopic \nor quantum particles are not always as certain as we might expect. Quantum particles behave as mys-\nteriously as Erwin’s socks—sometimes forgetting what we have already measured. Erwin’s adven-\nture with the mystery socks is farfetched because you know that everyday objects do not behave like \nhis socks. If you observe a sock to be black, it remains black no matter what other properties of the \nsock you observe. However, the Stern- Gerlach experiment goes against these ideas. Microscopic or \nquantum particles do not behave like the classical objects of your everyday experience. The act of \nobserving a quantum particle affects its measurable properties in a way that is foreign to our classical \nexperience.\nIn these ﬁrst three chapters, we focus on the Stern-Gerlach experiment because it is a conceptu-\nally simple experiment that demonstrates many basic principles of quantum mechanics. We discuss \na variety of experimental results and the quantum theory that has been developed to predict those \nresults. The mathematical formalism of quantum mechanics is based upon six postulates that we will \nintroduce as we develop the theoretical framework. (A complete list of these postulates is in Section 1.5.) \nWe use the Stern-Gerlach experiment to learn about quantum mechanics theory for two primary reasons: \n(1) It demonstrates how quantum mechanics works in principle by illustrating the postulates of quan-\ntum mechanics, and (2) it demonstrates how quantum mechanics works in practice through the use \nof Dirac notation and matrix mechanics to solve problems. By using a simple example, we can focus \non the principles and the new mathematics, rather than having the complexity of the physics obscure \nthese new aspects.\n1.1 \u0002 STERN-GERLACH EXPERIMENT\nIn 1922 Otto Stern and Walther Gerlach performed a seminal experiment in the history of quantum \nmechanics. In its simplest form, the experiment consisted of an oven that produced a beam of neu-\ntral atoms, a region of space with an inhomogeneous magnetic ﬁeld, and a detector for the atoms, as \ndepicted in Fig. 1.1. Stern and Gerlach used a beam of silver atoms and found that the beam was split \ninto two in its passage through the magnetic ﬁeld. One beam was deﬂected upwards and one down-\nwards in relation to the direction of the magnetic ﬁeld gradient.\nTo understand why this result is so at odds with our classical expectations, we must ﬁrst analyze \nthe experiment classically. The results of the experiment suggest an interaction between a neutral parti-\ncle and a magnetic ﬁeld. We expect such an interaction if the particle possesses a magnetic moment M.\nThe potential energy of this interaction is E = -M~B, which results in a force F = \u00021M~B2. In the \n",
    "2 \nStern-Gerlach Experiments\nStern-Gerlach experiment, the magnetic ﬁeld gradient is primarily in the z-direction, and the resulting \nz-component of the force is\n \n Fz =\n0\n0z\n 1M~B2 \n(1.1)\n \n \u0002 mz \n0Bz\n0z\n .\n \nThis force is perpendicular to the direction of motion and deﬂects the beam in proportion to the com-\nponent of the magnetic moment in the direction of the magnetic ﬁeld gradient.\nNow consider how to understand the origin of the atom’s magnetic moment from a classical view-\npoint. The atom consists of charged particles, which, if in motion, can produce loops of current that give \nrise to magnetic moments. A loop of area A and current I produces a magnetic moment\n \nm = IA \n(1.2)\nin MKS units. If this loop of current arises from a charge q traveling at speed v in a circle of radius r, \nthen\n \n m =\nq\n2pr>v\n pr 2 \n \n = qrv\n2\n \n(1.3)\n \n =\nq\n2m\n L ,\n \nwhere L = mrv is the orbital angular momentum of the particle. In the same way that the earth \nrevolves around the sun and rotates around its own axis, we can also imagine a charged particle in \nan atom having orbital angular momentum L and a new property, the intrinsic angular momen-\ntum, which we label S and call spin. The intrinsic angular momentum also creates current loops, \nso we expect a similar relation between the magnetic moment M and S. The exact calculation \nx\nOven\nCollimator\nMagnet\nDetector\nMagnet\nCross-Section\ny\nz\nS\nS\nN\nN\nFIGURE 1.1 Stern-Gerlach experiment to measure the spin component of neutral \nparticles along the z-axis. The magnet cross section at right shows the inhomogeneous \nﬁeld used in the experiment.\n",
    "1.1 Stern-Gerlach Experiment \n3\ninvolves an integral over the charge distribution, which we will not do. We simply assume that we \ncan relate the magnetic moment to the intrinsic angular momentum in the same fashion as Eq. (1.3), \ngiving\n \nM = g q\n2m\n S , \n(1.4)\nwhere the dimensionless gyroscopic ratio g contains the details of that integral.\nA silver atom has 47 electrons, 47 protons, and 60 or 62 neutrons (for the most common isotopes). \nThe magnetic moments depend on the inverse of the particle mass, so we expect the heavy protons and \nneutrons (\u00032000 me) to have little effect on the magnetic moment of the atom and so we neglect them. \nFrom your study of the periodic table in chemistry, you recall that silver has an electronic conﬁgura-\ntion 1s22s22p63s23p64s23d104p64d105s1, which means that there is only the lone 5s electron outside \nof the closed shells. The electrons in the closed shells can be represented by a spherically symmetric \ncloud with no orbital or intrinsic angular momentum (unfortunately we are injecting some quantum \nmechanical knowledge of atomic physics into this classical discussion). That leaves the lone 5s elec-\ntron as a contributor to the magnetic moment of the atom as a whole. An electron in an s state has no \norbital angular momentum, but it does have spin. Hence the magnetic moment of this electron, and \ntherefore of the entire neutral silver atom, is\n \nM = -g e\n2me\n S , \n(1.5)\nwhere e is the magnitude of the electron charge. The classical force on the atom can now be written as\n \nFz \u0002 -g e\n2me\n Sz \n0Bz\n0z\n . \n(1.6)\nThe deﬂection of the beam in the Stern-Gerlach experiment is thus a measure of the component (or pro-\njection) Sz of the spin along the z-axis, which is the orientation of the magnetic ﬁeld gradient.\nIf we assume that the 5s electron of each atom has the same magnitude 0 S0  of the intrinsic angular \nmomentum or spin, then classically we would write the z-component as Sz = 0 S0 cos u, where u is \nthe angle between the z-axis and the direction of the spin S. In the thermal environment of the oven, \nwe expect a random distribution of spin directions and hence all possible angles u. Thus we expect \nsome continuous distribution (the details are not important) of spin components from Sz = - 0 S0  to \nSz = + 0 S0 , which would yield a continuous spread in deﬂections of the silver atomic beam. Rather, \nthe experimental result that Stern and Gerlach observed was that there are only two deﬂections, indi-\ncating that there are only two possible values of the z-component of the electron spin. The magnitudes \nof these deﬂections are consistent with values of the spin component of\n \nSz = { U\n2\n ,  \n(1.7)\nwhere U is Planck’s constant h divided by 2p and has the numerical value\n \n U = 1.0546 * 10-34  J~s \n \n = 6.5821 * 10-16  eV~s. \n \n(1.8)\nThis result of the Stern-Gerlach experiment is evidence of the quantization of the electron’s \nspin angular momentum component along an axis. This quantization is at odds with our classical \n",
    "4 \nStern-Gerlach Experiments\nexpectations for this measurement. The factor of 1/2 in Eq. (1.7) leads us to refer to this as a \nspin-1/2 system.\nIn this example, we have chosen the z-axis along which to measure the spin component, but there \nis nothing special about this direction in space. We could have chosen any other axis and we would \nhave obtained the same results.\nNow that we know the ﬁne details of the Stern-Gerlach experiment, we simplify the experiment \nfor the rest of our discussions by focusing on the essential features. A simpliﬁed schematic representa-\ntion of the experiment is shown in Fig. 1.2, which depicts an oven that produces the beam of atoms, a \nStern-Gerlach device with two output ports for the two possible values of the spin component, and two \ncounters to detect the atoms leaving the output ports of the Stern-Gerlach device. The Stern-Gerlach \ndevice is labeled with the axis along which the magnetic ﬁeld is oriented. The up and down arrows \nindicate the two possible measurement results for the device; they correspond respectively to the \nresults Sz = {U>2 in the case where the ﬁeld is oriented along the z-axis. There are only two possible \nresults in this case, so they are generally referred to as spin up and spin down. The physical quantity \nthat is measured, Sz in this case, is called an observable. In our detailed discussion of the experiment \nabove, we chose the ﬁeld gradient in such a manner that the spin up states were deﬂected upwards. \nIn this new simpliﬁcation, the deﬂection itself is not an important issue. We simply label the output \nport with the desired state and count the particles leaving that port. The Stern-Gerlach device sorts \n(or ﬁlters, selects or analyzes) the incoming particles into the two possible outputs Sz = {U>2 in the \nsame way that Erwin sorted his socks according to color or length. We follow convention and refer to \na Stern-Gerlach device as an analyzer.\nIn Fig. 1.2, the input and output beams are labeled with a new symbol called a ket. We use the \nket 0  +9 as a mathematical representation of the quantum state of the atoms that exit the upper port \ncorresponding to Sz = +U>2. The lower output beam is labeled with the ket 0  -9, which corresponds \nto Sz = -U>2, and the input beam is labeled with the more generic ket 0  c9. The kets are representa-\ntions of the quantum states. They are used in mathematical expressions and they represent all the \ninformation that we can know about the state. This ket notation was developed by Paul A. M. Dirac \nand is central to the approach to quantum mechanics that we take in this text. We will discuss the \nmathematics of these kets in full detail later. With regard to notation, you will ﬁnd many different \nways of writing the same ket. The symbol within the ket brackets is any simple label to distinguish \nthe ket from other different kets. For example, the kets 0  +9, 0  +U>29, 0 Sz = +U>29, 0  +zn9, and 0 c9 \nare all equivalent ways of writing the same thing, which in this case signiﬁes that we have measured \nthe z-component of the spin and found it to be +U>2 or spin up. Though we may label these kets in \ndifferent ways, they all refer to the same physical state and so they all behave the same mathemati-\ncally. The symbol 0 {9 refers to both the 0  +9 and 0  -9 kets. The ﬁrst postulate of quantum mechanics \ntells us that kets in general describe the quantum state mathematically and that they contain all the \ninformation that we can know about the state. We denote a general ket as 0  c9.\nZ\n50\n50\n\u0002\u0002\u0003\n\u0002\u0003\u0003\n\u0002Ψ\u0003\nFIGURE 1.2 Simpliﬁed schematic of the Stern-Gerlach experiment, \ndepicting a source of atoms, a Stern-Gerlach analyzer, and two counters.\n",
    "1.1 Stern-Gerlach Experiment \n5\nPostulate 1\nThe state of a quantum mechanical system, including all the information you \ncan know about it, is represented mathematically by a normalized ket 0  c9.\nWe have chosen the particular simpliﬁed schematic representation of the Stern-Gerlach \nexperiment shown in Fig. 1.2, because it is the same representation used in the SPINS software \nprogram that you may use to simulate these experiments. The SPINS program allows you to per-\nform all the experiments described in this text. This software is freely available, as detailed in \nResources at the end of the chapter. In the SPINS program, the components are connected with \nsimple lines to represent the paths the atoms take. The directions and magnitudes of deﬂections \nof the beams in the program are not relevant. That is, whether the spin up output beam is drawn \nas deﬂected upwards, downwards, or not at all, is not relevant. The labeling on the output port is \nenough to tell us what that state is. Thus the extra ket label 0  +9 on the spin up output beam in Fig. \n1.2 is redundant and will be dropped soon.\nThe SPINS program permits alignment of Stern-Gerlach analyzing devices along all three axes \nand also at any angle f measured from the x-axis in the x-y plane. This would appear to be difﬁcult, if \nnot impossible, given that the atomic beam in Fig. 1.1 is directed along the y-axis, making it unclear \nhow to align the magnet in the y-direction and measure a deﬂection. In our depiction and discussion of \nStern-Gerlach experiments, we ignore this technical complication.\nIn the SPINS program, as in real Stern-Gerlach experiments, the numbers of atoms detected \nin particular states can be predicted by probability rules that we will discuss later. To simplify \nour schematic depictions of Stern-Gerlach experiments, the numbers shown for detected atoms \nare those obtained by using the calculated probabilities without any regard to possible statistical \nuncertainties. That is, if the theoretically predicted probabilities of two measurement possibilities \nare each 50%, then our schematics will display equal numbers for those two possibilities, whereas \nin a real experiment, statistical uncertainties might yield a 55%>45% split in one experiment and \na 47%>53% split in another, etc. The SPINS program simulations are designed to give statistical \nuncertainties, so you will need to perform enough experiments to convince yourself that you have a \nsufﬁciently good estimate of the probability (see SPINS Lab 1 for more information on statistics).\nNow let’s consider a series of simple Stern-Gerlach experiments with slight variations that help to \nillustrate the main features of quantum mechanics. We ﬁrst describe the experiments and their results \nand draw some qualitative conclusions about the nature of quantum mechanics. Then we introduce the \nformal mathematics of the ket notation and show how it can be used to predict the results of each of \nthe experiments.\n1.1.1 \u0002 Experiment 1\nThe ﬁrst experiment is shown in Fig. 1.3 and consists of a source of atoms, two Stern-Gerlach ana-\nlyzers both aligned along the z-axis, and counters for the output ports of the analyzers. The atomic \nbeam coming into the ﬁrst Stern-Gerlach analyzer is split into two beams at the output, just like the \noriginal experiment. Now instead of counting the atoms in the upper output beam, the spin compo-\nnent is measured again by directing those atoms into the second Stern-Gerlach analyzer. The result of \nthis experiment is that no atoms are ever detected coming out of the lower output port of the second \nStern-Gerlach analyzer. All atoms that are output from the upper port of the ﬁrst analyzer also pass \n",
    "6 \nStern-Gerlach Experiments\nthrough the upper port of the second analyzer. Thus we say that when the ﬁrst Stern-Gerlach analyzer \nmeasures an atom to have a z-component of spin Sz = +U>2, then the second analyzer also measures \nSz = +U>2 for that atom. This result is not surprising, but it sets the stage for results of experiments \nto follow.\nThough both Stern-Gerlach analyzers in Experiment 1 are identical, they play different roles in \nthis experiment. The ﬁrst analyzer prepares the beam in a particular quantum state 10  +92 and the \nsecond analyzer measures the resultant beam, so we often refer to the ﬁrst analyzer as a state prepa-\nration device. By preparing the state with the ﬁrst analyzer, the details of the source of atoms can be \nignored. Thus our main focus in Experiment 1 is what happens at the second analyzer because we \nknow that any atom entering the second analyzer is represented by the 0  +9 ket prepared by the ﬁrst \nanalyzer. All the experiments we will describe employ a ﬁrst analyzer as a state preparation device, \nthough the SPINS program has a feature where the state of the atoms coming from the oven is deter-\nmined but unknown, and the user can perform experiments to determine the unknown state using only \none analyzer in the experiment.\n1.1.2 \u0002 Experiment 2\nThe second experiment is shown in Fig. 1.4 and is identical to Experiment 1 except that the sec-\nond Stern-Gerlach analyzer has been rotated by 90° to be aligned with the x-axis. Now the second \nanalyzer measures the spin component along the x-axis rather the z-axis. Atoms input to the second \nanalyzer are still represented by the ket 0  +9 because the ﬁrst analyzer is unchanged. The result of this \nexperiment is that atoms appear at both possible output ports of the second analyzer. Atoms leaving \nthe upper port of the second analyzer have been measured to have Sx = +U>2, and atoms leaving \nZ\nZ\n50\n50\n0\n\u0002\u0002\u0003\n\u0002\u0002\u0003\n\u0002\u0003\u0003\n\u0002\u0003\u0003\n\u0002Ψ\u0003\nFIGURE 1.3 Experiment 1 measures the spin component along the z-axis twice in succession.\nX\nZ\n50\n25\n25\n\u0002\u0003\u0003\n\u0002\u0003\u0003x\n\u0002\u0002\u0003x\n\u0002\u0002\u0003\n\u0002Ψ\u0003\nFIGURE 1.4 Experiment 2 measures the spin component along the z-axis and then along the x-axis.\n",
    "1.1 Stern-Gerlach Experiment \n7\nthe lower port have Sx = -U>2. On average, each of these ports has 50% of the atoms that left the \nupper port of the ﬁrst analyzer. As shown in Fig. 1.4, the output states of the second analyzer have \nnew labels 0  +9x and 0  -9x, where the x subscript denotes that the spin component has been measured \nalong the x-axis. We assume that if no subscript is present on the quantum ket 1e.g., 0  +92, then the \nspin component is along the z-axis. This use of the z-axis as the default is a common convention \nthroughout our work and also in much of physics.\nA few items are noteworthy about this experiment. First, we notice that there are still only two \npossible outputs of the second Stern-Gerlach analyzer. The fact that it is aligned along a different axis \ndoesn’t affect the fact that we get only two possible results for the case of a spin-1/2 particle. Second, \nit turns out that the results of this experiment would be unchanged if we used the lower port of the ﬁrst \nanalyzer. That is, atoms entering the second analyzer in state 0  -9 would also result in half the atoms \nin each of the 0 {9x output ports. Finally, we cannot predict which of the second analyzer output ports \nany particular atom will come out. This can be demonstrated in actual experiments by recording the \nindividual counts out of each port. The arrival sequences at any counter are completely random. We \ncan say only that there is a 50% probability that an atom from the second analyzer will exit the upper \nanalyzer port and a 50% probability that it will exit the lower port. The random arrival of atoms at the \ndetectors can be seen clearly in the SPINS program simulations.\nThis probabilistic nature is at the heart of quantum mechanics. One might be tempted to say that \nwe just don’t know enough about the system to predict which port the atom will exit. That is to say, \nthere may be some other variables, of which we are ignorant, that would allow us to predict the results. \nSuch a viewpoint is known as a local hidden variable theory. John Bell proved that such theories are \nnot compatible with the experimental results of quantum mechanics. The conclusion to draw from this \nis that even though quantum mechanics is a probabilistic theory, it is a complete description of reality. \nWe will have more to say about this in Chapter 4.\nNote that the 50% probability referred to above is the probability that an atom input to the second \nanalyzer exits one particular output port. It is not the probability for an atom to pass through the whole sys-\ntem of Stern-Gerlach analyzers. It turns out that the results of this experiment (the 50>50 split at the sec-\nond analyzer) are the same for any combination of two orthogonal axes of the ﬁrst and second analyzers.\n1.1.3 \u0002 Experiment 3\nExperiment 3, shown in Fig. 1.5, extends Experiment 2 by adding a third Stern-Gerlach analyzer aligned \nalong the z-axis. Atoms entering the third analyzer have been measured by the ﬁrst Stern-Gerlach \nanalyzer to have spin component up along the z-axis, and by the second analyzer to have spin component \nup along the x-axis. The third analyzer then measures how many atoms have spin component up or down \n125\n125\n500\nX\nZ\nZ\n250\n\u0002\u0002\u0003\n\u0002\u0002\u0003\n\u0002\u0003\u0003\n\u0002\u0003\u0003\n\u0002Ψ\u0003\n\u0002\u0003\u0003x\n\u0002\u0002\u0003x\nFIGURE 1.5 Experiment 3 measures the spin component three times in succession.\n",
    "8 \nStern-Gerlach Experiments\nalong the z-axis. Classically, one would expect that the ﬁnal measurement would yield the result spin \nup along the z-axis, because that was measured at the ﬁrst analyzer. That is to say: classically the ﬁrst \ntwo analyzers tell us that the atoms have Sz = +U>2 and Sx = +U>2, so the third measurement must \nyield Sz = +U>2. But that doesn’t happen, as Erwin learned with his quantum socks in the Prologue. \nThe quantum mechanical result is that the atoms are split with 50% probability into each output port at \nthe third analyzer. Thus the last two analyzers behave like the two analyzers of Experiment 2 (except \nwith the order reversed), and the fact that there was an initial measurement that yielded Sz = +U>2 is \nsomehow forgotten or erased.\nThis result demonstrates another key feature of quantum mechanics: a measurement disturbs the \nsystem. The second analyzer has disturbed the system such that the spin component along the z-axis \ndoes not have a unique value, even though we measured it with the ﬁrst analyzer. Erwin saw this \nwhen he sorted, or measured, his socks by color and then by length. When he looked, or measured, \na third time, he found that the color he had measured originally was now random—the socks had \nforgotten about the ﬁrst measurement. One might ask: Can I be more clever in designing the experi-\nment such that I don’t disturb the system? The short answer is no. There is a fundamental incompat-\nibility in trying to measure the spin component of the atom along two different directions. So we say \nthat Sx and Sz are incompatible observables. We cannot know the measured values of both simul-\ntaneously. The state of the system can be represented by the ket 0  +9 = 0 Sz = +U>29 or by the ket \n0  +9x = 0 Sx = +U>29, but it cannot be represented by a ket 0 Sz = +U>2, Sx = +U>29 that speciﬁes \nvalues of both components. Having said this, it should be said that not all pairs of quantum mechanical \nobservables are incompatible. It is possible to do some experiments without disturbing some of the \nother aspects of the system. We will see in Section 2.4 that whether two observables are compatible or \nnot is very important in how we analyze a quantum mechanical system.\nNot being able to measure both the Sz and Sx spin components is clearly distinct from the classi-\ncal case where we can measure all three components of the spin vector, which tells us which direction \nthe spin is pointing. In quantum mechanics, the incompatibility of the spin components means that we \ncannot know which direction the spin is pointing. So when we say “the spin is up,” we really mean \nonly that the spin component along that one axis is up (vs. down). The quantum mechanical spin vec-\ntor cannot be said to be pointing in any given direction. As is often the case, we must check our classi-\ncal intuition at the door of quantum mechanics.\n1.1.4 \u0002 Experiment 4\nExperiment 4 is depicted in Fig. 1.6 and is a slight variation on Experiment 3. Before we get into the \ndetails, note a few changes in the schematic drawings. As promised, we have dropped the ket labels on \nthe beams because they are redundant. We have deleted the counters on all but the last analyzer and \ninstead simply blocked the unwanted beams and given the average number of atoms passing from one \nanalyzer to the next. The beam blocks are shown explicitly in Fig. 1.6 but will not be shown after this to \nbe consistent with the SPINS program. Note also that in Experiment 4c two output beams are combined \nas input to the following analyzer. This is simple in principle and in the SPINS program but can be \ndifﬁcult in practice. The recombination of the beams must be done properly so as to avoid “disturbing” \nthe beams. If you care to read more about this problem, see Feynman’s Lectures on Physics, volume 3. \nWe will have more to say about the “disturbance” later in Section 2.2. For now we simply assume that \nthe beams can be recombined in the proper manner.\nExperiment 4a is identical to Experiment 3. In Experiment 4b, the upper beam of the second ana-\nlyzer is blocked and the lower beam is sent to the third analyzer. In Experiment 4c, both beams are \ncombined with our new method and sent to the third analyzer. It should be clear from our previous \nexperiments that Experiment 4b has the same results as Experiment 4a. We now ask about the results of \n",
    "1.1 Stern-Gerlach Experiment \n9\nExperiment 4c. If we were to use classical probability analysis, then Experiment 4a would indicate that \nthe probability for an atom leaving the ﬁrst analyzer to take the upper path through the second analyzer \nand then exit through the upper port of the third analyzer is 25%, where we are now referring to the total \nprobability for those two steps. Likewise, Experiment 4b would indicate that the total probability to \ntake the lower path through the second  analyzer and exit through the upper port of the third analyzer is \nalso 25%. Hence the total probability to exit from the upper port of the third analyzer when both paths \nare available, which is Experiment 4c, would be 50%, and likewise for the exit from the lower port.\nHowever, the quantum mechanical result in Experiment 4c is that all the atoms exit the upper \nport of the third analyzer and none exits the lower port. The atoms now appear to “remember” that \nthey were initially measured to have spin up along the z-axis. By combining the two beams from \nthe second analyzer, we have avoided the quantum mechanical disturbance that was evident in \nExperiments 3, 4a, and 4b. The result is now the same as Experiment 1, which means it is as if the \nsecond analyzer is not there.\nTo see how odd this is, look carefully at what happens at the lower port of the third analyzer. In \nthis discussion, we refer to percentages of atoms leaving the ﬁrst analyzer, because that analyzer is \nthe same in all three experiments. In Experiments 4a and 4b, 50% of the atoms are blocked after the \nmiddle analyzer and 25% of the atoms exit the lower port of the third analyzer. In Experiment 4c, \n100% of the atoms pass from the second analyzer to the third analyzer, yet fewer atoms come out \nof the lower port. In fact, no atoms make it through the lower port! So we have a situation where \n25\n25\nX\nZ\nZ\n100\n50\n25\n25\nX\nZ\nZ\n100\n(a)\n(b)\n(c)\n50\n100\n0\nX\nZ\nZ\n100\n100\nFIGURE 1.6 Experiment 4 measures the spin component three times in succession \nand uses (a and b) one or (c) two beams from the second analyzer.\n",
    "10 \nStern-Gerlach Experiments\nallowing more ways or paths to reach a counter results in fewer counts. Classical probability theory \ncannot explain this aspect of quantum mechanics. It is as if you opened a second window in a room to \nget more sunlight and the room went dark!\nHowever, you may already know of a way to explain this effect. Imagine a procedure whereby \ncombining two effects leads to cancellation rather than enhancement. The concept of wave interfer-\nence, especially in optics, comes to mind. In the Young’s double-slit experiment, light waves pass \nthrough two narrow slits and create an interference pattern on a distant screen, as shown in Fig. 1.7. \nEither slit by itself produces a nearly uniform illumination of the screen, but the two slits combined \nproduce bright and dark interference fringes, as shown in Fig. 1.7(b). We explain this by adding \ntogether the electric ﬁeld vectors of the light from the two slits, then squaring the resultant vector to \nﬁnd the light intensity. We say that we add the amplitudes and then square the total amplitude to ﬁnd \nthe resultant intensity. See Section 6.6 or an optics textbook for more details about this experiment.\nWe follow a similar prescription in quantum mechanics. We add together amplitudes and then \ntake the square to ﬁnd the resultant probability, which opens the door to interference effects. Before \nwe discuss quantum mechanical interference, we must explain what we mean by an amplitude in \nquantum mechanics and how we calculate it.\n1.2 \u0002 QUANTUM STATE VECTORS\nPostulate 1 of quantum mechanics stipulates that kets are to be used for a mathematical description of a \nquantum mechanical system. These kets are abstract entities that obey many of the rules you know about \nordinary spatial vectors. Hence they are called quantum state vectors. As we will show in Example 1.3, \nthese vectors must employ complex numbers in order to properly describe quantum mechanical systems. \nQuantum state vectors are part of a vector space that we call a Hilbert space. The dimensionality of \nthe Hilbert space is determined by the physics of the system at hand. In the Stern-Gerlach example, \nthe two possible results for a spin  component measurement dictate that the vector space has only two \nPinhole\nSource\nDouble\nSlit\n(a)\n(b)\nScreen\nSingle Slit\nPatterns\nDouble Slit\nPattern\nFIGURE 1.7 (a) Young’s double-slit interference experiment and (b) resultant intensity patterns \nobserved on the screen, demonstrating single-slit diffraction and double-slit interference.\n",
    "1.2 Quantum State Vectors \n11\ndimensions. That makes this problem mathematically as simple as it can be, which is why we have chosen \nto study it. Because the quantum state vectors are abstract, it is hard to say much about what they are, \nother than how they behave mathematically and how they lead to physical predictions.\nIn the two-dimensional vector space of a spin-1/2 system, the two kets 0 {9 form a basis, just like \nthe unit vectors in  , jn , and kn form a basis for describing vectors in three-dimensional space. However, \nthe analogy we want to make with these spatial vectors is only mathematical, not physical. The spatial \nunit vectors have three important mathematical properties that are characteristic of a basis: the basis \nvectors in , jn , and kn are normalized, orthogonal, and complete. Spatial vectors are normalized if their \nmagnitudes are unity, and they are orthogonal if they are geometrically perpendicular to each other. \nThe basis is complete if any general vector in the space can be written as a linear superposition of the \nbasis vectors. These properties of spatial basis vectors can be summarized as follows:\n \n in~in = jn~jn = kn~kn = 1   normalization\n \n in~jn = in~kn = jn~kn = 0   orthogonality  \n(1.9)\n \n A = axin + ay jn + azkn   completeness,\nwhere A is a general vector. Note that the dot product, also called the scalar product, is central to the \ndescription of these properties.\nContinuing the mathematical analogy between spatial vectors and abstract vectors, we require that \nthese same properties (at least conceptually) apply to quantum mechanical basis vectors. For the Sz \nmeasurement, there are only two possible results, corresponding to the states 0  +9 and 0  -9, so these \ntwo states comprise a complete set of basis vectors. This basis is known as the Sz basis. We focus on \nthis basis for now and refer to other possible basis sets later. The completeness of the basis kets 0 {9 \nimplies that a general quantum state vector 0  c9 is a linear combination of the two basis kets:\n \n0  c9 = a0  +9 + b0  -9, \n(1.10)\nwhere a and b are complex scalar numbers multiplying each ket. This addition of two kets yields \nanother ket in the same abstract space. The complex scalar can appear either before or after the ket \nwithout affecting the mathematical properties of the ket 1i.e., a0  +9 = 0  +9a2. It is customary to use \nthe Greek letter c (psi) for a general quantum state. You may have seen c1x2 used before as a quan-\ntum mechanical wave function. However, the state vector or ket 0  c9 is not a wave function. Kets do \nnot have any spatial dependence as wave functions do. We will study wave functions in Chapter 5.\nTo discuss orthogonality and normalization (known together as orthonormality) we must ﬁrst \ndeﬁne scalar products as they apply to these new kets. As we said above, the machinery of quantum \nmechanics requires the use of complex numbers. You may have seen other ﬁelds of physics use com-\nplex numbers. For example, sinusoidal oscillations can be described using the complex exponential \neivt rather than cos(vt). However, in such cases, the complex numbers are not required, but are rather \na convenience to make the mathematics easier. When using complex notation to describe classical \nvectors like electric and magnetic ﬁelds, the deﬁnition of the dot product is generalized slightly, such \nthat one of the vectors is complex conjugated. A similar approach is taken in quantum mechanics. The \nanalog to the complex conjugated vector of classical physics is called a bra in the Dirac notation of \nquantum mechanics. Thus corresponding to a general ket 0  c9, there is a bra, or bra vector, which is \nwritten as 8c 0 . If a general ket 0  c9 is speciﬁed as 0  c9 = a0  +9 + b0  -9, then the corresponding bra \n8c 0  is deﬁned as\n \n8c 0 = a*8+ 0 + b*8- 0  , \n(1.11)\n",
    "12 \nStern-Gerlach Experiments\nwhere the basis bras 8  +  0  and 8  -  0  correspond to the basis kets 0  +9 and 0  -9, respectively, and the \ncoefﬁcients a and b have been complex conjugated.\nThe scalar product in quantum mechanics is deﬁned as the product of a bra and a ket taken in the \nproper order—bra ﬁrst, then ket second:\n \n18bra0210 ket92. \n(1.12)\nWhen the bra and ket are combined together in this manner, we get a bracket (bra ket)—a little physics \nhumor—that is written in shorthand as\n \n8bra0 ket9. \n(1.13)\nThus, given the basis kets 0  +9 and 0  -9, one inner product, for example, is written as\n \n18 + 0210  - 92 = 8 + 0  -9 \n(1.14)\nand so on. Note that we have eliminated the extra vertical bar in the middle. The scalar product in \nquantum mechanics is generally referred to as an inner product or a projection.\nSo how do we calculate the inner product 8+  0  +9? We do it the same way we calculate the dot \nproduct in~ in. We deﬁne it to be unity because we like basis vectors to be unit vectors. There is a little \nmore to it than that, because in quantum mechanics (as we will see shortly) using normalized basis \nvectors is more rooted in physics than in our personal preferences for mathematical cleanliness. But \nfor all practical purposes, if someone presents a set of basis vectors to you, you can probably assume \nthat they are normalized. So the normalization of the spin-1/2 basis vectors is expressed in this new \nnotation as 8+  0  +9 = 1 and 8-  0  -9 = 1.\nNow, what about orthogonality? The spatial unit vectors in, jn, and kn used for spatial vectors are \northogonal to each other because they are at 90° with respect to each other. That orthogonality is \nexpressed mathematically in the dot products in~jn = in~kn = jn~kn = 0. For the spin basis kets 0  +9 and \n0  -9, there is no spatial geometry involved. Rather, the spin basis kets 0  +9 and 0  -9 are orthogonal in \nthe mathematical sense, which we express with the inner product as 8+  0  -9 = 0. Again, we do not \nprove to you that these basis vectors are orthogonal, but we assume that a well-behaved basis set obeys \northogonality. Though there is no geometry in this property for quantum mechanical basis vectors, \nthe fundamental idea of orthogonality is the same, so we use the same language—if a general vector \n“points” in the direction of a basis vector, then there is no component in the “direction” of the other \nunit vectors.\nIn summary, the properties of normalization, orthogonality, and completeness can be expressed \nin the case of a two-state spin-1/2 quantum system as:\n \n8+  0  +9 = 1\n8-  0  -9 = 1 r    normalization\n \n \n8+ 0  -9 = 0\n8- 0  +9 = 0r    orthogonality\n \n \n(1.15)\n \n0  c9 = a0  +9 + b0  -9    completeness    . \nNote that a product of kets 1e.g., 0  +9 0  +92 or a similar product of bras 1e.g., 8 + 08 + 02 is meaningless \nin this new notation, while a product of a ket and a bra in the “wrong” order 1e.g., 0  + 98 + 02 has a \nmeaning that we will deﬁne in Section 2.2.3. Equations (1.15) are sufﬁcient to deﬁne how the basis \n",
    "1.2 Quantum State Vectors \n13\nkets behave mathematically. Note that the inner product is deﬁned using a bra and a ket, though it is \ncommon to refer to the inner product of two kets, where it is understood that one is converted to a bra \nﬁrst. The order does matter, as we will see shortly.\nUsing this new notation, we can learn a little more about general quantum states and derive some \nexpressions that will be useful later. Consider the general state vector 0  c9 = a0  +9 + b0  -9. Take the \ninner product of this ket with the bra 8 + 0  and obtain\n \n 8 + 0  c9 = 8 + 0  1a0  +9 + b0  -92  \n \n = 8 + 0 a0  +9 + 8 + 0 b0  -9 \n(1.16)\n \n = a 8 + 0  +9 + b8 + 0  -9  \n \n = a ,\n \nusing the properties that inner products are distributive and that scalars can be moved freely through \nbras or kets. Likewise, you can show that 8- 0 c9 = b. Hence the coefﬁcients multiplying the basis \nkets are simply the inner products or projections of the general state 0 c9 along each basis ket, albeit in \nan abstract complex vector space rather than the concrete three-dimensional space of normal vectors. \nUsing these results, we rewrite the general state as\n \n 0  c9 = a0  +9 + b0  -9\n \n \n = 0  +9a + 0  -9b\n \n(1.17)\n \n = 0  +958 + 0  c96 + 0  -958 - 0  c96, \nwhere the rearrangement of the second equation again uses the property that scalars 1e.g., a = 8 + 0  c92 \ncan be moved through bras or kets.\nFor a general state vector 0  c9 = a0  +9 + b0  -9, we deﬁned the corresponding bra to be \n8c 0 = a*8 + 0  +b*8 - 0 . Thus, the inner product of the state 0  c9 with the basis ket 0  +9 taken in the \nreverse order compared to Eq. (1.16) yields\n \n 8c 0  +9 = 8 + 0 a*0  +9 + 8 - 0 b*0  +9 \n \n = a*8 +\n 0  +9 + b*8 - 0  +9  \n \n = a*.\n \n \n(1.18)\nThus, we see that an inner product with the states reversed results in a complex conjugation of the \ninner product:\n \n8 + 0  c 9 = 8c 0  +9*. \n(1.19)\nThis important property holds for any inner product. For example, the inner product of two general \nstates is\n \n8f0  c9 = 8c 0 f 9*  . \n(1.20)\nNow we come to a new mathematical aspect of quantum vectors that differs from the use of vec-\ntors in classical mechanics. The rules of quantum mechanics (postulate 1) require that all state vectors \ndescribing a quantum system be normalized, not just the basis kets. This is clearly different from \nordinary spatial vectors, where the length or magnitude of a vector means something and only the unit \nvectors in, jn, and kn are normalized to unity. This new rule means that in the quantum mechanical state \n",
    "14 \nStern-Gerlach Experiments\nspace only the direction—in an abstract sense—is important. If we apply this normalization require-\nment to a general state 0  c9, then we obtain\n \n8c 0  c9 = 5a*8 + 0 + b*8- 0 65a 0  +9 + b0  -96 = 1 \n \n 1 a*a 8+ 0  +9 + a*b 8+ 0  -9 + b*a 8 - 0  +9 + b*b 8 - 0  -9 = 1\n \n 1 a*a + b*b = 1\n \n \n(1.21)\n \n 1 0 a0\n2 + 0 b0\n2 = 1 ,\nor using the expressions for the coefﬁcients obtained above,\n \n08 + 0  c9 0\n2 + 08 -\n 0  c9 0\n2 = 1. \n(1.22)\nExample 1.1 Normalize the vector 0  c9 = C110  +9 + 2i0  -92. The complex constant C is often \nreferred to as the normalization constant.\nTo normalize 0  c9, we set the inner product of the vector with itself equal to unity and then \nsolve for C—note the requisite complex conjugations\n \n 1 = 8c 0  c9\n \n \n = C*518 + 0 - 2i8- 0 6C510  +9 + 2i0  -96\n \n \n = C*C518 + 0  +9 + 2i8 + 0  -9 - 2i8- 0  +9 + 48 - 0  -96 \n(1.23)\n \n = 50 C0\n2\n \n \n 1 0 C0 =\n1\n25\n .\n \nThe overall phase of the normalization constant is not physically meaningful (Problem 1.3), so \nwe follow the standard convention and choose it to be real and positive. This yields C = 1> 15. \nThe normalized quantum state vector is then\n \n0  c9 =\n1\n25\n 110  +9 + 2i0  -92. \n(1.24)\nNow comes the crucial element of quantum mechanics. We postulate that each term in the sum \nof Eq. (1.22) is equal to the probability that the quantum state described by the ket 0  c9 is measured \nto be in the corresponding basis state. Thus\n \nPSz=+ U>2 = 08+  0  c9 0\n2 \n(1.25)\nis the probability that the state 0  c9 is found to be in the state 0  +9 when a measurement of Sz is made, \nmeaning that the result Sz = +U>2 is obtained. Likewise,\n \nPSz=- U>2 = 08-  0  c9 0\n2 \n(1.26)\nis the probability that the measurement yields the result Sz = -U>2. The subscript on the probability \nindicates the measured value. For the spin component measurements, we will usually abbreviate this \nto, for example, P+ for an Sz = +U>2 result or P-y for an Sy = -U>2 measurement.\n",
    "1.2 Quantum State Vectors \n15\nWe now have a prescription for predicting the outcomes of the experiments we have been dis-\ncussing. For example, the experiment shown in Fig. 1.8 has the state 0  c9 = 0  +9 prepared by the \nﬁrst Stern-Gerlach device and then input to the second Stern-Gerlach device aligned along the z-axis. \nTherefore the probabilities of measuring the input state 0  c9 = 0  +9 to have the two output values are \nas shown. Because the spin-1/2 system has only two possible measurement results, these two prob-\nabilities must sum to unity—there is a 100% probability of recording some value in the experiment. \nThis basic rule of probabilities is why the rules of quantum mechanics require that all state vectors \nbe properly normalized before they are used in any calculation of probabilities. The experimental \npredictions shown in Fig. 1.8 are an example of the fourth postulate of quantum mechanics, which is \npresented below.\n50\n0\nP\u0002\u0005\u0006\u0005\u0002\u0004\u0002\u0002\u0002\u0003\u00022 \u0006\u00051\nP\u0003\u0005\u0006\u0005\u0002\u0004\u0003\u0002\u0002\u0003\u00022 \u0006\u00050\nZ\nZ\n\u0002\u0002\u0003\n\u0002\u0002\u0003\n\u0002\u0003\u0003\nFIGURE 1.8 Probabilities of spin component measurements.\nPostulate 4 (Spin-1/2 system)\nThe probability of obtaining the value {U>2 in a measurement of the observ-\nable Sz on a system in the state 0  c9 is\nP{ = 08{ 0  c9 0\n2,\nwhere 0 {9 is the basis ket of Sz corresponding to the result {U>2.\nThis is labeled as the fourth postulate because we have written this postulate using the language of the \nspin-1/2 system, while the general statement of the fourth postulate presented in Section 1.5 requires \nthe second and third postulates of Section 2.1. A general spin component measurement is shown in \nFig. 1.9, along with a histogram that compactly summarizes the measurement results.\nBecause the quantum mechanical probability is found by squaring an inner product, we refer to \nan inner product, 8+ 0  c9 for example, as a probability amplitude or sometimes just an amplitude; \nmuch like a classical wave intensity is found by squaring the wave amplitude. Note that the conven-\ntion is to put the input or initial state on the right and the output or ﬁnal state on the left: 8out0 in9, so \none would read from right to left in describing a problem. Because the probability involves the com-\nplex square of the amplitude, and 8out0 in9 = 8in0 out9*, this convention is not critical for calculat-\ning probabilities. Nonetheless, it is the accepted practice and is important in situations where several \namplitudes are combined.\nArmed with these new quantum mechanical rules and tools, let’s continue to analyze the experi-\nments discussed earlier. Using the experimental results and the new rules we have introduced, we can \nlearn more about the mathematical behavior of the kets and the relationships among them. We will \nfocus on the ﬁrst two experiments for now and return to the others in the next chapter.\n",
    "16 \nStern-Gerlach Experiments\n1.2.1 \u0002 Analysis of Experiment 1\nIn Experiment 1, the ﬁrst Stern-Gerlach analyzer prepared the system in the 0  +9 state and the sec-\nond analyzer later measured this state to be in the 0  +9 state and not in the 0  -9 state. The results of \nthe experiment are summarized in the histogram in Fig. 1.10. We can use the fourth postulate to pre-\ndict the results of this experiment. We take the inner product of the input state 0  +9 with each of the \npossible output basis states 0  +9 and 0  -9. Because we know that the basis states are normalized and \northogonal, we calculate the probabilities to be\n \nP+ = 08+  0  +9 0\n2 = 1  \n \nP- = 08  - 0  +9 0\n2 = 0 . \n \n(1.27)\nThese predictions agree exactly with the histogram of experimental results shown in Fig. 1.10. A 0  +9 \nstate is always measured to have Sz = +U>2.\n1.2.2 \u0002 Analysis of Experiment 2\nIn Experiment 2, the ﬁrst Stern-Gerlach analyzer prepared the system in the 0  +9 state and the sec-\nond analyzer performed a measurement of the spin component along the x-axis, ﬁnding 50% prob-\nabilities for each of the two possible states 0  +9x and 0  -9x, as shown in the histogram in Fig. 1.11(a). \nFor this experiment, we cannot predict the results of the measurements, because we do not yet have \n\u0002Ψ\u0003\nZ\nP\u0002\u0005\u0006\u0005\u0002\u0004\u0002\u0002Ψ\u0003\u00022\nP\u0003\u0005\u0006\u0005\u0002\u0004\u0003\u0002Ψ\u0003\u00022\n(a)\n(b)\nSz\n1\nP\nP\u0003\nP\u0002\n\u0003\u0002\n2\n\u0002\n2\n\u0002\u0002\u0003\n\u0002\u0003\u0003\nFIGURE 1.9 (a) Spin component measurement for a general input state and \n(b) histogram of measurement results.\n1\nP\nP\u0003\nP\u0002\n\u0002Ψin\u0003\u0005\u0006\u0005\u0002\u0002\u0003\nSz\n\u0003\u0002\n2\n\u0002\n2\nFIGURE 1.10 Histogram of Sz spin component measurements \nfor Experiment 1 with 0 cin9 = 0  + 9.\n",
    "1.2 Quantum State Vectors \n17\nenough information about how the states 0  +9x and 0  -9x behave mathematically. Rather, we will use \nthe results of the experiment to determine these states. Recalling that the experimental results would \nbe the same if the ﬁrst analyzer prepared the system to be in the 0  -9 state [see Fig. 1.11(b)], we have \nfour results for the two experiments:\n \nP1,+x = 0 x8+  0  +9 0\n2 = 1\n2  \n \nP1,-x = 0 x8 - 0  +9 0\n2 = 1\n2  \n \nP2,+x = 0 x8+  0  -9 0\n2 = 1\n2  \n \n(1.28)\n \nP2,-x = 0 x8 - 0  -9 0\n2 = 1\n2. \nBecause the kets 0  +9 and 0  -9 form a complete basis, the kets describing the Sx measurement, 0  +9x \nand 0  -9x, can be written in terms of them. We do not yet know the speciﬁc coefﬁcients of the 0 {9x \nstates, so we use general expressions\n \n0  +9x = a0  +9 + b0  -9  \n \n0  -9x = c0  +9 + d0  -9, \n(1.29)\nand now our task is to use the results of Experiment 2 to determine the coefﬁcients a, b, c, and d. The \nﬁrst measured probability in Eq. (1.28) is\n \nP1,+x = 0 x8+  0  +9 0\n2 = 1\n2. \n(1.30)\nUsing the general expression for 0  +9x in Eq. (1.29), we calculate the probability that the 0  +9 input \nstate is measured to be in the 0  +9x output state, that is, to have Sx = +U>2:\n \n P1,+x = 0 x8+  0  +9 0\n2\n \n \n = 05a*8 + 0 + b*8  - 0 6 0  +9 0\n2 \n \n(1.31)\n \n = 0 a*0\n2 = 0 a0\n2 ,\n \nwhere we convert the 0  +9x ket to a bra x8 + 0  in order to calculate the inner product. Equating the \nexperimental result in Eq. (1.30) and the prediction in Eq. (1.31), we ﬁnd\n \n0 a0\n2 = 1\n2. \n(1.32)\n(a)\nP\u0003x\n1\nP\nSx\n(b)\nP\u0002x\nP\u0003x\n1\nP\nSx\n\u0003\u0002\n2\n\u0002\n2\n\u0003\u0002\n2\n\u0002\n2\nP\u0002x\n\u0002Ψin\u0003\u0005\u0006\u0005\u0002\u0003\u0003\n\u0002Ψin\u0003\u0005\u0006\u0005\u0002\u0002\u0003\nFIGURE 1.11 Histograms of Sx spin component measurements for Experiment 2 \nfor different input states (a) 0 cin9 = 0  + 9 and (b) 0 cin9 = 0  -9.\n",
    "18 \nStern-Gerlach Experiments\nSimilarly, one can calculate the other three probabilities to arrive at 0 b0\n2 = 0 c0\n2 = 0 d0\n2 = 1\n2 . (Prob-\nlem 1.4) Because each coefﬁcient is complex, each has an amplitude and phase. However, the overall \nphase of a quantum state vector is not physically meaningful (see Problem 1.3). Only the relative \nphase between different components of the state vector is physically measurable. Hence, we are free to \nchoose one coefﬁcient of each vector to be real and positive without any loss of generality. This allows \nus to write the desired states as\n \n 0  +9x =\n1\n12 3 0  +9 + eia0  -94   \n \n 0  -9x =\n1\n12 3 0  +9 + eib0  -94,\n \n \n(1.33)\nwhere a and b are relative phases that we have yet to determine. Note that these states are already nor-\nmalized because we used all of the experimental results, which reﬂect the fact that the probability for \nall possible results of an experiment must sum to unity.\nWe have used all the experimental results from Experiment 2, but the 0 {9x kets are still not deter-\nmined. We need some more information. If we perform Experiment 1 with both analyzers aligned \nalong the x-axis, the results will be as you expect—all 0  +9x states from the ﬁrst analyzer will be mea-\nsured to have Sx = +U>2 at the second analyzer, that is, all atoms exit in the 0  +9x state and none in the \n0  -9x . The probability calculations for this experiment are\n \nP+x = 0 x8+  0  +9x0\n2 = 1  \n \nP-x = 0 x8  - 0  +9x0\n2 = 0, \n \n(1.34)\nwhich tell us mathematically that the 0 {9x states are orthonormal to each other, just like the 0 {9 \nstates. This also implies that the 0 {9x kets form a basis, the Sx basis, which you might expect because \nthey correspond to the distinct results of a different spin component measurement. The general expres-\nsions we used for the 0 {9x kets are already normalized but are not yet orthogonal. That is the new \npiece of information we need. The orthogonality condition leads to\n \nx8  - 0  +9x = 0\n \n \n1\n12 38+  0 + e-ib8  - 0 4 1\n12 3 0 +9 + eia0  -94 = 0 \n \n1\n2 31 + ei1a-b24 = 0\n \n \n(1.35)\n \nei1a-b2 = -1\n \n \neia = -eib,\n \nwhere the complex conjugation of the second coefﬁcient of the x8  - 0  bra should be noted.\nWe now have an equation relating the remaining coefﬁcients a and b, but we need some more \ninformation to determine their values. Unfortunately, there is no more information to be obtained, so \nwe are free to choose the value of the phase a. This freedom comes from the fact that we have required \nonly that the x-axis be perpendicular to the z-axis, which limits the x-axis only to a plane rather than to \na unique direction. We follow convention here and choose the phase a \u0003 0. Thus we can express the \nSx basis kets in terms of the Sz basis kets as\n \n 0  +9x =\n1\n12 3 0  +9 + 0  -94  \n \n 0  -9x =\n1\n12 3 0  +9 - 0  -94. \n \n(1.36)\n",
    "1.2 Quantum State Vectors \n19\nWe generally use the Sz basis as the preferred basis for writing general states, but we could use \nany basis we choose. If we were to use the Sx basis, then we could write the 0 {9 kets as general states \nin terms of the 0 {9x kets. This can be done by solving Eq. (1.36) for the 0 {9 kets, yielding\n \n 0  +9 =\n1\n12 3 0  +9x + 0  -9x4  \n \n 0  -9 =\n1\n12 3 0  +9x - 0  -9x4. \n \n(1.37)\nWith respect to the measurements performed in Experiment 2, Eq. (1.37) tells us that the 0  +9 \nstate is a combination of the states 0  +9x and 0  -9x. The coefﬁcients tell us that there is a 50% probabil-\nity for measuring the spin component to be up along the x-axis, and likewise for the down possibility, \nwhich is in agreement with the histogram of measurements shown in Fig. 1.11(a). We must now take \na moment to describe carefully what a combination of states, such as in Eqs. (1.36) and (1.37), is and \nwhat it is not.\n1.2.3 \u0002 Superposition States\nA general spin-1/2 state vector 0  c9 can be expressed as a combination of the basis kets 0  +9 and 0  -9\n \n0  c9 = a0  +9 + b0  -9. \n(1.38)\nWe refer to such a combination of states as a superposition state. To understand the importance of a \nquantum mechanical superposition state, consider the particular state\n \n0  c9 =\n1\n12 10  +9 + 0  -92 \n(1.39)\nand measurements on this state, as shown in Fig. 1.12(a). Note that the state 0  c9 is none other \nthan the state 0  +9x that we found in Eq. (1.36), so we already know what the measurement results \nare. If we measure the spin component along the x-axis for this state, then we record the result \nSx = +U>2 with 100% probability (Experiment 1 with both analyzers along the x-axis). If we mea-\nsure the spin component along the orthogonal z-axis, then we record the two results Sz = {U>2 \nwith 50% probability each (Experiment 2 with the ﬁrst and second analyzers along the x- and \nz-axes, respectively). Based upon this second set of results, one might be tempted to consider the \nstate 0  c9 as describing a beam that contains a mixture of atoms with 50% of the atoms in the 0  +9 \nstate and 50% in the 0  -9 state. Such a state is called a mixed state and is very different from a \nsuperposition state.\nTo clarify the difference between a mixed state and a superposition state, let’s carefully exam-\nine the results of experiments on the proposed mixed-state beam, as shown in Fig. 1.12(b). If \nwe measure the spin component along the z-axis, then each atom in the 0  +9 state yields the result \nSz = +U>2 with 100% certainty and each atom in the 0  -9 state yields the result Sz = -U>2 with \n100% certainty. The net result is that 50% of the atoms yield Sz = +U>2 and 50% yield Sz = -U>2. \nThis is exactly the same result as that obtained with all atoms in the 0  +9x state, as seen in Fig. 1.12(a). \nIf we instead measure the spin component along the x-axis, then each atom in the 0  +9 state yields the \ntwo results Sx = {U>2 with 50% probability each (Experiment 2 with the ﬁrst and second analyzers \nalong the z- and x-axes, respectively). The atoms in the 0  -9 state yield the same results. The net result \nis that 50% of the atoms yield Sx = +U>2 and 50% yield Sx = -U>2. This is in stark contrast to the \nresults of Experiment 1, which tell us that once we have prepared the state to be 0  +9x, then subsequent \nmeasurements yield Sx = +U>2 with certainty, as seen in Fig. 1.12(a).\n",
    "20 \nStern-Gerlach Experiments\nHence we must conclude that the system described by the 0  c9 = 0  +9x state is not a mixed \nstate with some atoms in the 0  +9 state and some in the 0  -9 state. Rather, each atom in the 0  +9x \nbeam is in a state that itself is a superposition of the 0  +9 and 0  -9 states. A superposition state is \noften called a coherent superposition because the relative phase of the two terms is important. For \nexample, if the input beam were in the 0  -9x state, then there would be a relative minus sign between \nthe two coefﬁcients, which would result in an Sx = -U>2 measurement but would not affect the Sz \nmeasurement.\nZ\n50\n50\n\u0002Ψ\u0003\u0005\u0006\u0005\u0002\u0002\u0003x  \u0006\u0005(\u0002\u0002\u0003\u0005\u0002\u0005\u0002\u0003\u0003)\u0007\b2\nX\n100\n0\nZ\n50\n50\n50% \u0002\u0002\u0003\n50% \u0002\u0003\u0003\n50% \u0002\u0002\u0003\n50% \u0002\u0003\u0003\nX\n50\n50\n\u0002Ψ\u0003\u0005\u0006\u0005\u0002\u0002\u0003x \u0006\u0005(\u0002\u0002\u0003\u0005\u0002\u0005\u0002\u0003\u0003)\u0007\b2\n(b)\n(a)\nFIGURE 1.12 (a) Superposition state measurements and (b) mixed state measurements.\n",
    "1.2 Quantum State Vectors \n21\nWe will not have any further need to speak of mixed states, so any combination of states we use \nis a superposition state. Note that we cannot even write down a ket describing a mixed state. So if \n someone gives you a quantum state written as a ket, then it must be a superposition state and not a \nmixed state. The random option in the SPINS program produces a mixed state, while the unknown \nstates are all superposition states.\nExample 1.2 Consider the input state\n \n0 cin9 = 30  +9 + 40  -9.  \n(1.40)\nNormalize this state vector and ﬁnd the probabilities of measuring the spin component along the \nz-axis to be Sz = {U>2.\nTo normalize this state, introduce an overall complex multiplicative factor and solve for this \nfactor by imposing the normalization condition:\n \n0 cin9 = C 330  +9 + 40  -94\n \n \n8cin0 cin9 = 1\n \n \n5C* 338  +  0 + 48 - 0465C 330  +9 + 40  -946 = 1\n \n(1.41)\n \nC*C 398  +  0  +9 + 128  +  0  -9 + 128  - 0  +9 + 168  - 0  -94 = 1 \n \nC*C 3254 = 1\n \n \n0 C0\n2 = 1\n25.\n \nBecause an overall phase is physically meaningless, we choose C to be real and positive: C = 1>5. \nHence the normalized input state is\n \n@ cin9 = 3\n5 @  +9 + 4\n5 @  -9. \n(1.42)\nThe probability of measuring Sz = +U>2 is\n \n P+ = @8+ @cin9@\n2\n \n \n = @8  +  @33\n5 @  +9 + 4\n5 @  -94 @\n2 \n(1.43)\n \n = @ 3\n5 8  +  @  +9 + 4\n5 8  +  @  -9@\n2 \n \n = @ 3\n5 @\n2 =\n9\n25.\n \nThe probability of measuring Sz = -U>2 is\n \n P- = @8- @ cin9@\n2\n \n \n = @8- @33\n5 @  +9 + 4\n5 @  -94 @\n2 \n \n = @ 3\n5 8- @  +9 + 4\n5 8- @  -9@\n2 \n \n = @ 4\n5 @\n2 = 16\n25.\n \n(1.44)\n",
    "22 \nStern-Gerlach Experiments\nNote that the two probabilities add to unity, which indicates that we normalized the input state \nproperly. A histogram of the predicted measurement results is shown in Fig. 1.13.\n 1.3 \u0002 MATRIX NOTATION\nUp to this point, we have deﬁned kets mathematically in terms of their inner products with other kets. \nThus, in the general case we write a ket as\n \n0  c9 = 8+  0  c9 0  +9 + 8  - 0  c9 0  -9, \n(1.45)\nor in a speciﬁc case, we write\n \n 0  +9x = 8+  0  +9x 0  +9 + 8  - 0  +9x 0  -9 \n \n =\n1\n12 0  +9 +\n1\n12 0  -9.\n \n \n(1.46)\nIn both of these cases, we have chosen to write the kets in terms of the 0  +9 and 0  -9 basis kets. If we \nagree on that choice of basis as a convention, then the two coefﬁcients 8+  0  +9x and 8  - 0  +9x uniquely \nspecify the quantum state, and we can simplify the notation by using just those numbers. Thus, we \nrepresent a ket as a column vector containing the two coefﬁcients that multiply each basis ket. For \nexample, we represent 0  +9x as\n \n0  +9x \u0003\n1\n22\n ¢1\n1≤ , \n(1.47)\nwhere we have used the new symbol \u0003 to signify “is represented by,” and it is understood that we \nare using the 0  +9 and 0  -9 basis or the Sz basis. We cannot say that the ket equals the column vector, \nbecause the ket is an abstract vector in the state space and the column vector is just two complex num-\nbers. If we were to choose a different basis for representing the vector, then the complex coefﬁcients \nwould be different even though the vector is unchanged. We need to have a convention for the order-\ning of the amplitudes in the column vector. The standard convention is to put the spin up amplitude \nﬁrst (at the top). Thus, the representation of the 0  -9x state in Eq. (1.36) is\n \n0  -9x \u0003\n1\n22\n ¢ 1\n-1≤ d 0  +9\nd 0  -9, \n \n(1.48)\n1\nP\nSz\n\u0003\u0002\n2\n\u0002Ψin\u0003\u0005\u0006\u0005\u0005\u0005\u0005\u0005\u0002\u0002\u0003\u0005\u0002\u0005\u0005\u0005\u0005\u0005\u0002\u0003\u0003\n3\n5\n4\n5\n\u0002\n2\nP\u0003\nP\u0002\nFIGURE 1.13 Histogram of Sz spin component measurements.\n",
    "1.3 Matrix Notation \n23\nwhere we have explicitly labeled the rows according to their corresponding basis kets. Using this con-\nvention, it should be clear that the basis kets themselves are written as\n \n0  +9 \u0003 a1\n0b  \n \n0  -9 \u0003 a0\n1b. \n \n(1.49)\nThis demonstrates the important feature that basis kets are unit vectors when written in their own basis.\nThis new way of expressing a ket simply as the collection of coefﬁcients that multiply the basis \nkets is referred to as a representation. Because we have assumed the Sz kets as the basis kets, this is \ncalled the Sz representation. It is always true that basis kets have the simple form shown in Eq. (1.49) \nwhen written in their own representation. A general ket 0  c9 is written as\n \n0  c9 \u0003 ¢ 8+  0  c9\n8- 0  c9 ≤. \n(1.50)\nThis use of matrix notation simpliﬁes the mathematics of bras and kets. The advantage is not so evident for \nthe simple two-dimensional state space of spin-1/2 systems, but it is very evident for larger dimensional \nproblems. This notation is indispensable when using computers to calculate quantum mechanical results. \nFor example, the SPINS program employs matrix calculations coded in the Java computer language to \nsimulate the Stern-Gerlach experiments using the same probability rules you are learning here.\nWe saw earlier [Eq. (1.11)] that the coefﬁcients of a bra are the complex conjugates of the coef-\nﬁcients of the corresponding ket. We also know that an inner product of a bra and a ket yields a single \ncomplex number. In order for the matrix rules of multiplication to be used, a bra must be represented \nby a row vector, with the entries being the coefﬁcients ordered in the same sense as for the ket. For \nexample, if we use the general ket\n \n0  c9 = a0  +9 + b0  -9, \n(1.51)\nwhich is represented as\n \n0  c9 \u0003 aa\nbb, \n(1.52)\nthen the corresponding bra\n \n8c 0 = a*8 +  0 + b*8  - 0  \n(1.53)\nis represented by a row vector as\n \n8c 0 \u0003 1a* b*2. \n(1.54)\nThe rules of matrix algebra can then be applied to ﬁnd an inner product. For example,\n \n 8c 0  c9 = 1a* b*2aa\nbb \n \n = 0 a0\n2 + 0 b0\n2.  \n \n(1.55)\nSo a bra is represented by a row vector that is the complex conjugate and transpose of the column vec-\ntor representing the corresponding ket.\n",
    "24 \nStern-Gerlach Experiments\nExample 1.3 To get some practice using this new matrix notation, and to learn some more about \nthe spin-1/2 system, use the results of Experiment 2 to determine the Sy basis kets using the matrix \napproach instead of the Dirac bra-ket approach.\nConsider Experiment 2 in the case where the second Stern-Gerlach analyzer is aligned along \nthe y-axis. We said before that the results are the same as in the case shown in Fig. 1.4. Thus, we \nhave\n \n P1,+y = @ y8+  @  +9@\n2 = 1\n2  \n \n P1,-y = @ y8-  @  +9@\n2 = 1\n2  \n \n P2,+y = @ y8+  @  -9@\n2 = 1\n2  \n(1.56)\n \n P2,-y = @ y8-  @  -9@\n2 = 1\n2, \nas depicted in the histograms of Fig. 1.14.\nThese results allow us to determine the kets 0 {9y corresponding to the spin component up and \ndown along the y-axis. The argument and calculation proceeds exactly as it did earlier for the 0 {9x \nstates up until the point [Eq. (1.35)] where we arbitrarily chose the phase a to be zero. Having done \nthat for the 0 {9x states, we are no longer free to make that same choice for the 0 {9y states. Thus \nwe use Eq. (1.35) to write the 0 {9y states as\n \n 0  +9y =\n1\n22\n 3 0  +9 + eia0  -94 \u0003\n1\n22\n a 1\neiab\n \n \n 0  -9y =\n1\n22\n 3 0  +9 - eia0  -94 \u0003\n1\n22\n a 1\n-eiab. \n(1.57)\nTo determine the phase a, we use some more information at our disposal. Experiment 2 could be \nperformed with the ﬁrst Stern-Gerlach analyzer aligned along the x-axis and the second analyzer \nalong the y-axis. Again the results would be identical (50% at each output port), yielding\n \nP+y = @ y8+  @  +9x@\n2 = 1\n2 \n(1.58)\n(a)\nP\u0002y\nP\u0003y\n1\nP\nSy\n\u0003\u0002\n2\n\u0002\n2\n(b)\nP\u0002y\nP\u0003y\n1\nP\nSy\n\u0003\u0002\n2\n\u0002\n2\n\u0002Ψin\u0003\u0005\u0006\u0005\u0002\u0003\u0003\n\u0002Ψin\u0003\u0005\u0006\u0005\u0002\u0002\u0003\nFIGURE 1.14 Histograms of Sy spin component measurements for input states (a) 0 cin9 = 0  +9 \nand (b) 0 cin9 = 0  -9.\n",
    "1.4 General Quantum Systems \n25\nas one of the measured quantities. Now use matrix algebra to calculate this:\n \n  y8 + 0  +9x =\n1\n12 11 e-ia2 1\n12 a1\n1b\n \n \n = 1\n2 11 + e-ia2\n \n \n @ y8 + 0  +9x@\n2 = 1\n2 11 + e-ia21\n2 11 + eia2 \n(1.59)\n \n = 1\n4 11 + eia + e-ia + 12 \n \n \n = 1\n2 11 + cos a2 = 1\n2.\n \nThis result requires that cos a = 0, or that a = {p>2. The two choices for the phase correspond \nto the two possibilities for the direction of the y-axis relative to the already determined x- and z-axes. \nThe choice a = +p>2 can be shown to correspond to a right-handed coordinate system, which is the \nstandard convention, so we choose that phase. We thus represent the 0 {9y kets as\n \n 0  +9y \u0003\n1\n22\n a1\ni b\n \n \n 0  -9y \u0003\n1\n22\n a 1\n-ib. \n(1.60)\nNote that the imaginary components of these kets are required. They are not merely a mathemati-\ncal convenience as one sees in classical mechanics. In general, quantum mechanical state vectors \nhave complex coefﬁcients. But this does not mean that the results of physical measurements are \ncomplex. On the contrary, we always calculate a measurement probability using a complex square, \nso all quantum mechanics predictions of probabilities are real.\n 1.4 \u0002 GENERAL QUANTUM SYSTEMS\nThe machinery we have developed for spin-1/2 systems can be generalized to other quantum systems. \nFor example, if an observable A yields quantized measurement results an for some ﬁnite range of n, \nthen we generalize the schematic depiction of a Stern-Gerlach measurement to a measurement of the \nA\n\u0002a1\u0003\n\u0002a2\u0003\n\u0002a3\u0003\n\u0002Ψin\u0003\na2\na1\na3\nFIGURE 1.15 Generic depiction of the quantum mechanical measurement of observable A.\n",
    "26 \nStern-Gerlach Experiments\nobservable A, as shown in Fig. 1.15. The observable A labels the measurement device and the possible \nresults a1, a2, a3, etc. label the output ports. The basis kets corresponding to the results an are then 0 an9. \nThe mathematical rules about kets in this general case are\n \n 8ai@ aj9 = dij         orthonormality \n \n 0  c9 = a\ni\n8ai0  c9 0 ai9\n completeness,  \n \n(1.61)\nwhere we use the Kronecker delta\n \ndij = e0\n1 i \u0002 j\ni = j \n(1.62)\nto express the orthonormality condition compactly. In this case, the generalization of postulate 4 says \nthat the probability of a measurement of one of the possible results an is\n \nPan = 08an0 cin9 0\n2.  \n(1.63)\nExample 1.4 Imagine a quantum system with an observable A that has three possible measure-\nment results: a1, a2, and a3. The three kets 0 a19, 0 a29, and 0 a39 corresponding to these possible \nresults form a complete orthonormal basis. The system is prepared in the state\n \n0  c9 = 20 a19 - 30 a29 + 4i0 a39. \n(1.64)\nCalculate the probabilities of all possible measurement results of the observable A.\nThe state vector in Eq. (1.64) is not normalized, so we must normalize it before calculating \nprobabilities. Introducing a complex normalization constant C, we ﬁnd\n \n 1 = 8c 0  c9\n \n \n = C*128a10 - 38a20 - 4i8a302C120 a19 - 30 a29 + 4i0 a392 \n \n = 0 C0\n2548a10 a19 - 68a10 a29 + 8i8a10 a39\n \n \n- 68a20 a19 + 98a20 a29 - 12i8a20 a39\n \n(1.65)\n \n \n- 8i8a30 a19 + 12i8a30 a29 + 168a30 a396\n \n \n = 0 C0\n254 + 9 + 166 = 0 C0\n2 29\n \n \n 1 C =\n1\n129.\n \nThe normalized state is\n \n0  c9 =\n1\n129 120 a19 - 30 a29 + 4i0 a392. \n(1.66)\n",
    "1.5 Postulates \n27\nThe probabilities of measuring the results a1, a2, and a3 are\n \n Pa1 = 08a10  c9 0\n2 \n \n = @8a10\n1\n129520 a19 - 30 a29 + 4i0 a396@\n2\n \n \n =\n1\n29 0 28a10 a19 - 38a10 a29 + 4i8a10 a39 0\n2 =\n4\n29 \n(1.67)\n \n Pa2 = 0 a20  c9 0\n2 = @8a2@\n1\n12952@ a19 - 3@ a29 + 4i@ a396@\n2\n=\n9\n29 \n \nPa3 = 08a30  c9 @\n2 = @8a3@\n1\n12952@ a19 - 3@ a29 + 4i@ a396@\n2\n= 16\n29 . \nA schematic of this experiment is shown in Fig. 1.16(a) and a histogram of the predicted probabili-\nties is shown in Fig. 1.16(b).\n 1.5 \u0002 POSTULATES\nWe have introduced two of the postulates of quantum mechanics in this chapter. The postulates \nof quantum mechanics dictate how to treat a quantum mechanical system mathematically and \nhow to interpret the mathematics to learn about the physical system in question. These postulates \ncannot be proven, but they have been successfully tested by many experiments, and so we accept \nthem as an accurate way to describe quantum mechanical systems. New results could force us \nto reevaluate these postulates at some later time. All six postulates are listed below to give you \nan idea where we are headed and a framework into which you can place the new concepts as we \nconfront them.\n \nPostulates of Quantum Mechanics\n \n1. The state of a quantum mechanical system, including all the information you can know \nabout it, is represented mathematically by a normalized ket 0 c9.\n \n2. A physical observable is represented mathematically by an operator A that acts on kets.\n \n3. The only possible result of a measurement of an observable is one of the eigenvalues an of \nthe corresponding operator A. \nA a2\na1\na3\n(a)\n(b)\nPa2 \u0006\nPa1\nPa1\nPa2\nPa3\n\u0006\nPa3 \u0006\nA\n1\nP\n4\n29\n9\n29\n16\n29\na3\na2\na1\n\u0002Ψin\u0003\nFIGURE 1.16 (a) Schematic diagram of the measurement of observable A and (b) histogram of the \npredicted measurement probabilities.\n",
    "28 \nStern-Gerlach Experiments\n \n4. The probability of obtaining the eigenvalue an in a measurement of the observable A on the \nsystem in the state 0  c9 is\n \nPan = 08an0  c9 0\n2, \n \n where 0 an9 is the normalized eigenvector of A corresponding to the eigenvalue an.\n \n5. After a measurement of A that yields the result an, the quantum system is in a new state that \nis the normalized projection of the original system ket onto the ket (or kets) corresponding \nto the result of the measurement:\n \n0  c\u00049 =\nPn0  c9\n28c 0 Pn0  c9\n. \n \n6. The time evolution of a quantum system is determined by the Hamiltonian or total energy \noperator H(t) through the Schrödinger equation\n \niU d\ndt\n 0  c 1t29 = H 1t2 0  c 1t29. \nAs you read these postulates for the ﬁrst time, you will undoubtedly encounter new terms and \nconcepts. Rather than explain them all here, the plan of this text is to continue to explain them through \ntheir manifestation in the Stern-Gerlach spin-1/2 experiment. We have chosen this example because it \nis inherently quantum mechanical and forces us to break away from reliance on classical intuition or \nconcepts. Moreover, this simple example is a paradigm for many other quantum mechanical systems. \nBy studying it in detail, we can appreciate much of the richness of quantum mechanics.\nSUMMARY\nThrough the Stern-Gerlach experiment we have learned several key concepts about quantum mechan-\nics in this chapter.\n• Quantum mechanics is probabilistic. \n \nWe cannot predict the results of experiments precisely. We can predict only the probability \nthat a certain result is obtained in a measurement.\n• Spin measurements are quantized. \n \nThe possible results of a spin component measurement are quantized. Only these discrete \nvalues are measured.\n• Quantum measurements disturb the system. \n \nMeasuring one physical observable can “destroy” information about other observables.\nWe have learned how to describe the state of a quantum mechanical system mathematically using \na ket, which represents all the information we can know about that state. The kets 0  +9 and 0  -9 result \nwhen the spin component Sz along the z-axis is measured to be up or down, respectively. These kets \nform an orthonormal basis, which we denote by the inner products\n \n 8+  0  +9 = 1  \n \n 8- 0  -9 = 1  \n(1.68)\n \n 8+  0  -9 = 0. \n",
    "Problems \n29\nThe basis is also complete, which means that it can be used to express all possible kets as superposi-\ntion states\n \n0  c9 = a0  +9 + b0  -9. \n(1.69)\nFor spin component measurements, the kets corresponding to spin up or down along the three \nCartesian axes are\n \n 0  +9   0  +9x =\n1\n12 3 0  +9 + 0  -94   0  +9y =\n1\n12 3 0  +9 + i0  -94\n \n 0  -9   0  -9x =\n1\n12 3 0  +9 - 0  -94   0  -9y =\n1\n12 3 0  +9 - i0  -94. \n(1.70)\nWe also found it useful to introduce a matrix notation for calculations. In this matrix language the kets \nin Eq. (1.70) are represented by\n \n0  +9 \u0003 ¢1\n0≤ \n0  +9x \u0003\n1\n22\n ¢1\n1≤ \n0  +9y \u0003\n1\n22\n ¢1\ni ≤ \n \n0  -9 \u0003 ¢0\n1≤ \n0  -9x \u0003\n1\n22\n ¢ 1\n-1≤ \n0  -9y \u0003\n1\n22\n ¢ 1\n-i≤. \n(1.71)\nThe most important tool we have learned so far is the probability postulate (postulate 4). To \ncalculate the probability that a measurement on an input state 0 cin9 will yield a particular result, for \nexample Sz = U>2, we complex square the inner product of the input state with the ket corresponding \nto the measured result, 0  +9 in this case:\n \nP+ = 08 + 0 cin9 0\n2. \n(1.72)\nThis is generalized to other systems where a measurement yields a particular result an corresponding \nto the ket 0 an9 as:\n \nPan = 08an0 cin9 0\n2.  \n(1.73)\nPROBLEMS\n 1.1 Consider the following state vectors:\n \n 0 c19 = 30  +9 + 40  -9\n \n \n 0 c29 = 0  +9 + 2i0  -9\n \n \n 0 c39 = 30  +9 - eip>30  -9. \na) Normalize each state vector.\nb) For each state vector, calculate the probability that the spin component is up or down \nalong each of the three Cartesian axes. Use bra-ket notation for the entire calculation.\nc) Write each normalized state in matrix notation.\nd) Repeat part (b) using matrix notation for the entire calculation.\n",
    "30 \nStern-Gerlach Experiments\n 1.2 Consider the three quantum states:\n \n 0 c19 =\n1\n13 0  +9 + i 12\n13 0  -9\n \n \n 0 c29 =\n1\n15 0  +9 -\n2\n15 0  -9\n \n \n 0 c39 =\n1\n12 0  +9 + eip>4 1\n12 0  -9. \n \n Use bra-ket notation (not matrix notation) to solve the following problems. Note  \nthat 8+  0  +9 = 1, 8- 0  -9 = 1, and 8+  0  -9 = 0.\na) For each of the 0 ci9 above, ﬁnd the normalized vector 0 fi9 that is orthogonal to it.\nb) Calculate the inner products 8ci0 cj9 for i and j = 1, 2, 3.\n 1.3 Show that a change in the overall phase of a quantum state vector does not change \nthe probability of obtaining a particular result in a measurement. To do this, consider  \nhow the probability is affected by changing the state 0  c9 to the state eid0  c9.\n 1.4 Show by explicit bra-ket calculations using the states in Eq. (1.29) that the four \nexperimental results in Eq. (1.28) lead to the results 0 b0\n2 = 0 c0\n2 = 0 d0\n2 = 1\n2.\n 1.5 A beam of spin-1/2 particles is prepared in the state\n0  c9 =\n2\n113 0  +9 + i 3\n113 0  -9.\na) What are the possible results of a measurement of the spin component Sz, and with \nwhat probabilities would they occur?\nb) What are the possible results of a measurement of the spin component Sx, and with \nwhat probabilities would they occur?\nc) Plot histograms of the predicted measurement results from parts (a) and (b).\n 1.6 A beam of spin-1/2 particles is prepared in the state\n0  c9 =\n2\n113 0  +9x + i 3\n113 0  -9x.\na) What are the possible results of a measurement of the spin component Sz, and with \nwhat probabilities would they occur?\nb) What are the possible results of a measurement of the spin component Sx, and with \nwhat probabilities would they occur?\nc) Plot histograms of the predicted measurement results from parts (a) and (b).\n 1.7 A classical coin is thrown in the air and lands on the ground, where a measurement is \nmade of its state.\na) What are the possible results of this measurement?\nb) What are the predicted probabilities for these possible outcomes?\nc) Plot a histogram of the predicted measurement results.\n 1.8 A classical cubical die is thrown onto a table and comes to rest, where a measurement \nis made of its state.\na) What are the possible results of this measurement?\nb) What are the predicted probabilities for these possible outcomes?\nc) Plot a histogram of the predicted measurement results.\n",
    "Problems \n31\n 1.9 A pair of dice (classical cubes) are thrown onto a table and come to rest, where a \nmeasurement is made of the state of the system (i.e., the sum of the two dice).\na) What are the possible results of this measurement?\nb) What are the predicted probabilities for these possible outcomes?\nc) Plot a histogram of the predicted measurement results.\n 1.10 Consider the three quantum states:\n \n 0 c19 = 4\n5 0  +9 + i 3\n5 0  -9\n \n \n 0 c29 = 4\n5 0  +9 - i 3\n5 0  -9\n \n \n 0 c39 = -  4\n5 0  +9 + i 3\n5 0  -9. \na) For each of the 0 ci9 above, calculate the probabilities of spin component measurements \nalong the x-, y-, and z-axes.\nb) Use your results from (a) to comment on the importance of the overall phase and of the \nrelative phases of the quantum state vector.\n 1.11  A beam of spin-1/2 particles is prepared in the state\n0  c9 =\n3\n134 0  +9 + i 5\n134 0  -9.\na) What are the possible results of a measurement of the spin component Sz, and with what \nprobabilities would they occur?\nb) Suppose that the Sz measurement yields the result Sz = -U>2. Subsequent to that result \na second measurement is performed to measure the spin component Sx. What are the \npossible results of that measurement, and with what probabilities would they occur?\nc) Draw a schematic diagram depicting the successive measurements in parts (a) and (b).\n 1.12  Consider a quantum system with an observable A that has three possible measurement \nresults: a1, a2, and a3. Write down the orthogonality, normalization, and completeness \nrelations for the three kets comprising the basis corresponding to the possible results of the  \nA measurement.\n 1.13  Consider a quantum system with an observable A that has three possible measurement \nresults: a1, a2, and a3.\na) Write down the three kets 0 a19, 0 a29, and 0 a39 corresponding to these possible results \nusing matrix notation.\nb) The system is prepared in the state\n0  c9 = 10 a19 - 20 a29 + 50 a39.\n \nWrite this state in matrix notation and calculate the probabilities of all possible measurement \nresults of the observable A. Plot a histogram of the predicted measurement results.\nc) In a different experiment, the system is prepared in the state\n0  c9 = 20 a19 + 3i0 a29.\n \nWrite this state in matrix notation and calculate the probabilities of all possible measurement \nresults of the observable A. Plot a histogram of the predicted measurement results.\n",
    "32 \nStern-Gerlach Experiments\n 1.14  Consider a quantum system in which the energy E is measured and there are four possible \nmeasurement results: 2 eV, 4 eV, 7 eV, and 9 eV. The system is prepared in the state\n0  c9 =\n1\n139 530 2 eV9 - i0 4 eV9 + 2eip>70 7 eV9 + 50 9 eV96.\n \n Calculate the probabilities of all possible measurement results of the energy E. Plot a \nhistogram of the predicted measurement results.\n 1.15  Consider a quantum system described by a basis 0 a19, 0 a29, and 0 a39. The system is initially \nin a state\n0\n ci9 =\ni\n13 0 a19 + 4\n2\n3 0 a29.\n \n Find the probability that the system is measured to be in the ﬁnal state\n@\n cf9 = 1+i\n13 0 a19 +\n1\n16 0 a29 +\n1\n16 0 a39.\n 1.16  The spin components of a beam of atoms prepared in the state 0 cin9 are measured and the fol-\nlowing experimental probabilities are obtained:\n \nP+ = 1\n2 \nP+x = 3\n4 \nP+y = 0.067\n \nP- = 1\n2 \nP-x = 1\n4 \nP-y = 0.933. \n \n From the experimental data, determine the input state.\n 1.17  In part (1) of SPINS Lab #2, you measured the probabilities of all the possible spin compo-\nnents for each of the unknown initial states 0 ci9 (i = 1, 2, 3, 4). Using your data from that \nlab, ﬁnd the unknown states 0 c19, 0 c29, 0 c39, and 0 c49. Express each of the unknown states \nas a linear superposition of the Sz basis states 0  +9 and 0  -9. For each state, use your result \nto calculate the theoretical values of the probabilities for each component measurement and \ncompare these theoretical predictions with your experimental results.\nRESOURCES\nActivities \nSPINS: A software program to simulate Stern-Gerlach spin experiments. The Java software runs on \nall platforms and can be downloaded in two forms:\nOpen Source Physics framework\nwww.physics.oregonstate.edu/~mcintyre/ph425/spins/index_SPINS_OSP.html\nor\nStandalone Java\nwww.physics.oregonstate.edu/~mcintyre/ph425/spins\nThe bulleted activities are available at\nwww.physics.oregonstate.edu/qmactivities\n",
    "Resources \n33\n• SPINS Lab 1: An introduction to successive Stern-Gerlach spin-1/2 measurements. The random-\nness of measurements is demonstrated and students use statistical analysis to deduce probabilities \nfrom measurements.\n• SPINS Lab 2: Students deduce unknown quantum state vectors from measurements of spin projec-\ntions (part 3 requires material from Chapter 2 to do the calculations).\nStern-Gerlach simulation: A different simulation of the Stern-Gerlach experiment from the PHET \ngroup at the University of Colorado (somewhat Flashier version):\nhttp://phet.colorado.edu/en/simulation/stern-gerlach\nFurther Reading\nThe history of the Stern-Gerlach experiment and how a bad cigar helped are chronicled in  \na Physics Today article:\nB. Friedrich and D. Herschbach, “Stern and Gerlach: How a Bad Cigar Helped Reorient  \nAtomic Physics,” Phys. Today 56(12), 53–59 (2003). \n \nhttp://dx.doi.org/10.1063/1.1650229\nA different spin on the quantum mechanics of socks is discussed by John S. Bell in this article:\nJ. S. Bell, “Bertlmann’s socks and the nature of reality, ” J. Phys. Colloq. 42, C22 \nC2.41-C2.62 (1981).  \n \nhttp://cdsweb.cern.ch/record/142461\nNature has published a supplement on the milestones in spin physics. An extensive timeline \nof historical events, review articles, and links to original articles are included.\nNature Phys. 4, S1–S43 (2008). \n \nwww.nature.com/milestones/spin\nThe SPINS lab software is described in this pedagogical article:\nD. V. Schroeder and T. A. Moore, “A computer-simulated Stern-Gerlach laboratory,”  \nAm. J. Phys. 61, 798–805 (1993). \n \nhttp://dx.doi.org/10.1119/1.17172\nSome other textbooks that take a spins-ﬁrst approach or have an extensive treatment  \nof Stern-Gerlach experiments:\nR. P. Feynman, R. B. Leighton, and M. Sands, The Feynman Lectures on Physics, \nVolume 3, Quantum Mechanics, Reading, MA: Addison-Wesley Publishing Company, \nInc., 1965.\nJ. J. Sakurai, Modern Quantum Mechanics, Redwood City, CA: Addison-Wesley \nPublishing Company, Inc., 1985.\nJ. S. Townsend, A Modern Approach to Quantum Mechanics, New York: McGraw \nHill, Inc., 1992.\nC. Cohen-Tannoudji, B. Diu, and F. Laloë, Quantum Mechanics, New York: John Wiley & \nSons, 1977.\nD. F. Styer, The Strange World of Quantum Mechanics, Cambridge: Cambridge University \nPress, 2000.\n",
    "C H A P T E R \n2\nOperators and Measurement\nIn Chapter 1 we used the results of experiments to deduce a mathematical description of the spin-1/2 \nsystem. The Stern-Gerlach experiments demonstrated that spin component measurements along the \nx-, y-, or z-axes yield only {U>2 as possible results. We learned how to predict the probabilities of \nthese measurements using the basis kets of the spin component observables Sx, Sy, and Sz, and these \npredictions agreed with the experiments. However, the real power of a theory is its ability to predict \nresults of experiments that you haven’t yet done. For example, what are the possible results of a mea-\nsurement of the spin component Sn along an arbitrary direction nn and what are the predicted probabili-\nties? To make these predictions, we need to learn about the operators of quantum mechanics.\n2.1 \u0002 OPERATORS, EIGENVALUES, AND EIGENVECTORS\nThe mathematical theory we developed in Chapter 1 used only quantum state vectors. We said that \nthe state vector represents all the information we can know about the system and we used the state \nvectors to calculate probabilities. With each observable Sx, Sy, and Sz we associated a pair of kets \n corresponding to the possible measurement results of that observable. The observables themselves are \nnot yet included in our mathematical theory, but the distinct association between an observable and its \nmeasurable kets provides the means to do so.\nThe role of physical observables in the mathematics of quantum theory is described by the two \npostulates listed below. Postulate 2 states that physical observables are represented by mathematical \noperators, in the same sense that physical states are represented by mathematical vectors or kets (postu-\nlate 1). An operator is a mathematical object that acts or operates on a ket and transforms it into a new \nket, for example A0 c9 = 0 f9. However, there are special kets that are not changed by the operation \nof a particular operator, except for a possible multiplicative constant, which we know does not change \nanything measurable about the state. An example of a ket that is not changed by an operator would be \nA0 c9 = a0 c9. Such kets are known as eigenvectors of the operator A and the multiplicative constants \nare known as the eigenvalues of the operator. These are important because postulate 3 states that the only \npossible result of a measurement of a physical observable is one of the eigenvalues of the corresponding \noperator.\nPostulate 2\nA physical observable is represented mathematically by an operator A \nthat acts on kets.\n",
    "2.1 Operators, Eigenvalues, and Eigenvectors \n35\nWe now have a mathematical description of that special relationship we saw in Chapter 1 between \na physical observable, Sz say, the possible results {U>2, and the kets 0{9 corresponding to those \nresults. This relationship is known as the eigenvalue equation and is depicted in Fig. 2.1 for the case \nof the spin up state in the z-direction. In the eigenvalue equation, the observable is represented by an \noperator, the eigenvalue is one of the possible measurement results of the observable, and the eigen-\nvector is the ket corresponding to the chosen eigenvalue of the operator. The eigenvector appears on \nboth sides of the equation because it is unchanged by the operator.\nThe eigenvalue equations for the Sz operator in a spin-1/2 system are:\n \n Sz0  +9 = +  U\n2 0  +9  \n \n Sz0  -9 = -  U\n2 0  -9. \n \n(2.1)\nThese equations tell us that +U>2 is the eigenvalue of Sz corresponding to the eigenvector 0  +9 and \n-U>2 is the eigenvalue of Sz corresponding to the eigenvector 0  -9. Equations (2.1) are sufﬁcient to \ndeﬁne how the Sz operator acts mathematically on kets. However, it is useful to use matrix notation \nto represent operators in the same sense that we used column vectors and row vectors in Chapter 1 to \nrepresent bras and kets, respectively. For Eqs. (2.1) to be satisﬁed using matrix algebra with the kets \nrepresented as column vectors of size 1*  2, the operator Sz must be represented by a 2 *  2 matrix. The \neigenvalue equations (2.1) provide sufﬁcient information to determine this matrix.\nTo determine the matrix representing the operator Sz, assume the most general form for a 2 *  2 matrix\n \nSz \u0003 aa\nb\nc\ndb, \n(2.2)\nwhere we are again using the \u0003 symbol to mean “is represented by.” Now write the eigenvalue equa-\ntions in matrix form:\n \n aa\nb\nc\ndb a1\n0b = +  U\n2\n a1\n0b  \n \n aa\nb\nc\ndb a0\n1b = -  U\n2\n a0\n1b. \n \n(2.3)\nPostulate 3\nThe only possible result of a measurement of an observable is one of the \neigenvalues an of the corresponding operator A.\neigenvalue\neigenvector\noperator\n\u0002\n2\nSz \u0002\u0002\u0003\u0005\u0006\u0005\u0005\u0005\u0005\u0005\u0005\u0002\u0002\u0003\u0005\nFIGURE 2.1 Eigenvalue equation for the spin up state.\n",
    "36 \nOperators and Measurement\nNote that we are still using the convention that the 0{9 kets are used as the basis for the representation. \nIt is crucial that the rows and columns of the operator matrix are ordered in the same manner as used \nfor the ket column vectors; anything else would amount to nonsense. An explicit labeling of the rows \nand columns of the operator and the basis kets makes this clear:\n \nSz\n0  +9\n0  -9\n8+ 0\na\nb\n8- 0\nc\nd\n \n0  +9\n8+ 0\n1\n8- 0\n0\n \n0  -9\n8+ 0\n0\n8- 0\n1\n . \n(2.4)\nCarrying through the multiplication in Eqs. (2.3) yields\n \n aa\ncb = +  U\n2\n  a1\n0b  \n \n ab\ndb = -  U\n2\n a0\n1b, \n \n(2.5)\nwhich results in\n \na = + U\n2 \nb = 0 \n \nc = 0 \nd = -  U\n2. \n \n(2.6)\nThus the matrix representation of the operator Sz is\n \n Sz \u0003 aU>2\n0\n0\n-U>2b \n \n \u0003 U\n2\n a1\n0\n0\n-1b.\n \n \n(2.7)\nNote two important features of this matrix: (1) it is a diagonal matrix—it has only diagonal elements—\nand (2) the diagonal elements are the eigenvalues of the operator, ordered in the same manner as the \ncorresponding eigenvectors. In this example, the basis used for the matrix representation is that formed \nby the eigenvectors 0{9 of the operator Sz. That the matrix representation of the operator in this case \nis a diagonal matrix is a necessary and general result of linear algebra that will prove valuable as we \nstudy quantum mechanics. In simple terms, we say that an operator is always diagonal in its own \nbasis. This special form of the matrix representing the operator is similar to the special form that the \neigenvectors 0{9 take in this same representation—the eigenvectors are unit vectors in their own \nbasis. These ideas cannot be overemphasized, so we repeat them:\nAn operator is always diagonal in its own basis.  \nEigenvectors are unit vectors in their own basis.\nLet’s also summarize the matrix representations of the Sz operator and its eigenvectors:\n \nSz \u0003 U\n2\n a1\n0\n0\n-1b \n0  +9 \u0003 a1\n0b \n0  -9 \u0003 a0\n1b. \n(2.8)\n",
    "2.1 Operators, Eigenvalues, and Eigenvectors \n37\n2.1.1 \u0002 Matrix Representation of Operators\nNow consider how matrix representation works in general. Consider a general operator A describ-\ning a physical observable (still in the two-dimensional spin-1/2 system), which we represent by the \ngeneral matrix\n \nA \u0003 aa\nb\nc\ndb \n(2.9)\nin the Sz basis. The operation of A on the basis ket 0  +9 yields\n \nA0  +9 \u0003 aa\nb\nc\ndba1\n0b = aa\ncb. \n(2.10)\nThe inner product of this new ket A0  +9 with the ket 0  +9 (converted to a bra following the rules) results in\n \n8+ 0 A0  +9 = 11\n02aa\ncb = a , \n(2.11)\nwhich serves to isolate one of the elements of the matrix. Hence an individual element such as \n8+ 0 A0  +9 or 8+ 0 A0  -9 is generally referred to as a matrix element. This “sandwich” of a bra, an \noperator, and a ket\n \n8bra0 OPERATOR0 ket9 \n(2.12)\nplays an important role in many quantum mechanical calculations. Even in cases where the bra and ket \nare not basis kets, such as in 8c0 A0 f9, we still refer to this as a matrix element. A schematic diagram \nof a generic matrix element is depicted in Fig. 2.2(a).\nAll four elements of the matrix representation of A can be determined in the same manner as \nEq. (2.11), with the ﬁnal result\n \nA \u0003 ¢8+ 0 A0  +9\n8+ 0 A0  -9\n8- 0 A0  +9\n8- 0 A0  -9≤. \n(2.13)\nTo emphasize the structure of the matrix, let’s write it with explicit labeling of the rows and columns:\n \nA\n0  +9\n0  -9\n8+ 0\n8+ 0 A0 +9\n8+ 0 A0  -9\n8- 0\n8- 0 A0 +9\n8- 0 A0  -9\n . \n(2.14)\n(a) bra\nket\noperator\n\u0004bra\u0002OPERATOR\u0002ket\u0003\n(b) row\ncolumn\noperator\n\u0004n\u0002A\u0002m\u0003\n\u0004Φ\u0002A\u0002Ψ\u0003\nFIGURE 2.2 (a) Schematic diagram of a generic matrix element. (b) Schematic diagram \nof the row and column labeling convention for matrix elements.\n",
    "38 \nOperators and Measurement\nIn a more general problem with more than two dimensions in the complex vector space, the matrix \nrepresentation of an operator is\n \nA \u0003 •\nA11\nA12\nA13\ng\nA21\nA22\nA23\ng\nA31\nA32\nA33\ng\nf\nf\nf\nf\nμ, \n(2.15)\nwhere the matrix elements are\n \nAij = 8i0\n A0  j9 \n(2.16)\nand the basis is assumed to be the states labeled 0 i9, with the subscripts i and j labeling the rows and \ncolumns respectively, as depicted in Fig. 2.2(b). Using this matrix representation, the action of this\noperator on a general ket 0 c9 = a\ni\nci0 i9 is\n \nA0 c9 \u0003 •\nA11\nA12\nA13\ng\nA21\nA22\nA23\ng\nA31\nA32\nA33\ng\nf\nf\nf\nf\nμ•\nc1\nc2\nc3\nf\nμ = •\nA11c1 + A12c2 + A13c3 + g\nA21c1 + A22c2 + A23c3 + g\nA31c1 + A32c2 + A33c3 + g\nf\nμ. \n(2.17)\nIf we write the new ket 0  f9 = A0 c9 as 0 f9 = a\ni\nbi0 i9, then from Eq. (2.17) the coefﬁcients bi are\n \nbi = a\nj\nAij\n cj \n(2.18)\nin summation notation.\n 2.1.2 \u0002 Diagonalization of Operators\nIn the case of the operator Sz above, we used the experimental results and the eigenvalue equations to \nﬁnd the matrix representation of the operator in Eq. (2.7). It is more common to work the other way. \nThat is, one is given the matrix representation of an operator and is asked to ﬁnd the possible results of \na measurement of the corresponding observable. According to the third postulate, the possible results \nare the eigenvalues of the operator, and the eigenvectors are the quantum states representing them. In \nthe case of a general operator A in a two-state system, the eigenvalue equation is\n \nA0 an9 = an0 an9, \n(2.19)\nwhere we have labeled the eigenvalues an and we have labeled the eigenvectors with the correspond-\ning eigenvalues. In matrix notation, the eigenvalue equation is\n \n¢A11\nA12\nA21\nA22\n≤¢cn1\ncn2\n≤= an ¢cn1\ncn2\n≤, \n(2.20)\nwhere cn1 and cn2 are the unknown coefﬁcients of the eigenvector 0 an9 corresponding to the eigen-\nvalue an. This matrix equation yields the set of homogeneous equations\n \n 1A11 - an2cn1 + A12\n cn2 = 0  \n \n A21cn1 + 1A22 - an2cn2 = 0. \n \n(2.21)\n",
    "2.1 Operators, Eigenvalues, and Eigenvectors \n39\nThe rules of linear algebra dictate that a set of homogeneous equations has solutions for the unknowns \ncn1 and cn2 only if the determinant of the coefﬁcients vanishes:\n \n` A11 - an\nA12\nA21\nA22 - an\n` = 0. \n(2.22)\nIt is common notation to use the symbol l for the eigenvalues, in which case this equation is\n \ndet 1A - lI 2 = 0, \n(2.23)\nwhere I is the identity matrix\n \nI = a1\n0\n0\n1b. \n(2.24)\nEquation (2.23) is known as the secular or characteristic equation. It is a second order equation in the \nparameter l and the two roots are identiﬁed as the two eigenvalues a1 and a2 that we are trying to ﬁnd. \nOnce these eigenvalues are found, they are then individually substituted back into Eqs. (2.21), which \nare solved to ﬁnd the coefﬁcients of the corresponding eigenvector.\nExample 2.1 Assume that we know (e.g., from Problem 2.1) that the matrix representation for \nthe operator Sy is\n \nSy \u0003 U\n2\n a0\n-i\ni\n0 b . \n(2.25)\nFind the eigenvalues and eigenvectors of the operator Sy.\nThe general eigenvalue equation is\n \nSy0 l9 = l0 l9, \n(2.26)\nand the possible eigenvalues l are found using the secular equation\n \ndet0 Sy - lI0 = 0. \n(2.27)\nThe secular equation is\n \n∞\n-l\n-i U\n2\ni U\n2\n-l\n∞= 0, \n(2.28)\nand solving yields the eigenvalues\n \n l2 + i 2 a U\n2b\n2\n= 0 \n \n l2 - a U\n2b\n2\n= 0\n \n \n l2 = a U\n2b\n2\n \n \n l = { U\n2,\n \n(2.29)\n",
    "40 \nOperators and Measurement\nwhich was to be expected, because we know that the only possible results of a measurement of any \nspin component are {U>2.\nAs before, we label the eigenvectors 0{9y. The eigenvalue equation for the positive eigenvalue is\n \nSy0  +9y = +  U\n2 0  +9y, \n(2.30)\nor in matrix notation\n \nU\n2\n a0\n-i\ni\n0 b aa\nbb = + U\n2\n aa\nbb, \n(2.31)\nwhere we must solve for a and b to determine the eigenvector. Multiplying through and canceling \nthe common factor yields\n \na-ib\nia b = aa\nbb. \n(2.32)\nThis results in two equations, but they are not linearly independent, so we need some more infor-\nmation. The normalization condition provides what we need. Thus we have two equations that \ndetermine the eigenvector coefﬁcients:\n \n b = ia\n \n \n 0 a0\n2 + 0 b0\n2 = 1. \n(2.33)\nSolving these yields\n \n 0 a0\n2 + 0 ia0\n2 = 1 \n \n 0 a0\n2 = 1\n2.\n \n(2.34)\nAgain we follow the convention of choosing the ﬁrst coefﬁcient to be real and positive, resulting in\n \n a =\n1\n12  \n \n b = i 1\n12. \n(2.35)\nThus the eigenvector corresponding to the positive eigenvalue is\n \n0  +9y \u0003\n1\n12\n a1\ni b. \n(2.36)\nLikewise, one can ﬁnd the eigenvector for the negative eigenvalue to be\n \n0  -9y \u0003\n1\n12\n a 1\n-ib . \n(2.37)\nThese are, of course, the same states we found in Chapter 1 (Eq. 1.60).\nThis procedure of ﬁnding the eigenvalues and eigenvectors of a matrix is known as diagonaliza-\ntion of the matrix and is the key step in many quantum mechanics problems. Generally, if we ﬁnd a \nnew operator, the ﬁrst thing we do is diagonalize it to ﬁnd its eigenvalues and eigenvectors. However, \nwe stop short of the mathematical exercise of ﬁnding the matrix that transforms the original matrix to \nits new diagonal form. This would amount to a change of basis from the original basis to a new basis \nof the eigenvectors we have just found, much like a rotation in three dimensions changes from one \ncoordinate system to another. We don’t want to make this change of basis. In the example above, the \nSy matrix is not diagonal, whereas the Sz matrix is diagonal, because we are using the Sz basis. It is \n",
    "2.2 New Operators \n41\ncommon practice to use the Sz basis as the default basis, so you can assume that is the case unless you \nare told otherwise.\nIn summary, we now know three operators and their eigenvalues and eigenvectors. The spin com-\nponent operators Sx, Sy, and Sz all have eigenvalues {U>2. The matrix representations of the opera-\ntors and eigenvectors are (see Problem 2.1)\n \nSx \u0003 U\n2\n a0\n1\n1\n0b \n0  +9x \u0003\n1\n12\n a1\n1b \n0  -9x \u0003\n1\n12\n a 1\n-1b \n \nSy \u0003 U\n2\n a0\n-i\ni\n0 b \n0  +9y \u0003\n1\n12\n a1\ni b \n0  -9y \u0003\n1\n12\n a 1\n-ib    \n.\n \n \nSz \u0003 U\n2\n a1\n0\n0\n-1b \n0  +9 \u0003 a1\n0b \n0  -9 \u0003 a0\n1b \n(2.38)\n 2.2 \u0002 NEW OPERATORS\n 2.2.1 \u0002 Spin Component in a General Direction\nNow that we know the three operators corresponding to the spin components along the three Cartesian \naxes, we can use them to ﬁnd the operator Sn for the spin component along a general direction nn. This \nnew operator will allow us to predict results of experiments we have not yet performed. The direction \nnn is speciﬁed by the polar and azimuthal angles u and f as shown in Fig. 2.3. The unit vector nn is\n \nn = in sin u cos f + jn sin u sin f + kn cos u. \n(2.39)\nThe spin component along this direction is obtained by projecting the spin vector S onto this new unit \nvector\n \n Sn = S~nn\n \n \n = Sx sin u cos f + Sy sin u sin f + Sz cos u. \n \n(2.40)\nThe matrix representations we found for Sx, Sy, and Sz lead to the matrix representation of the spin \ncomponent operator Sn (Problem 2.6):\n \nSn \u0003 U\n2\n acos u\nsin u  e-if\nsin u  eif\n-cos u\nb. \n(2.41)\nn\nz\nx\ny\nΦ\nΘ\n\u0006\nFIGURE 2.3 General direction along which to measure the spin component.\n",
    "42 \nOperators and Measurement\nWe have found a new operator, so to learn about its properties, we diagonalize it. Following \nthe diagonalization procedure outlined in Section 2.1.2, we ﬁnd that the eigenvalues of Sn are {U>2 \n(Problem 2.7). So if we measure the spin component along any direction, we get only two possible \nresults. This is to be expected from the experiments in Chapter 1. The eigenvectors for these two pos-\nsible measurements are (Problem 2.7):\n \n0  +9n = cos u\n2 0  +9 + sin u\n2\n eif0  -9  \n \n0  -9n = sin u\n2 0  +9 - cos u\n2\n eif0  -9, \n \n(2.42)\nwhere we again use the convention of choosing the ﬁrst coefﬁcient to be real and positive. It is important \nto point out that the 0  +9n eigenstate (or equivalently the 0  -9n eigenstate) can be used to represent any \npossible ket in a spin-1/2 system, if one allows for all possible angles 0 … u 6 p and 0 … f 6 2p.\nWe generally write the most general state as 0 c9 = a0  +9 + b0  -9, where a and b are complex. Requir-\ning that the state be normalized and using the freedom to choose the ﬁrst coefﬁcient real and positive \nreduces this to\n \n0 c9 = 0 a0 0  +9 + 41 - 0 a0\n2\n eif0  -9. \n(2.43)\nIf we change the parametrization of 0 a0  to cos 1u>22, we see that 0  +9n is equivalent to the most general \nstate 0 c9. This correspondence between the 0  +9n eigenstate and the most general state is only valid in a \ntwo-state system such as spin 1/2. In systems with more dimensionality, it does not hold because more \nparameters are needed to specify the most general state than are afforded by the two angles u and f.\nExample 2.2 Find the probabilities of the measurements shown in Fig. 2.4, assuming that the \nﬁrst Stern-Gerlach analyzer is aligned along the direction nn deﬁned by the angles u = 2p>3 and \nf = p>4.\nThe measurement by the ﬁrst Stern-Gerlach analyzer prepares the system in the spin up state \n0  +9n along the direction nn. This state is then the input state to the second Stern-Gerlach analyzer. \nThe input state is\n \n 0 cin9 = 0  +9n = cos u\n2 0  +9 + sin u\n2\n eif0  -9 \n \n = cos p\n3 0  +9 + sin p\n3\n eip/40  -9\n \n \n = 1\n2 0  +9 + 23\n2\n eip/40  -9.\n \n(2.44)\nX\n?\n?\n^n\nP x\n2\nP x\nx\nn\n2\nx\nn\nFIGURE 2.4  Measurement of the spin component after state preparation in a new direction.\n",
    "2.2 New Operators \n43\nThe second analyzer is aligned along the x-axis, so the probabilities are\n \n P+x = 0 x8+ 0 cin9 0\n2 = 0 x8+ 0  +9n0\n2  \n \n P-x = 0 x8- 0 cin9 0\n2 = 0 x8- 0  +9n0\n2. \n(2.45)\nLet’s calculate the ﬁrst probability using bra-ket notation, recalling that 0  +9x =\n1\n12 30  +9 + 0  -94:\n \n P+x = 0 x8+ 0  +9n0\n2\n \n \n = @ 1\n12 38+ 0   +  8- 0 4 1\n2 3 0  +9 + 13eip/40  -94@\n2\n \n \n = @\n1\n212 31 + 13eip/44@\n2\n \n \n = 1\n8 31 + 13eip/4431 + 13e-ip/44\n \n \n = 1\n8 31 + 131eip/4 + e-ip/42 + 34\n \n \n = 1\n8 34 + 213 cos 1p>424\n \n \n = 1\n8 34 + 213> 124 \u0002 0.806.\n \n(2.46)\nLet’s calculate the second probability using matrix notation, recalling that 0  -9x =\n1\n12 30  +9 - 0  -94:\n \n P-x = 0 x8- 0  +9n0\n2\n \n \n = ` 1\n12 11\n-12 1\n2 a\n1\n13eip>4b `\n2\n \n \n = @\n1\n212 31 - 13eip/44@\n2\n \n \n = 1\n8 34 - 213 cos 1p>424\n \n \n = 1\n8 34 - 213> 124 \u0002 0.194. \n(2.47)\nThe two results sum to unity as they must. A histogram of the measured results is shown in Fig. 2.5.\n1\nP\nP\u0003x\nP\u0002x\n\u0002Ψin\u0003 \u0006\u0005\u0002\u0002\u0003n\nSx\n\u0003\u0002\n2\n\u0002\n2\nFIGURE 2.5 Histogram of spin component Sx measurement.\n",
    "44 \nOperators and Measurement\n2.2.2 \u0002 Hermitian Operators\nSo far we have deﬁned how operators act upon kets. For example, an operator A acts on a ket 0 c9 to \nproduce a new ket 0 f9 = A0 c9. The operator acts on the ket from the left; if the operator is on the \nright of the ket, the result is not deﬁned, which is clear if you try to use matrix representation. Simi-\nlarly, an operator acting on a bra must be on the right side of the bra\n \n8j0 = 8c0 A \n(2.48)\nand the result is another bra. However, the bra 8j0 = 8c0 A is not the bra 8f0  that corresponds to the \nket 0 f9 = A0 c9. Rather the bra 8f0  is found by deﬁning a new operator A† that obeys\n \n8f0 = 8c0 A†. \n(2.49)\nThis new operator A† is called the Hermitian adjoint of the operator A. We can learn something about the \nHermitian adjoint by taking the inner product of the state 0 f9 = A0 c9 with another (unspeciﬁed) state 0 b9\n \n 8f0 b9 = 8b0 f9*\n \n \n 38c0 A+4 0 b9 = 58b03A0 c946* \n \n 8c0 A+ 0 b9 = 8b0 A0 c9*,\n \n(2.50)\nwhich relates the matrix elements of A and A†. Equation (2.50) tells us that the matrix representing the \nHermitian adjoint A† is found by transposing and complex conjugating the matrix representing A. This \nis consistent with the deﬁnition of Hermitian adjoint used in matrix algebra.\nAn operator A is said to be Hermitian if it is equal to its Hermitian adjoint A†. If an operator is \nHermitian, then the bra 8c0 A is equal to the bra 8f0  that corresponds to the ket 0 f9 = A0 c9. That is, a \nHermitian operator can act to the right on a ket or to the left on a bra with the same result. In quantum \nmechanics, all operators that correspond to physical observables are Hermitian. This includes the spin \noperators we have already encountered as well as the energy, position, and momentum operators that \nwe will introduce in later chapters. The Hermiticity of physical observables is important in light of two \nfeatures of Hermitian matrices: (1) Hermitian matrices have real eigenvalues, which ensures that results \nof measurements are always real; and (2) the eigenvectors of a Hermitian matrix comprise a complete \nset of basis states, which ensures that we can use the eigenvectors of any observable as a valid basis.\n 2.2.3 \u0002 Projection Operators\nFor the spin-1/2 system, we now know four operators: Sx, Sy, Sz, and Sn. Let’s look for some other \noperators. Consider the ket 0 c9 written in terms of its coefﬁcients in the Sz basis\n \n 0 c9 = a0  +9 + b0  -9\n \n \n = 18+ 0 c92 0  +9 + 18- 0 c92 0  -9. \n \n(2.51)\nLooking for the moment only at the ﬁrst term, we can write it as a number times a ket, or as a ket times \na number:\n \n18+ 0 c92 0  +9 = 0  +918+ 0 c92 \n(2.52)\nwithout changing its meaning. Using the second form, we can separate the bra and ket that form the \ninner product and obtain\n \n0  +918+ 0 c92 = 10  +98+ 02 0 c9. \n(2.53)\n",
    "2.2 New Operators \n45\nThe new term in parentheses is a product of a ket and a bra but in the opposite order compared to the \ninner product deﬁned earlier. This new object must be an operator because it acts on the ket 0 c9 and\nproduces another ket: 18+ 0 c92 0  +9. This new type of operator is known as an outer product.\nReturning now to Eq. (2.51), we write 0 c9 using these new operators:\n \n 0 c9 = 8+ 0 c9 0  +9 + 8- 0 c9 0  -9  \n \n = 0  +98+ 0 c9 + 0  -98- 0 c9  \n \n = 10  +98+ 0 + 0  -98- 02 0 c9. \n \n(2.54)\nThe term in parentheses is a sum of two outer products and is clearly an operator because it acts on a \nket to produce another ket. In this special case, the result is the same as the original ket, so the operator \nmust be the identity operator 1. This relationship is often written as\n \n0  +98+ 0 + 0  -98- 0 = 1 \n(2.55)\nand is known as the completeness relation or closure. It expresses the fact that the basis states 0 {9 \ncomprise a complete set of states, meaning any arbitrary ket can be written in terms of them. To make \nit obvious that outer products are operators, it is useful to express Eq. (2.55) in matrix notation using \nthe standard rules of matrix multiplication:\n \n 0  +98+ 0 + 0  -98- 0 \u0003 a1\n0b11\n02 + a0\n1b10\n12 \n \n \u0003 a1\n0\n0\n0b + a0\n0\n0\n1b\n \n \n \n \u0003 a1\n0\n0\n1b.\n \n \n(2.56)\nEach outer product is represented by a matrix, as we expect for operators, and the sum of these two \nouter products is represented by the identity matrix, which we expected from Eq. (2.54).\nNow consider the individual operators 0  +98+ 0  and 0  -98- 0 . These operators are called projec-\ntion operators, and for spin 1/2 they are given by\n \n P+ = 0  +98+ 0 \u0003 a1\n0\n0\n0b  \n \n P- = 0  -98- 0 \u0003 a0\n0\n0\n1b. \n \n(2.57)\nIn terms of these new operators the completeness relation can also be written as\n \nP+ + P- = 1. \n(2.58)\nWhen a projection operator for a particular eigenstate acts on a state 0 c9, it produces a new ket that is \naligned along the eigenstate and has a magnitude equal to the amplitude (including the phase) for the \nstate 0 c9 to be in that eigenstate. For example,\n \nP+ 0 c9 = 0  +98+ 0 c9 = 18+ 0 c92 0  +9  \n \nP- 0 c9 = 0  -98- 0 c9 = 18- 0 c92 0  -9. \n \n(2.59)\n",
    "46 \nOperators and Measurement\nNote also that a projector acting on its corresponding eigenstate results in that eigenstate, and a projec-\ntor acting on an orthogonal state results in zero:\n \n P+ 0  +9 = 0  +98+ 0  +9 = 0  +9 \n \n P- 0  +9 = 0  -98- 0  +9 = 0.  \n \n(2.60)\nBecause the projection operator produces the probability amplitude, we expect that it must be inti-\nmately tied to measurement in quantum mechanics.\nWe found in Chapter 1 that the probability of a measurement is given by the square of the inner \nproduct of initial and ﬁnal states (postulate 4). Using the new projection operators, we rewrite the \nprobability as\n \n P+ = 08+ 0 c9 0\n2\n \n \n = 8+ 0 c9*8+ 0 c9 \n \n = 8c0  +98+ 0 c9  \n \n = 8c0 P+ 0 c9.\n \n \n(2.61)\nThus we say that the probability of the measurement Sz = U>2 can be calculated as a matrix element \nof the projection operator, using the input state 0 c9 and the projector P+ corresponding to the result.\nThe other important aspect of quantum measurement that we learned in Chapter 1 is that a mea-\nsurement disturbs the system. That is, if an input state 0 c9 is measured to have Sz = +U>2, then the \noutput state is no longer 0 c9 but is changed to 0  +9. We saw above that the projection operator does this \noperation for us, with a multiplicative constant of the probability amplitude. Thus, if we divide by this \namplitude, which is the square root of the probability, then we can describe the abrupt change of the \ninput state as\n \n0 c\u00049 =\nP+ 0 c9\n2\n 8c0 P+ 0 c9\n= 0  +9, \n(2.62)\nwhere 0 c\u00049 is the output state. This effect is described by the ﬁfth postulate, which is presented below \nand is often referred to as the projection postulate.\nPostulate 5\nAfter a measurement of A that yields the result an, the quantum system is in a \nnew state that is the normalized projection of the original system ket onto the \nket (or kets) corresponding to the result of the measurement:\n0 c\u00049 =\nPn0 c9\n2\n 8c0 Pn0 c9\n.\nThe projection postulate is at the heart of quantum measurement. This effect is often referred to as the \ncollapse (or reduction or projection) of the quantum state vector. The projection postulate clearly states \nthat quantum measurements cannot be made without disturbing the system (except in the case where the \ninput state is the same as the output state), in sharp contrast to classical measurements. The collapse of \n the quantum state makes quantum mechanics irreversible, again in contrast to classical mechanics.\n",
    "2.2 New Operators \n47\nWe can use the projection postulate to make a model of quantum measurement, as shown in the \nrevised depiction of a Stern-Gerlach measurement system in Fig. 2.6. The projection operators act on \nthe input state to produce output states with probabilities given by the squares of the amplitudes that \nthe projection operations yield. For example, the input state 0 cin9 is acted on the projection operator \nP+ = 0  +98+ 0 , producing an output ket 0 cout9 = 0  +918+ 0 cin92 with probability P+ = 08+ 0 cin9 0 2. \nThe output ket 0 cout9 = 0  +918+ 0 cin92 is really just a 0  +9 ket that is not properly normalized, so we \nnormalize it for use in any further calculations. We do not really know what is going on in the mea-\nsurement process, so we cannot explain the mechanism of the collapse of the quantum state vector. \nThis lack of understanding makes some people uncomfortable with this aspect of quantum mechan-\nics and has been the source of much controversy surrounding quantum mechanics. Trying to better \nunderstand the measurement process in quantum mechanics is an ongoing research problem. How-\never, despite our lack of understanding, the theory for predicting the results of experiments has been \nproven with very high accuracy.\n 2.2.4 \u0002 Analysis of Experiments 3 and 4\nWe can now return to Experiments 3 and 4 from Chapter 1 and analyze them with these new tools. \nRecall that Experiment 3 is the same as Experiment 4a, and Experiments 4a and 4b are similar in that \nthey each use only one of the output ports of the second Stern-Gerlach analyzer as input to the third \nanalyzer. Figure 2.7 depicts these experiments again, with Fig. 2.7(a) showing a hybrid experiment \nthat is essentially Experiment 4a in its upper half and Experiment 4b in its lower half, and Fig. 2.7(b) \nshowing Experiment 4c. In this problem, we discuss the probability that an atom leaving the ﬁrst \nanalyzer in the 0  +9 state is detected in one of the counters connected to the output ports of the third \nanalyzer. Such a probability involves two measurements at the second and third analyzers. The total \nprobability is the product of the individual probabilities of each measurement.\nFor the hybrid experiment shown in Fig. 2.7(a), the probability of measuring an atom at the top-\nmost counter is the probability of measuring Sx = +U>2 at the second analyzer, 0 x8+ 0  +9 0\n2, times the \nprobability of measuring Sz = +U>2 at the third analyzer, 08+ 0  +9x0\n2, giving\n \nPupper, + = 08+ 0  +9x0\n2\n 0 x8+ 0  +9 0\n2. \n(2.63)\nLikewise the probability of measuring the atom to have Sx = +U>2 and then Sz = -U>2 is\n \nPupper, - = 08- 0  +9x0\n2\n 0 x8+ 0  +9 0\n2, \n(2.64)\nZ\n\u0002\u0002\u0003 \u0004\u0002\u0002\n\u0002\u0003\u0003 \u0004\u0003\u0002\n\u0002\u0002\u0003\n\u0002\u0003\u0003\nProject\nNormalize\n\u0002Ψ\u0003\n\u0002\u0002\u0003\u0004\u0002\u0002Ψ\u0003\n\u0002\u0003\u0003\u0004\u0003\u0002Ψ\u0003\nFIGURE 2.6 Schematic diagram of the role of the projection operator in a Stern-Gerlach spin measurement.\n",
    "48 \nOperators and Measurement\nwhere we have written the product so as to be read from right to left as is the usual practice with \nquantum mechanical amplitudes and probabilities. For atoms that take the lower path from the second \nanalyzer, the ﬁnal probabilities are\n \nPlower, + = 08+ 0  -9x0\n2\n 0 x8- 0  +9 0\n2  \n \nPlower, - = 08- 0  -9x0\n2\n 0 x8- 0  +9 0\n2. \n \n(2.65)\nFor Experiment 4c, shown in Fig. 2.7(b), we have a new situation at the second analyzer. Both \noutput ports are connected to the third analyzer, which means that the probability of an atom from \nthe ﬁrst analyzer being input to the third analyzer is 100%. So we need only calculate the probability \nof passage through the third analyzer. The crucial step is determining the input state, for which we \nuse the projection postulate. Because both states are used, the relevant projection operator is the sum \nof the two projection operators for each port, P+x + P-x, where P+x = 0  +9x x8+ 0  and P-x = 0  -9x x8- 0 .\nThus the state after the second analyzer is\n \n 0 c29 =\n1P+x + P-x2 0 c19\n2\n 8c101P+x + P-x2 0 c19\n \n \n =\n1P+x + P-x2 0  +9\n2\n 8+ 01P+x + P-x2 0  +9\n . \n \n(2.66)\n100\n0\nX\nZ\nZ\n100\n100\n25\n25\nZ\nX\nZ\n100\n(a)\n(b)\n50\n25\n25\nZ\n50\nFIGURE 2.7 (a) Hybrid Experiment 4a and 4b, and (b) Experiment 4c.\n",
    "2.2 New Operators \n49\nIn this simple example, the projector P+x + P-x is equal to the identity operator because the two states \nform a complete basis. This clearly simpliﬁes the calculation, giving 0 c29 = 0  +9, but to illustrate our \npoint, let’s simplify only the denominator (which equals one), giving\n \n 0 c29 = 10  +9x x8+ 0 + 0  -9x x8- 02 0  +9  \n \n = 0  +9x x8+ 0  +9 + 0  -9x x8- 0  +9. \n \n(2.67)\nThus the beam entering the third analyzer can be viewed as a coherent superposition of the eigenstates \nof the second analyzer. Now calculate the probability of measuring spin up at the third analyzer:\n \n P+ = 08+ 0 c29 0\n2\n \n \n = 08+ 0  +9x x8+ 0  +9 + 8+ 0  -9x x8- 0  +9 0\n2. \n \n(2.68)\nThe probability of measuring spin down at the third analyzer is similarly\n \nP - = 08- 0 c29 0\n2\n \n \n = 08- 0  +9x x8+ 0  +9 + 8- 0  -9x x8- 0  +9 0\n2. \n \n(2.69)\nIn each case, the probability is a square of a sum of amplitudes, each amplitude being the amplitude \nfor a successive pair of measurements. For example, in P- the amplitude 8- 0  +9x x8+ 0  +9 refers to the \nupper path that the initial 0  +9 state takes as it is ﬁrst measured to be in the 0  +9x state and then mea-\nsured to be in the 0  -9 state (read from right to left). This amplitude is added to the amplitude for the \nlower path because the beams of the second analyzer are combined, in the proper fashion, to create the \ninput beam to the third analyzer. When the sum of amplitudes is squared, four terms are obtained, two \nsquares and two cross terms, giving\n \n P- = 08- 0  +9x x8+ 0  +9 0\n2 + 08- 0  -9x x8- 0  +9 0\n2 \n \n +8- 0  +9*\nx x8+ 0  +9*8- 0  -9x x8- 0  +9\n \n \n +8- 0  +9x x8+ 0  +98- 0  -9*\nx x8- 0  +9*\n \n \n = Pupper, - + Plower, - + interference terms. \n \n(2.70)\nThis tells us that the probability of detecting an atom to have spin down when both paths are used is the \nsum of the probabilities for detecting a spin down atom when either the upper path or the lower path is \nused alone plus additional cross terms involving both amplitudes, which are commonly called interference \nterms. It is these additional terms, which are not complex squares and so could be positive or negative, that \nallow the total probability to become zero in this case, illustrating the phenomenon of interference.\nThis interference arises from the nature of the superposition of states that enters the third analyzer. \nTo illustrate, consider what happens if we change the superposition state to a mixed state, as we dis-\ncussed previously in Section 1.2.3. Recall that a superposition state implies a beam with each atom in \nthe same state, which is a combination of states, while a mixed state implies that the beam consists of \natoms in separate states. As we have described it so far, Experiment 4c involves a superposition state \nas the input to the third analyzer. We can change this to a mixed state by “watching” to see which of \nthe two output ports of the second analyzer each atom travels through. There are a variety of ways to \nimagine doing this experimentally. The usual idea proposed is to illuminate the paths with light and \nwatch for the scattered light from the atoms. With proper design of the optics, the light can be localized \n",
    "50 \nOperators and Measurement\nsufﬁciently to determine which path the atom takes. Hence, such experiments are generally referred to \nas “Which Path” or “Welcher Weg” experiments. Such experiments can be performed in the SPINS \nprogram by selecting the “Watch” feature. Once we know which path the atom takes, the state is not \nthe superposition 0 c29 described above, but is either 0  +9x or 0  -9x, depending on which path produces \nthe light signal. To ﬁnd the probability that atoms are detected at the spin down counter of the third \nanalyzer, we add the probabilities for atoms to follow the path 0  +9 S 0  +9x S 0  -9 to the probability \nfor other atoms to follow the path 0  +9 S 0  -9x S 0  -9 because these are independent events, giving\n \n Pwatch,  - = 08- 0  +9x x8+ 0  +9 0\n2 + 08- 0  -9x x8- 0  +9 0\n2 \n \n = Pupper, - + Plower, - ,\n \n \n(2.71)\nin which no interference terms are present.\nThis interference example illustrates again the important distinction between a coherent superpo-\nsition state and a statistical mixed state. In a coherent superposition, there is a deﬁnite relative phase \nbetween the different states, which gives rise to interference effects that are dependent on that phase. In a \nstatistical mixed state, the phase relationship between the states has been destroyed and the interference \nis washed out. Now we can understand what it takes to have the beams “properly” combined after the \nsecond analyzer of Experiment 4c. The relative phases of the two paths must be preserved. Anything that \nrandomizes the phase is equivalent to destroying the superposition and leaving only a statistical mixture. \nIf the beams are properly combined to leave the superposition intact, the results of Experiment 4c are \nthe same as if no measurement were made at the second analyzer. So even though we have used a mea-\nsuring device in the middle of Experiment 4c, we generally say that no measurement was made there. \nWe can summarize our conclusions by saying that if no measurement is made on the intermediate state, \nthen we add amplitudes and then square to ﬁnd the probability, while if an intermediate measurement is \n performed (i.e., watching), then we square the amplitudes ﬁrst and then add to ﬁnd the probability. One \nis the square of a sum and the other is the sum of squares, and only the former exhibits interference.\n 2.3 \u0002 MEASUREMENT\nLet’s discuss how the probabilistic nature of quantum mechanics affects the way experiments are \nperformed and compared with theory. In classical physics, a theoretical prediction can be reliably \ncompared to a single experimental result. For example, a prediction of the range of a projectile can be \ntested by doing an experiment. The experiment may be repeated several times in order to understand \nand possibly reduce any systematic errors (e.g., wind) and measurement errors (e.g., misreading the \ntape measure). In quantum mechanics, a single measurement is meaningless. If we measure an atom to \nhave spin up in a Stern-Gerlach analyzer, we cannot discern whether the original state was 0  +9 or 0  -9x \nor any arbitrary state 0 c9 1except 0  -92. Moreover, we cannot repeat the measurement on the same \natom, because the original measurement changed the state, per the projection postulate.\nThus, one must, by necessity, perform identical measurements on identically prepared systems. \nIn the spin-1/2 example, an initial Stern-Gerlach analyzer is used to prepare atoms in a particular state \n0 c9. Then a second Stern-Gerlach analyzer is used to perform the same experiment on each identically \n prepared atom. Consider performing a measurement of Sz on N identically prepared atoms. Let N+ be the \nnumber of times the result +U>2 is recorded and N− be the number of times the result -U>2 is recorded. \nBecause there are only two possible results for each measurement, we must have N = N+ + N-. The \nprobability postulate (postulate 4) predicts that the probability of measuring +U>2 is\n \nP+ = 08+ 0 c9 0\n2. \n(2.72)\n",
    "2.3 Measurement \n51\nFor a ﬁnite number N of atoms, we expect that N+ is only approximately equal to P+ N due to the statis-\ntical ﬂuctuations inherent in a random process. Only in the limit of an inﬁnite number N do we expect \nexact agreement:\n \nlim\nNS \u0005\nN+\nN = P+ = 08+ 0 c9 0\n2. \n(2.73)\nIt is useful to characterize a data set in terms of the mean and standard deviation (see Appendix \nA for further information on probability). The mean value of a data set is the average of all the mea-\nsurements. The expected or predicted mean value of a measurement is the sum of the products of each \npossible result and its probability, which for this spin-1/2 measurement is\n \n8Sz9 = a+  U\n2b P+ + a-  U\n2b P-   , \n(2.74)\nwhere the angle brackets signify average or mean value. Using the rules of quantum mechanics we \nrewrite this mean value as\n \n 8Sz9 = +  U\n2\n 08+ 0 c9 0\n2 + a-  U\n2b 08- 0 c9 0\n2\n \n \n = +  U\n2\n 8c0  +98+ 0 c9 + a-  U\n2b8c0  -98- 0 c9 \n \n = 8c0 J\n +  U\n2 0  +98+ 0 c9 + a-  U\n2b 0  -98- 0 c9\n R \n \n = 8c03Sz0  +98+ 0 c9 + Sz0  -98- 0 c94\n \n \n = 8c0 Sz30  +98+ 0 + 0  -98- 04 0 c9.\n \n \n(2.75)\nAccording to the completeness relation, the term in square brackets in the last line is unity, so \nwe obtain\n \n8Sz9 = 8c0 Sz0 c9  . \n(2.76)\nWe now have two ways to calculate the predicted mean value, Eq. (2.74) and Eq. (2.76). Which you \nuse generally depends on what quantities you have readily available. The matrix element version in \nEq. (2.76) is more common and is especially useful in systems that are more complicated than the \n2-level spin-1/2 system. This predicted mean value is commonly called the expectation value, but \nit is not the expected value of any single experiment. Rather it is the expected mean value of a large \nnumber of experiments. It is not a time average, but an average over many identical experiments. For a \ngeneral quantum mechanical observable, the expectation value is\n \n8A9 = 8c0 A0 c9 = a\nn\nanPan   , \n(2.77)\nwhere an are the eigenvalues of the operator A.\nTo see how the concept of expectation values applies to our study of spin-1/2 systems, consider \ntwo examples. First consider a system prepared in the state 0  +9. The expectation value of Sz is\n \n8Sz9 = 8+ 0 Sz0  +9, \n(2.78)\n",
    "52 \nOperators and Measurement\nwhich we calculate with bra-ket notation\n \n 8Sz9 = 8+ 0 Sz0  +9 \n \n = 8+ 0 U\n2 0  +9  \n \n = U\n2\n 8+ 0  +9  \n \n = U\n2.\n \n \n(2.79)\nThis result should seem obvious because +U>2 is the only possible result of a measurement of Sz for \nthe 0  +9 state, so it must be the expectation value.\nNext consider a system prepared in the state 0  +9x. In this case, the expectation value of Sz is\n \n8Sz9 = x8+ 0 Sz0 +9x. \n(2.80)\nUsing matrix notation, we obtain\n \n 8Sz9 =\n1\n12  11\n12 U\n2\n a1\n0\n0\n-1b 1\n12 a1\n1b \n \n = U\n4\n  11\n12a 1\n-1b = 0 U.\n \n \n(2.81)\nAgain this is what you expect, because the two possible measurement results {U>2 each have 50% \nprobability, so the average value is zero. Note that the value of zero is never measured, so it is not the \nvalue “expected” for any given measurement, but rather the expected mean value of an ensemble of \nmeasurements.\nIn addition to the mean value, it is common to characterize a measurement by the standard devia-\ntion, which quantiﬁes the spread of measurements about the mean or expectation value. The standard \ndeviation is deﬁned as the square root of the mean of the square of the deviations from the mean, and \nfor an observable A is given by\n \n\u0006A = 481A - 8A9229, \n(2.82)\nwhere the angle brackets signify average value as used in the deﬁnition of an expectation value. This \nresult is also often called the root-mean-square deviation, or r.m.s. deviation. We need to square the \ndeviations, because the deviations from the mean are equally distributed above and below the mean in \nsuch a way that the average of the deviations themselves is zero. This expression can be simpliﬁed by \nexpanding the square and performing the averages, resulting in\n \n \u0006A = 481A2 - 2A8A9 + 8A9229 \n \n = 48A29 - 28A98A9 + 8A92 \n \n = 48A29 - 8A92,\n \n \n(2.83)\n",
    "2.3 Measurement \n53\nwhere one must be clear to distinguish between the square of the mean 8A9\n2 and the mean of the \nsquare 8A29. While the mean of the square of an observable may not be a common experimental quan-\ntity, it can be calculated using the deﬁnition of the expectation value\n \n8A29 = 8c0 A20 c9. \n(2.84)\nThe square of an operator means that the operator acts twice in succession:\n \nA20 c9 = AA0 c9 = A1A0 c92. \n(2.85)\nTo gain experience with the standard deviation, return to the two examples used above. To calcu-\nlate the standard deviation, we need to ﬁnd the mean of the square of the operator S z. In the ﬁrst case \nA 0  +9 initial stateB, we get\n \n 8S\n 2\nz 9 = 8+ @S\n 2\nz @  +9 = 8+ @Sz Sz@  +9 = 8+ @Sz U\n2\n @  +9\n \n = 8+ 0 a U\n2b\n2\n0  +9\n \n = a U\n2b\n2\n.\n \n(2.86)\nWe already have the mean of the operator Sz in Eq. (2.79) so the standard deviation is\n \n \u0006Sz = 48S\n 2\nz 9 - 8Sz9\n2  \n \n = Ca U\n2b\n2\n- a U\n2b\n2\n \n \n = 0 U,\n \n \n(2.87)\nwhich is to be expected because there is only one possible result, and hence no spread in the results of \nthe measurement, as shown in the histogram in Fig. 2.8(a).\n(a)\n1\nP\nP−\nP+\nSz\n\u0003\u0002\n2\n\u0002\n2\n1\nP\nP−\nP+\nSz\n\u0003\u0002\n2\n\u0002\n2\n\u000bSz \u0006 \u0002\n2\n\u000bSz \u0006\u00050\n\u0004Sz\u0003 \u0006\u0005\n\u0004Sz\u0003 \u0006\u00050\u0005\n\u0002\n2\n\u0002Ψin\u0003\u0005\u0006\u0005\u0002\u0002\u0003\n\u0002Ψin\u0003\u0005\u0006\u0005\u0002\u0002\u0003x\nFIGURE 2.8 Idealized measurements of Sz with (a) a 0  +9 input state and (b) with a 0  +9x input state.\n",
    "54 \nOperators and Measurement\nIn the second case 1 0  +9x initial state2, the mean of the square of the operator Sz is\n \n 8S\n 2\nz 9 = x8+ @ S\n 2\nz @  +9x\n \n \n =\n1\n12\n 11\n12  U\n2\n a1\n0\n0\n-1b U\n2\n a1\n0\n0\n-1b 1\n12\n a1\n1b \n \n = 1\n2\n a U\n2b\n2\n11\n12a1\n0\n0\n-1ba 1\n-1b\n \n \n = 1\n2\n a U\n2b\n2\n11\n12a1\n1b\n \n \n = a U\n2b\n2\n.\n \n \n(2.88)\nThe mean of the operator Sz is in Eq. (2.81), giving a standard deviation of \n \n \u0006Sz = 48S\n 2\nz 9 - 8Sz9\n2 \n \n = Ca U\n2b\n2\n- 0 U2 \n(2.89)\n \n = U\n2.\nAgain this makes sense because each measurement deviates from the mean (0 U) by the same value of \nU>2, as shown in the histogram in Fig. 2.8(b).\nThe standard deviation \u0006A represents the uncertainty in the results of an experiment. In quan-\ntum mechanics, this uncertainty is inherent and fundamental, meaning that you cannot design the \nexperiment any better to improve the result. What we have calculated then is the minimum uncertainty \nallowed by quantum mechanics. Any actual uncertainty may be larger due to experimental error. \nThis is another ramiﬁcation of the probabilistic nature of quantum mechanics and will lead us to the \nHeisenberg uncertainty relation in Section 2.5.\n 2.4 \u0002 COMMUTING OBSERVABLES\nWe found in Experiment 3 that two incompatible observables could not be known or measured simul-\ntaneously, because measurement of one somehow erased knowledge of the other. Let us now explore \nfurther what it means for two observables to be incompatible and how incompatibility affects the results \nof measurements. First we need to deﬁne a new object called a commutator. The commutator of two \noperators is deﬁned as the difference between the products of the two operators taken in alternate orders:\n \n3A, B4 = AB - BA. \n(2.90)\nIf the commutator is equal to zero, we say that the operators or observables commute; if it is not zero, we \nsay they don’t commute. Whether or not two operators commute has important ramiﬁcations in analyzing \na quantum system and in making measurements of the two observables represented by those operators.\n",
    "2.4 Commuting Observables \n55\nConsider what happens when two operators A and B do commute:\n \n 3A, B4 = 0\n \n \n AB - BA = 0\n \n \n AB = BA. \n(2.91)\nThus, for commuting operators the order of operation does not matter, whereas it does for noncom-\nmuting operators. Now let 0 a9 be an eigenstate of the operator A with eigenvalue a:\n \nA0 a9 = a0 a9. \n(2.92)\nOperate on both sides of this equation with the operator B and use the fact that A and B commute:\n \n BA0 a9 = Ba0 a9\n \n \n AB0 a9 = aB0 a9\n \n \n A1B0 a92 = a1B0 a92. \n(2.93)\nThe last equation says that the state B0 a9 is also an eigenstate of the operator A with the same eigen-\nvalue a. Assuming that each eigenvalue has a unique eigenstate (which is true if there is no degen-\neracy, but we haven’t discussed degeneracy yet), the state B0 a9 must be some scalar multiple of the \nstate 0 a9. If we call this multiple b, then we can write\n \nB0 a9 = b0 a9, \n(2.94)\nwhich is just an eigenvalue equation for the operator B. Thus, we must conclude that the state 0 a9 is \nalso an eigenstate of the operator B, with the eigenvalue b. The assumption that the operators A and B \ncommute has led us to the result that A and B have common or simultaneous sets of eigenstates. This \nresult bears repeating:\nCommuting operators share common eigenstates.\nThe ramiﬁcations of this result for experiments are very important. Recall that a measurement of \nthe observable A projects the initial state 0 c9 onto an eigenstate of A: 0 a9. A subsequent measurement \nof the observable B then projects the input state 0 a9 onto an eigenstate of B. But the eigenstates of \nthe commuting operators A and B are the same, so the second measurement does not change the state \n0 a9. Thus, another measurement of A following the measurement of B yields the same result as the \n initial measurement of A, as illustrated in Fig. 2.9. Thus we say that we can know the eigenvalues of \nthese two observables simultaneously. It is common to extend this language and say that these two \nobservables can be measured simultaneously, although, as illustrated in Fig. 2.9, we do not really measure \nthem simultaneously. What we mean is that we can measure one observable without erasing our knowl-\nedge of the previous results of the other observable. Observables A and B are said to be compatible.\n100\n0\n0\nA\nB\nA\na1\na1\na1\na2\na3\na1\na2\na3\na1\na2\na3\nb1\nb2\nb3\nFIGURE 2.9 Successive measurements of commuting observables.\n",
    "56 \nOperators and Measurement\nConversely, if two operators do not commute, then they are incompatible observables and cannot \nbe measured or known simultaneously. This is what we saw in Experiment 3 in Chapter 1. In that case, the \ntwo observables were Sx and Sz. Let’s take a look at their commutator to show that they are not compatible:\n \n3Sz, Sx4 \u0003 U\n2\n a1\n0\n0\n-1b U\n2\n a0\n1\n1\n0b - U\n2\n a0\n1\n1\n0b U\n2\n a1\n0\n0\n-1b \n \n\u0003 a U\n2b\n2\n c a 0\n1\n-1\n0b - a0\n-1\n1\n0 bd\n \n \n\u0003 a U\n2b\n2\na 0\n2\n-2\n0b\n \n \n= i USy. \n(2.95)\nAs expected, these two operators do not commute. In fact, none of the spin component operators com-\nmute with each other. The complete commutation relations are\n \n3Sx, Sy4 = i USz\n \n \n3Sy, Sz4 = i USx \n \n3Sz, Sx4 = i USy   , \n(2.96)\nso written to make the cyclic relations clear.\nWhen we represent operators as matrices, we can often decide whether two operators commute \nby inspection of the matrices. Recall the important statement: An operator is always diagonal in its \nown basis. If you are presented with two matrices that are both diagonal, they must share a common \nbasis, and so they commute with each other. To be explicit, the product of two diagonal matrices\n \nAB \u0003 §\na1\n0\n0\ng\n0\na2\n0\ng\n0\n0\na3\ng\nf\nf\nf\nf\n¥  §\nb1\n0\n0\ng\n0\nb2\n0\ng\n0\n0\nb3\ng\nf\nf\nf\nf\n¥ \n \n\u0003 §\na1b1\n0\n0\ng\n0\na2b2\n0\ng\n0\n0\na3b3\ng\nf\nf\nf\nf\n¥ , \n(2.97)\nis clearly independent of the order of the product. Note, however, that you may not conclude that two \noperators do not commute if one is diagonal and one is not, nor if both are not diagonal.\n 2.5 \u0002 UNCERTAINTY PRINCIPLE\nThe intimate connection between the commutator of two observables and the possible precision of \nmeasurements of the two corresponding observables is reﬂected in an important relation that we sim-\nply state here (see more advanced texts for a derivation). The product of the uncertainties or standard \ndeviations of two observables is related to the commutator of the two observables:\n \n\u0006A\u0006B Ú 1\n2 083A, B49 0   . \n(2.98)\n",
    "2.6 S2 Operator \n57\nThis is the uncertainty principle of quantum mechanics. Consider what it says about a simple Stern-\nGerlach experiment. The uncertainty principle for the Sx and Sy spin components is\n \n \u0006Sx\u0006Sy Ú 1\n2 083Sx, Sy49 0  \n \n Ú 1\n2 08i USz9 0\n \n \n Ú U\n2 08Sz9 0 .\n \n(2.99)\nThese uncertainties are the minimal quantum mechanical uncertainties that would arise in any experi-\nment. Any experimental uncertainties due to experimenter error, apparatus errors, and statistical limi-\ntations would be additional.\nLet’s now apply the uncertainty principle to Experiment 3 where we ﬁrst learned of the impact of \nmeasurements in quantum mechanics. If the initial state is 0  +9, then a measurement of Sz results in an \nexpectation value 8Sz9 = U>2 with an uncertainty \u0006Sz = 0, as illustrated in Fig. 2.8(a). Thus the uncer-\ntainty principle dictates that the product of the other uncertainties for measurements of the 0  +9 state is\n \n\u0006Sx\u0006Sy Ú a U\n2b\n2\n, \n(2.100)\nor simply\n \n\u0006Sx\u0006Sy \u0002 0. \n(2.101)\nThis implies that\n \n  \u0006Sx \u0002 0  \n \n \u0006Sy \u0002 0. \n(2.102)\nThe conclusion to draw from this is that while we can know one spin component absolutely (\u0006Sz = 0), \nwe can never know all three, nor even two, simultaneously. This is in agreement with our results from \nExperiment 3. This lack of ability to measure all spin components simultaneously implies that the spin \ndoes not really point in a given direction, as a classical spin or angular momentum does. So when we \nsay that we have measured “spin up,” we really mean only that the spin component along that axis is up, \nas opposed to down, and not that the complete spin angular momentum vector points up along that axis.\n 2.6 \u0002 S2 OPERATOR\nAnother indication that the spin does not point along the axis along which you measure the spin com-\nponent is obtained by considering a new operator that represents the magnitude of the spin vector but \nhas no information about the direction. It is common to use the square of the spin vector for this task. \nThis new operator is\n \nS2 = S2\nx + S2\ny + S2\nz , \n(2.103)\nand it is calculated in the Sz representation as\n \nS2 \u0003 a U\n2b\n2\n c a0\n1\n1\n0b a0\n1\n1\n0b + a0\n-i\ni\n0\n ba0\n-i\ni\n0\n b + a1\n0\n0\n-1ba1\n0\n0\n-1bd  \n \n\u0003 a U\n2b\n2\n c a1\n0\n0\n1b + a1\n0\n0\n1b + a1\n0\n0\n1bd  \n(2.104)\n \n\u0003 3\n4 U2 a1\n0\n0\n1b. \n",
    "58 \nOperators and Measurement\nThus the S2 operator is proportional to the identity operator, which means it must commute with all \nthe other operators Sx, Sy, and Sz. It also means that all states are eigenstates of the S2 operator. Thus, \nwe can write \n \nS20 c9 = 3\n4 U20 c9 \n(2.105)\nfor any state 0 c9 in the spin-1/2 system.\nFor the case of spin 1/2, note that the expectation value of the operator S2 is\n \n8S29 = 3\n4 U2, \n(2.106)\nwhich would imply that the “length” of the spin vector is\n \n0 S0 = 48S29 = 23 U\n2. \n(2.107)\nThis is appreciably longer than the measured component of U>2, implying that the spin vector can \nnever be fully aligned along any axis. A useful mental model of the spin vector and its component is \nshown in Fig. 2.10. In this vector model, one can imagine the total spin vector S precessing around the \nz-axis at a constant angle to form a cone, with a constant spin component Sz. For a spin-1/2 system in \nthe “spin up” state 0  +9, this classical model yields the same expectation values and uncertainties as the \nquantum model (Problem 2.9)\n \n 8Sz9 = U\n2   \u0006Sz = 0  \n \n 8Sx9 = 0   \u0006Sx \u0002 0  \n \n 8Sy9 = 0   \u0006Sy \u0002 0. \n(2.108)\nS\nz\ny\nx\n(a)\nS\n(b)\nz\n\u0002\n2\n√3\u0002\n2\n\u0002\u0002\n2\n\u0002\n2\n√3\u0002\n2\n\u0002\u0002\n2\nFIGURE 2.10 (a) Vector model illustrating the classical precision of a spin vector and the allowed \nquantum mechanical components. (b) Two-dimensional version of the vector model with constant spin \nvector length and two possible components.\n",
    "2.7 Spin-1 System \n59\n?\n?\n?\n0\n1\n0\n1\nZ\nFIGURE 2.11 Spin-1 Stern-Gerlach experiment.\nHowever, a quantum mechanical experiment on a spin component eigenstate does not yield the time \ndependence of the precession implied by the picture in Fig. 2.10(a). Rather, the quantum mechanical \nspin vector is more accurately thought of as smeared out over the whole cone in a uniform random sense. \nThis randomness is often termed quantum fuzziness and will be evident in other systems we will study \nlater. To avoid the inaccurate precession part of the vector model, it is often illustrated as in Fig. 2.10(b).\n 2.7 \u0002 SPIN-1 SYSTEM\nThe Stern-Gerlach experiment depicted in Fig. 1.1 can be performed on a variety of atoms or par-\nticles. Such experiments always result in a ﬁnite number of discrete beams exiting the analyzer. For \nspin-1/2 particles, there are two output beams. For the case of three output beams, the deﬂections are \nconsistent with magnetic moments arising from spin angular momentum components of 1U, 0 U, and \n-1U. For an analyzer aligned along the z-axis, the three output states are labeled 0 19, 0 09, and 0  -19, \nas shown in Fig. 2.11. This is what we call a spin-1 system. (Note that the SPINS software and our \nStern-Gerlach schematics use arrows for the 0 19 and 0  -19 output beams, but these outputs are not the \nsame as the spin-1/2 states that are also denoted with arrows.)\nThe three eigenvalue equations for the spin component operator Sz of a spin-1 system are\n \n Sz0 19 = U0 19\n \n \n Sz0 09 = 0 U0 09\n \n \n Sz0  -19 = -U0  -19. \n(2.109)\nAs we did in the spin-1/2 case, we choose the Sz basis as the standard basis in which to express kets \nand operators using matrix representation. In Section 2.1, we found that eigenvectors are unit vectors \nin their own basis and an operator is always diagonal in its own basis. Using the ﬁrst rule, we can \nimmediately write down the eigenvectors of the Sz operator:\n \n0 19 \u0003 °\n1\n0\n0\n¢  0 09 \u0003 °\n0\n1\n0\n¢  0  -19 \u0003 °\n0\n0\n1\n¢ , \n(2.110)\nwhere we again use the convention that the ordering of the rows follows the eigenvalues in descending \norder. Using the second rule, we write down the Sz operator\n \nSz \u0003 °\n1U\n0\n0\n0\n0 U\n0\n0\n0\n-1U\n¢ = U °\n1\n0\n0\n0\n0\n0\n0\n0\n-1\n¢ \n(2.111)\nwith the eigenvalues 1U, 0 U, and -1U ordered along the diagonal. The value zero is a perfectly valid \neigenvalue in some systems.\n",
    "60 \nOperators and Measurement\nThe same four experiments performed on the spin-1/2 system can be performed on a spin-1 sys-\ntem. Conceptually the results are the same. One important difference occurs in Experiment 2, where a \nmeasurement of Sz is ﬁrst performed to prepare a particular state, and then a subsequent measurement \nof Sx (or Sy) is performed. Based upon the results of the spin-1/2 experiment, one might expect each of \nthe possible components to have one-third probability. Such is not the case. Rather, one set of results is\n \n P1x = 0 x810 19 0\n2 = 1\n4\n \n \n P0x = 0 x800 19 0\n2 = 1\n2\n \n \n P-1x = 0 x8-10 19 0\n2 = 1\n4, \n(2.112)\nas illustrated in Fig. 2.12. These experimental results can be used to determine the Sx eigenstates in \nterms of the Sz basis\n \n 0 19x = 1\n2@ 19 +\n1\n12@ 09 + 1\n2 @ -19\n \n \n 0 09x =\n1\n12@ 19 -\n1\n12@  -19\n \n \n 0  -19x = 1\n2@ 19 -\n1\n12@ 09 + 1\n2@  -19. \n(2.113)\nLikewise, we can ﬁnd the Sy eigenstates:\n \n 0 19y = 1\n2@ 19 + i 1\n12@ 09 - 1\n2@  -19\n \n \n 0 09y =\n1\n12@ 19 +\n1\n12@  -19\n \n \n 0  -19y = 1\n2@ 19 - i 1\n12@ 09 - 1\n2@  -19. \n(2.114)\nThe matrix representations of the Sx and Sy operators are\n \nSx \u0003\nU\n12\n °\n0\n1\n0\n1\n0\n1\n0\n1\n0\n¢  Sy \u0003\nU\n12\n °\n0\n-i\n0\ni\n0\n-i\n0\ni\n0\n¢ . \n(2.115)\n25\n25\nX\n0\n50\nZ\n0\n1\n1 x\n0 x\n1 x\nFIGURE 2.12 Experiment 2 in the spin-1 case.\n",
    "2.7 Spin-1 System \n61\n0\nP\nP−1\nP1\nP0\n1\nSz\nΨin\n(2 1\ni 0\ni\n1 )\n√6\n1\nFIGURE 2.13 Histogram of measurements of z-component of spin for spin-1 particle.\nExample 2.3 A spin-1 system is prepared in the state\n \n0 cin9 =\n2\n16 0 19 -\ni\n16 0 09 +\ni\n16 0  -19. \n(2.116)\nFind the probabilities of measuring each of the possible spin components along the z-axis.\nThe probability of measuring Sz = +1U is\n \n P1 = 0810 cin90\n2\n \n \n = @810C 2\n16 0 19 -\ni\n16 0 09 +\ni\n16 0  -19D@\n2\n \n \n = @ 2\n16 810 19 -\ni\n16810 09 +\ni\n16 810  -19@\n2\n \n \n = @ 2\n16\n @\n2\n= 2\n3.\n \n(2.117)\nThe probability of measuring Sz = 0 U is\n \n P0 = 0800 cin90\n2\n \n \n = @800C 2\n16 0 19 -\ni\n16 0 09 +\ni\n16 0  -19D@\n2\n \n \n = @ -i\n16\n @\n2\n= 1\n6.\n \n(2.118)\nThe probability of measuring Sz = -1U is\n \n P-1 = 08-10 cin90\n2\n \n \n = @8-10C 2\n16 0 19 -\ni\n16 0 09 +\ni\n16 0  -19D@\n2\n \n \n = @ i\n16\n @\n2\n= 1\n6.\n \n(2.119)\nThe three probabilities add to unity, as they must. A histogram of the predicted measurement results \nis shown in Fig. 2.13.\n",
    "62 \nOperators and Measurement\nTo generalize to other possible spin systems, we need to introduce new labels. We use the label \ns to denote the spin of the system, such as spin 1/2, spin 1, spin 3/2. The number of beams exiting a \nStern-Gerlach analyzer is 2s + 1. In each of these cases, a measurement of a spin component along \nany axis yields results ranging from a maximum value of s U to a minimum value of -s U, in unit steps \nof the value U. We denote the possible values of the spin component along the z-axis by the label m, \nthe integer or half-integer multiplying U. A quantum state with speciﬁc values of s and m is denoted as \n0 sm9, yielding the eigenvalue equations\n \n S20 sm9 = s(s + 1) U20 sm9 \n \n Sz0 sm9 = m U0 sm9.\n \n \n(2.120)\nThe label s is referred to as the spin angular momentum quantum number or the spin quantum \nnumber for short. The label m is referred to as the spin component quantum number or the mag-\nnetic quantum number because of its role in magnetic ﬁeld experiments like the Stern-Gerlach \nexperiment. The connection between this new 0 sm9 notation and the spin-1/2 0{9 notation is\n \n @ 1\n2 1\n29 = 0  +9  \n \n @ 1\n2, -1\n29 = 0  -9. \n(2.121)\nFor the spin-1 case, the connection to this new notation is\n \n 0 119 = 0 19\n \n \n 0 109 = 0 09\n \n \n 0 1, -19 = 0  -19. \n(2.122)\nWe will continue to use the 0{9 notation, but we will ﬁnd the new notation useful later (Chapter 7).\n 2.8 \u0002 GENERAL QUANTUM SYSTEMS\nLet’s extend the important results of this chapter to general quantum mechanical systems. For a gen-\neral observable A with quantized measurement results an, the eigenvalue equation is\n \nA0 an9 = an0 an9. \n(2.123)\nIn the basis formed by the eigenstates 0 an9, the operator A is represented by a matrix with the eigen-\nvalues along the diagonal\n \nA \u0003 §\na1\n0\n0\ng\n0\na2\n0\ng\n0\n0\na3\ng\nf\nf\nf\nf\n¥ , \n(2.124)\nwhose size depends on the dimensionality of the system. In this same basis, the eigenstates are repre-\nsented by the column vectors\n \n0 a19 \u0003 §\n1\n0\n0\nf\n¥ , 0 a29 \u0003 §\n0\n1\n0\nf\n¥ , 0 a39 \u0003 §\n0\n0\n1\nf\n¥ , ... . \n(2.125)\n",
    "Summary \n63\nThe projection operators corresponding to measurement of the eigenvalues an are \n \nPan = 0 an98an0 . \n(2.126)\nThe completeness of the basis states is expressed by saying that the sum of the projection operators is \nthe identity operator\n \na\nn\nPan = a\nn\n0 an98an0 = 1. \n(2.127)\nSUMMARY\nIn this chapter we have extended the mathematical description of quantum mechanics by using \noperators to represent physical observables. The only possible results of measurements are the \neigenvalues of operators. The eigenvectors of the operator are the basis states corresponding to each \npossible eigenvalue. We ﬁnd the eigenvalues and eigenvectors by diagonalizing the matrix representing \nthe operator, which allows us to predict the results of measurements. The eigenvalue equations for the \nspin-1/2 component operator Sz are\n \n Sz0  +9 = +  U\n2 0  +9 \n \n Sz0  -9 = -  U\n2 0  -9. \n(2.128)\nThe matrices representing the spin-1/2 operators are\n \n Sx \u0003 U\n2\n a0\n1\n1\n0b\n \n Sy \u0003 U\n2\n a0\n-i\ni\n0 b  \n \n Sz \u0003 U\n2\n a1\n0\n0\n-1b  \n S2 \u0003 3U2\n4\n a1\n0\n0\n1b. \n(2.129)\nWe characterized quantum mechanical measurements of an observable A by the expectation value\n \n8A9 = 8c0 A0 c9 = a\nn\nanPan \n(2.130)\nand the uncertainty\n \n\u0006A = 48A29 - 8A92. \n(2.131)\nWe made a connection between the commutator [A, B] = AB - BA of two operators and the \nability to measure the two observables. If two operators commute, then we can measure both observ-\nables simultaneously, but if they do not commute, then we cannot measure them simultaneously. \nWe quantiﬁed this disturbance that measurement inﬂicts on quantum systems through the quantum \nmechanical uncertainty principle\n \n\u0006A\u0006B Ú 1\n2@83A, B49@ . \n(2.132)\nWe also introduced the projection postulate, which states how the quantum state vector is changed \nafter a measurement.\n",
    "64 \nOperators and Measurement\nPROBLEMS\n 2.1 Given the following information:\n \n Sx0{9x = { U\n2 0{9x\n \n Sy0{9y = { U\n2 0{9y\n \n 0{9x =\n1\n12 30  +9 { 0  -94  \n 0{9y =\n1\n12 30  +9 { i0  -94\n \n ﬁnd the matrix representations of Sx and Sy in the Sz basis.\n 2.2 From the previous problem we know that the matrix representation of Sx in the Sz basis is\nSx \u0003 U\n2\n a0\n1\n1\n0b.\n \n Diagonalize this matrix to ﬁnd the eigenvalues and the eigenvectors of Sx.\n 2.3 Find the matrix representation of Sz in the Sx basis for spin 1/2. Diagonalize this matrix to ﬁnd \nthe eigenvalues and the eigenvectors in this basis. Show that the eigenvalue equations for Sz are \nsatisﬁed in this new representation.\n 2.4 Show by explicit matrix calculation that the matrix elements of a general operator A (within a \nspin-1/2 system) are as shown in Eq. (2.13).\n 2.5 Calculate the commutators of the spin-1/2 operators Sx, Sy, and Sz, thus verifying Eqs. (2.96).\n 2.6 Verify that the spin component operator Sn along the direction nn has the matrix representation \nshown in Eq. (2.41).\n 2.7 Diagonalize the spin component operator Sn along the direction nn to ﬁnd its eigenvalues and \nthe eigenvectors.\n 2.8 Find the probabilities of the measurements shown below in Fig. 2.14. The ﬁrst Stern-Gerlach \nanalyzer is aligned along the direction nn deﬁned by the angles u = p>4 and f = 5p>3.\n 2.9 For the state 0 +9, calculate the expectation values and uncertainties for measurements of Sx, Sy, \nand Sz in order to verify Eq. (2.108).\n 2.10 For the state 0 +9y, calculate the expectation values and uncertainties for measurements of Sx, \nSy, and Sz. Draw a diagram of the vector model applied to this state and reconcile your quan-\ntum mechanical calculations with the classical results.\n 2.11 Show that the S2 operator commutes with each of the spin component operators of Sx, Sy, and \nSz. Do this once with matrix notation for a spin-1/2 system and a second time using only the \ncomponent commutation relations in Eqs. (2.96) and the deﬁnition of S2 in Eq. (2.103).\nY\n?\n?\nP\u0002y\nP\u0003y\n^n\nFIGURE 2.14 Measurement of spin components (Prob. 2.8).\n",
    " 2.12 Diagonalize the Sx and Sy operators in the spin-1 case to ﬁnd the eigenvalues and the eigenvec-\ntors of both operators.\n 2.13 For a spin-1 system, show by explicit matrix calculation that the spin component operators \nobey the commutation relations in Eqs. (2.96).\n 2.14 Find the matrix representation of the S2 operator for a spin-1 system. Do this once by explicit \nmatrix calculation and a second time by inspection of the S2 eigenvalue equation (2.120).\n 2.15 A beam of spin-1 particles is prepared in the state\n0 c9 =\n2\n129 0 19 + i 3\n129 0 09 -\n4\n129 0  -19.\na) What are the possible results of a measurement of the spin component Sz, and with what \nprobabilities would they occur?\nb) What are the possible results of a measurement of the spin component Sx, and with what \nprobabilities would they occur?\nc) Plot histograms of the predicted measurement results from parts (a) and (b), and calculate \nthe expectation values for both measurements.\n 2.16 A beam of spin-1 particles is prepared in the state\n0 c9 =\n2\n129 0 19y + i 3\n129 0 09y -\n4\n129 0  -19y.\na) What are the possible results of a measurement of the spin component Sz, and with what \nprobabilities would they occur?\nb) What are the possible results of a measurement of the spin component Sy, and with what \nprobabilities would they occur?\nc) Plot histograms of the predicted measurement results from parts (a) and (b), and calculate \nthe expectation values for both measurements.\n 2.17 A spin-1 particle is in the state\n0 c9 \u0003\n1\n130\n °\n1\n2\n5i\n¢ .\na) What are the possible results of a measurement of the spin component Sz, and with what \nprobabilities would they occur? Calculate the expectation value of the spin component Sz.\nb) Calculate the expectation value of the spin component Sx. Suggestion: Use matrix mechan-\nics to evaluate the expectation value.\n 2.18 A spin-1 particle is prepared in the state\n0 c9 =\n1\n114 0 19 -\n3\n114 0 09 + i 2\n114 0  -19.\na) What are the possible results of a measurement of the spin component Sz, and with what \nprobabilities would they occur?\nb) Suppose that the Sz measurement on the particle yields the result Sz = -U. Subsequent to \nthat result a second measurement is performed to measure the spin component Sx. What are \nthe possible results of that measurement, and with what probabilities would they occur?\nc) Draw a schematic diagram depicting the successive measurements in parts (a) and (b).\nProblems \n65\n",
    "66 \nOperators and Measurement\n 2.19 A spin-1 particle is prepared in the state\n0 ci9 = 4\n1\n6 0 19 - 4\n2\n6 0 09 + i 4\n3\n6 0  -19.\n \n Find the probability that the system is measured to be in the ﬁnal state\n0 cf9 = 1+i\n17 0 19y +\n2\n17 0 09y - i 1\n17 0  -19y.\n 2.20 In part (2) of SPINS Lab #3, you measured the spin components of the unknown (spin 1) ini-\ntial states 0 ci9 (i \u0003 1, 2, 3, 4) along the three axes. Using your measured values, deduce the \nunknown initial states.\n 2.21 In part (3) of SPINS Lab #3, you built a spin-1 interferometer and measured the relative prob-\nabilities after the ﬁnal Stern-Gerlach analyzer for the seven possible cases where one beam, \na pair of beams, or all three beams from the second Stern-Gerlach analyzer were used. Show \nhow you used the projection postulate to calculate the theoretical probabilities.\n 2.22 A beam of spin-1/2 particles is sent through a series of three Stern-Gerlach analyzers, as shown \nin Fig. 2.15. The second Stern-Gerlach analyzer is aligned along the nn direction, which makes \nan angle \u0007 in the x-z plane with respect to the z-axis.\na) Find the probability that particles transmitted through the ﬁrst Stern-Gerlach analyzer are \nmeasured to have spin down at the third Stern-Gerlach analyzer?\nb) How must the angle \u0007 of the second Stern-Gerlach analyzer be oriented so as to maximize \nthe probability that particles are measured to have spin down at the third Stern-Gerlach \nanalyzer? What is this maximum fraction?\nc) What is the probability that particles have spin down at the third Stern-Gerlach analyzer if \nthe second Stern-Gerlach analyzer is removed from the experiment?\n 2.23 Consider a three-dimensional ket space. In the basis deﬁned by three orthogonal kets 0 19, 0 29, \nand 0 39, the operators A and B are represented by\n \nA \u0003 °\na1\n0\n0\n0\na2\n0\n0\n0\na3\n ¢  B \u0003 °\nb1\n0\n0\n0\n0\nb2\n0\nb2\n0\n¢ ,\n \n where all the quantities are real.\na) Do the operators A and B commute?\nb) Find the eigenvalues and normalized eigenvectors of both operators.\nZ\n?\n?\n^n\nZ\nFIGURE 2.15 Measurement of spin components (Prob. 2.22).\n",
    "Resources \n67\nc) Assume the system is initially in the state 0 29. Then the observable corresponding to the oper-\nator B is measured. What are the possible results of this measurement and the probabilities of \neach result? After this measurement, the observable corresponding to the operator A is mea-\nsured. What are the possible results of this measurement and the probabilities of each result?\nd) How are questions (a) and (c) above related?\n 2.24 If a beam of spin-3/2 particles is input to a Stern-Gerlach analyzer, there are four output beams \nwhose deﬂections are consistent with magnetic moments arising from spin angular momentum \ncomponents of 3\n2 U, 1\n2 U, -  1\n2 U, and -  3\n2 U. For a spin-3/2 system:\na) Write down the eigenvalue equations for the Sz operator.\nb) Write down the matrix representation of the Sz eigenstates.\nc) Write down the matrix representation of the Sz operator.\nd) Write down the eigenvalue equations for the S2 operator.\ne) Write down the matrix representation of the S2 operator.\n 2.25 Are the projection operators P+ and P- Hermitian? Explain.\nRESOURCES\nActivities\nThis activity is available at\nwww.physics.oregonstate.edu/qmactivities\nSpins Lab 3: Stern-Gerlach measurements of a spin-1 system.\n",
    "C H A P T E R \n3\nSchrödinger Time Evolution\nThis chapter marks our ﬁnal step in developing the mathematical basis of a quantum theory. In \n Chapter 1, we learned how to use kets to describe quantum states and how to predict the probabili-\nties of results of measurements. In Chapter 2, we learned how to use operators to represent physical \nobservables and how to determine the possible measurement results. The key missing aspect is the \nability to predict the future. Physics theories are judged on their predictive power. Classical mechan-\nics relies on Newton’s second law F = ma to predict the future of a particle’s motion. The ability to \npredict the quantum future started with Erwin Schrödinger and bears his name.\n3.1 \u0002 SCHRÖDINGER EQUATION\nThe sixth postulate of quantum mechanics says that the time evolution of a quantum system is \n governed by the differential equation\n \niU d\ndt\n 0  c 1t29 = H 1t2 0  c 1t29, \n(3.1)\nwhere the operator H corresponds to the total energy of the system and is called the Hamiltonian \noperator of the system because it is derived from the classical Hamiltonian. This equation is known as \nthe Schrödinger equation.\nPostulate 6\nThe time evolution of a quantum system is determined by the  Hamiltonian \nor total energy operator H1t2 through the Schrödinger equation\niU d\ndt\n 0  c 1t29 = H 1t2 0  c 1t29.\nThe Hamiltonian is a new operator, but we can use the same ideas we developed in Chapter 2 to \nunderstand its basic properties. The Hamiltonian H is an observable, so it is a Hermitian operator. The \neigenvalues of the Hamiltonian are the allowed energies of the quantum system, and the eigenstates \nof H are the energy eigenstates of the system. If we label the allowed energies as En, then the energy \neigenvalue equation is\n \nH 0 En9 = En0 En9 . \n(3.2)\n",
    "3.1 Schrödinger Equation \n69\nIf we have the Hamiltonian H in a matrix representation, then we diagonalize the matrix to ﬁnd the \neigenvalues En and the eigenvectors 0 En9 just as we did with the spin operators in Chapter 2. For the \nmoment, let’s assume that we have already diagonalized the Hamiltonian [i.e., solved Eq. (3.2)] so that \nwe know the eigenvalues En and the eigenvectors 0 En9, and let’s see what we can learn about quantum \ntime evolution in general by solving the Schrödinger equation.\nThe eigenvectors of the Hamiltonian form a complete basis because the Hamiltonian is an observ-\nable, and therefore a Hermitian operator. Because H is the only operator appearing in the Schrödinger \nequation, it would seem reasonable (and will prove invaluable) to consider the energy eigenstates as \nthe basis of choice for expanding general state vectors:\n \n0 c 1t29 = a\nn\ncn1t2 0 En9. \n(3.3)\nThe basis of eigenvectors of the Hamiltonian is also orthonormal, so\n \n8Ek\u0004 En9 = dkn. \n(3.4)\nWe refer to this basis as the energy basis.\nFor now, we assume that the Hamiltonian is time independent (we will do the time-dependent case \nH(t) in Section 3.4). The eigenvectors of a time-independent Hamiltonian come from the diagonaliza-\ntion procedure we used in Chapter 2, so there is no reason to expect the eigenvectors themselves to \ncarry any time dependence. Thus if a general state 0 c9 is to be time dependent, as the Schrödinger equa-\ntion implies, then the time dependence must reside in the expansion coefﬁcients cn1t2, as expressed in \nEq. (3.3). Substitute this general state into the Schrödinger equation (3.1)\n \niU d\ndt a\nn\ncn1t2 0 En9 = Ha\nn\ncn1t2 0 En9 \n(3.5)\nand use the energy eigenvalue equation (3.2) to obtain\n \niUa\nn\ndcn1t2\ndt\n0 En9 = a\nn\ncn1t2En0 En9. \n(3.6)\nEach side of this equation is a sum over all the energy states of the system. To simplify this equation, \nwe isolate single terms in these two sums by taking the inner product of the ket on each side with one \nparticular ket 0 Ek9 (this ket can have any label k, but must not have the label n that is already used in \nthe summation). The orthonormality condition 8Ek\u0004En9 = dkn then collapses the sums:\n \n 8Ek0 iUa\nn\ndcn1t2\ndt\n0 En9 = 8Ek0 a\nn\ncn1t2En0 En9 \n \n iUa\nn\ndcn1t2\ndt\n 8Ek0 En9 = a\nn\ncn1t2En8Ek0 En9  \n \n iUa\nn\ndcn1t2\ndt\n  dkn = a\nn\ncn1t2Endkn\n \n \n iU \ndck1t2\ndt\n= ck1t2Ek.\n \n(3.7)\nWe are left with a single differential equation for each of the possible energy states of the systems \nk = 1, 2, 3, ... . This ﬁrst-order differential equation can be rewritten as\n \ndck1t2\ndt\n= -i Ek\nU\n ck1t2. \n(3.8)\n",
    "70 \nSchrödinger Time Evolution\nThe solution to Eq. (3.8) is a complex exponential\n \nck1t2 = ck102e-iEkt>U. \n(3.9)\nIn Eq. (3.9), we have denoted the initial condition as ck102, but we denote it simply as ck hereafter. \nEach coefﬁcient in the energy basis expansion of the state obeys the same form of the time dependence \nin Eq. (3.9), but with a different exponent due to the different energies. The time-dependent solution \nfor the full state vector is summarized by saying that if the initial state of the system at time t \u0003 0 is\n \n0 c1029 = a\nn\ncn0 En9, \n(3.10)\nthen the time evolution of this state under the action of the time-independent Hamiltonian H is\n \n0 c1t29 = a\nn\ncne-iEnt>U0 En9  . \n(3.11)\nSo the time dependence of the original state vector is found by multiplying each energy eigenstate \ncoefﬁcient by its own phase factor e-iEnt>U that depends on the energy of that eigenstate. Note that the \nfactor E>U is an angular frequency, so that the time dependence is of the form e-ivt, a form commonly \nfound in many areas of physics. It is important to remember that one must use the energy eigenstates for \nthe expansion in Eq. (3.10) in order to use the simple phase factor multiplication in Eq. (3.11) to account \nfor the Schrödinger time evolution of the state. This key role of the energy basis accounts for the impor-\ntance of the Hamiltonian operator and for the common practice of ﬁnding the energy eigenstates to use \nas the preferred basis.\nA few examples help to illustrate some of the important consequences of this time evolution of \nthe quantum mechanical state vector. First, consider the simplest possible situation where the system \nis initially in one particular energy eigenstate:\n \n0 c1029 = 0 E19, \n(3.12)\nfor example. The prescription for time evolution tells us that after some time t the system is in the state\n \n0 c1t29 = e-iE1t>U0 E19. \n(3.13)\nBut this state differs from the original state only by an overall phase factor, which we have said before \ndoes not affect any measurements (Problem 1.3). For example, if we measure an observable A, then \nthe probability of measuring an eigenvalue aj is given by\n \n Paj = 08aj0 c1t290\n2\n \n \n = 08aj0 e-iE1t>U0 E190\n2 \n \n = 08aj0 E190\n2.\n \n(3.14)\nThis probability is time independent and is equal to the probability at the initial time. Thus, we \n conclude that there is no measureable time evolution for this state. Hence, the energy eigenstates are \ncalled stationary states. If a system begins in an energy eigenstate, then it remains in that state.\nNow consider an initial state that is a superposition of two energy eigenstates:\n \n0 c1029 = c10 E19 + c20 E29. \n(3.15)\nIn this case, time evolution takes the initial state to the later state\n \n0 c1t29 = c1e-iE1t>U0 E19 + c2e-iE2t>U0 E29. \n(3.16)\n",
    "3.1 Schrödinger Equation \n71\nA measurement of the system energy at the time t would yield the value E1 with a probability\n \n PE1 = 08E10 c1t290\n2\n \n \n = 08E103c1e-iE1t>U0 E19 + c2e-iE2t>U0 E294 0\n2 \n \n = 0 c10\n2,\n \n(3.17)\nwhich is independent of time. The same is true for the probability of measuring the energy E2. Thus, \nthe probabilities of measuring the energies are stationary, as they were in the ﬁrst example.\nHowever, now consider what happens if another observable is measured on this system in this \nsuperposition state. There are two distinct situations: (1) If the other observable A commutes with the \nHamiltonian H, then A and H have common eigenstates. In this case, measuring A is equivalent to mea-\nsuring H because the inner products used to calculate the probabilities use the same eigenstates. Hence, \nthe probability of measuring any particular eigenvalue of A is time independent, as in Eq. (3.17). (2) If \nA and H do not commute, then they do not share common eigenstates. In this case, the eigenstates of A \nin general consist of superpositions of energy eigenstates. For example, suppose that the eigenstate of \nA corresponding to the eigenvalue a1 were\n \n0 a19 = a10 E19 + a20 E29. \n(3.18)\nThen the probability of measuring the eigenvalue a1 would be\n \n Pa1 = 08a10 c1t290\n2\n \n \n = 03a*\n18E10 + a*\n28E2043c1e-iE1t>U0 E19 + c2e-iE2t>U0 E294 0\n2 \n \n = @ a*\n1c1e-iE1t>U + a*\n2c2e-iE2t>U@\n2.\n \n(3.19)\nFactoring out the common phase gives\n \n Pa1 = @ e-iE1t>U@\n2\n @ a*\n1c1 + a*\n2c2e-i1E2-E12t>U@\n2\n \n \n = 0 a10\n20 c10\n2 + 0 a20\n20 c20\n2 + 2Re1a1c*\n1a*\n2c2e-i1E2-E12t>U2. \n(3.20)\nThe different time-evolution phases of the two components of 0 c1t29 lead to a time dependence in the \nprobability. The overall phase in Eq. (3.20) drops out, and only the relative phase remains in the prob-\nability calculation. Hence, the time dependence is determined by the difference of the energies of the \ntwo states involved in the superposition. The corresponding angular frequency of the time evolution\n \nv 21 = E2 - E1\nU\n \n(3.21)\nis called the Bohr frequency.\nTo summarize, we list below a recipe for solving a standard time-dependent quantum mechanics \nproblem with a time-independent Hamiltonian.\nGiven a Hamiltonian H and an initial state 0 c1029, what is the probability that \nthe eigenvalue aj of the observable A is measured at time t?\n 1. Diagonalize H (ﬁnd the eigenvalues En and eigenvectors 0 En92.\n 2. Write 0 c1029 in terms of the energy eigenstates 0 En9.\n 3. Multiply each eigenstate coefﬁcient by e-iEnt>U to get 0 c1t29.\n 4. Calculate the probability Paj = 08aj0 c1t290\n2.\n",
    "72 \nSchrödinger Time Evolution\n3.2 \u0002 SPIN PRECESSION\nNow apply this new concept of Schrödinger time evolution to the case of a spin-1/2 system. The Ham-\niltonian operator represents the total energy of the system, but because only energy differences are \nimportant in time-dependent solutions (and because we can deﬁne the zero of potential energy as \nwe wish), we need consider only energy terms that differentiate between the two possible spin states \nin the system. Our experience with the Stern-Gerlach apparatus tells us that the magnetic potential \nenergy of the magnetic dipole differs for the two possible spin-component states. So to begin, we \nconsider the potential energy of a single magnetic dipole (e.g., in a silver atom) in a uniform magnetic \nﬁeld as the sole term in the Hamiltonian. Recalling that the magnetic dipole is given by\n \nM = g q\n2me\n S, \n(3.22)\nthe Hamiltonian is\n \n H = -M~B\n \n \n = -g q\n2me\n S~B \n \n = e\nme\n S~B,\n \n(3.23)\nwhere q = -e and g = 2 have been used in the last line. The gyromagnetic ratio, g, is slightly differ-\nent from 2, but we ignore that detail.\n3.2.1 \u0002 Magnetic Field in the z-Direction\nFor our ﬁrst example, we assume that the magnetic ﬁeld is uniform and directed along the z-axis. \n Writing the magnetic ﬁeld as\n \nB = B0\n zn \n(3.24)\nallows the Hamiltonian to be simpliﬁed to\n \n H = eB0\nme\n Sz \n \n = v0 Sz,  \n(3.25)\nwhere we have introduced the deﬁnition\n \nv0 K eB0\nme\n. \n(3.26)\nThis deﬁnition of an angular frequency simpliﬁes the notation now and will have an obvious \n interpretation at the end of the problem.\nThe Hamiltonian in Eq. (3.25) is proportional to the Sz operator, so H and Sz commute and \n therefore share common eigenstates. This is clear if we write the Hamiltonian as a matrix in the \nSz  representation:\n \nH \u0003 U v0\n2\n a1\n0\n0\n-1b. \n(3.27)\n",
    "3.2 Spin Precession \n73\nBecause H is diagonal, we have already completed step 1 of the Schrödinger time-evolution recipe. \nThe eigenstates of H are the basis states of the representation, while the eigenvalues are the diagonal \nelements of the matrix in Eq. (3.27). The eigenvalue equations for the Hamiltonian are thus\n \n H 0  +9 = v0Sz0  +9 = U v0\n2\n 0  +9 = E+ 0  +9\n \n \n H 0  -9 = v0Sz0  -9 = -  U v0\n2\n 0  +9 = E - 0  -9, \n(3.28)\nwith eigenvalues and eigenvectors given by\n \n E + = U v 0\n2\n \n E - = -  U v 0\n2  \n \n 0\n E +9 = 0  +9\n \n 0\n E -9 = 0  -9.  \n(3.29)\nThe information regarding the energy eigenvalues and eigenvectors is commonly presented in a \ngraphical diagram, which is shown in Fig. 3.1 for this case. The two energy states are separated \nby the energy E+ - E- = U v0, so the angular frequency v0 characterizes the energy scale of this \nsystem. The spin-up state 0  +9 has a higher energy because the magnetic moment is aligned against \nthe ﬁeld in that state; the negative charge in Eq. (3.22) causes the spin and magnetic moment to be \nantiparallel.\nNow we look at a few examples to illustrate the key features of the behavior of a spin-1/2 system \nin a uniform magnetic ﬁeld. First, consider the case where the initial state is spin up along the z-axis:\n \n0\n c1029 = 0  +9. \n(3.30)\nThis initial state is already expressed in the energy basis (step 2 of the Schrödinger recipe), so the \nSchrödinger equation time evolution takes this initial state to the state\n \n 0\n c1t29 = e-iE +\n t>U0  +9 \n \n = e-iv 0\n t>20  +9 \n(3.31)\n\u00030.5\n\u00030.25\n0.0\n0.25\n0.5\nE/\u0002Ω0\n\u0002\u0002\u0003\n\u0002Ω0\nE\u0002\u0005 \u0002Ω0\n2\n\u0003\nE\u0003\u0005\n\u0002Ω0\n2\n\u0002\u0003\u0003\nFIGURE 3.1 Energy level diagram of a spin-1/2 particle in a uniform magnetic ﬁeld.\n",
    "74 \nSchrödinger Time Evolution\naccording to step 3 of the Schrödinger recipe. As we saw before [(Eq. (3.13)], because the initial state is \nan energy eigenstate, the time-evolved state acquires an overall phase factor, which does not represent \na physical change of the state. The probability for measuring the spin to be up along the z-axis is (step 4 \nof the Schrödinger recipe)\n \n P+ = 08+ 0 c1t290\n2\n \n \n = @8+ 0 e-iv0t>20  +9@\n2 \n \n(3.32)\n \n = 1.\n \n \nAs expected, this probability is not time dependent, and we therefore refer to 0  +9 as a stationary state \nfor this system. A schematic diagram of this experiment is shown in Fig. 3.2, where we have intro-\nduced a new element to represent the applied ﬁeld. This new depiction is the same as the depictions in \nthe SPINS software, where the number in the applied magnetic ﬁeld box (42 in Fig. 3.2) is a measure \nof the magnetic ﬁeld strength. In this experiment, the results shown are independent of the applied \nﬁeld strength, as indicated by Eq. (3.32), and as you can verify with the software.\nNext, consider the most general initial state, which we saw in Chapter 2 corresponds to spin \nup along an arbitrary direction deﬁned by the polar angle u and the azimuthal angle f. The initial \nstate is\n \n0 c1029 = 0  +9n = cos u\n2 0  +9 + sin u\n2\n eif0  -9, \n(3.33)\nor using matrix notation:\n \n0 c1029 \u0003 a cos1u>22\neif sin1u>22b. \n(3.34)\nSchrödinger time evolution introduces a time-dependent phase term for each component, giving\n \n 0 c1t29 \u0003 a e-iE+t>U cos1u>22\ne-iE-t>Ueif sin1u>22b\n \n \n \u0003 a e-iv0t>2 cos1u>22\neiv0t>2eif sin1u>22b\n \n \n(3.35)\n \n \u0003 e-iv0t>2 a\ncos1u>22\nei1f+v0t2 sin1u>22b. \nZ\nZ\n100\n0\n42\nZ\nFIGURE 3.2 Schematic diagram of a Stern-Gerlach measurement with an applied uniform magnetic ﬁeld \nrepresented by the box in the middle, with the number 42 representing the strength of the magnetic ﬁeld.\n",
    "3.2 Spin Precession \n75\nNote again that an overall phase does not have a measurable effect, so the evolved state is a spin up \neigenstate along a direction that has the same polar angle u as the initial state and a new azimuthal \nangle f + v0t. The state appears to have simply rotated around the z-axis, the axis of the magnetic \nﬁeld, by the angle v0t. Of course, we have to limit our discussion to results of measurements, so let’s \nﬁrst calculate the probability for measuring the spin component along the z-axis:\n \n P+ = 08+  0 c1t290\n2\n \n \n = 2 11 02e-iv0t>2 a\ncos1u>22\nei1f+v0t2 sin1u>22b 2\n2\n \n \n = 0 e-iv0t>2 cos1u>22 0\n2\n \n \n = cos21u>22.\n \n \n(3.36)\nThis probability is time independent because the Sz eigenstates are also energy eigenstates for this \nproblem (i.e., H and Sz commute). The probability in Eq. (3.36) is consistent with the interpretation \nthat the angle u that the spin vector makes with the z-axis does not change.\nThe probability for measuring spin up along the x-axis is\n \n P+x = 0 x8+  0 c1t290\n2\n \n \n = 2 1\n12 11  12e-iv0t>2a\ncos1u>22\nei(f +v0t) sin1u>22b 2\n2\n \n \n = 1\n2 @ cos1u>22 + ei1f+v0t2 sin1u>22 @\n2\n \n(3.37)\n \n = 1\n2 3cos21u>22 + cos1u>22sin1u>221ei1f+v0t2 + e-i1f +v0t22 + sin21u>224 \n \n = 1\n2 31 + sin u cos1f + v0t24.\n \nThis probability is time dependent because the Sx eigenstates are not stationary states (i.e., H and Sx \ndo not commute). The time dependence in Eq. (3.37) is consistent with the spin precessing around \nthe z-axis.\nTo illustrate this spin precession further, it is useful to calculate the expectation values for each of \nthe spin components. For Sz, we have\n \n 8Sz9 = 8c1t2 0 Sz0 c1t29\n \n \n = eiv0t>2 a\n cos a u\n2b  e-i1f +v0t2 sin a u\n2b b  U\n2\n a1\n0\n0\n-1b e-iv0t>2 a\ncos1u>22\nei1f+v0t2 sin1u>22b \n \n = U\n2\n 3cos21u>22 - sin21u>224\n \n \n = U\n2\n cos u,\n \n(3.38)\n",
    "76 \nSchrödinger Time Evolution\nwhile the other components are\n \n 8Sy9 = 8c1t2 0 Sy0 c1t29\n \n \n = eiv0t>2 a\n cos a u\n2b  e-i1f+v0t2 sin a u\n2b b U\n2\n a0\n-i\ni\n0 be-iv0t>2 a\ncos1u>22\nei1f +v0t2 sin1u>22b (3.39)\n \n = U\n2\n sin u sin1f + v0t2 \nand\n \n 8Sx9 = 8c1t2 0 Sx0 c1t29\n \n \n = U\n2\n sin u cos1f + v0t2. \n \n(3.40)\nThe expectation value of the total spin vector 8S9 is shown in Fig. 3.3, where it is seen to precess \naround the magnetic ﬁeld direction with an angular frequency v0. The precession of the spin vector is \nknown as Larmor precession and the frequency of precession is known as the Larmor frequency.\nThe quantum mechanical Larmor precession is analogous to the classical behavior of a magnetic \nmoment in a uniform magnetic ﬁeld. A classical magnetic moment M experiences a torque M * B \nwhen placed in a magnetic ﬁeld. If the magnetic moment is associated with an angular momentum L, \nthen we can write\n \nM =\nq\n2m\n L, \n(3.41)\nwhere q and m are the charge and mass, respectively, of the system. The equation of motion for the \nangular momentum\n \nd L\ndt = M * B \n(3.42)\nthen results in\n \ndM\ndt =\nq\n2m\n M * B. \n(3.43)\ny\nx\nz\n\u0004S(t)\u0003\n\u0004S(0)\u0003\nB\nΩ0t\nFIGURE 3.3 The expectation value of the spin vector precesses in a uniform magnetic ﬁeld.\n",
    "3.2 Spin Precession \n77\nBecause the torque M * B is perpendicular to the angular momentum L = 2mM>q, it causes the \nmagnetic moment to precess about the ﬁeld with the classical Larmor frequency vcl = qB>2m.\nIn the quantum mechanical example we are considering, the charge q is negative (meaning the \nspin and magnetic moment are antiparallel), so the precession is counterclockwise around the ﬁeld. A \npositive charge would result in clockwise precession. This precession of the spin vector makes it clear \nthat the system has angular momentum, as opposed to simply having a magnetic dipole moment. The \nequivalence of the classical Larmor precession and the expectation value of the quantum mechanical \nspin vector is one example of Ehrenfest’s theorem, which states that quantum mechanical expecta-\ntion values obey classical laws.\nPrecession experiments like the one discussed here are of great practical value. For example, if \nwe measure the magnetic ﬁeld strength and the precession frequency, then the gyromagnetic ratio can \nbe determined. This spin precession problem is also of considerable theoretical utility because it is \nmathematically equivalent to many other quantum systems that can be modeled as two-state systems. \nThis utility is broader than you might guess at ﬁrst glance, because many multistate quantum systems \ncan be reduced to two-state systems if the experiment is designed to interact only with two of the many \nlevels of the system.\nExample 3.1 A spin-1/2 particle with a magnetic moment is prepared in the state 0  -9x and is \nsubject to a uniform applied magnetic ﬁeld B = B0\n zn. Find the probability of measuring spin up in \nthe x-direction after a time t. This experiment is depicted in Fig. 3.4.\nWe solve this problem using the four steps of the Schrödinger time-evolution recipe from \nSection 3.1. The initial state is\n \n0 c1029 = 0  -9x. \n(3.44)\nThe applied magnetic ﬁeld is in the z-direction, so the Hamiltonian is H = v0Sz and the energy \neigenstates are 0{9 with energies E{ = {U v0>2 (step 1). The Larmor precession frequency is \nv0 = eB0>me. We must express the initial state in the energy basis (step 2):\n \n0 c1029 = 0  -9x =\n1\n12 0  +9 -\n1\n12 0  -9. \n(3.45)\nThe time-evolved state is obtained by multiplying each energy eigenstate coefﬁcient by the appro-\npriate phase factor (step 3):\n \n 0 c1t29 =\n1\n12 e-iE +t>U0  +9 -\n1\n12 e-iE -t>U0  -9 \n(3.46)\n \n =\n1\n12 e-iv0t>20  +9 -\n1\n12 e+iv0t>2 0 -9.\nX\n?\n?\n42\nZ\nX\nFIGURE 3.4 Spin precession experiment.\n",
    "78 \nSchrödinger Time Evolution\nThe measurement probability is found by projecting 0 c1t29 onto the measured state and complex \nsquaring (step 4):\n \n P+x = 0 x8+ 0\n c1t29 0\n2\n \n \n = @x8+ 0A 1\n12 e-iv0t>20  +9 -\n1\n12 e+iv0t>20  -9B @\n2\n \n \n = @ A 1\n128+ 0 +\n1\n128- 0 BA 1\n12 e-iv0t>20  +9 -\n1\n12 e+iv0t>20  -9B @\n2\n \n(3.47)\n \n = 1\n4 @e-iv0t>2 - e+iv0t>2@\n2\n \n \n = sin2 1v0t>22.\n \nThe probability that the system has spin up in the x-direction oscillates between zero and unity \nas time evolves, as shown in Fig. 3.5(a), which is consistent with the model of the spin vector \nprecessing around the applied ﬁeld, as shown in Fig. 3.5(b).\n3.2.2 \u0002 Magnetic Field in a General Direction\nFor our second example, consider a more general direction for the magnetic ﬁeld by adding a magnetic \nﬁeld component along the x-axis to the already existing ﬁeld along the z-axis. The simplest approach \nto solving this new problem would be to redeﬁne the coordinate system so the z-axis pointed along the \ndirection of the new total magnetic ﬁeld. Then the solution would be the same as was obtained above, \nwith a new value for the magnitude of the magnetic ﬁeld being the only change. This approach would \nbe considered astute in many circumstances, but we will not take it because we want to get practice \nsolving this new type of problem and because we want to address some issues that are best posed in the \noriginal coordinate system. Thus, we deﬁne a new magnetic ﬁeld as\n \nB = B0zn + B1xn . \n(3.48)\nt\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP\u0002x\ny\nx\nz\n(a)\n(b)\nΩ0t\nB\n\u0004S(0)\u0003\n\u0004S(t)\u0003\n2Π\nΩ0\n4Π\nΩ0\n6Π\nΩ0\nFIGURE 3.5 (a) Probability of a spin component measurement and (b) the corresponding \nprecession of the expectation value of the spin.\n",
    "3.2 Spin Precession \n79\nThis ﬁeld is oriented in the xz-plane at an angle u with respect to the z-axis, as shown in Fig. 3.6. In \nlight of the solution above, it is useful to deﬁne Larmor frequencies associated with each of the ﬁeld \ncomponents:\n \nv0 K eB0\nme\n, \nv1 K eB1\nme\n. \n(3.49)\nUsing these deﬁnitions, the Hamiltonian becomes\n \n H = - M~B\n \n \n = v0 Sz + v1Sx, \n(3.50)\nor in matrix representation\n \nH \u0003 U\n2\n av0\nv1\nv1\n-v0\nb. \n(3.51)\nThis Hamiltonian is not diagonal, so its eigenstates are not the same as the eigenstates of Sz. Rather we \nmust use the diagonalization procedure to ﬁnd the new eigenvalues and eigenvectors. The characteristic \nequation determining the energy eigenvalues is\n \n ∞ \nU\n2\n  v0 - l\nU\n2\n  v1\nU\n2\n  v1\n-  U\n2\n  v0 - l\n∞= 0  \n \n - a U\n2\n v0b\n2\n+ l2 - a U\n2\n v1b\n2\n= 0, \n \n(3.52)\nwith solutions\n \nl = { U\n2\n 4v2\n0 + v2\n1. \n(3.53)\nNote that the energy eigenvalues are {1U v0>22 when v1 = 0, which they must be given our previ-\nous solution. Rather than solve directly for the eigenvectors, let’s make them obvious by rewriting the \nHamiltonian. From Fig. 3.6 it is clear that the angle is determined by the equation\n \ntan u = B1\nB0\n= v1\nv0\n. \n(3.54)\nB0\nB1\nB\nθ\nFIGURE 3.6 A uniform magnetic ﬁeld in a general direction.\n",
    "80 \nSchrödinger Time Evolution\nUsing this, the Hamiltonian can be written as\n \nH \u0003 U\n2\n 4v2\n0 + v2\n1 acos u\nsin u\nsin u\n-cos ub. \n(3.55)\nIf we let nn be the unit vector in the direction of the total magnetic ﬁeld, then the Hamiltonian is propor-\ntional to the spin component Sn along the direction nn:\n \nH = 4v2\n0 + v2\n1 Sn. \n(3.56)\nThis is what we expected at the beginning: that the problem could be solved by using the ﬁeld direc-\ntion to deﬁne a coordinate system. Thus, the eigenvalues are as we found in Section 2.2.1 and the \neigenstates are the spin up and down states along the direction nn, which are\n \n0  +9n = cos u\n2\n 0  +9 + sin u\n2\n 0  -9 \n \n0  -9n = sin u\n2\n 0  +9 - cos u\n2\n 0  -9 \n \n(3.57)\nfor this case, because the azimuthal angle f is zero. These are the same states you would ﬁnd by \ndirectly solving for the eigenstates of the Hamiltonian. Because we have already done that for the Sn \ncase, we do not repeat it here.\nNow consider performing the following experiment: begin with the system in the spin-up state \nalong the z-axis, and measure the spin component along the z-axis after the system has evolved in \nthis magnetic ﬁeld for some time, as depicted in Fig. 3.7. Let’s speciﬁcally calculate the probabil-\nity that the initial 0  +9 is later found to have evolved to the 0  -9 state. This is commonly known as a \nspin ﬂip. According to our time-evolution prescription, we must ﬁrst write the initial state in terms \nof the energy eigenstates of the system. In the previous examples, this was trivial because the energy \neigenstates were the 0{9 states that we used to express all general states. But now this new problem is \nmore involved, so we proceed more slowly. The initial state\n \n0\n c1029 = 0  +9 \n(3.58)\nmust be written in the 0{9n basis. Because the 0{9n basis is complete, we can use the completeness \nrelation [Eq. (2.55)] to decompose the initial state\n \n 0\n c1029 = 10  +9n n8+ 0 + 0  -9n n8- 02 0  +9 \n \n = 0  +9n n8+ 0  +9 + 0  -9n n8- 0  +9 \n \n = n8+ 0  +9 0  +9n + n8- 0  +9 0  -9n  \n \n = cos u\n2\n 0  +9n + sin u\n2\n 0  -9n.\n \n \n(3.59)\nZ\n?\n?\n42\n^n\nZ\nFIGURE 3.7 A spin precession experiment with a uniform magnetic ﬁeld aligned in a general direction nn.\n",
    "3.2 Spin Precession \n81\nNow that the initial state is expressed in the energy basis, the time-evolved state is obtained by multi-\nplying each coefﬁcient by a phase factor dependent on the energy of that eigenstate:\n \n0\n c1t29 = e-iE+t>U cos u\n2\n 0  +9n + e-iE-t>U sin u\n2\n 0  -9n. \n(3.60)\nWe leave it in this form and substitute the energy eigenvalues\n \nE{ = { U\n2 4v2\n0 + v2\n1 \n(3.61)\nat the end of the example.\nThe probability of a spin ﬂip is\n \nP+ S  - = 08-  0\n c1t290\n2 \n \n= 2 8- 0 Je-iE+t>U cos u\n2 0  +9n + e-iE-t>U sin u\n2 0  -9n R 2\n2\n \n \n= 2 e-iE+t>U cos u\n2\n 8- 0  +9n + e-iE-t>U sin u\n2\n 8- 0  -9n 2\n2\n \n \n= 2 e-iE+t>U cos u\n2 sin u\n2\n + e-iE-t>U sin u\n2\n a-cos u\n2b 2\n2\n \n(3.62)\n \n= cos2 u\n2 sin2\n  u\n2\n @ 1 - ei1E+-E-2t>U@\n2 \n \n= sin2 u\n  sin2 a1E+ - E-2t\n2U\nb. \nThe probability oscillates at the frequency determined by the difference in energies of the eigen-\nstates. This time dependence results because the initial state was a superposition state, as we saw in \nEq. (3.20). In terms of the Larmor frequencies used to deﬁne the Hamiltonian in Eq. (3.51), the prob-\nability of a spin ﬂip is\n \nP+ S - =\nv2\n1\nv2\n0 + v2\n1\n sin2\n a 2v2\n0 + v2\n1\n2\n tb  . \n(3.63)\nEq. (3.63) is often called Rabi’s formula, and it has important applications in many problems as we \nshall see.\nTo gain insight into Rabi’s formula, consider two simple cases. First, if there is no added ﬁeld in \nthe x-direction, then v1 \u0003 0 and P+ S - = 0 because the initial state is a stationary state. Second, if \nthere is no ﬁeld component in the z-direction, then v0 \u0003 0 and P+ S - oscillates between 0 and 1 at the \nfrequency v1, as shown in Fig. 3.8(a). The second situation corresponds to spin precession around the \napplied magnetic ﬁeld in the x-direction, as shown in Fig. 3.8(b), with a complete spin ﬂip from 0  +9 to \n0  -9 and back again occurring at the precession frequency v1. In the general case where both magnetic \nﬁeld components are present, the probability does not reach unity and so there is no time at which the \nspin is certain to ﬂip over. If the x-component of the ﬁeld is small compared to the z-component, then \nv1 << v0 and P+ S - oscillates between 0 and a value much less than 1 at a frequency approximately \nequal to v0, as shown in Fig. 3.9.\n",
    "82 \nSchrödinger Time Evolution\nExample 3.2 A spin-1/2 particle with a magnetic moment is prepared in the state 0  -9 and is sub-\nject to a uniform applied magnetic ﬁeld B = B0yn. Find the probability of measuring spin up in the \nz-direction after a time t.\nThe initial state is\n \n0\n c1029 = 0  -9. \n(3.64)\nThe applied magnetic ﬁeld is in the y-direction, so the Hamiltonian is H = v0Sy and the energy \neigenstates are 0{9y with energies E{ = {U v0>2 (step 1). The Larmor precession frequency is \nt\n0\n0.2\n0.4\n0.6\n0.8\n1.0\n)\nb\n(\n)\na\n(\ny\nx\nz\nB\n〈S(0)〉\n〈S(t)〉\n2\u0002\n\u00030\n4\u0002\n\u00030\n6\u0002\n\u00030\nP+→−\nFIGURE 3.9 (a) Spin-ﬂip probability for a uniform magnetic ﬁeld with x- and z-components and \n(b) the corresponding precession of the expectation value of the spin.\nt\n)\nb\n(\n)\na\n(\ny\nx\nz\nB\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP+→−\nS(0)\nS(t)\n2Π\nΩ1\n4Π\nΩ1\n6Π\nΩ1\nFIGURE 3.8 (a) Spin-flip probability for a uniform magnetic ﬁeld in the x-direction and (b) the \ncorresponding precession of the expectation value of the spin.\nP+→−\n",
    "3.2 Spin Precession \n83\nv0 = eB0>me. We must express the initial state in the energy basis (step 2), which in this case is \nthe Sy basis:\n \n 0\n c1029 = 0  -9 = 10  +9y y0  + 0 + 0  -9y y8- 02 0  -9 \n \n = 0  +9y y8+ 0  -9 + 0  -9y y8- 0  -9\n \n \n = y8+ 0  -9 0  +9y + y8- 0  -9 0  -9y\n \n(3.65)\n \n =\n-i\n12 0  +9y +\ni\n12 0  -9y.\n \nThe time evolved state is obtained by multiplying each energy eigenstate coefﬁcient by a phase \nfactor (step 3):\n \n 0\n c1t29 =\n-i\n12\n e-iE +t>U0  +9y +\ni\n12\n e-iE -t>U0  -9y \n \n =\n-i\n12\n e-iv0t>20  +9y +\ni\n12\n e+iv0t>20  -9y. \n(3.66)\nThe measurement probability is found by projecting onto the measured state and squaring (step 4):\n \n P+ = 08+ 0\n c1t290\n2\n \n \n = @8+ 0A -i\n12 e-iv0t>20  +9y +\ni\n12 e+iv0t>20  -9y2@\n2\n \n \n = @ A -i\n12 e-iv0t>28+ 0  +9y +\ni\n12 e+iv0t>28+ 0  -9yB @\n2\n \n \n(3.67)\n \n = @ A -i\n12 e-iv0t>2 A 1\n12B +\ni\n12 e+iv0t>2 A 1\n12B B @\n2\n \n \n = 1\n4 @-ie-iv0t>2 + ie+iv0t>2@\n2\n= 1\n4 @-2sin1v0t>22 @\n2\n \n \n = sin2\n 1v0t>22.\n \n \nThe probability oscillates between zero and unity as time evolves, as shown in Fig. 3.10(a), which \nis consistent with the model of the spin vector precessing around the applied ﬁeld, as shown in \nFig. 3.10(b).\nt\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ny\nx\nz\nΩ0t\nB\n(a)\n(b)\nP+→−\n\u0004S(t)\u0003\n\u0004S(0)\u0003\n2Π\nΩ0\n4Π\nΩ0\n6Π\nΩ0\nFIGURE 3.10 (a) Spin measurement probability and (b) the corresponding precession \nof the expectation value of the spin.\n",
    "84 \nSchrödinger Time Evolution\nThough we have derived Rabi’s formula [Eq. (3.63)] in the context of a spin-1/2 particle in a \nuniform magnetic ﬁeld, its applicability is much more general. If we can express the Hamiltonian \nof any two-state system in the matrix form of Eq. (3.51) with the parameters v0 and v1, then we can \nuse Rabi’s formula to ﬁnd the probability that the system starts in the “spin-up” state 0  +9 and is then \nmeasured to be in the “spin-down” state 0  -9 after some time t. In the general case, the 0  +9 and 0  -9 \nstates are whatever states of the system are used to represent the Hamiltonian operator in the form of \nEq. (3.51). In the next section, we’ll look at the example of neutrino oscillations to see how this exam-\nple can be applied more generally.\n3.3 \u0002 NEUTRINO OSCILLATIONS\nNeutrinos have enjoyed an almost mystical history in particle physics because they are very hard to \ndetect and yet play an important role in many fundamental processes. In 1930, the neutrino was pos-\ntulated by Wolfgang Pauli as a solution to the beta decay problem. A free neutron decays to a proton \nand an electron with a lifetime of about 10 minutes in the most basic beta decay process. However, the \ndecay scheme n S p + e- violates conservation of angular momentum, and experimental data sug-\ngest that conservation of energy is also violated. That’s not good. Rather than reject these two basic \nconservation laws, as some suggested, Pauli proposed that a third particle is involved in the decay \nprocess. Enrico Fermi named this new particle the “neutrino.” Fermi developed a theory that used the \nneutrino to properly explain beta decay, but it was 25 more years before a neutrino was detected.\nNeutrinos are uncharged, relativistic particles. In nuclear beta decay, neutrinos are produced in \nprocesses such as\n \nn S p + e- + ne  \n \np S n + e+ + ne, \n \n(3.68)\nwhere the subscript labels the neutrino ne as an electron neutrino and the bar labels ne as an antineu-\ntrino. In the standard model of particle physics, neutrinos are massless, like photons. Neutrinos are so \nelusive because they interact via the weak force or weak interaction, which is the weakest of the four \nfundamental forces—the strong nuclear force, electromagnetism, and gravity being the other three.\nThe reaction p S n + e+ + ne is part of the thermonuclear reaction chain in the sun and other \nstars, so we earthlings are constantly bombarded with neutrinos along with the essential photons we \nreceive from the sun. In the 1960s and 70s, landmark experiments indicated that there are only about \nhalf as many solar neutrinos arriving on earth as we would expect, given reliable models of stellar ther-\nmonuclear reactions. This solar neutrino problem has recently been solved by experiments detecting \nneutrinos from the sun and from nuclear reactors that demonstrate that neutrinos have nonzero mass. \nThese results are counter to the standard model and so have profound implications for particle physics \nand cosmology. Understanding how these experiments provide information on the neutrino mass is a \npowerful illustration of the applicability of Rabi’s formula to other two-state systems.\nIn addition to the electron neutrinos in Eq. (3.68), there are other types of neutrinos associated \nwith other reactions, such as\n \n p+ S m+ + nm\n \n \n m- S e- + nm + ne, \n \n(3.69)\nwhich represent the decay of a pion (p) to a muon (m) and the decay of a muon to an electron, respectively. \nA muon behaves exactly like an electron but has a larger mass. Electrons, muons, and a third particle \n(tau) and their associated neutrinos are collectively called leptons. In reactions involving these particles \n",
    "3.3 Neutrino Oscillations \n85\nit is convenient to define a lepton “ﬂavor” quantum number L, with the assigned values Le = 1 for the \nelectron e− and its associated neutrino ne, Le = -1 for the positron e+ and the antineutrino ne, Lm = 1\nfor the muon μ− and its associated neutrino nm, and Lm = -1 for the μ+ and nm. With these assignments, \nthe individual electron and muon ﬂavor numbers are conserved in the processes shown above. However, \nthere is no theoretical basis for this conservation, and so we allow for the possibility that these quantum \nnumbers are only approximately conserved. This possibility then allows for reactions of the type\n \nne 4 nm, \n(3.70)\nwhere an electron neutrino changes its ﬂavor and becomes a muon neutrino, or the reverse. Such \nchanges are called neutrino mixing or neutrino oscillations.\nThe labeling of neutrinos according to their association with electrons or muons arises from their behavior \nin the weak interaction processes described above. In other words, the quantum states 0 ne9 and 0 nm9 are \neigenstates of the Hamiltonian describing the weak interaction. However, when neutrinos propagate in \nfree space, the weak interaction is not relevant and the only Hamiltonian of relevance is that due to the \nrelativistic energy of the particles, which includes their rest masses and momenta. The eigenstates of this \nHamiltonian are generally referred to as the mass eigenstates. If the masses of the two types of neutrinos \n(electron and muon) are different, then, in general, the mass eigenstates do not coincide with the weak \ninteraction eigenstates. This distinction between sets of eigenstates allows for ﬂavor-changing processes.\nTo see why this is so, let the mass eigenstates be labeled 0 n19 and 0 n29. Either one of the two bases \n(mass or weak eigenstates) can be used as a complete basis upon which to expand any general state in \nthis system. Let’s assume that the relation between the bases is\n \n 0 ne9 = cos u\n2\n 0 n19 + sin u\n2\n 0 n29  \n \n 0 nm9 = sin u\n2\n 0 n19 - cos u\n2\n 0 n29. \n \n(3.71)\nThe angle u>2 is generally referred to as the mixing angle (some treatments drop the factor 1/2, but \nwe retain it to be consistent with the previous spin-1/2 discussion). If the mixing angle is small, then \nthe relations become\n \n 0 ne9 \u0003 0 n19  \n \n 0 nm9 \u0003 0 n29. \n \n(3.72)\nAssume that an electron neutrino is created in some weak interaction process and then propagates \nthrough free space to a detector. We wish to know the probability that a muon neutrino is detected, \nwhich is the signature of neutrino ﬂavor mixing. The initial state vector is\n \n 0 c1029 = 0 ne9\n \n \n = cos u\n2\n 0 n19 + sin u\n2\n 0 n29. \n \n(3.73)\nDuring the free-space propagation, the energy eigenstates of the system are the mass eigenstates \nbecause there is no weak interaction present. Thus the Schrödinger time evolution for this state is\n \n0 c1t29 = cos u\n2\n e-iE1t>U0 n19 + sin u\n2\n e-iE2t>U0 n29. \n(3.74)\nThe energy eigenvalues are simply the relativistic energies, which are determined by the rest masses \nand the momenta:\n \nEi = 41pc22 + 1mi c222,  i = 1, 2. \n(3.75)\n",
    "86 \nSchrödinger Time Evolution\nAssuming that the neutrinos are highly relativistic 1mc2 V pc2, we ﬁnd\n \n Ei = pc c1 + ami c2\npc b\n2\nd\n1>2\n \n \n \u0002 pc c1 + 1\n2\n amic2\npc b\n2\nd  \n \n(3.76)\n \n \u0002 pc + 1mi c222\n2pc\n.\n \nThe beauty of studying two-level systems such as spin-1/2 particles and neutrino oscillations is \nthat they are formally identical. In the spin-1/2 case, we phrased the problem in terms of ﬁnding the \nprobability of a spin ﬂip, whereas here we are looking for a change in the ﬂavor of the neutrino. In \nboth cases, the initial and ﬁnal states are not energy eigenstates, but rather orthogonal states in a dif-\nferent basis. The problems are mathematically identical, so the probability of a transition between the \northogonal states takes the same form. The probability of a neutrino oscillation is thus given by the \nsame equation as the spin-ﬂip probability, Eq. (3.62),\n \n PneSnm = 08nm0 c1t290\n2\n \n \n = sin2 u\n  sin2 a1E1 - E22t\n2U\nb, \n \n(3.77)\nwhere the parameter u has been deﬁned the same in both problems and the energy difference E+ - E- \nhas been changed to the energy difference E1 - E2. This energy difference is\n \n E1 - E2 = 1m1c22\n2\n2pc\n- 1m2c22\n2\n2pc\n \n \n = c3\n2p\n 1m2\n1 - m2\n22.\n \n \n(3.78)\nNeutrinos move at nearly the speed of light c, so we approximate the time from the creation of the \nelectron neutrino to the detection of the muon neutrino as t \u0002 L>c, where L is the distance from the \nsource to the detector. We also approximate the relativistic momentum as p = E>c. This gives a prob-\nability for neutrino ﬂavor change of\n \n PneSnm = sin2 u sin2 a1m2\n1 - m2\n22Lc3\n4E U\nb . \n(3.79)\nAs a function of the distance L, the probability oscillates from 0 to a maximum value of sin2 u—hence \nthe term neutrino oscillation. By measuring the fractions of different neutrino ﬂavors at a distance \nfrom a neutrino source (e.g., the sun or a reactor) and comparing to a model for the expected fractions, \nexperimenters have been able to infer the masses of the different neutrinos, or at least the differences \nof the squares of the masses. Recent results from solar neutrino and reactor neutrino experiments \nindicate a squared mass difference of approximately\n \nm2\n1 - m2\n2 \u0005 8 * 10-5 eV 2>c4. \n(3.80)\nThese experiments also provide information on the mixing angle u, with recent results indicating\n \nu \u0005 69\b. \n(3.81)\nNeutrino experiments such as these continue to provide information about the fundamental physics of \nthe universe.\n",
    "3.4 Time-Dependent Hamiltonians \n87\n3.4 \u0002 TIME-DEPENDENT HAMILTONIANS\nUp to now, we have studied the time evolution of quantum mechanical systems where the Hamiltonian \nis time independent. We solved the Schrödinger equation once for the general case and developed \na recipe for the time evolution of the system that we can apply to all cases with time-independent \nHamiltonians. However, if the Hamiltonian is time dependent, then we cannot use that simple recipe. \nWe must know the form of the Hamiltonian time dependence in order to solve the Schrödinger equa-\ntion. Fortunately, there are common forms of time dependence that we can solve in general and then \napply in many cases. The most common form of time dependence is sinusoidal time dependence at one \nfrequency. We will solve this problem in the context of a spin-1/2 particle in a magnetic ﬁeld and then \nalso apply it to atom-light interactions.\n3.4.1 \u0002 Magnetic Resonance\nIn the spin precession example in Section 3.2.2, we concluded that a complete spin ﬂip required a large \nmagnetic ﬁeld in the x-direction, which represents a large change or perturbation compared to the \ninitial situation of a magnetic ﬁeld in the z-direction. Now consider whether we can induce a complete \nspin ﬂip without such a large perturbation. That is, what small magnetic ﬁeld can we add to the system \nthat will cause a 0  +9 state to ﬂip to a 0  -9 state? The answer is that we must apply a time-dependent \nmagnetic ﬁeld that oscillates at a frequency close to the Larmor precession frequency v0 that charac-\nterizes the energy difference between the spin-up and spin-down states, as shown in Fig. 3.1. By mak-\ning the oscillating magnetic ﬁeld resonant with the Larmor frequency, we induce transitions between \nthe energy states shown in Fig. 3.1. This effect is known as magnetic resonance. I. I. Rabi won the \nNobel Prize in physics in 1944 for his work in developing the magnetic resonance technique and using \nit to measure the magnetic moments of nuclei. Following Rabi’s work, nuclear magnetic resonance \n(NMR) became a widely used tool for studying the properties of materials. The Larmor frequency \ndepends on the magnetic ﬁeld magnitude at the location of the particular nucleus being studied. This \nmagnetic ﬁeld includes the applied external ﬁeld and any internal ﬁelds created by the local environ-\nment, such that measuring the resonance frequency provides valuable information about the environ-\nment of the nucleus. In biology and chemistry, NMR has been used extensively to distinguish different \ntypes of bonds and identify structures. More recently, magnetic resonance imaging (MRI) has been \ndeveloped for medical diagnosis.\nTo understand how magnetic resonance works, it is instructive to consider the classical problem \nﬁrst. A classical magnetic moment aligned with an angular momentum precesses around the direc-\ntion of an applied magnetic ﬁeld. Now imagine going to a reference frame that rotates about the ﬁeld \n(assumed to be in the z-direction) with the same frequency as the precession. An observer in the rotat-\ning frame would see the magnetic moment stationary and so would conclude that there is no magnetic \nﬁeld in that frame. If that rotating observer were asked to ﬂip the magnetic moment from up to down \nalong the z-axis, she would answer, “Simple, just impose a small magnetic ﬁeld perpendicular to the \nz-axis, which will cause the spin to precess around that direction.” Because that ﬁeld is the only ﬁeld \nacting in the rotating frame, it can be as small as one likes. The magnitude simply determines the time \nfor the spin to ﬂip.\nIn this situation, the transverse applied ﬁeld is stationary in the rotating frame, so it will appear to \nbe rotating at the precessional frequency in the original frame. Thus, we could write it as\n \nB = B1 cos1vt2xn + B1 sin1vt2yn, \n(3.82)\nwhere we allow the frequency v to differ from the precessional frequency v0 in order to solve the \nproblem more generally. In that case, there would be some residual precession in the rotating frame, \nand so the rotating observer would conclude that there is some residual ﬁeld in the z-direction. Hence, \n",
    "88 \nSchrödinger Time Evolution\nwe expect that the added transverse ﬁeld would not cause a complete ﬂipping of the magnetic moment \nfrom up to down in this general case.\nLet’s now apply this reasoning to the quantum mechanical case. Assume a magnetic ﬁeld of the form\n \nB = B0zn + B13cos1vt2xn + sin1vt2yn4, \n(3.83)\nwhere the role of B0 is to split the energies of the spin-up and spin-down states and the role of B1 is to \nﬂip the spin between the the up and down states. The Hamiltonian is\n \n H = -M~B\n \n \n = v0 Sz + v13cos1vt2Sx + sin1vt2Sy4, \n \n(3.84)\nwhere we again deﬁne the Larmor frequencies corresponding to the two magnetic ﬁeld components,\n \nv0 K eB0\nme\n,   v1 K eB1\nme\n. \n(3.85)\nThe matrix representation of the Hamiltonian is\n \nH \u0003 U\n2\n ¢ v0\nv1e-ivt\nv1eivt\n-v0\n≤. \n(3.86)\nThis Hamiltonian is time dependent, so we can no longer use our simple recipe for Schrödinger \ntime evolution. Rather, we must return to the Schrödinger equation and solve it with these new time-\ndependent terms. Because we are not using our recipe for Schrödinger time evolution, we are not \nbound to use the energy basis as the preferred basis. The obvious choice would be to use the basis we \nhave used for representing the Hamiltonian as a matrix, which becomes the basis of energy states if the \ntransverse part B1 of the magnetic ﬁeld vanishes. Using this basis, we write the state vector as\n \n0 c1t29 = c+1t2 0  +9 + c-1t2 0  -9 \u0003  ¢c+1t2\nc-1t2≤. \n(3.87)\nSchrödinger’s equation\n \niU d\ndt\n 0\n c1t29 = H1t2 0\n c1t29 \n(3.88)\nin matrix form is\n \niU d\ndt\n ¢c+1t2\nc-1t2≤= U\n2\n ¢ v0\nv1e-ivt\nv1eivt\n-v0\n≤¢c+1t2\nc-1t2≤ \n(3.89)\nand leads to the differential equations\n \niUc#\n+ 1t2 = U v0\n2  c+1t2 + U v1\n2  e-ivtc-1t2 \n \niUc#\n- 1t2 = U v1\n2  eivt c+1t2 - U v0\n2  c-1t2, \n \n(3.90)\nwhere c#\n+1t2 denotes a time derivative. To solve these time-dependent coupled differential equations, \nit is useful to follow the lead of the classical discussion and consider the problem from the rotating \n",
    "3.4 Time-Dependent Hamiltonians \n89\nframe. Though we don’t yet have the complete tools to know how to effect this transformation, we take \nit on faith that after a frame transformation the state vector is\n \n0 c\u00031t29 = c+1t2eivt>2 0  +9 + c-1t2e-ivt>2 0  -9 \u0003 ¢ c+1t2eivt>2\nc-1t2e-ivt>2≤, \n(3.91)\nwhere 0 c\u00031t29 is the state vector as viewed from the rotating frame. If we call the coefﬁcients of this \nvector a{1t2, then we can write\n \n0 c\u00031t29 = a+1t2 0  +9 + a-1t2 0  -9 \u0003 ¢a+1t2\na-1t2≤, \n(3.92)\nwhere the relations between the sets of coefﬁcients are\n \n c+1t2 = e-ivt>2a+1t2 \n \n c-1t2 = eivt>2a-1t2. \n \n(3.93)\nThe state vector in the nonrotating frame can thus be written as\n \n0 c1t29 = a+1t2e-ivt>2 0  +9 + a-1t2eivt>2 0  -9 \u0003 ¢a+1t2e-ivt>2\na-1t2eivt>2 ≤. \n(3.94)\nAnother way of viewing this transformation is to say that based upon earlier solutions of similar \nproblems [Eq. (3.35)], we expect the coefﬁcients c{1t2 to have time dependence of the form e|ivt>2, \nand so we have extracted that part of the solution and now need to solve for the remaining time depen-\ndence in the coefﬁcients a{1t2. In this view, we have simply performed a mathematical trick to make \nthe solution easier.\nIf we now substitute the expressions for c{1t2 in terms of a{1t2 into the differential \nequations (3.90), then we obtain\n \n iUa#\n+1t2 = -  U\u0006v\n2\n a+1t2 + U v1\n2\n a-1t2 \n \n iUa#\n-1t2 = U v1\n2\n a+1t2 + U\u0006v\n2\n a-1t2,  \n \n(3.95)\nwhere we have deﬁned a new term\n \n\u0006v K v - v0, \n(3.96)\nwhich is the difference between the angular frequencies of the rotating ﬁeld and the Larmor preces-\nsion due to the z-component of the magnetic ﬁeld. Because a{1t2 are the coefﬁcients of the trans-\nformed state vector 0 c\u00031t29, these differential equations can be considered as comprising a transformed \nSchrödinger equation\n \niU d\ndt\n 0\n c\u00031t29 = H\u0003\n 0\n c\u00031t29, \n(3.97)\nwhere the new Hamiltonian H\u0003 has the matrix representation\n \nH\u0003 \u0003 U\n2\n a-\u0006v\nv1\nv1\n\u0006vb. \n(3.98)\n",
    "90 \nSchrödinger Time Evolution\nThus, we have transformed (by rotation or mathematical sleight of hand) the original problem \ninto a new problem that has a time-independent Hamiltonian. Once we solve the new problem, we can \nuse the transformation equations to ﬁnd the solution to the original problem. However, because the \nnew Hamiltonian H\u0003 is time independent, we already know the solution. That is, this new problem has \nthe same form of the Hamiltonian as the spin precession problem in Section 3.2.2. Comparing the spin \nprecession Hamiltonian in Eq. (3.51) with the transformed Hamiltonian in Eq. (3.98), we note that the \nterm v0 is replaced by the new term -\u0006v. We are interested in ﬁnding the same probability P+ S - that \nan initial 0  +9 state is later found to have evolved to the 0  -9 state. The rotational transformation does \nnot alter the 0{9 basis states so if\n \n0 c1029 = 0  +9, \n(3.99)\nthen\n \n0 c\u00031029 = 0  +9. \n(3.100)\nThe probability for a spin flip is given by\n \n P+ S - = 08- 0\n c1t290\n2 \n \n = 0 c-1t20\n2.\n \n \n(3.101)\nFrom Eq. (3.93) relating the coefﬁcients, we have\n \n 0 c-1t20\n2 = 0 e-ivt>2a-1t20\n2 \n \n = 0 a-1t20\n2\n \n \n(3.102)\n \n = 08- 0 c\u00031t290\n2,  \nwhich means that the probability we desire is\n \nP+ S - = 08- 0 c\u00031t290\n2. \n(3.103)\nWe obtain this spin-ﬂip probability using Rabi’s formula in Eq. (3.63), with the change v0 S -\u0006v, \nresulting in\n \n P+ S - =\nv2\n1\n\u0006v2 + v2\n1\n  sin2 ¢ 4\u0006v2 + v2\n1\n2\n t≤ \n \n =\nv2\n1\n1v - v022 + v2\n1\n  sin2 ¢41v - v022 + v2\n1\n2\n t≤. \n \n(3.104)\nThis spin-ﬂip probability is a generalization of Rabi’s formula. Note that Eq. (3.104) reduces to \nEq. (3.63) for the case v = 0, which is expected because the applied ﬁeld in Eq. (3.83) is static and \naligned the same as the static ﬁeld in Eq. (3.48) for the case v = 0. The static magnetic ﬁeld case is \ngenerally referred to as spin precession, while the rotating ﬁeld case is referred to as Rabi ﬂopping. \nThough we have used their similarities to help us derive Eq. (3.104), it is important to clarify their dif-\nferences. In the static applied magnetic ﬁeld case, the resulting spin precession is a manifestation of \nthe natural Bohr oscillation of a quantum system that starts in a superposition of energy eigenstates. \nThe initial superposition remains intact and there is no exchange of energy between the system and \nthe applied ﬁeld. In the rotating applied magnetic ﬁeld case, the Rabi ﬂopping represents transitions \nbetween energy eigenstates, and there is exchange of energy between the system and the applied ﬁeld. \nThe energy exchange occurs because the Hamiltonian is time dependent.\n",
    "3.4 Time-Dependent Hamiltonians \n91\nThe probability of a Rabi spin ﬂip oscillates with an angular frequency given by\n \n\t = 41v - v022 + v2\n1, \n(3.105)\nthat is typically referred to as the generalized Rabi frequency. The term Rabi frequency generally \nrefers to the frequency v1, which is the value of the generalized Rabi frequency when the frequency v \nof the rotating ﬁeld is on resonance (i.e., v is set equal to the Larmor precession frequency v0 of the \nsystem in the presence of the magnetic ﬁeld B0 alone). For this choice of v = v0, the probability of a \nspin ﬂip becomes\n \nP+ S - = sin2 av1\n2\n tb, \n(3.106)\nwhich implies that the spin is ﬂipped with 100% probability at an angular frequency v1. For other off-\nresonance choices of the frequency v, the probability of a spin ﬂip oscillates with an amplitude smaller \nthan one. The amplitude of the spin-ﬂip oscillation, as a function of the frequency v of the rotating \nﬁeld, is plotted in Fig. 3.11. This curve has the form of a Lorentzian curve and clearly exhibits the \nimportant resonant behavior of the spin-ﬂip probability. The full width at half maximum (FWHM) of \nthe resonance curve is 2v1.\nFor the resonance condition v = v0, the probability of a spin ﬂip as a function of time is plotted \nin Fig. 3.12. Because the frequency v1 is proportional to the applied ﬁeld B1, the rate of spin ﬂipping \nincreases with increasing rotating magnetic ﬁeld strength. However, it is important to note that there \nis still 100% probability of a spin ﬂip for very small ﬁelds. This is the property we were looking for at \nthe beginning of the problem—a way to ﬂip the spin without perturbing the system appreciably. After \na time t given by v1t = p, the probability for a spin ﬂip is 100%. We have assumed that the applied \nﬁeld is on continuously, but this spin ﬂip can also be produced by a pulsed ﬁeld with a magnitude and \nduration that satisfy v1t = p. Such a pulse is often called a P-pulse and is used to ﬂip a spin, or more \ngenerally to make a transition from one energy state to another with 100% certainty. The diagram on \nthe right of Fig. 3.12 illustrates the energy levels of the spin in the magnetic ﬁeld and how the spin-ﬂip \noscillations are associated with transitions between the two energy levels. A transition from the upper \nlevel to the lower level takes energy from the atom and gives it to the magnetic ﬁeld and is known as \nemission, while the opposite process takes energy from the ﬁeld and is known as absorption.\n0\n0.5\n1.0\n2Ω1\nΩ0\nΩ\nP+→−,max\nFIGURE 3.11 Magnetic resonance curve showing the probability \nof a spin ﬂip as a function of the applied  frequency.\n",
    "92 \nSchrödinger Time Evolution\n3.4.2 \u0002 Light-Matter Interactions\nThis same model of the interaction between a two-level system and an applied time-dependent ﬁeld is \nused to explain how atoms absorb and emit light. In the magnetic resonance example above, the oscil-\nlating magnetic ﬁeld interacts with the magnetic dipole and energy is exchanged between the ﬁeld and \nthe dipole. In the interaction of atoms with light, the oscillating electric ﬁeld of the light wave interacts \nwith the electric dipole of the atom, and energy exchange between the ﬁeld and the atom corresponds to \nabsorption and emission of photons. We can use the Rabi ﬂopping formula of Eq. (3.104) to model the \natom-light interaction as long as we express the Hamiltonian of the system in the form of Eq. (3.86). \nThough atoms have more than two energy levels, we can reduce the problem to a two-level system if \nthe frequency v of the applied light ﬁeld is close to just one of the Bohr frequencies of the atom.\nConsider two levels of an atom, as shown in Fig. 3.13. Following the convention used in this com-\nmon problem, we label the lower state 0 g9 (for ground state) and the upper state 0 e9 (for excited state). \nThe energy difference between the two levels is deﬁned to be\n \nEe - Eg = U v0 \n(3.107)\nto connect to the spin notation. The applied light ﬁeld (e.g., laser beam) has a frequency v that is close \nto, but not necessarily equal to, the atomic Bohr frequency v0. Using the same notation as the spin \nproblem [Eq. (3.86)], we express the Hamiltonian for this atom-light system in two parts\n \nH \u0003 U\n2\n a v0\nv1e-ivt\nv1eivt\n-v0\nb = U\n2\n av0\n0\n0\n-v0\nb + U\n2\n a\n0\nv1e-ivt\nv1eivt\n0\nb \n(3.108)\nEe\nEg\n\u0002Ω0\n\u0002Ω\n\u0002e\u0003\n\u0002g\u0003\nFIGURE 3.13 Energy level diagram of a two-level atom interacting with \nan applied light ﬁeld of frequency v.\nt\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nemission\nabsorption\nemission\nabsorption\nemission\nabsorption\nP+→−\nΠ\nΩ1\n2Π\nΩ1\n3Π\nΩ1\n4Π\nΩ1\n\u0002\u0004\u0003\n\u0002\u0005\u0003\nFIGURE 3.12 Rabi oscillations of the spin-ﬂip probability for the resonance condition.\n",
    "Summary \n93\nand identify the ﬁrst term as the atomic Hamiltonian and the second term as the interaction Hamilto-\nnian. In this way, we see that the parameter v1 is really an off-diagonal matrix element of the interac-\ntion Hamiltonian that connects the two states:\n \nv1 = 2\nU\n 8e0 Hint0 g9. \n(3.109)\nThe Rabi formula in Eq. (3.104) then gives the probability for the light ﬁeld to cause transitions \nbetween the two atomic energy states. Transitions between the atomic states correspond to absorption \n10 g9 S 0 e92 and emission 10 e9 S 0 g92 of photons in the light ﬁeld. Total energy is conserved as it is \nexchanged between the atom and the light ﬁeld.\nStudying these induced transitions is the most powerful tool we have for discovering what the \nenergy levels of a system are and ultimately for determining the Hamiltonian of the system. This \ntool is known as spectroscopy and has played a pivotal role in relating experiments and theory in \nquantum mechanics. As we encounter new quantum mechanical systems in this text, we will point \nout the spectroscopic aspects of these systems. For now, we can make a few general comments. If the \nmatrix element of the interaction Hamiltonian in Eq. (3.109) happens to be zero, then the transition \nprobability between the two levels is zero and we say that this is a forbidden transition. By studying \nthe general properties of the matrix elements 8e0 Hint0 g9 for a system and an interaction, we can dis-\ncover a set of basic rules governing whether transitions are allowed or forbidden. These are known as \nselection rules and are often representative of some underlying symmetry in the system. We will discuss \nselection rules brieﬂy as we encounter new systems and then will study them more fully in Chapter 14.\nSUMMARY\nIn this chapter we have learned the key aspect of quantum mechanics—how to predict the future. \nSchrödinger’s equation\n \niU d\ndt\n 0\n c1t29 = H1t20\n c1t29 \n(3.110)\ntells us how quantum state vectors evolve with time. In the common case where the Hamiltonian \nis time independent, the solution to Schrödinger’s equation has the same form no matter the problem. The \ntime-evolved state includes energy-dependent phase factors for each component of the superposition \nthat the system starts in:\n \n0 c1t29 = a\nn\ncne-iEnt>U0 En9. \n(3.111)\nThe general recipe for solving time-dependent problems is\nGiven a Hamiltonian H and an initial state 0 c1029, what is the probability that \nthe eigenvalue aj of the observable A is measured at time t?\n 1. Diagonalize H (ﬁnd the eigenvalues En and eigenvectors 0 En92.\n 2. Write 0 c1029 in terms of the energy eigenstates 0 En9.\n 3. Multiply each eigenstate coefﬁcient by e-iEnt>U to get 0 c1t29.\n 4. Calculate the probability Paj = 08aj0 c1t290\n2.\nWe will use this recipe throughout the rest of the book to study the time evolution of quantum mechan-\nical systems where the Hamiltonian is time independent.\n",
    "94 \nSchrödinger Time Evolution\nPROBLEMS\n 3.1 Write out the Schrödinger equation as expressed in Eq. (3.5) in matrix form for the two-state \nsystem and verify the result in Eq. (3.8).\n 3.2 Show that the probability of a measurement of the energy is time independent for a general state \n \n 0 c1t29 = a\nn\ncn1t2 0 En9 that evolves due to a time-independent Hamiltonian. Show that the \n \n probability of measurements of other observables are also time independent if those observables \ncommute with the Hamiltonian.\n 3.3 Show that the Hamiltonian in Eq. (3.51) can be written in the simple form of Eq. (3.56). \nDiagonalize the Hamiltonian in Eq. (3.55) and conﬁrm the results in Eq. (3.57).\n 3.4 Consider a spin-1/2 particle with a magnetic moment placed in a uniform magnetic ﬁeld \naligned with the z-axis. Verify by explicit matrix calculations that the Hamiltonian commutes \nwith the spin component operator in the z-direction but not with spin component operators in \nthe x- and y-directions. Comment on the relevance of these results to spin precession.\n 3.5 Consider a spin-1/2 particle with a magnetic moment. At time t = 0, the state of the particle is \n0 c1t = 029 = 0  +9.\na) If the observable Sx is measured at time t = 0, what are the possible results and the \nprobabilities of those results?\nb) Instead of performing the above measurement, the system is allowed to evolve in a uniform \nmagnetic ﬁeld B = B0yn. Calculate the state of the system (in the Sz basis) after a time t.\nc) At time t, the observable Sx is measured. What is the probability that a value U>2 will be \nfound?\nd) Draw a schematic diagram of the experiment in parts (b) and (c), similar to Fig. 3.2.\n 3.6 Consider a spin-1/2 particle with a magnetic moment.\na) At time t = 0, the observable Sx is measured, with the result U>2. What is the state vector \n0 c1t = 029 immediately after the measurement?\nb) Immediately after the measurement, a magnetic ﬁeld B = B0zn is applied and the particle is \nallowed to evolve for a time T. What is the state of the system at time t \u0003 T?\nc) At t = T, the magnetic ﬁeld is very rapidly changed to B = B0yn. After another time inter-\nval T, a measurement of Sx is carried out once more. What is the probability that a value U>2 \nis found?\n 3.7 A beam of identical neutral particles with spin 1/2 travels along the y-axis. The beam passes \nthrough a series of two Stern-Gerlach spin-analyzing magnets, each of which is designed to \nanalyze the spin component along the z-axis. The ﬁrst Stern-Gerlach analyzer allows only \nparticles with spin up (along the z-axis) to pass through. The second Stern-Gerlach analyzer \nallows only particles with spin down (along the z-axis) to pass through. The particles travel at \nspeed v between the two analyzers, which are separated by a region of length d in which there \nis a uniform magnetic ﬁeld B0 pointing in the x-direction. Determine the smallest value of d \nsuch that 25% of the particles transmitted by the ﬁrst analyzer are transmitted by the second \nanalyzer.\n 3.8 A beam of identical neutral particles with spin 1/2 is prepared in the 0  +9 state. The beam enters \na uniform magnetic ﬁeld B0, which is in the xz-plane and makes an angle u with the z-axis. \nAfter a time T in the ﬁeld, the beam enters a Stern-Gerlach analyzer oriented along the y-axis. \nWhat is the probability that particles will be measured to have spin up in the y-direction? Check \nyour result by evaluating the special cases u = 0 and u = p>2.\n",
    " 3.9 Consider a spin-1/2 particle with a magnetic moment. At time t = 0, the state of the particle is \n0 c1t = 029 = 0  +9n with the direction n = 1xn + yn2> 12. The system is allowed to evolve in \na uniform magnetic ﬁeld B = B0zn. What is the probability that the particle will be measured to \nhave spin up in the y-direction after a time t?\n 3.10 Consider a spin-1/2 particle with a magnetic moment. At time t = 0, the state of the \nparticle is 0 c1t = 029 = 0  +9. The system is allowed to evolve in a uniform magnetic ﬁeld \nB = B01xn + zn2> 12. What is the probability that the particle will be measured to have spin \ndown in the z-direction after a time t?\n 3.11 Consider a spin-1/2 particle with a magnetic moment. At time t = 0, the state of the particle is \n0 c1t = 029 = 0  +9n with the direction n = 1xn + yn2> 12. The system is allowed to evolve in \na uniform magnetic ﬁeld B = B01xn + zn2> 12. What is the probability that the particle will be \nmeasured to have spin up in the y-direction after a time t?\n 3.12 Consider a two-state quantum system with a Hamiltonian\nH \u0003 aE1\n0\n0\nE2\nb .\n \n Another physical observable A is described by the operator\nA \u0003 a0\na\na\n0b,\n \n where a is real and positive. Let the initial state of the system be 0 c1029 = 0 a19, where 0 a19 is \nthe eigenstate corresponding to the larger of the two possible eigenvalues of A. What is the \nfrequency of oscillation (i.e., the Bohr frequency) of the expectation value of A?\n 3.13 Let the matrix representation of the Hamiltonian of a three-state system be\nH \u0003 °\nE0\n0\nA\n0\nE1\n0\nA\n0\nE0\n ¢\n \n using the basis states 0 19, 0 29, and 0 39.\na) If the state of the system at time t = 0 is 0 c1029 = 0 29, what is the probability that the \nsystem is in state 0 29 at time t?\nb) If, instead, the state of the system at time t = 0 is 0 c1029 = 0 39, what is the probability that \nthe system is in state 0 39 at time t?\n 3.14 A quantum mechanical system starts out in the state\n0 c1029 = C130 a19 + 40 a292,\n \n where 0 ai9 are the normalized eigenstates of the operator A corresponding to the eigenvalues ai. \nIn this 0 ai9 basis, the Hamiltonian of this system is represented by the matrix\nH \u0003 E0 a2\n1\n1\n2b.\na) If you measure the energy of this system, what values are possible, and what are the \nprobabilities of measuring those values?\nb) Calculate the expectation value 8A9 of the observable A as a function of time.\nProblems \n95\n",
    "96 \nSchrödinger Time Evolution\n 3.15 Show that the general energy state superposition 0 c1t29 = a\nn\ncne-iEnt>U0 En9 satisﬁes the \n \n Schrödinger equation, but not the energy eigenvalue equation.\n 3.16 For a spin-1/2 system undergoing Rabi oscillations, assume that the resonance condition \nv = v0 holds.\na) Solve the differential equations for the coefﬁcients a{1t2. Use your results to ﬁnd the \ntransformed state vector 0 c\u00031t29 and the state vector 0 c1t29, assuming the most general \ninitial state of the system.\nb) Verify that a p-pulse (v1t = p) produces a complete spin ﬂip. Calculate both the \ntransformed state vector 0 c\u00031t29 and the state vector 0 c1t29.\nc) Assume that the interaction time is such that v1t = p>2. Find the effect on the system \nif the initial state is 0  +9.\nd) Discuss the differences between the original reference frame and the rotating reference \nframe in light of your results.\n 3.17 Consider an electron neutrino with an energy of 8 MeV. How far must this neutrino travel \nbefore it oscillates to a muon neutrino? Assume the neutrino mixing parameters given in the \ntext. How many complete oscillations (ne S nm S ne) will take place if this neutrino travels \nfrom the sun to the earth? Through the earth?\n 3.18 Many weak decay processes produce neutrinos with a spectrum of energies. Assume electron \nneutrinos are produced with a uniform distribution from 4 MeV to 8 MeV. By averaging the \nprobability over the energy spectrum, calculate and plot, as a function of the travel distance L, \nthe probability that electron neutrinos are measured at the detector. Compare the result with the \nprobability for monoenergetic neutrinos at 8 MeV. The integral required for the averaging does \nnot yield an elementary expression, so a computer is advisable. Assume the neutrino mixing \nparameters given in the text.\nRESOURCES\nActivities\nThis activity is available at\nwww.physics.oregonstate.edu/qmactivities\nSpins Lab 4: Students design experiments to study spin precession in a magnetic ﬁeld.\nFurther Reading\nPedagogical articles on neutrino oscillations:\nW. C. Haxton and B. R. Holstein, “Neutrino physics,” Am. J. Phys. 68, 15–32 (2000).\nW. C. Haxton and B. R. Holstein, “Neutrino physics: An update,” Am. J. Phys. 72, 18–24 (2004).\nE. Sassaroli, “Neutrino oscillations: A relativistic example of a two-level system,” Am. J. Phys. \n67, 869–875 (1999).\nC. Waltham, “Teaching neutrino oscillations,” Am. J. Phys. 72, 742–752 (2004).\nThe application of Rabi oscillations to atomic physics is the main focus of this book:\nL. Allen and J. H. Eberly, Optical Resonance and Two-Level Atoms, New York: Dover \nPublications, Inc., 1987.\n",
    " \n97\nC H A P T E R \n4\nQuantum Spookiness\nAs we have seen in the previous chapters, many aspects of quantum mechanics run counter to our \nphysical intuition, which is formed from our experience living in the classical world. The probabilistic \nnature of quantum mechanics does not agree with the certainty of the classical world—we have no \ndoubt that the sun will rise tomorrow. Moreover, the disturbance of a quantum mechanical system \nthrough the action of measurement makes us part of the system, rather than an independent observer. \nThese issues and others make us wonder what is really going on in the quantum world. As quantum \nmechanics was being developed in the early twentieth century, many of the world’s greatest physicists \ndebated the “true meaning” of quantum mechanics. They often developed gedanken experiments or \nthought experiments to illustrate their ideas. Some of these gedanken experiments have now actually \nbeen performed and some are still being pursued.\nIn this chapter, we present a few of the gedanken and real experiments that demonstrate the \nspookiness of quantum mechanics. We present enough details to give a ﬂavor of the spookiness and \nprovide references for further readings on these topics at the end of the chapter.\n4.1 \u0002  EINSTEIN-PODOLSKY-ROSEN PARADOX\nAlbert Einstein was never comfortable with quantum mechanics. He is famously quoted as saying \n“Gott würfelt nicht” or “God does not play dice,” to express his displeasure with the probabilistic \nnature of quantum mechanics. But his opposition to quantum mechanics ran deeper than that. He felt \nthat properties of physical objects have an objective reality independent of their measurement, much \nas Erwin felt that his socks were black or white, or long or short, independent of his pulling them out \nof the drawer. In quantum mechanics, we cannot say that a particle whose spin is measured to be up \nhad that property before the measurement. It may well have been in a superposition state. Moreover, \nwe can only know one spin component of a particle, because measurement of one component disturbs \nour knowledge of the other components. Because of these apparent deﬁciencies, Einstein believed that \nquantum mechanics was an incomplete description of reality.\nIn 1935, Einstein, Boris Podolsky, and Nathan Rosen published a paper presenting a gedan-\nken experiment designed to expose the shortcomings of quantum mechanics. The EPR Paradox \n(Einstein-Podolsky-Rosen) tries to paint quantum mechanics into a corner and expose the “absurd” \nbehavior of the theory. The essence of the argument is that if you believe that measurements on two \nwidely separated particles cannot inﬂuence each other, then the quantum mechanics of an ingeniously \nprepared two-particle system leads you to conclude that the physical properties of each particle are \nreally there—they are elements of reality in the authors’ words.\n",
    "98 \nQuantum Spookiness\nThe experimental situation is depicted in Fig. 4.1 (this version of the EPR experiment is due to \nDavid Bohm and has been updated by N. David Mermin). An unstable particle with spin 0 decays into \ntwo spin-1/2 particles, which by conservation of angular momentum must have opposite spin compo-\nnents and by conservation of linear momentum must travel in opposite directions. For example, a neu-\ntral pi meson decays into an electron and a positron: p0 S e- + e+. Observers A and B are on opposite \nsides of the decaying particle and each has a Stern-Gerlach apparatus to measure the spin component \nof the particle headed in its direction. Whenever one observer measures spin up along a given direc-\ntion, then the other observer measures spin down along that same direction. The quantum state of this \ntwo-particle system is\n \n0\n c9 =\n1\n12 10  +91 0  -92 - 0  -91 0  +922 , \n(4.1)\nwhere the subscripts label the particles and the relative minus sign ensures that this is a spin-0 state \n(as we’ll discover in Chapter 11). The use of a product of kets 1e.g., 0  +91 0  -922 is required here to \ndescribe the two-particle system (Problem 4.1). The kets and operators for the two particles are inde-\npendent, so, for example, operators act only on their own kets\n \nS1z0  +91 0  -92 = 1S1z0  +912 0  -92 = + U\n2\n 0  +91 0  -92, \n(4.2)\nand inner products behave as\n \n118+ 0 2 8-  0210  +91 0  -922 = 118+\n 0  +912128-  0  -922 = 1. \n(4.3)\nAs shown in Fig. 4.1, observer A measures the spin component of particle 1 and observer B mea-\nsures the spin component of particle 2. The probability that observer A measures particle 1 to be spin \nup is 50% and the probability for spin down is 50%. The 50-50 split is the same for observer B. For a \nlarge ensemble of decays, each observer records a random sequence of spin up and spin down results, \nwith a 50>50 ratio. But, because of the correlation between the spin components of the two particles, \nif observer A measures spin up (i.e., S1z = +U>2), then we can predict with 100% certainty that the \nresult of observer B’s measurement will be spin down (S2z = -U>2). The result is that even though \neach observer records a random sequence of ups and downs, the two sets of results are perfectly anticor-\nrelated. The state 0\n c9 in Eq. (4.1) that produces this strange mixture of random and correlated measure-\nment results is known as an entangled state. The spins of the two particles are entangled with each \nother and produce this perfect correlation between the measurements of observer A and observer B.\nImagine that the two observers are separated by a large distance, with observer B slightly farther \nfrom the decay source than observer A. Once observer A has made the measurement S1z = +U>2, we \nknow that the measurement by observer B in the next instant will be spin down 1S2 z = -U>22. We con-\nclude that the state 0\n c9 in Eq. (4.1) instantaneously collapses onto the state 0  +91 0  -92 , and the measure-\nment by observer A has somehow determined the measurement result of observer B. Einstein referred \nto this as “spooky action at a distance” (spukhafte Fernwirkungen). The result that observer B records is \nstill random, it is just that its randomness is perfectly anticorrelated with observer A’s random result. \nA\nB\nParticle 1 \nParticle 2 \nSpin 0\nSource \nS2z\nS1z\nFIGURE 4.1 Einstein-Podolsky-Rosen gedanken experiment.\n",
    "4.1 Einstein-Podolsky-Rosen Paradox \n99\nHence, there is no problem with faster-than-light communication here because there is no information \ntransmitted between the two observers.\nThe EPR argument contends that because we can predict a measurement result with 100% cer-\ntainty 1e.g., S2z = -U>22, then that result must be a “real” property of the particle—it must be an ele-\nment of reality. Because the particles are widely separated, this element of reality must be independent \nof what observer A does, and hence, must have existed all along. The independence of the elements of \nreality of the two  particles is called Einstein’s locality principle, and is a fundamental assumption of \nthe EPR argument.\nThe correlation of spin measurements of the two observers is independent of the choice of mea-\nsurement direction, assuming the same direction for both observers. That is, if observer A measures \nthe x-component of spin and records S1x = +U>2, then we know with 100% certainty that observer B \nwill measure S2x = -U>2. Observer A is free to choose to measure S1x, S1y, or S1z, so EPR argue that \nS2x, S2y, and S2z must all be elements of reality for particle 2. However, quantum mechanics maintains \nthat we can know only one spin component at a time for a single particle. EPR conclude that quantum \nmechanics is an incomplete description of physical reality because it does not describe all the elements \nof reality of the particle.\nIf the EPR argument is correct, then the elements of reality, which are also called hidden vari-\nables or instruction sets, are really there, but for some reason we cannot know all of them at once. \nThus, one can imagine constructing a local hidden variable theory wherein there are different types of \nparticles with different instruction sets that determine the results of measurements. The theory is local \nbecause the instruction sets are local to each particle so that measurements by the two observers are \nindependent. The populations or probabilities of the different instruction sets can be properly adjusted \nin a local hidden variable theory to produce results consistent with quantum mechanics. Because quan-\ntum mechanics and a local hidden variable theory cannot be distinguished by experiment, the question \nof which is correct is then left to the realm of metaphysics. For many years, this was what many physi-\ncists believed. After all, it doesn’t seem unreasonable to believe that there are things we cannot know!\nHowever, in 1964, John Bell showed that the hidden variables that we cannot know cannot even \nbe there! Bell showed that there are speciﬁc measurements that can be made to distinguish between a \nlocal hidden variable theory and quantum mechanics. The results of these quantum mechanics experi-\nments are not compatible with any local hidden variable theory. Bell derived a very general relation, \nbut we present a speciﬁc one here for simplicity.\nBell’s argument relies on observers A and B making measurements along a set of different direc-\ntions. Consider three directions an, bn, cn in a plane as shown in Fig. 4.2, each 120° from any of the other \ntwo. Each observer makes measurements of the spin projection along one of these three directions, \nchosen randomly. Any single observer’s result can be only spin up or spin down along that direction, \nbut we record the results independent of the direction of the Stern-Gerlach analyzers, so we denote \none observer’s result simply as  + or  -, without noting the axis of measurement. The results of the pair \nParticle 1 \nParticle 2 \na∧\nb\n∧\nc∧\nA\nB\nSpin 0\nSource \na∧\nb\n∧\nc∧\nFIGURE 4.2 Measurement of spin components along three directions as proposed by Bell.\n",
    "100 \nQuantum Spookiness\nof measurements from one correlated pair of particles (i.e., one decay from the source) are denoted \n+ -, for example, which means observer A recorded a + and observer B recorded a -. There are only \nfour possible system results: + +,  + -,  - +, or  - -. Even more simply, we classify the results as \neither the same, + + or  - -, or opposite, + - or  - +.\nA local hidden variable theory needs a set of instructions for each particle that speciﬁes ahead \nof time what the results of measurements along the three directions an, bn, cn will be. For example, the \ninstruction set 1an  +, bn  +, cn +2 means that a measurement along any one of the three directions will \nproduce a spin up result. For the entangled state of the system given by Eq. (4.1), measurements by the \ntwo observers along the same direction can yield only the results + - or  - +. To reproduce this aspect \nof the data, a local hidden variable theory would need the eight instruction sets shown in Table 4.1. For \nexample, the instruction set 1an  +, bn  -, cn +2 for particle 1 must be paired with the set 1an  -, bn  +, cn -2 for \nparticle 2 in order to produce the proper correlations of the entangled state. Beyond that requirement, \nwe allow the proponent of the local hidden variable theory freedom to adjust the populations Ni (or \nprobabilities) of the different instruction sets as needed to make sure that the hidden variable theory \nagrees with the quantum mechanical results.\nNow use the instruction sets (i.e., the local hidden variable theory) to calculate the prob-\nability that the results of the spin component measurements are the same 1Psame = P+ + + P- -2 \nand the probability that the results are opposite 1Popp = P+ - + P+ -2, considering all possible \norientations of the spin measurement devices. There are nine different combinations of measure-\nment directions for the pair of observers: anan, anbn, ancn, bnan, bnbn, bncn, cnan, cnbn, cncn. If we consider particles \nof type 1 (i.e., instruction set 1), then for each of these nine possibilities, the results are opposite \n(+ -). The results are never the same for particles of type 1. The same argument holds for type \n8 particles. For type 2 particles, the instruction sets 1an  +, bn  +, cn -2 and 1an  -, bn  -, cn +2 yield the\nnine possible results + -,  + -,  + +,  + -,  + -,  + +,  - -,  - -,  - + with four possibilities of \nrecording the same results and ﬁve possibilities for recording opposite results. Thus, we arrive at the \nfollowing probabilities for the different particle types:\n \nPopp = 1\nPsame = 0 r types 1 & 8 \n \nPopp = 5\n9\nPsame = 4\n9\nt types 2 S 7 . \n \n(4.4)\nTable 4.1 Instruction Sets (Hidden Variables)\nPopulation\nParticle 1\nParticle 2\nN1\nN2\nN3\nN4\nN5\nN6\nN7\nN8\n1an\n  +, bn\n +, cn\n +2\n1an\n  +, bn\n +, cn\n -2\n1an\n  +, bn\n -, cn\n +2\n1an\n  +, bn\n -, cn\n -2\n1an\n  -, bn\n +, cn\n +2\n1an\n  -, bn\n +, cn\n -2\n1an\n  -, bn\n -, cn\n +2\n1an\n  -, bn\n -, cn\n -2\n1an\n  -, bn\n -, cn\n -2\n1an\n  -, bn\n -, cn\n +2\n1an\n  -, bn\n +, cn\n -2\n1an\n  -, bn\n +, cn\n +2\n1an\n  +, bn\n -, cn\n -2\n1an\n  +, bn\n -, cn\n +2\n1an\n  +, bn\n +, cn\n -2\n1an\n  +, bn\n +, cn\n +2\n",
    "4.1 Einstein-Podolsky-Rosen Paradox \n101\nTo ﬁnd the probabilities of recording the same or opposite results in all the measurements, we \nperform a weighted average over all the possible particle types. The weight of any particular particle \ntype, for example type 1, is simply N1  \u0006aNi (recall we will adjust the actual values later as needed). \nThus, the averaged probabilities are:\n \n Psame =\n1\na\ni\nNi\n 4\n9\n 1N2 + N3 + N4 + N5 + N6 + N72 … 4\n9\n \n \n Popp =\n1\na\ni\nNi\n aN1 + N8 + 5\n9\n 1N2 + N3 + N4 + N5 + N6 + N72b Ú 5\n9\n ,  \n(4.5)\nwhere the inequalities follow because the sum of all the weights for the different particle types must \nbe unity. In summary, we can adjust the populations all we want, but that will always produce prob-\nabilities of the same or opposite measurements that are bound by the above inequalities. That is what \nis meant by a Bell inequality.\nWhat does quantum mechanics predict for these probabilities? For this system of two spin-1/2 \nparticles, we can calculate the probabilities using the concepts from the previous chapters. Assume \nthat observer A records a “+” along some direction (of the three). Deﬁne that direction as the z-axis \n(no law against that). Observer B measures along a direction nn at some angle u with respect to the \nz-axis. The probability that observer A records a “+” along the z-axis and observer B records a “+” \nalong the nn direction is\n \nP+ + = 0118+ 0   n   \n2n 8+ 02 0\n c9 0\n2 . \n(4.6)\nSubstituting the entangled state 0\n c9 and the direction eigenstate 0  +9nn  gives\n \nP+ + = 2 18+ 0 ¢cos u\n2  28+ 0 + e-if sin u\n2  28- 0 ≤ 1\n12 10  +91 0  -92 - 0  -91 0  +922 2\n2\n \n \n= 2 1\n12 ¢cos u\n2  28+ 0 + e-if sin u\n2  28- 0 ≤10  -922 2\n2\n \n \n= 1\n2\n sin2  u\n2\n . \n \n(4.7)\nThe same result is obtained for the probability that observer A records a “-” along the z-axis and \nobserver B records a “-” along the nn direction. Hence, the result for the same measurements is\n \nPsame = P+ + + P- - = sin2 u\n2\n . \n(4.8)\nThe probability that observer B records a “-” along the direction nn, when A records a “+” is\n \nP+ - = 0118+ 0     n \n2n 8- 02 0\n c9 0\n2 \n \n= 2 18+ 0 ¢sin u\n2  28+ 0 - e-if cos u\n2  28- 0 ≤ 1\n12 10  +91 0  -92 - 0  -91 0  +922 2\n2\n \n \n= 2 1\n12 ¢sin u\n2  28+ 0 - e-if cos u\n2  28- 0 ≤ 1 0  -922 2\n2\n \n \n= 1\n2\n  cos2  u\n2\n , \n \n(4.9)\n",
    "102 \nQuantum Spookiness\nand the probability for opposite results is\n \nPopp = P+  - + P-  + = cos2 u\n2\n . \n(4.10)\nThe angle u between the measurement directions of observers A and B is 0° in 1>3 of the mea-\nsurements and 120° in 2>3 of the measurements, so the average probabilities are\n \nPsame = 1\n3 ~ sin2 0\b\n2 + 2\n3 ~ sin2 120\b\n2\n= 1\n3 ~ 0 + 2\n3 ~ 3\n4 = 1\n2  \n \nPopp = 1\n3 ~ cos2 0\b\n2 + 2\n3 ~ cos2 120\b\n2\n= 1\n3 ~ 1 + 2\n3 ~ 1\n4 = 1\n2\n . \n \n(4.11)\nThese predictions of quantum mechanics are inconsistent with the range of possibilities that we \nderived for local hidden variable theories in Eq. (4.5). Because these probabilities can be measured, \nwe can do experiments to test whether local hidden variable theories are possible. The results of exper-\niments performed on systems that produce entangled quantum states have consistently agreed with \nquantum mechanics and hence, exclude the possibility of local hidden variable theories. We are forced \nto conclude that quantum mechanics is an inherently nonlocal theory.\nThe EPR paradox also raises issues regarding the collapse of the quantum state and how a mea-\nsurement by A can instantaneously alter the quantum state at B. However, there is no information \ntransmitted instantaneously and so there is no violation of relativity. What observer B measures is not \naffected by any measurements that A makes. The two observers notice only when they get together \nand compare results that some of the measurements (along the same axes) are correlated.\nThe entangled states of the EPR paradox have truly nonclassical behavior and so appear spooky \nto our classically trained minds. But when you are given lemons, make lemonade. Modern quantum \nresearchers are now using the spookiness of the entangled states to enable new technologies that take \nadvantage of the way that quantum mechanics stores information in these correlated systems. Quan-\ntum computers, quantum communication, and quantum information processing in general are active \nareas of research and promise to enable a new revolution in information technology.\n 4.2 \u0002 SCHRÖDINGER CAT PARADOX\nThe Schrödinger cat paradox is a gedanken experiment designed by Schrödinger to illustrate some of \nthe problems of quantum measurement, particularly in the extension of quantum mechanics to classi-\ncal systems. The apparatus of Schrödinger’s gedanken experiment consists of a radioactive nucleus, a \nGeiger counter, a hammer, a bottle of cyanide gas, a cat, and a box, as shown in Fig. 4.3. The nucleus \nhas a 50% probability of decaying in one hour. The components are assembled such that when the \nnucleus decays, it triggers the Geiger counter, which causes the hammer to break the bottle and release \nthe poisonous gas, killing the cat. Thus, after one hour there is a 50% probability that the cat is dead.\nAfter the one hour, the nucleus is in an equal superposition of undecayed and decayed states:\n \n0 cnucleus9 =\n1\n12 10 cundecayed9 + 0\n cdecayed92. \n(4.12)\nThe apparatus is designed such that there is a one-to-one correspondence between the undecayed \nnuclear state and the live-cat state and a one-to-one correspondence between the decayed nuclear state \nand the dead-cat state. Though the cat is macroscopic, it is made up of microscopic particles and so \nshould be describable by a quantum state, albeit a complicated one. Thus, we expect that the quantum \nstate of the cat after one hour is\n \n0 ccat9 =\n1\n12 10 calive9 + 0 cdead92. \n(4.13)\n",
    "4.2 Schrödinger Cat Paradox \n103\nBoth quantum calculations and classical reasoning would predict 50>50 probabilities of observ-\ning an alive or a dead cat when we open the box. However, quantum mechanics would lead us to \nbelieve that the cat was neither dead nor alive before we opened the box, but rather was in a super-\nposition of states, and the quantum state collapses to the alive state 0 calive9 or dead state 0 cdead9 only \nwhen we open the box and make the measurement by observing the cat. But our classical experiences \nclearly run counter to this. We would say that the cat really was dead or alive, we just did not know \nit yet. (Imagine that the cat is wearing a cyanide sensitive watch—the time will tell us when the cat \nwas killed, if it is dead!)\nWhy are we so troubled by a cat in a superposition state? After all, we have just ﬁnished three \nchapters of electrons in superposition states! What is so inherently different about cats and electrons? \nExperiment 4 that we studied in Chapters 1 and 2 provides a clue. The superposition state in that \nexperiment exhibits a clear interference effect that relies on the coherent phase relationship between \nthe two parts of the superposition state vector for the spin-1/2 particle. No one has ever observed such \nan interference effect with cats, so our gut feeling that cats and electrons are different appears justiﬁed.\nThe main issues raised by the Schrödinger cat gedanken experiment are (1) Can we describe mac-\nroscopic states quantum mechanically? and (2) What causes the collapse of the wave function?\nThe Copenhagen interpretation of quantum mechanics championed by Bohr and Heisenberg \nmaintains that there is a boundary between the classical and quantum worlds. We describe micro-\nscopic systems (the nucleus) with quantum states and macroscopic systems (the cat, or even the Gei-\nger counter) with classical rules. The measurement apparatus causes the quantum state to collapse and \nto produce the single classical or meter result. The actual mechanism for the collapse of the wave func-\ntion is not speciﬁed in the Copenhagen interpretation, and where to draw the line between the classical \nand the quantum world is not clear. Others have argued that the human consciousness is responsible \nfor collapsing the wave function, while some have argued that there is no collapse, just bifurcation into \nalternate, independent universes. Many of these different points of view are untestable experimentally \nand thus raise more metaphysical than physical questions.\nThese debates about the interpretation of quantum mechanics arise when we use words, which \nare based on our classical experiences, to describe the quantum world. The mathematics of quantum \nNucleus\nCyanide\nGeiger Counter\nCat\nFIGURE 4.3 Schrödinger cat gedanken experiment.\n",
    "104 \nQuantum Spookiness\nmechanics is clear and allows us to calculate precisely. No one is disagreeing about the probability \nthat the cat will live or die. The disagreement is all about “what it really means!” To steer us toward \nthe clear mathematics, Richard Feynman admonished us to “Shut up and calculate!” Two physicists \nwho disagree on the words they use to describe a quantum mechanical experiment generally agree on \nthe mathematical description of the results.\nRecent advances in experimental techniques have allowed experiments to probe the boundary \nbetween the classical and quantum worlds and address the quantum measurement issues raised by \nthe Schrödinger cat paradox. The coupling between the microscopic nucleus and the macroscopic \ncat is representative of a quantum measurement whereby a classical meter (the cat) provides a clear \nand unambiguous measurement of the state of the quantum system (the nucleus). In this case, the two \npossible states of the nucleus (undecayed or decayed) are measured by the two possible positions on \nthe meter (cat alive or cat dead). The quantum mechanical description of this complete system is the \nentangled state\n \n0 csystem9 =\n1\n12 10 cundecayed9 0 calive9 + 0 cdecayed9 0 cdead92. \n(4.14)\nThe main issue to be addressed by experiment is whether Eq. (4.14) is the proper quantum mechanical \ndescription of the system. That is, is the system in a coherent quantum mechanical superposition, as \ndescribed by Eq. (4.14), or is the system in a 50>50 statistical mixed state of the two possibilities? As \ndiscussed above, we can distinguish these two cases by looking for interference between the two states \nof the system.\nTo build a Schrödinger cat experiment, researchers use a two-state atom as the quantum system \nand an electromagnetic ﬁeld in a cavity as the classical meter (or cat). The atom can either be in the \nground 0\n g9 or excited 0\n e9 state. The cavity is engineered to be in a coherent state 0\n a9 described \nby the complex number a, whose magnitude is equal to the square root of the average number of \nphotons in the cavity. For large a, the coherent state is equivalent to a classical electromagnetic \nﬁeld, but for small a, the ﬁeld appears more quantum mechanical. The beauty of this experiment is \nthat the experimenters can tune the value of a between these limits to study the region between the \nmicroscopic and macroscopic descriptions of the meter (cat). In this intermediate range, the meter is \na mesoscopic system.\nAtoms travel through the cavity and disturb the electromagnetic ﬁeld in the cavity. Each atom is \nmodeled as having an index of refraction that alters the phase of the electromagnetic ﬁeld. The sys-\ntem is engineered such that the ground and excited atomic states produce opposite phase shifts {f. \nBefore the atom enters the cavity, it undergoes a p-pulse that places it in an equal superposition of \nground and excited states\n \n0 catom9 =\n1\n12 10\n e9 + 0\n g92, \n(4.15)\nas shown in Fig. 4.4. Each component of this superposition produces a different phase shift in the \ncavity ﬁeld such that after the atom passes through the cavity, the atom-cavity system is in the entan-\ngled state\n \n0 catom +cavity9 =\n1\n12 10\n e9 0 ae if9 + 0\n g9 0 ae -if92 \n(4.16)\nthat mirrors the Schrödinger cat state in Eq. (4.14). The state of the cavity ﬁeld is probed by sending \na second atom into the cavity and looking for interference effects in the atom that are produced by the \ntwo components of the ﬁeld. In this experiment, the two ﬁeld states are classically distinguishable, \nakin to the alive and dead cat states. For small values of the phase difference 2f between the two ﬁeld \ncomponents, the interference effect is evident. However, for large values of the phase difference 2f \n",
    "Problems \n105\nbetween the two ﬁeld components, the interference effect vanishes, indicating that the superposition \nstate in Eq. (4.16) has lost the ﬁxed phase relationship between the two parts of the entangled state and \ncan no longer produce interference effects. The system has undergone decoherence due to its interac-\ntion with the random aspects of the environment. The decoherence effect also increases as the number \nof photons in the cavity ﬁeld increases, which makes the cavity ﬁeld more like a classical state. Hence, \nthe experiment demonstrates that the quantum coherence of a superposition state is rapidly lost when \nthe state becomes complex enough to be considered classical. Further details on this recent experiment \nare available in the references below (Brune et al.).\nPROBLEMS\n 4.1 Show that the quantum state vector of a two-particle system must be a product 0\n c910\n f92 of \ntwo single-particle state vectors rather than a sum 0\n c91 + 0\n f92. Hint: consider the action of a \nsingle-particle state operator on the two-particle state vector.\n 4.2 Consider the two-particle entangled state\n0 c9 =\n1\n12 10  +91 0  -92 - 0  -91 0  +922.\na) Show that 0\n c9 is not an eigenstate of the spin component operator S1z for particle 1.\nb) Show that 0\n c9 is properly normalized.\n 4.3 Consider the two-particle entangled state\n0 c9 =\n1\n12 10  +91 0  -92 - 0  -91 0  +922.\n \n Show that the probability of observer A measuring particle 1 to have spin up is 50% for any \norientation of the Stern-Gerlach detector used by observer A. To ﬁnd this probability, sum over \nall the joint probabilities for observer A to measure spin up and observer B to measure anything.\n 4.4 Show that the state\n0 ca9 =\n1\n12 10  +91 0  -92 - 0  -91 0  +922\n \n is equivalent to the state\n0 cb9 =\n1\n12 10  +91x 0  -92  x - 0  -91x 0  +92  x2.\n \n That is, the two observers record perfect anticorrelations independent of the orientation of their \ndetectors, as long as both are aligned along the same direction.\nAtom Source\nΠ\u00062 Pulse\nCavity\n1\n2 \u0002g\u0003+\u0002e\u0003\n\u0002g\u0003\nFIGURE 4.4 Schrödinger cat experiment with atoms in a cavity.\n",
    "106 \nQuantum Spookiness\n 4.5 Calculate the quantum mechanical probabilities in Eqs. (4.7) and (4.9) without assuming that \nobserver A’s Stern-Gerlach device is aligned with the z-axis. Let the direction of observer A’s \nmeasurements be described by the angle u1 and the direction of observer B’s measurements be \ndescribed by the angle u2. Show that the averaged results in Eq. (4.11) are still obtained.\nRESOURCES\nFurther Reading\nThe EPR Paradox and Bell’s theorem are discussed in these articles:\nF. Laloe, “Do we really understand quantum mechanics? Strange correlations, paradoxes, \nand  theorems,” Am. J. Phys. 69, 655–701 (2001); “Erratum: Do we really understand \nquantum mechanics? Strange correlations, paradoxes, and theorems,” Am. J. Phys. 70, \n556 (2002).\nN. D. Mermin, “Bringing home the atomic world: Quantum mysteries for anybody,” Am. J. \nPhys. 49, 940–943 (1981).\nN. D. Mermin, “Is the moon there when nobody looks? Reality and the quantum theory,” \nPhys. Today 38(5), 38–47 (1985).\nN. D. Mermin, “Quantum mysteries revisited,” Am. J. Phys. 58, 731–734 (1990).\nN. D. Mermin, “Not quite so simply no hidden variables,” Am. J. Phys. 60, 25–27 (1992).\nN. D. Mermin, “Quantum mysteries reﬁned,” Am. J. Phys. 62, 880–887 (1994).\nN. D. Mermin, “Nonlocal character of quantum theory?” Am. J. Phys. 66, 920–924 (1998).\nN. D. Mermin, “What is quantum mechanics trying to tell us?” Am. J. Phys. 66, 753–767 \n(1998).\nSchrödinger’s cat is discussed in these references:\nT. J. Axon, “Introducing Schrodinger’s cat in the laboratory,” Am. J. Phys. 57, 317–321 (1989).\nM. Brune, E. Hagley, J. Dreyer, X. MaÓtre, A. Maali, C. Wunderlich, J. M. Raimond, and  \nS. Haroche, “Observing the progressive decoherence of the ‘meter’ in a quantum \nmeasurement,” Phys. Rev. Lett. 77, 4887–4890 (1996).\nB. S. DeWitt, “Quantum mechanics and reality,” Phys. Today 23(9), 30–35 (1970).\nA. J. Legett, “Schrodinger’s cat and her laboratory cousins,” Contemp. Phys. 25, 583–598 \n(1984). \nJ. G. Loeser, “Three perspectives on Schrodinger’s cat,” Am. J. Phys. 52, 1089–1093 (1984).\nW. H. Zurek, “Decoherence and the transition from quantum to classical,” Phys. Today \n44(10), 36–44 (1991).\nRichard Feynman’s directive to “Shut up and calculate!” is discussed in: \nN. D. Mermin, “What’s wrong with this pillow?” Phys. Today 42(4), 9–11 (1989).\nN. D. Mermin, “Could Feynman have said this?” Phys. Today 57(5), 10–11 (2004).\n",
    " \n107\nC H A P T E R \n5\nQuantized Energies:  \nParticle in a Box\nIn the ﬁrst part of this book we used the spin system to illustrate the basic concepts and tools of quan-\ntum mechanics. With a ﬁrm foundation in how quantum mechanics works, we are ready to address the \ncentral question that quantum mechanics was designed to answer: How do we explain the structure of \nthe microscopic world? All around us are nuclei, atoms, molecules, and solids with unique properties \nthat cannot be explained with classical physics but require quantum mechanics. For example, quantum \nmechanics can tell us why sodium lamps are yellow, why laser diodes have a unique color, and why \nuranium is radioactive.\nThe key to understanding the structure of microscopic systems lies in the energy states that the \nsystems are allowed to have. Each microscopic system has a unique set of energy levels that gives that \nsystem a “ﬁngerprint” that sets it apart from other systems. With the tools of quantum mechanics, we \ncan build a theoretical model for the system, predict that ﬁngerprint, and compare it to the experimen-\ntal measurement. Our goal in this chapter and the ones that follow is to learn how to predict this energy \nﬁngerprint. In this chapter we will study a particularly simple model system that exhibits most of the \nimportant features that are shared by all microscopic systems.\n5.1 \u0002 SPECTROSCOPY\nThe energy ﬁngerprint of a system not only identiﬁes that system uniquely, but the allowed energies \ndetermine the time evolution of the system through the Schrödinger equation, as we learned in Chapter 3. \nOne of the primary experimental techniques for measuring the energy ﬁngerprint of a system is spectros-\ncopy. We saw a hint of this in the magnetic resonance example of Section 3.4: absorption and emission of \nphotons causes transitions between quantized energy levels of the system only when the photon energy \nmatches the spacing between the energy eigenstates. Historically, the spectrum of hydrogen was a key \ningredient in the development of quantum mechanics, and spectroscopy continues to play an important \nrole in characterizing new quantum systems and in verifying the rules of quantum mechanics.\nIn the magnetic resonance example of Section 3.4, the two quantized energy levels arose from the \ntwo possible spin components (up or down) and their different interactions with an applied magnetic \nﬁeld. The more common situation that gives rise to quantized energy levels is where two or more \nparticles interact in a way that limits their spatial motion and binds them together into a compos-\nite system. Bound systems such as nuclei, atoms, molecules, and solids are everyday examples that \nare characterized by distinct spectral lines associated with quantized energy states, (i.e., eigenstates \nof the Hamiltonian with discrete energy eigenvalues). For example, the hydrogen atom energy levels \n",
    "108 \nQuantized Energies: Particle in a Box\nand the corresponding optical spectrum are shown in Fig. 5.1. The spectral lines appear when elec-\ntrons make transitions between energy levels. Downward transitions emit photons and give rise to an \nemission spectrum, while upward transitions absorb photons and yield an absorption spectrum. For \nevery pair of energy eigenvalues Ei and Ej  , there is a possible spectral line with photon energy Ei - Ej\n , \nand photon frequency fij and wavelength \nij given by\n \nfij =\nvij\nU =\nEi - Ej\nh\n \n \nlij = c\nfij\n=\nhc\nEi - Ej\n , \n(5.1)\nassuming that Ei 7 Ej\n . The set of spectral lines of atomic hydrogen that share a common lower level \nforms a series that is named after its discoverer. The ﬁrst three series in hydrogen are shown in Fig. 5.1 \nand listed in Table 5.1. The lowest energy state (n \u0003 1 for hydrogen) is called the ground state, and \nthe levels above that are called excited states. Though the word spectrum often refers to the observed \noptical lines, the set of quantized energy states is also commonly referred to as the energy spectrum \nof the system.\n1\n2\n3\n4\n5\nn\nLyman\nBalmer\nPaschen\n0\n\u00051\n\u00052\n\u00053\n\u00054\n\u00055\n\u00056\n\u00057\n\u00058\n\u00059\n\u000510\n\u000511\n\u000512\n\u000513\n\u000514\nE\nf\nΛ\n\b\nEnergy (eV)\nFIGURE 5.1 Hydrogen energy levels and the corresponding optical spectrum as a function \nof energy,  frequency, and wavelength (the wavelength scale is not a linear scale).\n",
    "5.1 Spectroscopy \n109\nA spectroscopy experiment can be considered to be a measurement of the energy of a quantum \nstate. A spectroscopic energy measurement is depicted in Fig. 5.2(a) in a simpliﬁed schematic that is \nanalogous to the Stern-Gerlach spin measurement we discussed earlier. A system is prepared in an \n initial state 0 c9, and we measure the probability that the state is measured to have a particular energy \nEi. If we write the energy eigenstates as 0\n Ei9, then the probability of a particular energy measurement is\n \nPEi = 08Ei0 c90\n2. \n(5.2)\nAs we did in the spins problem, we represent the collection of measurements on an ensemble of iden-\ntical states as a histogram, as shown in Fig. 5.2(b). In a real spectroscopy experiment, the measured \nenergies are really energy differences between levels, so it can be a bit of a puzzle to decode the energy \nlevels from the observed spectrum. We assume that this decoding process can be done and we assume \nthat the histogram in Fig. 5.2(b) faithfully represents the energy levels of the system. The energy levels \nEi and the eigenstates 0\n Ei9 are solutions to the energy eigenvalue equation\n \nHn  0 Ei9 = Ei0\n Ei9, \n(5.3)\nso the spectroscopic measurement is how the theoretical Hamiltonian is compared with experiment. \nOur task in this chapter is to learn how to predict the allowed energy eigenstates of a particular system \ngiven the Hamiltonian of the system.\n)\nb\n(\n)\na\n(\nPE1\nE1\n2\nPEi\nPE2\nPE3\nPE4\nPE5\nE5\n2\nE\nH\nE1\nE2\nE3\nE4\nE5\nE1\n2\nE4\n2\nE5\n2\nE1 E2\nE3\nE4\nE5\nE3\n2\nE2\n2\nE4\n2\nE2\n2\nE3\n2\nFIGURE 5.2 (a) Energy measurement and (b) histogram of results.\nTable 5.1 Hydrogen Transition Wavelengths\nFinal state\nInitial state\nSeries\n2\n3\n4\n5\n1\n122 nm\n103 nm\n97 nm\n95 nm\nLyman\n2\n656 nm\n486 nm\n434 nm\nBalmer\n3\n1875 nm\n1282 nm\nPaschen\n",
    "110 \nQuantized Energies: Particle in a Box\n5.2 \u0002 ENERGY EIGENVALUE EQUATION\nIn classical mechanics, we often solve problems by using Newton’s second law F = ma to predict the \nposition r1t2 of a particle subject to some known forces. Another common method is the energy method, \nwhereby we use conservation of energy and the relation E = T + V between the total energy (E ) \nand the kinetic (T ) and potential (V ) energies to predict the motion. Of course, the two methods are \nrelated because the force is related to the potential energy by\n \nFx = -  dV\ndx  \n(5.4)\nin one dimension. Hence the potential energy function V(x) is what determines the classical motion of \na particle.\nThe potential energy is also the key element in quantum mechanics, because of the important role \nit plays in the Hamiltonian of the system in question. The Hamiltonian determines the energy states \nthrough the energy eigenvalue equation\n \nHn 0\n Ei9 = Ei0\n Ei9. \n(5.5)\nNote that many other textbooks refer to Eq. (5.5) as the time-independent Schrödinger equation \nbecause it can be derived from the Schrödinger equation by separating the time and space parts; how-\never, we refer to it always as the energy eigenvalue equation. The prescription for ﬁnding a quantum \nmechanical Hamiltonian operator is to ﬁnd the classical form of the energy and replace the physical \nobservables with their quantum mechanical operators. For a moving particle, the classical mechanical \nenergy is the sum of the kinetic energy and the potential energy, which in one dimension is\n \nE =\np2\nx\n2m + V1x2. \n(5.6)\nWe use the position x and momentum p as the primary physical observables in quantum mechanics, \nfollowing the Hamiltonian approach to classical mechanics. Hence the quantum mechanical Hamilto-\nnian operator for a particle moving in one dimension is\n \nHn =\npn 2\nx\n2m + V1xn2. \n(5.7)\nWe use carets or hats on operators on occasion to distinguish them from the same symbol used as a \nvariable. If the distinction is clear from the context, then that notation may be dropped.\nSo now what? What are these new operators xn and pn for position and momentum? And how do \nwe use them to solve the energy eigenvalue equation? In the spins chapters, we learned much of the \nmachinery of quantum mechanics and would rightly expect to be able to use it in this new problem \non particle motion. However, position and momentum are different enough from spin that we need to \nredevelop some of the mathematical machinery we have already learned.\nWhen we discussed spin quantum states, we either used abstract kets, such as 0  +9 or 0  -9x, or we \nused column vectors to represent the abstract kets in a particular basis of eigenstates. For example, we \noften used the eigenstates of the Sz operator as the preferred basis, in which case the abstract kets 0  +9 \nand 0  -9x are expressed as\n \n0  +9 \u0003 a1\n0b \n(5.8)\n",
    "5.2 Energy Eigenvalue Equation \n111\nand\n \n0  -9x \u0003\n1\n12\n a 1\n-1b. \n(5.9)\nIn fact, there are very few quantum mechanical problems that can be solved using abstract kets. It is \ngenerally necessary to use a representation of the kets that is convenient for solving the problem. In \nthe problems that we wish to address now, it is most convenient to represent abstract quantum states as \nspatial functions, so we need to explain what that means.\nThe spatial functions we use to represent quantum states are called wave functions and are gener-\nally written using the Greek letter c as\n \nc 1x2. \n(5.10)\nThe wave function is a representation of the abstract quantum state, so we can use our representation \nnotation to write\n \n0\n c9 \u0003 c 1x2. \n(5.11)\nWe call this representation the position representation, which means that we are using the position \neigenstates as the preferred basis (more on these eigenstates later). For clarity, we will use the Greek \nletter c when referring to generic quantum states and other Greek letters to denote speciﬁc eigenstates. \nFor example, in the case of the energy eigenstates, we write the wave functions representing them as\n \n0\n Ei9 \u0003 wEi1x2 \n(5.12)\nto distinguish them as speciﬁc eigenstates.\nUsing this new wave function notation, the energy eigenvalue equation Eq. (5.5) becomes\n \nHnwEi1x2 = Ei\n wEi1x2. \n(5.13)\nTo solve this equation, we must know how to represent the operators in the Hamiltonian of Eq. (5.7) \nusing the position representation. It turns out that in the position representation, the action of the posi-\ntion operator xn is represented by multiplication by the position variable x, while the action of the \nmomentum operator pn is represented by application of a derivative with respect to position (see an \nadvanced text for justiﬁcation or take these as postulates). Using our representation notation, these two \nstatements are\n \n xn \u0003 x\n \n \n pn \u0003 -iU d\ndx  . \n \n(5.14)\nThe momentum operator has a factor of -iU to get the dimensions correct and to ensure that the mea-\nsurable results are real (not imaginary).\nWith these representations of the position and momentum operators, we now begin to solve the \nenergy eigenvalue equation. Inserting Eq. (5.14) into the energy eigenvalue equation gives\n \n HnwEi1x2 = Ei\n wEi1x2  \n \n a pn 2\n2m + V 1xn2\n b wEi1x2 = Ei\n wEi1x2  \n(5.15)\n \n a 1\n2m a-iU d\ndxb\n2\n+ V 1x2\n b wEi1x2 = Ei\n wEi1x2.\n",
    "112 \nQuantized Energies: Particle in a Box\nThe result is that the energy eigenvalue equation becomes a differential equation\n \na-  U2\n2m\n d 2\ndx2 + V1x2\n b wE1x2 = EwE1x2  . \n(5.16)\nThis differential equation is a big change from the matrix eigenvalue equations we encountered in the \nspin problems. This result is a common occurrence when using the wave function approach: operator \nequations turn into differential equations. Hence, when we use the wave function approach to ﬁnd \nthe allowed energy eigenstates of a system, we typically solve differential equations. We will solve \nthis differential equation for several different potential energy functions V1x2 in the remainder of this \nbook, but ﬁrst we pause to examine the wave function idea more carefully.\n5.3 \u0002 THE WAVE FUNCTION\nTo better understand the new concept of a wave function c1x2, let’s see how it relates to the quantum \nstate vector 0\n c9 we used in spins. In the spin case, we found that a useful way to represent a state vec-\ntor was as a column vector of numbers, with each number being the probability amplitude for the state \n0\n c9 to be measured in a particular spin eigenstate. For example, we could write the state 0\n c9 using the \nSz representation as\n \n0\n c9 \u0003 ¢8+ 0\n c9\n8- 0\n c9≤ d Sz = +U>2\n d Sz = -U>2. \n(5.17)\nThe numbers 8{0\n c9 in the column vector are the projections of the state vector 0\n c9 onto the Sz \neigenstates 0{9, corresponding to the two possible eigenvalues. If we measure the spin projection, as \ndepicted in Fig. 5.3(a), then the amplitudes 8{0\n c9 are used to calculate the probabilities\n \nP{ = 08{ 0\n c90\n2 \n(5.18)\nshown in the histogram in Fig. 5.3(b).\nIf we now consider an energy measurement, such as depicted in Fig. 5.2(a), then the basis of \nenergy eigenstates is the appropriate basis for representing the state vector:\n \n0\n c9 \u0003 •\n8E1@ c9\n8E2@ c9\n8E3@c9\nf\nμ \nd E = E1\nd E = E2\nd E = E3.\nf\n  \n(5.19)\nP\n2\n2\n(a)\n(b)\n1\nP\nZ\n2\n2\nSz\nP\n2\n2\nFIGURE 5.3 (a) Spin measurement and (b) probability histogram.\n",
    "In such an energy measurement, the probabilities shown in Fig. 5.2(b) are calculated using the pro-\njections 8Ei0\n c9 of the state 0\n c9 onto the energy eigenstates 0\n Ei9. The probabilities of measuring the \nquantized energies are\n \nPEi = 08Ei 0\n c90\n2\n . \n(5.20)\nIn analogy to these two examples, the wave function is a representation of a quantum state using \nthe eigenstates of the position operator xn as the basis states. If we call the position eigenstates 0\n xi9, \nthen the analog to Eqs. (5.17) and (5.19) would be\n \n0\n c9 \u0003 •\n8x1@ c9\n8x2@ c9\n8x3@ c9\nf\nμ \nd x1\nd x2\nd x3 ,\nf\n  \n(5.21)\nwhere the projection 8xi0 c9 is  the probability amplitude for the state 0 c9 to be measured in the posi-\ntion eigenstate 0\n xi9. However, experiment tells us that the physical observable x is not quantized. \nRather, all values of position x are allowed. This is in stark contrast to the case of the spin component \nSz, where only two results were possible. We say that the spectrum of eigenvalues of position is con-\ntinuous and the spectrum of eigenvalues of spin is discrete. Future experiments may shed new light on \nthis, but to date, space appears to be continuous. “Discrete vs. continuous” is an important distinction \nthat affects how we use and interpret the quantum state vector, the probability amplitudes, and the \nprobabilities when position is the relevant quantum mechanical observable.\nFor a continuous variable like position, the column vector representation of Eq. (5.21) is not con-\nvenient because we cannot write down the inﬁnite number of components. Even if the number were \nﬁnite but large, say 100, then we would ﬁnd a column vector cumbersome. Instead, we might choose to \nrepresent the 100 discrete numbers 8xi0 c9 as points in a graph, such as shown in Fig. 5.4(a). However, \nbecause the position spectrum is continuous, there is an inﬁnite continuum of the probability ampli-\ntudes 8x0 c9, and the natural way to represent such a continuous set of numbers is as a continuous func-\ntion, as shown in Fig. 5.4(b). This function is what we call the quantum mechanical wave  function c1x2. \nThe wave function is the collection of numbers that represents the quantum state vector in terms of the \nposition eigenstates, in the same way that the column vector used to represent a general spin state is a \ncollection of numbers that represents the quantum state vector in terms of the spin eigenstates. Whether \nyou write the wave function as c1x2 or as 8x0 c9 is ultimately a matter of taste. It is more common to \n(a)\n(b)\nx\nx\n\u0004x1\u0002Ψ\u0003\n\u0004x2\u0002Ψ\u0003\n\u0004x3\u0002Ψ\u0003\n\u0004x\u0002Ψ\u0003\nΨ(x)\nΨ(x)\n\u0004x4\u0002Ψ\u0003\u0004x5\u0002Ψ\u0003\n\u0004x6\u0002Ψ\u0003\n\u0004x7\u0002Ψ\u0003\n\u0004x8\u0002Ψ\u0003\n\u0004x9\u0002Ψ\u0003\n\u0004x10\u0002Ψ\u0003\nx1 x2 x3 x4 x5 x6 x7 x8 x9 x10\nFIGURE 5.4 (a) Discrete basis representation and (b) continuous basis representation.\n5.3 The Wave Function \n113\n",
    "114 \nQuantized Energies: Particle in a Box\nsee the form c1x2 used as the wave function, and we will follow that convention mostly, using the \nDirac notation when convenient. But it is important to remember both forms, so we repeat them here:\n \nc1x2 = 8x0 c9  . \n(5.22)\nIn words, we say that the wave function c1x2 is the probability amplitude for the quantum state 0 c9 to be \nmeasured in the position eigenstate 0 x9. We will say more about the position eigenstates in Chapter 6 and \nthen also make more connections between the wave function language and the Dirac bra-ket notation.\nContinuing with the analogy to the spin and energy examples above, we expect that the prob-\nability of measuring a particular value of position is obtained by taking the absolute square of the \nprojection 8x0 c9, as was done in Eqs. (5.18) and (5.20) for spin and energy representations. However, \nbecause the projection 8x0 c9 is the continuous wave function c1x2, the absolute square yields a con-\ntinuous probability function (actually a probability density, as we’ll ﬁnd in a moment), which we write \nas P1x2 so as to distinguish it from the discrete case Ae.g. PSz=+U>2B by making x an argument rather than \na subscript. In wave function notation, this new probability function is\n \nP1x2 = 0 c1x20\n2  . \n(5.23)\nThus, given a wave function c1x2, such as shown in Fig. 5.5(a), we use Eq. (5.23) to calculate the prob-\nability function P1x2, which is shown in Fig. 5.5(b). The probability function in Fig. 5.5(b) is analogous \nto the histograms of discrete probabilities in Figs. 5.2(b) and 5.3(b). We must stress that measuring the \nprobability function P1x2 does not allow us to infer the wave function c1x2. We saw in the spin measure-\nments of Chapters 1 and 2 that measurements of three different observables, Sx, Sy, and Sz, were required \nto deduce the state vector 0 c9 because the probability amplitudes are complex numbers. The relative \nphases between the probability amplitudes are not accessible from measurement of a single observable.\nHaving a continuous function for the probability rather than a set of discrete values raises some \nimportant issues. In quantum mechanics we require that the sum of all possible probabilities be equal \nto unity (i.e., the state vector must be normalized). In the discrete spins case this meant that:\n \na\n{\nP{ = a\n{\n08{0 c90\n2 = 1. \n(5.24)\nIf position were discrete instead of continuous, then the normalization condition would be:\n \na\nn\nPxn = a\nn\n08xn0 c90\n2 = 1. \n(5.25)\n(a)\n(b)\nx\nP(x)\nΨ(x)\nx\nFIGURE 5.5 (a) Wave function and (b) corresponding probability density.\n",
    "5.3 The Wave Function \n115\nHowever, because the spectrum of position eigenvalues is continuous rather than discrete, the sum \nover discrete probabilities must be changed to an integral over the continuous probability function \nP1x2, with the requisite differential term dx added. For now, we restrict the discussion to one spatial \ndimension. Thus the normalization condition is\n \nL\n\u0005\n- \u0005\nP1x2dx =\n \nL\n\u0005\n- \u0005\n0 c1x20\n2\n dx = 1. \n(5.26)\nThe differential dx has dimensions of length and the total integrated probability must be dimension-\nless, so the probability function P1x2 must have dimensions of inverse length. This means that P1x2 is \na probability density (in one dimension a probability per unit length) rather than a probability. Hence \nwe interpret the quantity\n \nP1x2dx \n(5.27)\nas the inﬁnitesimal probability of detecting a particle at position x within an inﬁnitesimal region of \nwidth dx [i.e., between x and x + dx, as shown in Fig. 5.6(a)]. To calculate the probability that a par-\nticle is measured to be in a ﬁnite interval a 6 x 6 b, we add all the inﬁnitesimal probabilities in that \ninterval, which is the integral\n \nPa6x6b =\n \nL\nb\na\n0 c1x20\n2\n dx \n(5.28)\nas depicted in Fig. 5.6(b). Equation (5.28) is an incredibly important formula. We use it, for example, \nto ﬁnd the probability that an electron is in a certain region of an atom (extended to three dimensions, \nof course).\nTo calculate other experimental quantities, such as expectation values, we must learn how to trans-\nlate bra-ket rules for discrete basis systems to wave function rules for continuous basis systems. We can \nlearn some rules for this translation by comparing the new wave function form of the normalization \ncondition in Eq. (5.26) to the bra-ket normalization condition. In Dirac notation, the requirement of \nprobability normalization is expressed in terms of the inner product of the state vector with itself:\n \n8c0 c9 = 1. \n(5.29)\nRewrite the wave function normalization condition Eq. (5.26) to make it look more like the bra-ket form:\n \nL\n\u0005\n- \u0005\nc*1x2c1x2dx = 1. \n(5.30)\n(a)\n(b)\nx x + dx\na\nb\nP(x)\nP(x)\nx\nx\nFIGURE 5.6 Probability for measuring a particle to be in the position range (a) x to x + dx, and (b) a to b.\n",
    "116 \nQuantized Energies: Particle in a Box\nComparing Eq. (5.29) and Eq. (5.30), we postulate the following rules for translating bra-ket formulae \nto wave function formulae:\n 1) Replace ket with wave function \n0 c9 S c1x2\n 2) Replace bra with wave function conjugate \n8c 0 S c*1x2\n 3) Replace bracket with integral over all space \n8 0 9 S\nL\n\u0005\n- \u0005\n dx\n 4) Replace operator with position representation An S A1x2\nwhere we have added a rule about operators that will become obvious in a moment.\nExample 5.1 Normalize the wave function\n \nc1x2 = Ce-a0 x -  20. \n(5.31)\nUse Eq. (5.26) for the normalization condition and integrate over all space\n \n 1 =\n \nL\n\u0005\n- \u0005\n0 c1x20\n2\n dx\n \n \n =\n \nL\n\u0005\n- \u0005\n@ Ce-a0 x -  2@ 0\n2\n dx\n \n \n =\n \nL\n\u0005\n- \u0005\n0 C0\n2 e-2a0 x -  20  dx. \n(5.32)\nBreak the integral into two pieces to remove the absolute value:\n \n 1 =\n \nL\n2\n- \u0005\n0 C0\n2 e2a1x -  22 dx +\n \nL\n\u0005\n2\n0 C0\n2 e-2a1x -  22 dx \n \n = J 0 C0 2\n2a\n e2a 1x -  22 R\n2\n- \u0005\n+ J 0 C0 2\n-2a\n e-2a 1x  -22 R\n\u0005\n2\n \n \n = 0 C0\n2\na\n .\n \n(5.33)\nOnce again, we have freedom to choose the overall phase, so we let C be real and positive:\n \nC = 1a \n(5.34)\ngiving the normalized wave function\n \nc1x2 = 1a e\n -  a0 x -  2 0 . \n(5.35)\nUsing the rules for translating bra-ket notation to wave function notation, a general state vector \nprojection or probability amplitude expressed in wave function language is\n \n8f0 c9 =\n \nL\n\u0005\n- \u0005\nf*1x2c1x2dx . \n(5.36)\n",
    "5.3 The Wave Function \n117\nThe square of this probability amplitude is the probability that the state c1x2 is measured to be in the \nstate f1x2\n \nPcSw = 08w0 c90\n2 = 2\nL\n\u0005\n- \u0005\nw*1x2cx1x2dx 2\n2   \n . \n(5.37)\nTechnically, we should say that this is the probability that the system prepared in state c1x2 is measured \nto have the physical observable for which f1x2 is the eigenstate, because we measure observables, not \nstates. But the looser language is common and does not create any ambiguity in the calculation. If we \nmeasure the energy, for example, then the probability of obtaining the result En is\n \nPEn = 08En0 c90\n2 = 2\nL\n\u0005\n- \u0005\nw*\nn1x2c1x2dx 2\n2\n, \n(5.38)\nwhere wn1x2 is the energy eigenstate with energy En. Note that Eq. (5.28) and Eq. (5.37) look simi-\nlar but have important differences. In Eq. (5.28) we integrate the probability density (wave function \ncomplex squared) over a ﬁnite range of position in order to sum the probabilities of measuring many \ndifferent positions. In Eq. (5.37) we integrate the product of two wave functions over all space to deter-\nmine their mutual overlap, and then we complex square that result to get the probability of measuring \na single result.\nTo transform an expectation value to wave function language, we must consider the operator. The \nexpectation value of an observable A is the matrix element of the operator\n \n8An9 = 8c0 An 0 c9. \n(5.39)\nIf we rewrite the expectation value as\n \n8An9 = 8c 05An0 c96, \n(5.40)\nwe see that it is an inner product where one ket has been transformed by the operator An. To write this \nin terms of wave functions, we must make sure to use the position representation form of the operator. \nFor example, the position operator xn in the position representation is simply multiplication by the sca-\nlar position x. Using the translation rules to write the expectation value of the position in wave function \nnotation yields\n \n 8xn9 = 8c0 xn 0 c9\n \n \n =\n \nL\n\u0005\n- \u0005\nc*1x2x c1x2dx \n \n =\n \nL\n\u0005\n- \u0005\nx0 c1x20\n2 dx,\n \n \n(5.41)\nwhere we have used the fact that scalar multiplication is commutative. For the expectation value of the \nmomentum, we ﬁnd\n \n 8pn9 = 8c0 pn 0 c9\n \n \n =\n \nL\n\u0005\n- \u0005\nc*1x2a-iU d\ndxb c1x2dx, \n \n(5.42)\n",
    "118 \nQuantized Energies: Particle in a Box\nwhich cannot be simpliﬁed more without knowing the wave function. In the next section, we will solve \nthe energy eigenvalue equation for a speciﬁc potential energy to allow us to calculate these expectation \nvalues explicitly.\nExample 5.2 Consider the wave function from Example 5.1:\n \nc1x2 = 1a e\n -  a0 x -  2 0 . \n(5.43)\nCalculate the expectation value of the position and the probability that the particle is measured to \nbe in the interval 4 6 x 6 6.\nThe expectation value of position is given by Eq. (5.41)\n \n 8xn9 =\n \nL\n\u0005\n- \u0005\nx0 c1x20\n2\n dx\n \n \n =\n \nL\n\u0005\n- \u0005\nx 11ae -a 0 x -  202\n2\n dx\n \n \n = a\nL\n\u0005\n- \u0005\nxe -2a 0 x-\n 20  dx\n \n \n = a\nL\n2\n- \u0005\nxe2a 1x-22 dx + a\nL\n\u0005\n2\nxe-2a 1x-22 dx\n \n(5.44)\n \n = a Je2a 1x-22 1-1 + 2ax2\n4a2\nR\n2\n- \u0005\n+ a Je-2a 1x-22 1-1 - 2ax2\n4a2\nR\n\u0005\n2\n \n \n = a J1-1 + 4a2\n4a2\n- 0 + 0 - 1-1 - 4a2\n4a2\nR\n \n \n = 2.\n \nThis is what you expect based upon the plot of wave function shown in Fig. 5.7(a) and the probabil-\nity density in Fig. 5.7(b), which are symmetric about the point x = 2.\n(a)\n(b)\n\u00022\n0\n2\n4\n6\n\u00022\n0\n2\n4\n6\nP(x)\n(x)\nx\nx\nFIGURE 5.7 (a) Wave function and (b) corresponding probability density. The hatched region \nin (b) represents the probability for the particle to be measured in the region 4 6 x 6 6.\n",
    "5.4 Inﬁnite Square Well \n119\nTo calculate the probability of ﬁnding the particle in the interval, use Eq. (5.28)\n \n P46x66 =\n \nL\n6\n4\n2 1a e -a 0 x -\n 20 2\n2\n dx \n \n =\n \nL\n6\n4\nae -2a1x-22 dx\n \n(5.45)\n \n = c a\n-2a\n e -2a 1x-22 d\n6\n4\n \n \n = e -4a\n2\n 31 - e -4a4 .\n \nThis probability is shown as the hatched region in Fig. 5.7(b). The actual value of the probability \ndepends on the value of the parameter a.\n5.4 \u0002 INFINITE SQUARE WELL\nOur task now is to solve the energy eigenvalue equation, which we found to be a differential equation\n \na-  U2\n2m d 2\ndx2 + V 1x2\n b wE 1x2 = E wE 1x2. \n(5.46)\nAs you might expect, the solutions to this differential equation depend critically on the functional \ndependence of the potential energy V1x2. A generic potential energy function is depicted in Fig. 5.8 \nin a potential energy diagram that illustrates some important aspects of the motion of the particle. \nMost of the interesting systems to which we will apply Eq. (5.46) resemble the potential energy func-\ntion depicted in Fig. 5.8 in that V1x2 has a minimum, so we refer to the potential energy function as a \nx\nE1\nX1\nX2\nEnergy\nT \u0002\u0003E \u0004\u0003V(x)\nV(x)\nClassical\nturning points\nClassically allowed region\nClassically\nforbidden region\nClassically\nforbidden region\nFIGURE 5.8 A generic potential energy well.\n",
    "120 \nQuantized Energies: Particle in a Box\npotential well. The particle energy is conserved, so the kinetic energy T1x2 = E - V1x2 is illustrated \nin the potential energy diagram by the vertical arrow between the ﬁxed energy E1 and the potential \nenergy V1x2. For a classical particle, the kinetic energy cannot be negative, so a classical particle with \nthe energy E1 chosen in Fig. 5.8 has its motion constrained to the region between x1 and x2. These \nextreme points of the classical motion are called classical turning points and the region within the \nturning points is called the classically allowed region, while the regions beyond are called classically \nforbidden regions. Particles that have their motion constrained by the potential well are said to be \nin bound states. Particles with energies above the top of the potential well do not have their motion \nconstrained and so are in unbound states. Note that the extent of the classically forbidden and allowed \nregions depends on the speciﬁc value of the energy, E1, for a particular bound state.\nSolving Eq. (5.46) for various important potential energy functions is the subject of this and later \nchapters. In this chapter, our goal is to study a simple potential energy system and learn the mathemat-\nics required for this new wave function approach.\nWe begin our journey to energy quantization with the simplest example of a particle that is con-\nﬁned to a region of space. The classical picture is a super ball bouncing between two perfectly elastic \nwalls. We call this system a particle in a box. We observe three important characteristics of this \nclassical system: (1) the ball ﬂies freely between the walls, (2) the ball is reﬂected perfectly at each \nbounce, and (3) the ball remains in the box no matter how large its energy. These three observations \nare consistent with (1) zero force on the ball when it is between the walls, (2) inﬁnite force on the ball \nat the walls, and (3) inﬁnite potential energy outside the box.\nThe mathematical model that is consistent with these three observations of the motion of a par-\nticle in a box is given by the potential energy function shown in Fig. 5.9. The potential energy is zero \nwithin the well (any constant would sufﬁce, but we choose zero for simplicity), and it is inﬁnite out-\nside the well. The discontinuity at the sides of the well requires us to write the potential energy func-\ntion in a piecewise fashion\n \nV1x2 = •\n\u0005,\n0 ,\n\u0005,   \nx 6 0\n0 6 x 6 L\nx 7 L.\n \n(5.47)\nBecause of the shape of the potential energy in Fig. 5.9, this system is also referred to as an inﬁnite \nsquare well. Though this model is too simple to accurately represent any real quantum mechanical \nsystem, it does illustrate most of the important features of a particle bound to a limited region of space.\n0\nL/2\nL\nx\nV(x)\n\u0005\nFIGURE 5.9 Inﬁnite square potential energy well.\n",
    "5.4 Inﬁnite Square Well \n121\nOur goal is to ﬁnd the energy eigenstates and eigenvalues of the system by solving the energy \neigenvalue equation using the potential energy in Eq. (5.47). The potential energy is piecewise, so we \nmust solve the differential equation (5.46) separately inside and outside the box. Outside the box, the \npotential energy is inﬁnite and the energy eigenvalue equation is\n \na-  U2\n2m d 2\ndx2 + \u0005b wE 1x2 = E wE 1x2,   outside box. \n(5.48)\nWe are looking for solutions with ﬁnite energy E, so Eq. (5.48) is satisﬁed only if the energy eigenstate \nwave function wE1x2 is zero everywhere outside the box. This means that the quantum mechanical \nparticle is excluded from the classically forbidden regions in this example. This correspondence with \nthe classical situation holds only for the case of inﬁnite potential energy walls on the potential well.\nInside the box, the potential energy is zero and the energy eigenvalue equation is\n \na-  U2\n2m\n d 2\ndx2 + 0b wE1x2 = EwE1x2,   inside box. \n(5.49)\nThus our task reduces to solving the differential equation inside the box:\n \n-  U2\n2m\n d 2\ndx2 wE1x2 = EwE1x2. \n(5.50)\nIt is worth reminding ourselves at this point what is known and what is not. The particle has a mass \nm and is conﬁned to a box of size L. These quantities are known, as is U, a fundamental constant. The \nunknowns that we need to ﬁnd are the energy E and the wave function wE1x2, which is what it means to \nsolve an eigenvalue problem (now posing as a differential equation).\nIt is convenient to rewrite the differential equation (5.50) as\n \n d 2\ndx2 wE1x2 = -  2mE\nU2  wE1x2 \n \n = -k2wE1x2,\n \n(5.51)\nwhere we have deﬁned a new parameter\n \nk2 = 2mE\nU2 , \n(5.52)\nwhich is positive because the energy E is positive in this problem. The parameter k is called the wave \nvector, and its physical interpretation will be evident in Eq. (5.67). Equation (5.51) says that the \nenergy eigenstate wE1x2 is a function whose second derivative is equal to that function itself times a \nnegative constant. We can write the solution either in terms of complex exponential functions\n \nwE1x2 = A\u0004e ikx + B\u0004e -ikx \n(5.53)\nor in terms of sine and cosine functions\n \nwE1x2 = A sin kx + B cos kx. \n(5.54)\nEither solution includes two as yet unknown constants, as you would expect for a second-order differ-\nential equation. It turns out that bound state energy eigenstates can always be written as real functions, \nso we choose to work with the sine and cosine form of the general solution (if you choose the complex \n",
    "122 \nQuantized Energies: Particle in a Box\nexponential form, you will arrive at the sine and cosine solutions at the end of the problem anyway: \nProblem 5.3). Hence the energy eigenstate wave function throughout space is\n \nwE 1x2 = •\n0 ,\nA sin kx + B cos kx ,\n0 ,\n   \nx 6 0\n0 6 x 6 L\nx 7 L.\n \n(5.55)\nWe now need some more information to reach the ﬁnal solution. There are three unknowns in \nthe problem: A, B, and k [which contains the energy E through Eq. (5.52)], so we expect to need three \npieces of information to solve for the three unknowns. We get two of these pieces of information from \nimposing boundary conditions on the wave function. To make sure that the mathematical solutions \nproperly represent real physical systems, we require that the wave function be continuous across each \nboundary between different regions of space where different solutions exist. Applying this require-\nment on the continuity of the wave function at the sides of the box x = 0 and L yields two boundary \ncondition equations:\n \n wE102: A sin102 + B cos102 = 0 \n \n wE1L2: A sin kL + B cos kL = 0.  \n(5.56)\nThe boundary condition at the left side of the box yields\n \nB = 0. \n(5.57)\nThis tells us that the cosine part of the general solution is not allowed because the cosine solution is not \nzero at the edge of the box and so does not match the wave function outside the box. The exclusion of \nthe cosine part of the solution arises because we chose to locate our box with one side at x \u0003 0; if the \nbox is located differently, then both sine and cosine solutions may be allowed. Given that the allowed \nwave functions must be sine functions, the boundary condition at the right side of the box yields\n \nA sin kL = 0. \n(5.58)\nThis equation is satisﬁed if A \u0003 0, but that yields a wave function that is zero everywhere, so it is unin-\nteresting. The more interesting possibility is that\n \nsin kL = 0. \n(5.59)\nThis is a transcendental equation that places limitations on the allowed values of the wave vector k. We \nwill ﬁnd other transcendental equations when we study other potentials. This transcendental equation \nhas solutions when the sinusoid function is zero. Hence the wave vectors that satisfy this equation are\n \n kL = np\n \n \n kn = n p\nL\n ,   n = 1, 2, 3, ... . \n(5.60)\nOnly discrete wave vectors are allowed, so this is termed the quantization condition. The index n is \nthe quantum number, which we use to label the quantized states and energies. The value n = 0 is \nexcluded because that would yield a wave function equal to zero, which is uninteresting. The nega-\ntive values of n are excluded because they yield the same states as the corresponding positive n values, \n",
    "5.4 Inﬁnite Square Well \n123\nrecalling that an overall phase A-1 = eip in this caseB does not change the physical state. Using the deﬁ-\nnition of the wave vector in Eq. (5.52), we relate the quantized wave vectors to the quantized energies\n \nEn = U2k 2\nn\n2m . \n(5.61)\nHence, the wave vector quantization condition in Eq. (5.60) results directly in the energy quantization \nfor this system:\n \nEn = n2p2U2\n2mL2 ,   n = 1, 2, 3, ...  . \n(5.62)\nThese allowed energies scale with the square of the quantum number n and produce the set of energy \nlevels shown in Fig. 5.10. The ground state is the n \u0003 1 level.\nThe allowed energy eigenstate wave functions are:\n \nwn1x2 = A sin npx\nL ,   n = 1, 2, 3, ...  . \n(5.63)\nThe constant A was not determined by the boundary conditions. To determine A, we need the third \npiece of information, which is that the wave function is normalized to unity:\n \n1 = 8En 0\n En9 =\n \nL\n\u0005\n- \u0005\nw*\nn1x2wn1x2dx =\n \nL\n\u0005\n- \u0005\n0 wn1x20\n2\n dx. \n(5.64)\n0\n5\n10\n15\n20\n25\nE /E1\nn \u0002\u00031\nn \u0002\u00032\nn \u0002\u00033\nn \u0002\u00034\nn \u0002\u00035\nE1 \u0002 1E1\nE2 \u0002 4E1\nE3 \u0002 9E1\nE4 \u0002 16E1\nE5 \u0002 25 E1\nFIGURE 5.10 Energy spectrum of the inﬁnite square potential energy well.\n",
    "124 \nQuantized Energies: Particle in a Box\nSubstitute the wave function from Eq. (5.63) and note that the wave function is zero for x 6 0 and \nx 6 L to limit the range of integration, resulting in\n \n1 =\n \nL\nL\n0\n0 A0\n2 sin2 kn\n x dx = 0 A0\n2 L\n2 . \n(5.65)\nWe are free to choose the normalization constant to be real and positive, because an overall phase is \nnot measurable. Thus the normalization constant is A = 12>L and the properly normalized energy \neigenstates are\n \nwn1x2 = A\n2\nL sin npx\nL ,   n = 1, 2, 3, ...  . \n(5.66)\nThe ﬁrst few allowed energy states are shown in Fig. 5.11. From these plots, it is now clear why \nwe call c1x2 the wave function. These energy eigenstates have a “wavy” spatial dependence, much \nlike the modes on a guitar string. For the inﬁnite square well, the waves “ﬁt” into the potential well \nsuch that there are an integer number of half wavelengths within the well. If we relate the wave vector \nk to a wavelength l through the relation\n \nk = 2p\nl , \n(5.67)\nthen we can rewrite the quantization condition in terms of the wavelength\n \n kn = n p\nL  \n \n 2p\nln\n= n p\nL  \n \n ln = 2L\nn\n \n(5.68)\n \n L = n ln\n2 . \nIn words, the well must contain an integer number of half wavelengths. This is the sense in which the \nwaves must “ﬁt” into the well. This is the same as the classical result for the allowed standing waves \non a vibrating string, such as a guitar string. The distinction between the classical wave and the quan-\ntum wave is that the classical wave does not have a quantized energy. The energy of a vibrating guitar \nn \u0002\u00031\nL/2\nL\nx\nΨ(x)\n(a)\nn \u0002\u00032\nL/2\nL\nx\nΨ(x)\n(b)\nn \u0002\u00033\nL/2\nL\nx\nΨ(x)\n(c)\nFIGURE 5.11 Wave functions of the ﬁrst three energy eigenstates of the inﬁnite \nsquare potential energy well.\n",
    "5.4 Inﬁnite Square Well \n125\nstring depends on the amplitude of oscillation, not on the wavelength or wave vector, and so it can have \nany energy value. The amplitude of the quantum wave function is determined by the normalization \ncondition and is independent of the energy for the inﬁnite square well.\nThe wave properties of this quantum system are a new aspect that is not evident in the classical \ndescription of a particle. In classical mechanics, waves and particles are clearly distinct, whereas in \nquantum mechanics a system exhibits properties that remind us of classical particles but also exhibits \nproperties of classical waves. This is often referred to as wave-particle duality. We will see more of \nthis in the next chapter when we discuss free particles.\nExample 5.3 It is useful to put some numbers into these expressions to get a sense of scale. For \nexample, if we conﬁne an electron 1me = 511 k eV>c22 in a box of size 0.2 nm (about the size of an \natom), the ground state (n \u0003 1) energy is\n \n E1 =\np2U2\n2me L2\n \n \n =\np216.58 * 10-16 eV s2\n2\n210.511 * 106 eV>c2210.2 * 10-9 m2\n2 \n(5.69)\n \n = 9.4 eV. \nThis is comparable to typical atomic binding energies.\nThe spectrum of this system will include the transition between the ground state and the ﬁrst excited \nstate. The ﬁrst excited state has energy E2 = 22E1 = 4E1, so the wavelength of light for this transition is\n \n l21 =\nhc\nE2 - E1\n= hc\n3E1\n \n \n = 1240 eV nm\n319.4 eV2\n= 44 nm . \n(5.70)\nNote that l21 is the wavelength of the photon emitted or absorbed in the transition, not the wave-\nlength of the bound particle that is associated with the wave vector of the wave function, which is \n0.4 nm for the ground state and 0.2 nm for the excited state, in agreement with Eq. (5.68).\nNow that we have found the energy eigenstates, we have what we need to calculate probabilities \nand expectation values to compare with experiments. The square of the wave function gives us the \nprobability density\n \n Pn1x2 = 0 wn1x2 0\n2\n \n \n = 2\nL sin2 npx\nL , \n(5.71)\nwhich is shown in Fig. 5.12 for the ﬁrst three states. Note that the probability density is zero outside \nthe well, so the probability of ﬁnding the particle anywhere outside the well is zero, just as in the clas-\nsical case. However, in the quantum system there are positions within the well where the probability \nof ﬁnding the particle is zero, which does not happen in the classical case. These positions are at the \nnodes of the wave function and hence are characteristic of the wave nature of the particle.\n",
    "126 \nQuantized Energies: Particle in a Box\nExample 5.4 Find the expectation value of the position for a particle in the ground state of an \ninﬁnite square potential energy well.\nThe expectation value of position is given by Eq. (5.41)\n \n 8xn9 = 8E10\n xn  0 E19 =\n \nL\n\u0005\n- \u0005\nw*\n11x2xw11x2dx =\n \nL\n\u0005\n- \u0005\nx  0\n w11x20\n2\n dx \n \n = 2\nL L\nL\n0\nx sin2a px\nL b dx = 2\nL\n a L\npb\n2\nL\np\n0\ny sin21y2dy\n \n \n = 2\nL\n a L\npb\n2\n c y2\n4 - y sin 2y\n4\n-\n cos 2y\n8\nd\np\n0\n \n(5.72)\n \n = 2\nL\n a L\npb\n2\n c p2\n4 -\np sin12p2\n4\n-\n cos12p2\n8\n+ 1\n8 d\n \n \n = 2\nL\n a L\npb\n2\n c p2\n4 d\n \n \n = L\n2 .\n \nThis is what we would expect to get given the symmetry of the problem. There is no preference for \nthe left or right side of the well, so the average value of a set of position measurements must be the \nmidpoint of the well. We get the same result for any energy eigenstate of the system.\nTo summarize, we have solved the problem of a particle bound in an inﬁnitely deep square poten-\ntial energy well, which means we have found the energy eigenvalues and eigenstates. The well is \ndepicted in Fig. 5.13(a), the spectrum of allowed energies is depicted in Fig. 5.13(b), and the wave \nfunctions of the energy eigenstates are depicted in Fig. 5.13(c). It is common practice to unify the three \ndiagrams of Fig. 5.13 in a single diagram, shown in Fig. 5.14, that represents the quantum mechani-\ncal potential energy well problem and its solution. The well, the energies, and the wave functions are \nsuperimposed on each other, such that different aspects of the diagram have different vertical axes. \nThe wave function for each energy eigenstate has its vertical coordinate origin located at the energy of \nthat state.\nn \u0002\u00033\n0\n(c)\nL/2\nL\nx\n\u0002Ψ\u00022\nn \u0002\u00031\n0\n(a)\nL/2\nL\nx\n\u0002Ψ\u00022\nn \u0002\u00032\n0\n(b)\nL/2\nL\nx\n\u0002Ψ\u00022\nFIGURE 5.12 Probability densities of the ﬁrst three energy eigenstates of the \ninﬁnite square potential energy well.\n",
    "5.4 Inﬁnite Square Well \n127\n0\nL\nx\nV(x)\n(a)\n\u0005\n0\n5\n10\n15\n20\n25\n(b)\nE /E1\nn \u0002\u00031\nn \u0002\u00032\nn \u0002\u00033\nn \u0002\u00034\nn \u0002\u00035\nL\nx\nn \u0002\u00035\nL\nx\nn \u0002\u00034\nL\nx\nn \u0002\u00033\nL\nx\nn \u0002\u00032\nL\n(c)\nx\nn \u0002\u00031\nΨ(x)\nFIGURE 5.13 (a) Inﬁnite square potential energy well, (b) spectrum of allowed energies, and \n(c) energy eigenstate wave functions.\n0\nL\nx\nn \u0002\u00031\nn \u0002\u00032\nn \u0002\u00033\nn \u0002\u00034\nn \u0002\u00035\nFIGURE 5.14 Uniﬁed schematic diagram of inﬁnite square well problem and solution. \nNote that two vertical scales are implied. For the potential energy well and the energy  \nspectrum, the vertical scale is energy with the origin at the bottom of the well. For the  \nwave functions, the vertical scale is probability amplitude 11>length1>22 with the \nc = 0 origin for each state centered on the energy of that state.\n",
    "128 \nQuantized Energies: Particle in a Box\nThe take home message of this problem is that the imposition of boundary conditions on the \nwave function limits the possible states that can “ﬁt” into the well and directly leads to the quantiza-\ntion of energy. This is a general result that we will return to time and again as we study other potential \nwell landscapes.\n5.5 \u0002 FINITE SQUARE WELL\nNow let’s make the problem a little more realistic by having the potential energy outside the well be ﬁnite \ninstead of inﬁnite. We still assume that the well is square, which still results in an inﬁnite force at the \nwalls. However, this new problem illustrates several important features of bound energy states that were \nnot evident in the inﬁnite well. A ﬁnite well can be used to model many real systems, such as an electron \nin a thin semiconductor. In Section 5.8, we use this model to discuss quantum well semiconductor lasers.\nThe ﬁnite square well potential energy is shown in Fig. 5.15 and is written as\n \nV1x2 = •\nV0,\nx 6 -a\n  0,\n-a 6 x 6 a\nV0,  \nx 7 a,\n \n(5.73)\nwhere we have deliberately chosen a different position origin from the inﬁnite well case in order to \ngive you practice and also for convenience. For now, we look for bound state solutions, that is, for \nenergies below the potential V0. Energies above V0 correspond to unbound states that we will discuss \nin the next chapter.\nWith this new potential energy function, the energy eigenvalue equation is\n \n a-  U2\n2m d 2\ndx 2 + 0b wE1x2 = EwE1x2,    inside box  \n \n a-  U2\n2m d 2\ndx 2 + V0b wE1x2 = EwE1x2,          outside box . \n(5.74)\n0\n\u0004a\na\nx\nV(x)\nV0\nFIGURE 5.15 Finite square potential energy well.\n",
    "5.5 Finite Square Well \n129\nIn the inﬁnite well problem, we found it useful to use the wave vector k\n \nk = A\n2mE\nU2 . \n(5.75)\nIn this case, it is also useful to deﬁne a similar constant outside the well\n \nq = A\n2m\nU2 1V0 - E2. \n(5.76)\nFor bound states, 0 6 E 6 V0, and therefore both k and q are real. We use these two constants to \nrewrite the energy eigenvalue equation:\n \n \nd 2wE1x2\ndx2\n= -k2wE1x2,    inside box  \n \n \nd 2wE1x2\ndx 2\n= q 2wE1x2,       outside box . \n(5.77)\nThe energy eigenvalue equation inside the box is identical to the one we solved for the inﬁnite well poten-\ntial. The differential equation outside the box is similar except the constant is positive instead of negative, \ngiving real exponential solutions rather than complex exponentials. Thus the solution outside the box is\n \nwE1x2 = Aeqx + Be-qx. \n(5.78)\nThis solution in the classically forbidden region is exponentially decaying, or growing, with a decay \nlength, or growth length, of 1Nq.\nThe energy eigenstate must be constructed by connecting solutions in the three regions shown in \nFig. 5.15. We write the general solution as\n \nwE1x2 = •\nAeqx + Be-qx,\nC sin kx + D cos kx,\nFeqx + Ge-qx,    x 6 -a\n -a 6 x 6 a\n x 7 a.\n \n(5.79)\nAs we discussed in the inﬁnite well problem, the solutions in the three regions must satisfy bound-\nary conditions where the regions connect. In constructing the inﬁnite well solutions, we used the \ncondition that the wave function must be continuous across a boundary. We now introduce a second \nrequirement that the slope of the wave function be continuous across a boundary. If the slope were \ndiscontinuous, that would imply an inﬁnite kinetic energy. However, this requirement has one excep-\ntion: it does not apply if the potential is inﬁnite (Problem 5.24), which is why we did not use it in the \ninﬁnite well problem. You can see in Fig. 5.14 that the inﬁnite well solutions have a change in slope \nat the edges of the box where the potential energy becomes inﬁnite. We now summarize these two \nboundary conditions:\n 1) wE1x2 is continuous\n 2) dwE1x2\ndx\n is continuous unless V = \u0005.\nBefore we impose the boundary conditions, we make two immediate simpliﬁcations to the gen-\neral solutions in Eq. (5.79). In the regions outside the well, the wave function must be a decaying \n",
    "130 \nQuantized Energies: Particle in a Box\nexponential because a growing exponential term all the way out to inﬁnity would not permit the wave \nfunction to be normalized. This normalization condition, which can also be termed a boundary condi-\ntion at inﬁnity, requires that B = F = 0 in Eq. (5.79). The second simpliﬁcation comes from recog-\nnizing that the potential energy is symmetric with respect to the origin 3V1x2 = V1-x24. This means \nthat the energy eigenstates will either be symmetric or antisymmetric (even or odd). This symmetry \nis evident in the inﬁnite well solutions shown in Fig. 5.14. (This can also be discussed in terms of the \ncommutation of the Hamiltonian and the parity operator, which we discuss in Section 7.6.4) We can \nthus solve for the two sets of solutions independently. If you don’t impose this symmetry condition \nnow, it will come out naturally after some algebra on the general solutions anyway (Problem 5.14). \nWith these two simpliﬁcations, the even solutions reduce to\n \nweven1x2 = •\nAeqx,\nD cos1kx2,\nAe-qx,\n   \nx 6 -a\n-a … x … a\nx 7 a.\n \n(5.80)\nThe odd solutions are\n \nwodd1x2 = •\nAeqx,\nC sin1kx2,\n-Ae-qx,\n   \nx 6 -a\n-a … x … a\nx 7 a.\n \n(5.81)\nLet’s ﬁrst do the even solutions. The boundary conditions at the right side of the well 1x = a2 give\n \n weven1a2: D cos1ka2 = Ae-qa\n \n \n \ndweven1x2\ndx\n`\nx=a \n: -kD sin1ka2 = -qAe-qa. \n(5.82)\nThe boundary conditions at the left side of the well 1x = -a2 yield the same equations, which must be \ntrue because of the symmetry. The two equations above have three unknowns: the amplitudes A and D \nand the energy E, which is contained in the parameters k and q. The normalization condition provides \nthe third equation required to solve for all three unknowns. We ﬁnd the energy condition rather simply \nby dividing the two equations, which eliminates the amplitudes and yields\n \nk tan1ka2 = q. \n(5.83)\nBecause both k and q are functions of the energy, this equation gives us a formula to ﬁnd the allowed \nenergies. It is independent of the constants A and D, which are found by applying the normalization \ncondition and using Eq. (5.82) again. As usual with these types of problems, the eigenvalue condi-\ntion is obtained ﬁrst, and then the eigenfunctions are obtained later. To make the energy dependence \nexplicit, we use Eqs. (5.75) and (5.76) to write Eq. (5.83) as\n \nA\n2m\nU2  E tana A\n2m\nU2  E ab = A\n2m\nU2  1V0 - E2. \n(5.84)\nThe next step is to solve this transcendental equation for the energy E.\nFor the odd solutions, a similar argument leads to the transcendental equation (Problem 5.15)\n \n-k cot1ka2 = q. \n(5.85)\n",
    "5.5 Finite Square Well \n131\nA graphical solution for the allowed energies using these two transcendental equations is most useful \nhere. There are many ways of doing this. One way involves deﬁning some new dimensionless parameters:\n \n z = ka = B\n2mEa2\nU2\n \n \n z0 = B\n2mV0a2\nU2\n \n \n qa = B\n2m1V0 - E2a2\nU2\n, \n \n(5.86)\nwhere the variable z parameterizes the energy of the state and the constant z0 characterizes the strength \nof the potential energy well. These deﬁnitions lead to the convenient expressions\n \n 1ka2\n2 + 1qa2\n2 = z 2\n0\n \n \n 1qa2\n2 = z 2\n0 - 1ka2\n2 = z 2\n0 - z 2. \n \n(5.87)\nThis allows us to write the transcendental equations in this form:\n \n ka tan1ka2 = qa  S  z tan1z2 = 4z 2\n0 - z 2\n \n \n -ka cot1ka2 = qa  S  -z cot1z2 = 4z 2\n0 - z 2. \n \n(5.88)\nIn each of these new transcendental equations, the left side is a modiﬁed trig function, while the right \nside is a circle with radius z 0. These functions are plotted in Fig. 5.16 as a function of the parameter \nz. The intersection points of these curves determine the allowed values of z and hence the allowed \nenergies En through Eq. (5.86). Because the constant z 0 is the radius of the circle, there are a limited \nnumber of allowed energies, and that number grows as z 0 gets larger. Wells that are deeper and wider \nhave more allowed bound energy states. \nThat’s it for the energies. There is no simple formula—the transcendental equations must be \nsolved graphically or numerically for each different well. For example, the curves in Fig. 5.16 corre-\nspond to a well with z 0 = 6, which results in four intersection points and hence four bound states. The \nintersection points and four allowed energies are\n \n z1 = 1.34 S E1 = 1.81 U2\n2ma2\n \n \n z2 = 2.68 S E2 = 7.18 U2\n2ma2\n \n \n z3 = 3.99 S E3 = 15.89 U2\n2ma2  \n \n z4 = 5.23 S E4 = 27.31 U2\n2ma2 . \n \n(5.89)\nThe energy eigenstate wave functions are characterized by the allowed values of the parameters \nk and q from Eq. (5.86). All that remains to do is normalize the wave function, which is straightfor-\nward but tedious (Problem 5.16). Once again, we use a uniﬁed diagram to show the potential energy \nwell, the allowed energies, and the allowed eigenstate wave functions superimposed in Fig. 5.17. \n",
    "132 \nQuantized Energies: Particle in a Box\n0\n\u0004a\na\nx\nn \u0002\u00031\nn \u0002\u00032\nn \u0002\u00034\nn \u0002\u00033\nFIGURE 5.17 Uniﬁed schematic diagram of the ﬁnite potential energy well and the bound state \nsolutions, showing the well, the allowed energies, and the energy eigenstate wave functions.\n0\n\u000b\n2\n\u000b\n3\u000b\n2\nz\n\u000b\n2\n\u000b\n3\u000b\n2\n2\u000b\nf(z)\nz tanz\n_z cotz\nz02\u000b\nz0\nz2\n0 \n_ z 2\nFIGURE 5.16 Graphical solution of the transcendental equations for the allowed energies of a ﬁnite \nsquare well 1z0 = 62.\n",
    "5.6 Compare and Contrast \n133\nNote that the ﬁnite well eigenstates share many features with the inﬁnite well states, with one major \nexception—they extend into the classically forbidden region. Quantum mechanical particles have a \nﬁnite probability of being found where classical particles may not exist! This is a purely quantum \nmechanical effect and is commonly referred to as barrier penetration. The ability of the particle \nto penetrate the potential energy barrier leads to the phenomenon of tunneling, an example of which \nis radioactive decay. We’ll say more about these wave functions in a bit, but let’s ﬁrst check that our \nsolution is consistent with the solution we derived earlier for the inﬁnite energy well case.\nThe limit of an inﬁnitely deep well corresponds to the radius z0 in Fig. 5.16 going to inﬁnity, in \nwhich case the allowed values of z become the asymptotes of the modiﬁed trig functions, shown by the \ndashed lines in Fig. 5.16. These limits are the same as for the simple trig functions and yield\n \n zn = n p\n2 1 kna = n p\n2 \n \n kn = np\n2a ,\n \n \n(5.90)\nfrom which we recover the inﬁnite well energy eigenvalues:\n \nEn =\nn2p2 U2\n2m12a2\n2 . \n(5.91)\nNote that the width of the well is 2a here, whereas we called the width L in the inﬁnite well case. The \ninﬁnite well eigenstate wave functions for this symmetric well position are\n \n wn1x2 = A\n2\n2a\n cos npx\n2a ,    n = 1, 3, 5, ...  \n \n wn1x2 = A\n2\n2a\n sin npx\n2a ,    n = 2, 4, 6, ... . \n \n(5.92)\nThere are two sets of solutions because we chose a different coordinate system to solve the problem. \nIn the limit z0 S \u0005, the decay length q becomes zero and the energy eigenstates are zero outside the \nwell, as expected. The inﬁnite well eigenstates are shown in Fig. 5.18(a) for this new choice of coor-\ndinates. Comparing the wave functions in Fig. 5.18(a) with those from Fig. 5.14, though, we see that \nthese are the same eigenstate wave functions that we found before. In Fig. 5.18(b) we show the ﬁnite \nwell states for comparison.\n5.6 \u0002 COMPARE AND CONTRAST\nNow that we have solved two similar problems, the inﬁnite and ﬁnite square wells, let’s discuss some \nof the important features of these solutions and see which features are common to both problems and \nothers, and which are distinct.\n 5.6.1 \u0002 Wave Function Curvature\nThe ﬁrst common feature is that the wave function is oscillatory 1sin kx or cos kx2 inside the well and \nexponentially decaying (e-qx or eqx) outside the well. This aspect is explained by examining the curvature \n(i.e., second derivative) of the wave function. To see this, we rewrite the energy eigenvalue equation\n \nd 2wE 1x2\ndx2\n= -  2m\nU2  3E - V 1x24wE 1x2, \n(5.93)\n",
    "134 \nQuantized Energies: Particle in a Box\nwhich then directly relates the wave function curvature to the difference between the energy E and \nthe potential energy V(x). Thus, inside the well, in the classically allowed region, we have E 7 V1x2 \nand the differential equation admits only sinusoidal solutions characterized by the wave vector k or \nwavelength l = 2p>k. Outside the well, in the classically forbidden region, we have E 6 V1x2 and \nthe differential equation admits only real exponential solutions with a decay length of 1>q, which is \nzero for the inﬁnite square well. The growing exponential terms in these problems are excluded by the \nnormalization requirement (i.e., the boundary condition at inﬁnity).\nThese comments can be generalized as shown in Fig. 5.19. Equation (5.93) tells us that in a clas-\nsically allowed region where E 7 V, the curvature has the opposite sign to the wavefunction, and in \nthe classically forbidden region where E 6 V, the curvature has the same sign as the wavefunction. \nThis means that in the classically allowed region the wave function is concave toward the axis, while \nin the classically forbidden region the wave function is convex toward the axis, as shown in Fig. 5.19.\nWe can also make some general observations regarding the length scales of the wave functions. In \na general potential well, the wave vector is given by\n \nk =\n22m 1E - V2\nU\n. \n(5.94)\nHence, the oscillatory part of the wave function (inside the well) has a characteristic wavelength\n \nl = 2p\nk\n=\nh\n22m 1E - V2\n\f\n1\n2T\n. \n(5.95)\nSo the larger the energy difference between the eigenvalue and the potential energy (i.e., the larger \nthe kinetic energy), the smaller the wavelength. That relationship is evident in the eigenstates shown \nin Fig. 5.18; the higher the energy, the more “wiggly” the wave function. In the forbidden region, the \ndecay constant \n \nq =\n22m 1V - E2\nU\n \n(5.96)\n\u0004a\n0\na\nx\n0\n\u0004a\na\nx\n(a)\n(b)\nFIGURE 5.18 (a) Inﬁnite and (b) ﬁnite well energy eigenstates.\n",
    "5.6 Compare and Contrast \n135\ndecreases as the energy increases toward V, which means that the decay length becomes larger. Hence, \nfor higher energy states the wave function penetrates further into the classically forbidden region \n(Problem 5.17). This increasing penetration with increasing energy is evident in the ﬁnite well states \nof Fig. 5.17.\nIn comparing the ﬁnite and inﬁnite well energies in Fig. 5.18, we also note that a given ﬁnite well \nenergy eigenvalue En lies below the corresponding inﬁnite well energy eigenvalue. This is consistent \nwith the longer wavelength of the ﬁnite well eigenstate compared to the corresponding inﬁnite well \nstate. For the ﬁnite well eigenstate to “ﬁt” in the well, the wavelength can be longer because part of the \nwave function is outside the well. The increasing penetration of the wave function into the classically \nforbidden region with increasing energy implies that the difference in energies between the ﬁnite and \ninﬁnite wells is larger for higher energies, as is also evident in Fig. 5.18 (Problem 5.19).\n 5.6.2 \u0002 Nodes\nThe ground state has a single antinode in the wave function, with each subsequent higher state acquiring \nan extra antinode. Thus the nth energy level has n antinodes and 1n - 12 nodes. This is a general char-\nacteristic of the energy eigenstates of any potential energy well. In the inﬁnite well we found an inﬁnite \nnumber of states. In the ﬁnite well we found a ﬁnite number of states, but we looked only for bound \nstates. We will see later that there are an inﬁnite number of unbound states with E 7 V0, which means \nthat there are an inﬁnite number of total allowed energy states. The inﬁnite number of states is a common \nfeature of potential energy wells. In the ﬁnite well, if the well is small enough (small V0 and/or small a), \nthen there might be only one bound state, but there is always at least one bound state. This is generally \ntrue for any well shape. The delta-function potential is an extreme case (Problem 5.25).\n 5.6.3 \u0002 Barrier Penetration\nIn the ﬁnite potential well, the wave function is nonzero in the classically forbidden region. This \nimplies a ﬁnite probability that the quantum mechanical particle can be found where the classical \nparticle cannot. As mentioned above, this penetration of the wave function into the potential energy \nx\nE0\nE, Ψ\nExponential\nOscillatory\nAllowed\nForbidden\nV(x)\nΨ > 0, Ψ\" < 0\nΨ < 0, Ψ\" > 0\nΨ > 0, Ψ\" > 0\nΨ < 0, Ψ\" < 0\nFIGURE 5.19 Curvature of the energy eigenstate wave functions in the allowed and forbidden regions.\n",
    "136 \nQuantized Energies: Particle in a Box\nbarrier leads to the phenomenon of tunneling, which we explore in the next chapter. The wave function \nplots in Fig. 5.18 indicate that the barrier penetration is more pronounced for higher energy levels and \ncan become quite large for energies close to the top of the well. This aspect is clear quantitatively if we \nnote that the decay constant q in the forbidden region decreases as the energy increases, which means \nthat the decay length becomes larger, so more of the wave function is outside the well.\n 5.6.4 \u0002 Inversion Symmetry and Parity\nIn both square well problems, the allowed wave functions are either symmetric (even) or antisym-\nmetric (odd) with respect to the center of the well. In both cases, the potential energy well, and hence \nthe Hamiltonian, is symmetric with respect to the well center. We say that the Hamiltonian is invariant \nunder the parity operation x S -x. Because the Hamiltonian is invariant under the parity operation, \nit must commute with the parity operator, and hence the energy eigenstates are also eigenstates of \nthe parity operator. The symmetric states satisfy wn1x2 = +wn1-x2, have a parity eigenvalue +1, \nand are called even parity states. The antisymmetric states satisfy wn1x2 = -wn1-x2, have a parity \neigenvalue -1, and are called odd parity states. Identifying the parity of an energy eigenstate is useful \nbecause the parity of the state often indicates whether a particular matrix element involving that state \nis zero or not. For example, the probability of a transition between two energy eigenstates caused by \nincident laser light is proportional to the matrix element of the electric dipole operator (-ex in one \ndimension) between the two states:\n \n8wm 0\n -ex 0\n wn9 = -\nL\n\u0005\n- \u0005\nwm1x2ex wm1x2d 3r. \n(5.97)\nThis integral is zero if the integrand has odd parity. The electric dipole operator has odd parity, so the \nenergy eigenstates must have different parity for the transition to be allowed. If the integral is zero, then \nthe transition is a forbidden transition. Many of the selection rules that determine which transitions \nare allowed and which are forbidden come from these types of parity arguments. More complete dis-\ncussion of electric dipole transitions must wait until we discuss time-dependent perturbation theory in \nChapter 14.\n 5.6.5 \u0002 Orthonormality\nThe energy eigenstates form an orthonormal set, as we have found for other sets of eigenstates, such \nas spin states. The normalization is not an intrinsic property of the solutions but rather something that \nwe impose so that the total probability of ﬁnding the particle somewhere is unity. The orthogonality is \na fundamental trait of eigenstates of Hermitian operators. The orthonormality condition is expressed \nin Dirac notation as\n \n8En0 Em9 = dnm \n(5.98)\nand in wave function language as\n \nL\n\u0005\n- \u0005\nw*\nn 1x2wm1x2dx = dnm. \n(5.99)\nThis condition is straightforward to show for the inﬁnite well states (Problem 5.12) but is a little tedious \nfor the ﬁnite well states because of the lack of a general expression for the allowed wave vectors.\n",
    "5.7 Superposition States and Time Dependence \n137\n 5.6.6 \u0002 Completeness\nThe energy eigenstates form a complete basis, as we have found for other sets of basis states. Com-\npleteness is also a fundamental trait of eigenstates of Hermitian operators. Completeness means \nthat we can use these basis functions to construct all possible solutions to the Schrödinger equation \nH0 c9 = iU d 0 c9>dt for this problem. The wave function of a general superposition state is\n \nc1x2 = a\nn\ncn\n wn1x2. \n(5.100)\nNote that the energy eigenvalue equation Hwn1x2 = En\n wn1x2 is satisﬁed by each particular energy \neigenstate in turn but is not satisﬁed by general superposition states. For the inﬁnite well, Eq. (5.100) \nis exact, while for the ﬁnite well we must also include unbound energy states above the well in the sum \nover basis states. Obviously, for a well that is so small that there is only one bound state, we would \nexpect to need more states to form a complete basis. The completeness relation is also called the clo-\nsure relation and, as we saw in the spins problem, is expressed as a sum of all the projection operators\n \na\nn\n0 En98En0 = 1, \n(5.101)\nwhere the right-hand side is understood to be the identity operator\n 5.7 \u0002 SUPERPOSITION STATES AND TIME DEPENDENCE\nSolving for the energy eigenvalues and eigenstates is an important aspect of any problem, but it is not \nthe only goal. As physicists, our aim is to predict the future of a physical system. In quantum mechan-\nics, we do this through the Schrödinger equation\n \nH0 c9 = iU d\ndt\n 0 c9 \n(5.102)\nthat governs the time evolution of any quantum system. Though different systems clearly have differ-\nent Hamiltonians, we need not solve the Schrödinger equation for the time evolution separately for \neach system. We have already solved it in Chapter 3 for a time-independent Hamiltonian, where we \nfound that the most general time-dependent solution to the Schrödinger equation is\n \n0 c1t29 = a\nn\ncn\n e-iEnt>U0 En9. \n(5.103)\nThat is, the energy eigenstates form the preferred basis in which to expand a general quantum state \nvector, with the time evolution determined by phase factors dependent on the energy of each compo-\nnent state. In a general superposition, each energy eigenstate acquires a different phase. It is critical \nto remember that one must use the energy basis in order to use this simple recipe for time evolu-\ntion. This is why we spend much of our time ﬁnding energy eigenstates.\nTo use Eq. (5.103) we need to know the expansion coefﬁcients cn for the particular state in ques-\ntion. The quantum state at time t = 0 is\n \n0 c 1029 = a\nn\ncn0 En9, \n(5.104)\n",
    "138 \nQuantized Energies: Particle in a Box\nso the expansion coefﬁcients cn are determined by the initial state of the system. The coefﬁcients cn are \nthe probability amplitudes for the state 0 c1029 to be in the energy eigenstates 0 En9\n \ncn = 8En0 c1029. \n(5.105)\nTo show this again, we perform a manipulation with the closure relation in Eq. (5.101). The identity \noperator does not change the state vector, so we act on the state vector to obtain\n \n 0 c1029 = 10 c1029\n \n \n = e a\nn\n0 En98En0f 0 c1029 \n \n = a\nn\n0 En98En0 c1029\n \n \n = a\nn\n8En0 c1029 0 En9\n \n \n(5.106)\nand hence identify the coefﬁcients cn as given in Eq. (5.105).\nOf course, once we know the probability amplitudes, we can calculate the probabilities for mea-\nsuring the system to have one of the energy eigenvalues:\n \nPEn = 08En0 c1029 0\n2 = 0 cn0\n2. \n(5.107)\nWe showed in Chapter 3 that the probabilities of energy measurements are time independent, but let’s \ndo it again here, using the time-dependent state vector in Eq. (5.103)\n \nP En = 08En0 c1t29 0\n2 \n \n = `8En0 a\nm\ncm0 Em9e-iEmt>U `\n2\n= ` a\nm\ncm8En0 Em9e-iEmt>U `\n2\n \n \n = ` a\nm\ncmdmne-iEmt>U `\n2\n= 0 cne-iEnt>U0\n2 \n \n = 0 cn0\n2.\n \n \n(5.108)\nThe Kronecker delta from the energy eigenstate orthonormality condition collapses the sum to a single \nterm. Time independence of the energy probabilities implies that the expectation value of the energy is \nalso time independent:\n \n8H9 = a\nn\nPEnEn = a\nn\n0 cn0\n2En. \n(5.109)\n",
    "5.7 Superposition States and Time Dependence \n139\nWe can also show this by explicit calculation with the time-dependent states:\n \n 8H9 = 8c1t2 0 H0 c1t29\n \n \n = a\nm\nc*\nm8Em0 eiEmt>UHa\nn\ncn0 En9e-iEnt>U \n \n = a\nm,n\nc*\nmcneiEmt>Ue-iEnt>U8Em0 H0 En9\n \n \n = a\nm,n\nc*\nmcnei 1Em-En2 t>U En8Em0 En9\n \n \n = a\nm,n\nc*\nmcnei 1Em-En2 t>U Endmn\n \n \n = a\nn\nc*\nncnEn\n \n \n = a\nn\n0 cn0\n2\n En.\n \n \n(5.110)\nNote that we had no need to use wave function notation in these calculations. Wave function calcula-\ntions of Eqs. (5.108) and (5.110) would require spatial integrals that would also yield the Kronecker \ndelta from the energy eigenstate orthonormality condition that collapses the sums. The results would \nclearly be the same, so the message is: if you can avoid integrals by using Dirac notation instead of \nwave function notation, do so.\nWe need to use wave function language to answer questions about the spatial distribution of the \nparticle, so let’s use the rules we developed in Section 5.3 to translate the Dirac notation equations \nto wave function notation. The time evolution of the state vector [Eq. (5.103)], in wave function \nlanguage, is\n \nc1x, t2 = a\nn\ncn\n wn1x2e-iEnt>U. \n(5.111)\nTo ﬁnd the expansion coefﬁcients cn (i.e., the probability amplitudes), we translate Eq. (5.105) to wave \nfunction language:\n \ncn =\n \nL\n\u0005\n- \u0005\nw*\nn1x2c1x, 02dx. \n(5.112)\nSo, given the initial wave function of the system c1x, 02, the expansion coefﬁcients are overlap inte-\ngrals between each energy eigenstate and the initial wave function. These overlap integrals are analo-\ngous to the integrals used to ﬁnd Fourier expansion coefﬁcients. Let’s brieﬂy illustrate the Fourier \napproach for calculating the coefﬁcients cn. Set the time equal to zero in Eq. (5.111) to ﬁnd the initial \nwave function superposition:\n \nc1x, 02 = a\nn\ncn\n wn1x2. \n(5.113)\n",
    "140 \nQuantized Energies: Particle in a Box\nProject both sides of Eq. (5.113) onto the energy eigenstates by multiplying each side by w*\nm1x2 and \nintegrating over all space:\n \n \nL\n\u0005\n- \u0005\nw*\nm1x2c1x, 02dx =\n \nL\n\u0005\n- \u0005\nw*\nm1x2 a\nn\ncn\n wn1x2dx  \n \n = a\nn\ncn L\n\u0005\n- \u0005\nw*\nm1x2wn1x2dx \n(5.114)\n \n = a\nn\ncndnm\n \n \n = cm,\n \n \nyielding\n \ncm =\n \nL\n\u0005\n- \u0005\nw*\nm1x2c1x, 02dx \n(5.115)\nas we expected from Eq. (5.112). Once we have the wave function expansion coefﬁcients in the energy \nbasis, we can predict the future time evolution of the system. Then we can calculate any physical quan-\ntities we need to, such as probabilities and expectation values.\nExample 5.4 Consider a particle in an inﬁnite square well with the initial wave function\n \nc 1x, 02 = A J a x\nLb\n3\n- 11\n7\n a x\nLb\n2\n+ 4\n7\n a x\nLb R  \n(5.116)\nin the interval 0 6 x 6 L and zero elsewhere, as shown in Fig. 5.20. Find (i) the wave function at a \nlater time, (ii) the probabilities of energy measurements, and (iii) the expectation value of the energy.\n(i) First we must normalize the state to ﬁnd the constant A:\n \n 8c0 c9 = 1 =\n \nL\nL\n0\n0 c 1x, 020\n2\n dx\n \n \n = 0 A0\n2\nL\nL\n0\n J a x\nLb\n3\n- 11\n7\n a x\nLb\n2\n+ 4\n7\n a x\nLb R\n2\n dx = 0 A0\n2\n L\n735. \n(5.117)\nWe choose the constant to be real and positive and the normalized wave function is\n \nc1x, 02 = B\n735\nL  J a x\nLb\n3\n- 11\n7\n a x\nLb\n2\n+ 4\n7\n a x\nLb R. \n(5.118)\nNow perform the overlap integral to ﬁnd the expansion coefﬁcients:\n \n cn = 8En0 c9 =\n \nL\n\u0005\n- \u0005\nw*\nn 1x2  c 1x, 02dx\n \n \n =\n \nL\nL\n0 B\n2\nL\n sin anpx\nL b B\n735\nL  J a x\nLb\n3\n- 11\n7\n a x\nLb\n2\n+ 4\n7\n a x\nLb Rdx . \n(5.119)\n",
    "5.7 Superposition States and Time Dependence \n141\nDo the integral\n \n cn = 7230\nL\n b c3L \n1npx>L2\n2 - 2\n1np24\n sin anpx\nL b - L \n1npx>L2\n3 - 61npx>L2\n1np24\n cos anpx\nL bd\nL\n0\n \n \n -   11\n7\n c 2L \n1npx>L2\n1np23\n sin anpx\nL b - L \n1npx>L2\n2 - 2\n1np23\n cos anpx\nL b d\nL\n0\n \n(5.120)\n \n + 4\n7\n c L \n1\n1np22 sin anpx\nL b - L \n1npx>L2\n1np22\n cos anpx\nL b d\nL\n0\nr .\nEvaluate the limits and simplify:\n \n cn = 322 + 201-12\nn4230\n1np23\n \n \n = e\n2230\n1np2\n3  , if n is odd\n42230\n1np2\n3\n , if n is even .\n \n(5.121)\nThe ﬁrst few coefﬁcients are\n \nc1 = 0.3533  \n \nc2 = 0.9274  \n \nc3 = 0.0131  \n \nc4 = 0.1159 , \n(5.122)\nso the state is composed mostly of the ﬁrst excited state, which is evident from the shape of the \nwave function in Fig. 5.20.\nL/2\nL\nx\nΨ(x)\nFIGURE 5.20 An initial state wave function [Eq. (5.116)] in the inﬁnite square well.\n",
    "142 \nQuantized Energies: Particle in a Box\nThe wave function at later times is the superposition with each energy state evolved at its pre-\nscribed frequency\n \n c 1x, t2 = a\n\u0005\nn=1\ncn\n wn 1x2  e-iEnt = a\n\u0005\nn=1\ncnA\n2\nL\n sin npx\nL\n e-i n2p2Ut>2mL2 \n \n = A\n60\nL a\n\u0005\nn=1\n322 + 201-12\nn4\n1np2\n3\n sin npx\nL\n e-i n2p2Ut>2mL2.\n \n(5.123)\n(ii)  The probabilities of measuring the energy eigenvalues are the squares of the expansion \ncoefﬁcients:\n \n PEn = 08En0 c\n 1t290\n2 = 0 cn0\n2\n \n \n =\n30\n1np26 322 + 201-12\nn4\n2\n \n \n =\n120\n1np26 3221 + 2201-12\nn4. \n(5.124)\nThe energy probabilities are shown in the histogram in Fig. 5.21, reﬂecting the predominance of the \nsecond state.\n(iii) The expectation value of the energy is\n \n 8H9 = a\nn\nPEnEn = a\nn\n0 cn0\n2\n En\n \n \n = a\n\u0005\nn=1\n120\n1np26 1221 + 2201-12\nn2an2p2  U2\n2mL2 b\n \n \n =\na\n\u0005\nn=1,3,5...\n120\n1np26 an2p2U2\n2mL2 b +\na\n\u0005\nn=2,4,6...\n12014412\n 1np26\n an2p2U2\n2mL2 b \n \n =\n60U2\np4mL2 c\na\n\u0005\nn=1,3,5...\n 1\nn4 + 441 \na\n\u0005\nn=2,4,6...\n 1\nn4 d\n \n \n =\n60U2\np4mL2 c p4\n96 + 441 p4\n1440 d\n \n \n = 19 U2\nmL2 = 38\np2 E1 \u0002 3.85E1 ,\n \n(5.125)\nwhich is slightly smaller than the energy (E2 = 4E1) of the ﬁrst excited state, as expected from the \nhistogram in Fig. 5.21.\n",
    "5.7 Superposition States and Time Dependence \n143\nNotice that the energy expectation value, such as we calculated in Eq. (5.125), is time inde-\npendent regardless of whether the system is in an energy eigenstate or a general superposition of \nenergy eigenstates. On the other hand, the expectation values of position or momentum are time \nindependent when the system is in an energy eigenstate, but they are time dependent for a general \nsuperposition state. Let’s demonstrate this in the inﬁnite square well where the time dependence of \na general state is\n \nc 1x, t2 = a\nn\ncnA\n2\nL\n sin npx\nL\n e-i n2p2Ut>2mL2 . \n(5.126)\nConsider a simple superposition of two states in an inﬁnite well. If the initial state is\n \n0 c 1029 =\n1\n12 0 E19 +\n1\n12 0 E29, \n(5.127)\nthen the time-evolved state is\n \n0 c 1t29 =\n1\n12 0 E19e-iE1t>U +\n1\n12 0 E29e-iE2t>U. \n(5.128)\nThe wave function representation is\n \n c 1x, t2 =\n1\n12 w11x2e-iE1t>U +\n1\n12 w21x2e-iE2t>U\n \n \n = A\n1\nL\n c sin px\nL\n e-iE1t>U + sin 2px\nL\n e-iE2t>U d . \n \n(5.129)\nNow ﬁnd the expectation value of the position:\n \n 8x9 = 8c 1t2 0 x0 c 1t29\n \n \n = E 1\n12 8E10 eiE1t>U +\n1\n12 8E20 eiE2t>UF\n \nx E 1\n12 0 E19e-iE1t>U +\n1\n12 0 E29e-iE2t>UF\n (5.130)\n \n = 1\n2 38E10 x0 E19 + 8E20 x0 E29 + 8E10 x0 E29ei1E1-E22t>U + 8E20 x0 E19e-i1E1-E22t>U4. \nE1\nE2\nE3\nE4\nE\n0.5\n1.0\nP\nPE1\nPE2\nPE3\nPE4\nFIGURE 5.21 Histogram of the probabilities of energy measurements.\n",
    "144 \nQuantized Energies: Particle in a Box\nAgain notice that we are using Dirac notation to simplify the calculation. However, at this point we \nneed to use integrals to calculate the matrix elements. Let’s deﬁne them in general:\n \n 8x9n = 8En0 x0 En9 =\n \nL\nL\n0\nw*\nn1x2  x wn1x2dx =\n \nL\nL\n0\nx0 wn1x2 0 2 dx \n \n 8x9nk = 8En0 x0 Ek9 =\n \nL\nL\n0\nw*\nn1x2  x wk1x2dx . \n \n(5.131)\nWe calculated the ﬁrst matrix element, which is the expectation value of position in an energy eigen-\nstate, in Example 5.3. We saw that the answer is the midpoint of the well L>2. The second integral \ncomes from the cross term in the superposition:\n \n 8x9nk =\n \nL\nL\n0\nw*\nn1x2  x wk1x2dx\n \n \n = 2\nL L\nL\n0\n sin anpx\nL b x sin akpx\nL b dx \n \n = 2\nL\n a L\npb\n2\nL\np\n0\ny sin1ny2 sin1k y2 dy . \n \n(5.132)\nSimplify with a trig identity and integrate\n \n 8x9nk = 2\nL\n a L\npb\n2\nL\np\n0\ny 1\n2 3 cos1n - k2y -  cos1n + k2y4dy\n \n \n = 1\nL\n a L\npb\n2\n c\n cos1n - k2y\n1n - k22\n+\ny sin1n - k2y\n1n - k2\n-\n cos1n + k2y\n1n + k22\n-\ny sin1n + k2y\n1n + k2\nd\np\n0\n \n \n = 1\nL\n a L\npb\n2\n c\n cos1n - k2p\n1n - k22\n-\n cos1n + k2p\n1n + k22\n-\n1\n1n - k22 +\n1\n1n + k22d ,\n \n(5.133)\nyielding\n \n8x9nk =\n-4Lnk\np21n2 - k222 C1 - 1-12\nn+kD . \n(5.134)\nThis result is zero for states where n + k is even (i.e., if the states have the same parity). The results for \nthe two-state example are\n \n 8x91 = 8x92 = L\n2\n \n \n 8x912 = 8x921 = -  16L\n9p2 , \n \n(5.135)\n",
    "5.7 Superposition States and Time Dependence \n145\ngiving the ﬁnal result\n \n 8x9 = 8c1t2 0 x0 c1t29\n \n \n = 1\n2\n c L\n2 + L\n2 - 16L\n9p2 ei 1E1-E22 t>U - 16L\n9p2 e-i 1E1-E22 t>U d  \n \n = L\n2\n c 1 - 32\n9p2 cos a 3p2U\n2mL2 tb d .\n \n(5.136)\nThe position of this two-state superposition oscillates at the Bohr frequency 1E2 - E12>U.\nThe time-dependent position is also evident in the spatial probability density:\n \n P1x, t2 = 08x0 c1t290\n2 = 0 c1x, t20\n2\n \n \n = ` A\n1\nL\n c  sin px\nL\n e-iE1t>U +  sin 2px\nL\n e-iE2t>U d `\n2\n \n \n = 1\nL\n csin2 px\nL +  sin2 2px\nL\n+ 2 sin px\nL\n sin 2px\nL\n cos 1E2 - E12t\nU\nd . \n(5.137)\nThe oscillation of the probability density is depicted in the animation frames shown in Fig. 5.22, \nwhere the constant t is the oscillation period t = 2p>vBohr (see activity on time evolution of inﬁnite \nwell solutions). The superposition probability distribution “sloshes back and forth” in the well at the \nBohr frequency. This motion of the superposition state provides a model for how atoms and other \nbound systems radiate light. An electron undergoing this oscillatory motion accelerates and hence \nradiates electromagnetic energy. So far, our model does not account for the energy loss from this \nradiation, but we will address that in Chapter 14.\n0\nL/2\nL\nt/\u0004 = 0.0\nt/\u0004 = 0.1\nt/\u0004 = 0.2\nt/\u0004 = 0.3\nt/\u0004 = 0.4\nt/\u0004 = 0.5\nFIGURE 5.22 Time dependence of the probability distribution of a superposition state.\n",
    "146 \nQuantized Energies: Particle in a Box\nA calculation of the momentum expectation value (Problem 5.27) also yields a time-dependent result:\n \n 8  p9 = 8c1t20 p0 c1t29\n \n \n =\n \nL\nL\n0\nc*1x, t2aU\ni\n  d\ndxb c 1x, t2dx \n \n = 8\n3 U\nL\n sin a 3p2U\n2mL2 tb.\n \n(5.138)\nIf we compare Eqs. (5.136) and (5.138), we notice that the quantum mechanical position and momen-\ntum obey the classical relation p = mv, provided we restrict the relation to expectation values:\n \n8p1t29 = m \nd 8x1t29\ndt\n. \n(5.139)\nThis is another example of Ehrenfest’s theorem, which says that quantum mechanical expectation \nvalues obey classical laws.\n 5.8 \u0002 MODERN APPLICATION: QUANTUM WELLS AND DOTS\nThe square well potential problem has been a staple of quantum mechanics textbooks since the early \ndays. However, for many years it was only a textbook problem because no systems in nature could be \nmodeled accurately as a square well. The progress of semiconductor fabrication technology has changed \nthat, as we are now able to make artiﬁcial systems of square potential energy wells. Semiconductor \nquantum wells are now routinely used to fabricate diode lasers and other semiconductor devices.\nThe key advance that allowed fabrication of quantum well devices was the ability to grow pure \ncrystals of semiconductors using techniques such as molecular beam epitaxy (MBE) and metal-\norganic chemical vapor deposition (MOCVD). With these techniques, layers of semiconductors can be \ngrown with atomic scale precision, yielding structures with layers thin enough (several nm or less) for \nquantum effects to be important.\nA typical quantum well structure is shown in Fig. 5.23(a). Alternate layers of GaAs and AlGaAs \nare grown epitaxially on a GaAs substrate. GaAs and AlGaAs have similar crystal unit cell sizes that \npermit dislocation-free crystals to be grown. This lattice-matched growth is crucial to obtaining reli-\nGaAs substrate\nAlGaAs\nGaAs\nAlGaAs\nGaAs\nsubstrate\nAlGaAs\nGaAs\nAlGaAs\nconduction band\nvalence band\nEg\nGaAs\nEg\nAlGaAs\nVc(x)\nVv(x)\n(b)\n(a)\nFIGURE 5.23 (a) Structure and (b) potential energy diagram of a GaAs quantum well.\n",
    "5.9 Asymmetric Square Well: Sneak Peek at Perturbations \n147\nable devices. The band gap of GaAs (1.42 eV) is smaller than the band gap of AlGaAs (2.67 eV), so the \nelectrons in the conduction band and the holes in the valence band experience the different potentials \nshown in Fig. 5.23(b). Because the layers change on the atomic scale, this is as close to a square well \nas nature allows.\nWe can calculate the energy levels in the well using the same analysis we used for the ﬁnite square \nwell. Figure 5.24 shows the energy levels and how they vary with changes in the GaAs layer thickness. \nNote that there are only two or three bound states in the well for the range of thickness shown.\nFor making practical devices with quantum wells, there are two important features. First, the \nenergy levels can be adjusted, or “tuned,” by changing the thickness of the quantum well layer, as \nshown in Fig. 5.24, or by changing the stoichiometry of the surrounding AlxGa1 -  xAs layers to adjust \nthe band gap and hence the potential energy depth of the well. Second, the quantization of the electron \nenergy in the conﬁned well increases the number of electrons with speciﬁc energies (compared to the \ncontinuum of energies of unconﬁned electrons), which in turn increases the probability of creating \nphotons with the corresponding wavelengths. Hence, a semiconductor diode laser made with quantum \nwells is more efﬁcient than one made with bulk material, so quantum well diode lasers are now the \nmost common type of diode lasers in use.\nThe quantum well structure shown in Fig. 5.23 conﬁnes the electron in one dimension, but the \nelectrons are not conﬁned in the plane of the thin well. Further conﬁnement leads to quantum wires \n(2D conﬁnement) and quantum dots (3D conﬁnement). Quantum dots are semiconductor nanocrys-\ntals with a typical size range of 2–20 nm. The size of the dot determines the conﬁnement size and \nhence the wavelength of light emitted by the dot. A simple Web search reveals beautiful pictures of \nquantum dots glowing in a rainbow of colors.\n 5.9 \u0002 ASYMMETRIC SQUARE WELL: SNEAK PEEK AT PERTURBATIONS\nWhile the square potential wells we have studied in this chapter illustrate many of the ideas of bound \nstate wave functions, there is one important aspect that we have not encountered. All the square well \nsolutions have a constant wave vector and a constant wave function amplitude throughout the well, \nbecause the potential is constant throughout the well. To see how the wave vector and amplitude of \n5\n10\n15\n20 Well width (nm)\n20\n40\n60\n80\n100\nEnergy (meV)\n\u00021\u0003\n\u00022\u0003\n\u00023\u0003\nFIGURE 5.24 Energy levels in a GaAs quantum well as the thickness of the GaAs layer is changed.\n",
    "148 \nQuantized Energies: Particle in a Box\nan eigenstate can vary within the well, let’s make a slight modiﬁcation to the inﬁnite square well. \nConsider the well shown in Fig. 5.25, which is commonly referred to as the asymmetric square \nwell. By adding a “shelf” within the well, we now have two regions of constant but different poten-\ntial energy.\nThe potential energy for this asymmetric square well is\n \nV1x2 = μ\n\u0005,\n0,\nV0,\n\u0005,\n      \nx 6 0\n0 6 x 6 L>2\nL>2 6 x 6 L\nx 7 L.\n  \n(5.140)\nWe know that the inﬁnite potential outside the well demands that the energy eigenstates are zero outside \nthe well. Inside the well, we now have different energy eigenvalue equations in the left and right halves:\n \n a-  U2\n2m d 2\ndx2 + 0b wE 1x2 = EwE 1x2,    left half\n \n \n a-  U2\n2m d 2\ndx2 + V0b wE 1x2 = EwE 1x2, \n right half. \n \n(5.141)\nFor this discussion, let’s assume that the energy E is greater than the potential V0 so that the \nsolutions in each half of the well are sinusoidal. We then have different wave vectors in each half, \ndeﬁned by\n \n k1 = B\n2mE\nU2 , \n left half\n \n \n k2 = B\n2m 1E - V02\nU2\n,    right half, \n \n(5.142)\nwhich yields a smaller wave vector 1k2 6 k12 and hence larger wavelength of the wave in the right \nhalf. We know that the left-half solution must be a sine function in order to match the zero wave func-\ntion outside the well, so the general solution is\n \nwE 1x2 = e\nA sin k1x ,\nB sin k2x + C cos k2x ,   0 6 x 6 L>2\nL>2 6 x 6 L. \n(5.143)\n0\nL/2\nL\nV0\nx\nV(x)\n\u0002\nFIGURE 5.25 Asymmetric square well.\n",
    "5.9 Asymmetric Square Well: Sneak Peek at Perturbations \n149\nNow we apply the boundary condition on the wave function continuity at the middle and right side of \nthe well and the boundary condition on the continuity of the ﬁrst derivative of the wave function at the \nmiddle of the well (recall that the inﬁnite potential on the right means that the derivative condition is \nnot applicable). The three boundary conditions are\n \n wE 1L>22: A sin1k1L>22 = B sin1k2L>22 + C cos1k2L>22\n \n \n dwE1x2\ndx\n2\nx=L>2\n: k1A cos1k1L>22 = k2B cos1k2L>22 - k2C sin1k2L>22 \n \n wE 1L2: B sin k2L + C cos k2L = 0.\n \n \n(5.144)\nThese three equations contain four unknowns: the amplitudes A, B, and C, and the energy E through \nthe wave vectors k1 and k2. The normalization condition supplies the fourth equation required to solve \nfor all unknowns. By eliminating the amplitude coefﬁcients from the three boundary condition equa-\ntions, we arrive at a transcendental equation for the energy eigenvalues (Problem 5.28):\n \nk1 cos1k1L>22sin1k2L>22 + k2 cos1k2L>22sin1k1L>22 = 0. \n(5.145)\nThis looks a bit intimidating, so how do we know it’s correct? Well, we know what the solutions are \nfor the inﬁnite (symmetric) square well, which is the case where V0 = 0; so we can check to see if our \nsolution agrees with the inﬁnite square well solutions. This won’t tell us whether our solution is cor-\nrect, but we can at least make sure that it is not obviously wrong. If V0 = 0, then the two wave vectors \nare equal and the transcendental equation becomes:\n \n k1cos1k1L>22sin1k1L>22 + k1 cos1k1L>22sin1k1L>22 = 0 \n \n k1 sin31k1L>22 + 1k1L>224 = 0\n \n \n k1 sin k1L = 0.\n \n \n(5.146)\nIf we divide this result by k1, then we have the same equation  sin k1L = 0 that we had for the inﬁnite \nsquare well. So our intimidating result may well be correct.\nIn order to compare the asymmetric square well with the inﬁnite square well, it is useful to divide \neach transcendental equation by the factor k1 and plot the energy eigenvalue equations for the asym-\nmetric square well\n \ncos1k1L>22sin1k2L>22 + k2\nk1\n cos1k2L>22sin1k1L>22 = 0 \n(5.147)\nand for the inﬁnite square well:\n \nsin1k1L2 = 0. \n(5.148)\nA plot of the two equations as a function of k1L is shown in Fig. 5.26 for the case where the potential \nstep height is 0.75 times the energy of the ground state in the inﬁnite well case. The inﬁnite square well \neigenstates occur at the values k1L = np marked on the axis. The eigenstates for the asymmetric well \nare each slightly larger, with the difference decreasing as the energy increases. This is a sneak preview \nof perturbation theory that we will study in Chapter 10.\nLet’s now use these solutions to draw the energy eigenstates. A plot of a typical energy eigen-\nstate is shown in Fig. 5.27. The wavelength and the amplitude of the wave in the right half are larger, \nmeaning that the probability to ﬁnd the particle in the right half is larger than in the left half. This is \nconsistent with our classical expectation, because a classical particle moves more slowly in the right \nhalf where its kinetic energy is lower, and so it spends more time in the right half with an increased \nprobability to ﬁnd it there.\n",
    "150 \nQuantized Energies: Particle in a Box\n 5.10 \u0002 FITTING ENERGY EIGENSTATES BY EYE OR BY COMPUTER\n 5.10.1 \u0002 Qualitative (Eyeball) Solutions\nThe problems we have solved in this chapter illustrate most of the important features of bound states in \npotential wells. Using these common traits allows us to make qualititative estimates of energy eigen-\nstate solutions to other potential well problems. The important features are\n 1(a). Oscillatory wave solution inside well\n 1(b). Wavelength proportional to 1> 2E - V1x2\n 2(a). Exponentially decaying solution outside well\n 2(b). Decay length proportional to 1> 2V1x2 - E\n3.  Amplitude inside well related to wavelength\n4.  Match wE1x2 and dwE1x2>dx at boundaries.\nUsing these rules of thumb, we can get a very good idea of the wave function before we tackle the dif-\nferential equation that gives us the exact solution.\nConsider the potential shown in Fig. 5.28. It has an inﬁnite wall, a ﬂat potential region, a sloped \npotential region, and a ﬁnite wall. Given our rules, we draw the approximate wave function. From left \nx\nE7\n0\nL/2\nL\nV0\nFIGURE 5.27 An energy eigenstate of the asymmetric square well.\nΠ\n2Π\n3Π\n4Π\nk1L\nF(k1L)\nFIGURE 5.26 Transcendental equations for the energy eigenvalues of \nthe asymmetric square well (solid) and the inﬁnite square well (dashed).\n",
    "5.10 Fitting Energy Eigenstates by Eye or by Computer \n151\nto right, starting at zero at the inﬁnite wall, the wave function oscillates with a constant wavelength and \nhas a constant amplitude over the ﬂat potential region; it oscillates with an increasing wavelength and \nhas an increasing amplitude over the sloped potential region; and then it exponentially decays in the \nclassically forbidden region. The wave function is drawn qualitatively and the main features are indi-\ncated. This wave function represents the 17th energy state because there are 17 antinodes in the wave \nfunction. Remember that the wave function oscillates about the value zero in the well and decays to \nzero outside the well. The ﬁgure shows the wave function c1x2 drawn superimposed on the potential \nwell, so you have to imagine a “c axis” with its zero as indicated by the dashed line.\n 5.10.2 \u0002 Numerical Solutions\nWe can be more quantitative by using a computer to help us “draw” the wave functions. Rather than \nfollow the rules listed above, we directly solve the energy eigenvalue equation by numerical integra-\ntion, which is a common technique for solving differential equations and is easily accomplished in \ncommon mathematical packages like Matlab, Mathematica, and Maple, and even in a spreadsheet. The \nenergy eigenvalue equation is\n \nd 2wE1x2\ndx2\n= -  2m\nU2  3E - V1x24wE1x2. \n(5.149)\nYou may not yet know how to solve such a differential equation, but you do know how to solve a very \nsimilar one—Newton’s second law, F = ma, which yields the differential equation\n \nd 2x\ndt 2 = F\nm. \n(5.150)\nIn the case where the acceleration a = F>m is constant, one integral of Eq. (5.150) gives\n \nv = dx\ndt = v0 + at, \n(5.151)\nE,Ψ\nMatch Ψ\nMatch Ψ,dΨ\u0005dx\nOscillating wave\nExponential decay\nConstant Λ,\namplitude\nIncreasing Λ,\namplitude\nΨ\u0007\b\u0007\t\n0\nL/2\nL\nV0\nV0/2\nx\nFIGURE 5.28 Drawing approximate energy eigenstate solutions.\n",
    "152 \nQuantized Energies: Particle in a Box\nand a second integration gives\n \nx = x0 + v0\n t + 1\n2\n at 2, \n(5.152)\nwhich are the equations of motion you learned in introductory physics. With these equations, one can \npredict the future if one knows the initial position x0, the initial velocity v0, and the acceleration a.\nIn the Newtonian case, the motion function x(t) is determined by its curvature d 2x>dt 2, which is \nthe acceleration a. In the quantum case, the wave function is determined by its curvature d 2c>dx 2, which \ndepends on the energy, the potential, and the wave function itself. The potential and the wave function \nboth depend on position, so the wave function curvature is not constant and the simple integrations in \nEqs. (5.151) and (5.152) cannot be used. However, if the acceleration in the Newtonian example is not \nconstant, then we can modify Eqs. (5.151) and (5.152) for use on a computer by using them to predict \nmotion only in the very near future, say from t to t + \u0006t:\n \n x1t + \u0006t2 = x1t2 + v1t2\u0006t + 1\n2\n a1t21\u0006t2\n2 \n \n v1t + \u0006t2 = v1t2 + a1t2\u0006t.\n \n(5.153)\nAs long as we choose the time steps \u0006t small enough that the acceleration does not vary appreciably from \none time step to the next, then these equations can be used to reliably update the position and velocity at \neach time step. These update equations produce estimates of the full motion by iterating from step to step.\nThis method works well but suffers from one failing: the update equations use “old” information \nabout the velocity and the acceleration. We can improve this slightly by using the new acceleration in \nthe velocity update equation:\n \n x1t + \u0006t2 = x1t2 + v1t2\u0006t + 1\n2\n a1t21\u0006t2\n2\n \n \n v1t + \u0006t2 = v1t2 + 1\n2 3a1t2 + a1t + \u0006t24\u0006t. \n(5.154)\nWe can’t use the new acceleration in the position update equation because the acceleration typically \ndepends on position (through the potential), so we do the position update ﬁrst and then the modiﬁed \nvelocity update. This method is known as the velocity Verlet algorithm and yields more reliable \nresults than Eq. (5.153).\nTo solve the energy eigenvalue equation, we use the wave function and its spatial derivatives \nrather than the position and its time derivatives used in the Newtonian case. Thus, we generalize the \nposition and velocity update equations (5.154) to\n \n wE1x + \u0006x2 = wE1x2 + adwE\ndx b\nx  \n\u0006x + 1\n2\n ad 2wE\ndx2 b\nx\n1\u0006x2\n2\n \n \n adwE\ndx b\nx+\u0006x\n= adwE\ndx b\nx\n+ 1\n2\n c a d 2wE\ndx2 b\nx\n+ ad 2wE\ndx2 b\nx+\u0006x\nd \u0006x. \n(5.155)\nSo, given the wave function (analogous to “position”), the slope of the wave function (“velocity”), and \nthe curvature of the wave function (“acceleration”) at any position x (“time”), we can predict the wave \nfunction and its slope at the next position x + \u0006x. At each step we calculate the wave function curva-\nture using the energy eigenvalue equation\n \nd 2wE1x2\ndx2\n= -  2m\nU2  3E - V1x24wE1x2. \n(5.156)\n",
    "5.10 Fitting Energy Eigenstates by Eye or by Computer \n153\nWe don’t have to impose the continuity conditions on wE1x2 and dwE1x2>dx at boundaries; the \nupdate equations guarantee that they are met. What we do need are initial values of the wave function \nand the ﬁrst derivative to get the update equations started. In principle, we should start at x = - \u0005 \nand integrate (i.e., update) all the way to x = + \u0005. In practice, it sufﬁces to start a reasonable way \ninto the left-hand forbidden region, integrate into and through the potential well, and then integrate \na reasonable way into the right-hand forbidden region. The wave function in the forbidden region \nshould be decaying toward zero as it approaches x = { \u0005, which indicates how we should choose \nthe initial values of the wave function and the ﬁrst derivative. Recall, however, that the energy eigen-\nvalue equation is linear in the wave function wE1x2, so we can scale the wave function by any factor \nand it will still solve the differential equation. This means that we can choose the initial wave function \narbitrarily, but the resultant wave function will not be normalized. In principle, the initial wave func-\ntion slope should be chosen to have the appropriate decay length. In practice, the method is insensitive \nto this choice.\nNotice that the calculation of the wave function curvature from the energy eigenvalue equation \n(5.156) requires us to know the energy. But we don’t know the energy—we are trying to ﬁnd it! So we \nguess a value of the energy and then we solve for the resultant wave function and see if it “ﬁts” into the \npotential well. From the problems above we have plenty of practice recognizing wave functions that \nﬁt, so it should be clear. And it is, as you will see.\nAs an example of how this numerical technique works, let’s try it out on the ﬁnite square well \nand compare to the results in Eq. (5.89). We choose an energy and start integrating with Eq. (5.155). \nThis is well suited to a spreadsheet, and the results shown in Fig. 5.29 are from an Excel worksheet. \nThe trademark results of this technique are illustrated in Fig. 5.29(a). If the chosen energy does not \nmatch an energy eigenstate solution, then as we integrate toward x = + \u0005 the wave function solution \nthat should decay starts to grow exponentially, because as the integration crossed the boundary into \nthe classically forbidden region (at x = a) there was a small component of the growing exponential \nsolution contained in the numerical wave function. Only by choosing the energy exactly equal to one \nof the allowed energies can this “bad” component be eliminated from the integration. Because of the \nseverity of exponential growth, combined with the discreteness of computer calculations, it is impos-\nsible to ﬁnd the energy solution exactly. However, as Fig. 5.29 illustrates, you can ﬁnd nearby energies \nthat cause the wave function to grow either negatively [Fig. 5.29(a)] or positively [Fig. 5.29(c)]. These \nsolutions then bracket the approximate solution [Fig. 5.29(b)]. The ﬁnite square well used for the cal-\nculation in Fig. 5.29 is the same as the well used for Fig. 5.16, and the resultant energy eigenvalue of \nthis fourth energy level matches well with the result in Eq. (5.89). To obtain a more accurate value, one \nhas to be more careful about the initial conditions.\nΨ(x)\nΨ(x)\nΨ(x)\n(a)\n(b)\n(c)\n\na\na\nx\n\na\na\nx\n\na\na\nx\nE = 27.30454\nE = 27.30455\nE = 27.30456\nFIGURE 5.29 Numerical integration for solution of the ﬁnite square well eigenvalue equation.\n",
    "154 \nQuantized Energies: Particle in a Box\n 5.10.3 \u0002 General Potential Wells\nGiven our approximate and numerical techniques, we can solve for the bound states in any potential \nwell, in principle. A typical bound state solution is shown in Fig. 5.30. It exhibits the key features that \nwe have mentioned above for bound state solutions:\n• Oscillatory in allowed region\n• Exponential decay in forbidden region\n• Oscillatory wave becomes less wiggly near classical turning point as kinetic energy \ndecreases\n• Amplitude becomes larger near classical turning points\nThus, though potential energy wells may appear quite different at ﬁrst glance, they all can be \ncalled “particle-in-a-box” systems, albeit with differently shaped boxes. Some common boxes are \nshown in Fig. 5.31: (a) inﬁnite square well, (b) ﬁnite square well, (c) harmonic oscillator (mass on a \nspring), and (d) linear potential (bouncing ball potential).\nSUMMARY\nIn this chapter we learned the language of the wave function, which is the representation of the quan-\ntum state vector in position space. We express this as\n \n 0 c9 \u0003 c1x2  \n \n c1x2 = 8x0 c9. \n(5.157)\nThe complex square of the wave function yields the spatial probability density\n \nP1x2 = 0 c1x20\n2. \n(5.158)\nThe normalization condition is\n \n1 = 8c@ c9 =\n \nL\n\u0005\n- \u0005\n@ c1x2@\n2 dx = 1. \n(5.159)\nThe rules for translating bra-ket formulae to wave function formulae are:\n1) Replace ket with wave function \n0 c9 S c1x2\n2) Replace bra with wave function conjugate \n8c0 S c*1x2\n3) Replace bracket with integral over all space \n8@ 9 S\n \nL\n\u0005\n- \u0005\n dx\n4) Replace operator with position representation \nAn S A1x2.\nThe probability of measuring the position of a particle to be in a ﬁnite spatial region is\n \nPa6x6b =\n \nL\nb\na\n0 c1x20\n2 dx. \n(5.160)\n",
    "Summary \n155\nx\nE,Ψ\nFIGURE 5.30 Bound state in a generic potential energy well.\n0\n(a)\nL/2\nL\nx\n0\n(b)\n\na\na\nx\nx\n(c)\nx\n(d)\nFIGURE 5.31 Different versions of the particle-in-a-box: (a) inﬁnite square well, \n(b) ﬁnite square well, (c) harmonic oscillator (quadratic potential), and (d) linear potential.\n",
    "156 \nQuantized Energies: Particle in a Box\nThe probability of measuring the energy to be En is\n \nPEn = @8En@ c9@\n2 = 2\nL\n\u0005\n- \u0005\nw*\nn1x2c1x2dx 2\n2\n, \n(5.161)\nwhere wn1x2 = 8x0 En9 is the wave function representation of the energy eigenstate.\nPosition and momentum operators in the position representation are\n \n xn \u0003 x\n \n \n pn \u0003 -iU d\ndx \n(5.162)\nand lead to the energy eigenvalue equation becoming a differential equation:\n \na-  U2\n2m d 2\ndx2 + V1x2\n b wE1x2 = EwE1x2. \n(5.163)\nIn solving the energy eigenvalue equation, two boundary conditions are imposed upon the wave function:\n1) wE1x2 is continuous\n2) \ndwE1x2\ndx\n is continuous unless V = \u0005.\nIn an inﬁnite square potential energy well, the allowed energies are\n \nEn = n2p2\n U2\n2mL2\n ,   n = 1, 2, 3, ..., \n(5.164)\nand the allowed energy eigenstates are\n \nwn1x2 = A\n2\nL\n sin  npx\nL ,   n = 1, 2, 3, ... . \n(5.165)\nThe energy eigenstates obey the following properties:\nProperty\nDirac notation\nWave function notation\nNormalization\n8En0En9 = 1\nL\n\u0005\n- \u0005\n0wn1x20\n2dx = 1\nOrthogonality\n8En0Em9 = dnm\nL\n\u0005\n- \u0005\nw*\nn1x2wm1x2dx = dnm\nCompleteness\n0c9 = a\nn\ncn0En9\nc1x2 = a\nn\ncnwn1x2\nPROBLEMS\n 5.1 Show that the operators xn and pn do not commute.\n 5.2 A particle in an inﬁnite square well potential has an initial state vector \n0 c1t = 029 = A10 w19- 0 w29 + i0 w392 where 0 wn9 are the energy eigenstates.\n",
    "Problems \n157\na) Normalize the state vector.\nb)  What are the possible outcomes of a measurement of the energy, and with what probabilities \nwould they occur?\nc) What is the average value of the energy?\nd) Find the state vector at some later time, t.\ne)  At time t = U>E1, what are the possible outcomes of a measurement of the energy, and with \nwhat probabilities would they occur?\n 5.3 Solve the inﬁnite square well problem using the complex exponential form of the general solu-\ntion in Eq. (5.53) as the assumed form of the wave function inside the well. Assume that the \npotential well boundaries are at x = 0 and x = L.\n 5.4 Solve the inﬁnite square well problem with the well boundaries at x = {a. Comment on the \ndifferences and similarities with the solution in the text.\n 5.5 Calculate the expectation values and the uncertainties of position and momentum for the inﬁ-\nnite square well energy eigenstates.\n 5.6 For a particle in an inﬁnite square well, calculate the probability of ﬁnding the particle in the \nrange 3L>4 6 x 6 L for each of the ﬁrst three energy eigenstates.\n 5.7 A particle in an inﬁnite square well potential has an initial state vector \n0 c1t = 029 = 10 w19 - 2i0 w292> 15 where the 0 wn9 are the eigenfunctions of the Hamiltonian \noperator. Find the time evolution of the state vector.\n 5.8 A particle in an inﬁnite square well potential has an initial wave function \nc1x, t =\n 02 = Ax1L - x2. Find the time evolution of the state vector. Find the expectation \nvalue of the position as a function of time.\n 5.9 A particle in an inﬁnite square well has the initial wave function\n \nc1x, 02 = A c a x\nLb\n3\n-\n 3\n2\n a x\nLb\n2\n+ 1\n2\n a x\nLbd \n \n in the interval 0 6 x 6 L and zero elsewhere. Find (a) the wave function at a later time, (b) the \nprobabilities of energy measurements, and (c) the expectation value of the energy.\n 5.10 A particle at t = 0 is known to be in the right half of an inﬁnite square well with a probability \ndensity that is uniform in the right half of the well. What is the initial wave function of the par-\nticle? Calculate the expectation value of the energy. Find the probabilities that the particle is \nmeasured to have energy E1, E2, or E3.\n 5.11 A particle is in the ground state of an inﬁnite square well. The potential wall at x = L suddenly \nmoves to x = 3L such that the well is now three times its original size. Find the probabilities \nthat the particle is measured to have the ground state energy or the ﬁrst excited state energy of \nthe new well.\n 5.12 Show that the energy eigenstates of the inﬁnite square well are orthogonal.\n 5.13 Use the closure relation in Eq. (5.101) to show that the normalization condition is\n \n1 = 8c@ c9 = a\nn\n@8En@ c9@\n2. \n 5.14 Solve the energy eigenvalue problem for the ﬁnite square well without using the symmetry \nassumption and show that the energy eigenstates must be either even or odd.\n",
    "158 \nQuantized Energies: Particle in a Box\n 5.15 Derive the transcendental equation (5.85) for the energy eigenvalues of the odd states in the \nﬁnite square well.\n 5.16 Normalize the energy eigenstates of the ﬁnite square well.\n 5.17 Find the probability that a particle in the ground state of a ﬁnite square well is measured to \nhave a position outside of the well. Derive a general relation involving only the parameters z \nand z0 deﬁned in Eqs. (5.86). Show that the probability increases as the energy increases.\n 5.18 An electron is bound in a ﬁnite square well of depth V0 = 5 eV and width 2a = 1.5 nm. Find \nthe allowed energies of the bound states in the well using the transcendental equations (5.88).\n 5.19 Give a qualitative, graphical argument that the difference in energy eigenvalues between the \nﬁnite and inﬁnite square wells is larger for higher energy states.\n 5.20 Find the bound energy eigenstates and eigenvalues of a “half-inﬁnite” square well (i.e., a \nsquare well with inﬁnite potential for x 6 0 and ﬁnite potential with value V0 for x 7 L).\n 5.21 Consider a quantum system with a set of energy eigenstates 0 Ei9. The system is in the state\n \n0 c9 =\n1\n130 0 E19 +\n2\n130 0 E29 +\n3\n130 0 E39 +\n4\n130 0 E49, \n \n where the energies are given by En = nE1. Find the probabilities for measuring the energy \neigenvalues and make a histogram similar to Fig. 5.2(b). Find the expectation value of the \nenergy. Find the uncertainty of the energy.\n 5.22 Consider a quantum system with a set of energy eigenstates 0 En9 where the energies are given \nby En = 1n + 1\n22U v for n = 0, 1, 2, ... . The system is in the state\n \n0 a9 = a\n\u0005\nn=0\nane-a2>2\n2n!\n0 En9, \n \n where a is a positive real number. Find the probabilities for measuring the energy eigenvalues \nand make a histogram similar to Fig. 5.2(b). Find the expectation value of the energy. Find the \nuncertainty of the energy.\n 5.23 Consider the following wave functions\n \n c1x2 = Ae-x2>3\n \n c1x2 = B \n1\nx2 + 2\n \n c1x2 = C secha x\n5b.\n \n In each case, normalize the wave function, plot the wave function, and ﬁnd the probability that \nthe particle is measured to be in the range 0 6 x 6 1.\n 5.24 Demonstrate the requirement that the ﬁrst derivative of the wave function be continuous, \nunless the potential is inﬁnite. To do this, integrate the energy eigenvalue equation from -e \nto +e and take the limit as e S 0 to derive a condition on the difference of the wave function \nderivatives between two adjacent points.\n 5.25 Find the energy eigenstates and eigenvalues of a particle conﬁned to a delta function potential \nV1x2 = -b d1x2, where b is a positive real constant. Note that you will need to follow the \napproach in the previous problem to properly address how the inﬁnite potential at the origin affects \nthe wave function derivative. How many bound energy states exist in this potential energy well?\n",
    "Resources \n159\n 5.26 Find the energy eigenstates and eigenvalues of a particle conﬁned to a double delta function \npotential V1x2 = -b 1d1x - a2 + d1x + a22, where b is a positive real constant. How many \nbound energy states exist in this potential energy well?\n 5.27 Calculate the expectation value of the momentum for the two-state superposition in Eq. (5.128) \nand verify Eq. (5.138).\n 5.28 Solve the boundary condition equations (5.144) for the asymmetric square well and verify \nEq. (5.145).\n 5.29 Find the transcendental equation that determines the energy eigenvalues in an asymmetric \nsquare well for the case E 6 V0. Compare with Eq. (5.145) for the E 7 V0 case and comment.\n 5.30 Implement the update equations (5.155) using a spreadsheet or other computer program and \nﬁnd the numerical solutions for the energy eigenvalues of a ﬁnite square well with a well \nparameter z0 = 6. Compare your results with Eq. (5.89).\n 5.31 Use a spreadsheet or other computer program to ﬁnd the numerical solution of the ground \nstate and ﬁrst excited state energy eigenvalues and wave functions for a ﬁnite square well with \nparameters V0 = 5 eV, 2a = 1.5 nm, and m = me. Compare your results with the transcen-\ndental equations (5.88).\n 5.32 Reproduce the results for the GaAs quantum well states shown in Fig. 5.24 using the transcen-\ndental equations (5.88). The relevant GaAs parameters are V0 = 0.1 eV and m = 0.067 me.\n 5.33 For each of the potential wells shown in Fig. 5.32, make a qualitative sketch of the two energy \neigenstate wave functions whose energies are indicated. For each energy state, identify the clas-\nsically allowed and forbidden regions. Discuss the important qualitative features of each state.\n 5.34 Sketch a copy of Fig. 5.30 and identify the classically allowed and forbidden regions. Which \nenergy eigenstate is drawn in Fig. 5.30? Make a similar plot for the next lower energy eigenstate.\nRESOURCES\nActivities\nThe bulleted activities are available at\nwww.physics.oregonstate.edu/qmactivities\n• Operators and Functions: Students investigate the differential forms of quantum mechanical \noperators and identify eigenfunctions and eigenvalues of quantum mechanical operators.\n• Solving the Energy Eigenvalue Equation for the Finite Well: Students solve the energy eigenvalue \nequation for different regions of the ﬁnite well and make their solutions match at the boundaries.\nE,Ψ\nx\nE11\nE4\nE,Ψ\nx\nE10\nE5\nFIGURE 5.32 Potential wells for Problem 5.33.\n",
    "160 \nQuantized Energies: Particle in a Box\n• Time Evolution of Inﬁnite Well Solutions: Students animate wave functions consisting of linear \ncombinations of eigenstates.\nQuantum Bound States: This simulation experiment from the PHET group at the \nUniversity of Colorado animates wave function superpositions in bound states: \nhttp://phet.colorado.edu/en/simulation/bound-states\nShooting Method Model: This program from the Open Source Physics group implements \nthe shooting method to numerically solve the energy eigenvalue equation: \nhttp://www.compadre.org/osp/items/detail.cfm?ID=6987\nFurther Reading\nQuantum wells are discussed in these Physics Today articles:\nD. Chemla, “Quantum wells for photonics,” Phys. Today 38(5), 57–64 (1985): \nhttp://dx.doi.org/10.1063/1.880974\nD. Gammon, D. Steel, “Optical studies of single quantum dots,” Phys. Today 55(10), 36–41 (2002): \nhttp://dx.doi.org/10.1063/1.1522165\nFurther details on numerical solutions of the energy eigenvalue equation are available in these \nreferences:\nR. H. Landau, M. J. Páez and C. C. Bordeianu, A Survey of Computational Physics: Introductory \nComputational Science, Princeton, NJ: Princeton University Press, 2008.\nH. Gould, J. Tobochnik, and W. Christian, An Introduction to Computer Simulation Methods: \n Applications to Physical Systems (3rd edition), San Francisco, CA: Addison-Wesley, 2007.\n",
    " \n161\nC H A P T E R \n6\nUnbound States\nIn the last chapter we learned how to use the new concept of wave functions to describe the motion of \na particle in a potential well. We found that states corresponding to particles conﬁned within the poten-\ntial well had quantized energies. We now turn our attention to unbound states, and we will ﬁnd that the \nenergies are no longer quantized. The simplest case is that of the free particle with no potential affect-\ning the particle motion at all. The free particle states help us better understand the wave-particle dual-\nity of quantum mechanics. We then consider the case of particles that are affected by potentials but are \nnot bound. This includes potential wells where the energy is larger than the well depth and cases where \nthe potential has no localized minimum. Studying these unbound states is important in understanding \nscanning tunneling microscopy, nuclear alpha decay, and the scattering of particles.\nIn all cases, we are still charged with solving the energy eigenvalue equation\n \nHn 0 E9 = E0 E9 \n(6.1)\nwith the Hamiltonian operator\n \nHn = pn2\n2m\n + V 1xn2. \n(6.2)\nAs we did in the last chapter, we work in wave function language (i.e., in the position representation), \nand so the energy eigenvalue equation becomes a differential equation:\n \n HnwE 1x2 = EwE 1x2  \n \n a-  U2\n2m\n d 2\ndx 2 + V  1x2b  wE 1x2 = EwE 1x2  \n \n -  U2\n2m\n d 2\ndx2 wE 1x2 + V 1x2wE 1x2 = EwE 1x2. \n \n(6.3)\n6.1 \u0002 FREE PARTICLE EIGENSTATES\n6.1.1 \u0002 Energy Eigenstates\nFor a free particle, the potential energy function V1x2 is zero everywhere and the energy eigenvalue \ndifferential equation is\n \nd 2\ndx2 wE 1x2 =  -  2mE\nU2  wE 1x2. \n(6.4)\n",
    "162 \nUnbound States\nThis is the same differential equation we solved in Chapter 5 inside the square potential energy well. \nAgain, it is convenient to deﬁne a wave vector\n \nk2 = 2mE\nU2  \n(6.5)\nand write the differential equation as\n \nd 2\ndx2 wE 1x2 =  -k2wE 1x2. \n(6.6)\nThe solutions to this differential equation are the familiar sinusoidal functions, which we can \nexpress either as the trigonometric functions sin kx and cos kx or the complex exponential functions \ne+ikx and e-ikx. Note that the energy E must be positive, so the wave vector is real for this problem. It \nis more convenient in this problem to use the complex exponential functions, so we write the general \nsolution to the energy eigenvalue equation as\n \nwE 1x2 = Ae+ikx + Be-ikx, \n(6.7)\nwhere we need to account for both possible signs of the wave vector and A and B are normalization \nconstants.\nThe critical physical difference between a free particle 1with V1x2 = 02 and a bound particle is \nthe lack of a conﬁning potential. Because the wave function of the free particle is not required to “ﬁt” \ninto the potential energy well, there are no limitations on the wave functions and hence no quantization \nof the energy. Mathematically, there are not enough constraints on the two normalization constants A \nand B and the energy E (through the wave vector k). There are three unknowns in Eq. (6.7), but the \nnormalization condition is the only constraining equation. The result is that the energy is a continuous \nvariable, not quantized, in contrast to the bound-state solutions in Chapter 5. The continuous nature of \nthe energy has important ramiﬁcations, which we will explore. But ﬁrst, let’s look more closely at the \nphysics of quantum wave motion.\nTo understand free particle wave motion, let’s look at the time evolution of the energy eigenstates \nof Eq. (6.7). The time dependence of this state is obtained by applying the recipe for Schrödinger time \nevolution that we learned in Chapter 3. Because the state is already written in the energy basis, the \nSchrödinger time-evolution recipe says to multiply by a phase factor dependent on the energy of the \nstate, giving\n \n cE1x, t2 = wE1x2e-i Et>U\n \n \n = 1Aeikx + Be-ikx2e-i Et>U. \n(6.8)\nIf we use the Einstein energy relation E = U v, we can rewrite Eq. (6.8) in a suggestive way:\n \n cE1x, t2 = 1Aeikx + Be-ikx2e-ivt\n \n \n = Aei1kx-vt2 + Be-i1kx+vt2\n \n \n = Aeik1x-vt>k2 + Be-ik1x+vt>k2. \n \n(6.9)\nThis quantum wave function has the same form we know from classical waves—a function f  1x { vt2 \nwith the argument 1x { vt2. This functional form represents a wave that retains its shape as it moves, \nand any given point on that shape moves with a speed determined by the parameter \r, which in this \ncase yields 0 v0 = v>k. For the sinusoidal waves of this free particle state, such points of constant \nphase move at the phase velocity. The energy eigenstate has two parts—the ei1kx-vt2 part moving in \nthe positive x-direction and the e-i1kx+vt2 part moving in the negative x-direction. So now we know \n",
    "6.1 Free Particle Eigenstates \n163\nthat whenever we see a wave function with spatial dependence e{ikx, the sign of the wave vector in the \nexponent indicates the direction of motion. It is convenient to work with the wave  vector eigenstates\n \nwk1x2 = Aeikx \n(6.10)\nas long as we remember that we must use both positive and negative k values to make a general energy \neigenstate.\n6.1.2 \u0002 Momentum Eigenstates\nTo learn more about the phase velocity of the wave vector eigenstates, it is useful to study the momen-\ntum of these wave functions. Let’s operate on one of the states with the momentum operator, which is \na differential operator in the position representation:\n \n pnwk1x2 = a-iU d\ndxb Aeikx \n \n = -iU1ik2Aeikx\n \n \n = Ukwk1x2.\n \n \n(6.11)\nThus the action of the momentum operator on a wave vector eigenstate yields the same state with \na constant multiplier. Well, that is an eigenvalue equation! So the wave vector eigenstates are also \nmomentum eigenstates. The momentum eigenvalue equation is\n \npnwp1x2 =  pwp1x2 \n(6.12)\n 1 pn 0  p9 = p0  p9 in bra@ket notation2, so we have identiﬁed\n \np = Uk \n(6.13)\nas the momentum eigenvalue and\n \nwp1x2 = Aei px>U \n(6.14)\nas the momentum eigenstate. The momentum eigenstate wave function wp1x2 is a function of position \nand not of momentum—x is a variable and p is the particular momentum eigenvalue. The wave \nvector is related to the wavelength through k = 2p>l, so we can rewrite Eq. (6.13) as\n \np = h\nl  . \n(6.15)\nThis equation was introduced in the early days of quantum mechanics by Louis de Broglie and pro-\nvides the connection between the particle properties (momentum) and the wave properties (wave-\nlength) of a system. The de Broglie relation between momentum and wavelength is at the heart of \nthe wave-particle duality of quantum mechanics. We can turn Eq. (6.15) around to write an equation \ndeﬁning the de Broglie wavelength of a particle with momentum p:\n \nlde Broglie = h\np  . \n(6.16)\nThe momentum eigenstates are also energy eigenstates for the free particle, with energy [Eq. (6.5)]\n \nE = p2\n2m. \n(6.17)\n",
    "164 \nUnbound States\nThe fact that the momentum and energy operators share eigenstates is an important aspect of the free \nparticle problem and is a consequence of the general rule we discussed in Section 2.4 that commuting \noperators have common eigenstates 1like Sz and S2 sharing 0{9 states2 (Problem 6.5). A given momen-\ntum eigenstate has a deﬁnite energy given by Eq. (6.17), but a given energy state does not necessarily \nhave a deﬁnite momentum, because a general energy eigenstate is a superposition of the two momentum \nstates 0  p9 \u0003 wp1x2 and 0  -p9 \u0003 w-p1x2 with opposite momenta, as in Eq. (6.7). Because a given \nenergy state corresponds to multiple momentum states, we say that the energy state is degenerate with \nrespect to momentum. In the free particle case, the energy states are two-fold degenerate. This is our \nﬁrst example of degeneracy, but it will be more common once we address two- and three-dimensional \nsystems in Chapter 7.\nThe wave nature of the quantum mechanical description of the free particle is evident in Fig. 6.1, \nwhich shows the wave function of a momentum eigenstate. It is evident that a single wavelength char-\nacterizes the wave function, consistent with the single momentum of the eigenstate and the de Broglie \nrelation between wavelength and momentum. The wave function is complex, so we must plot both the \nreal and imaginary parts to completely describe the state.\nLet’s now return to the question of the phase velocity of the free particle eigenstates. A momentum \neigenstate has time dependence\n \n cp1x, t2 = wp1x2e-i Ept>U\n \n \n = Aei px>U e-i p2t>2m U \n \n = Aei p>U1x-pt>2m2.  \n \n(6.18)\nThis wave is moving at a speed of v = p>2m, which is half the speed of a classical particle \nvclassical = p>m. This apparent contradiction exists because we are using the phase velocity of the \nwave. As we will see in Section 6.2, the proper way to use a wave to describe a particle leads us to the \nconcept of “group velocity of a wave packet” as the more appropriate velocity.\nA more serious problem with the momentum eigenstates becomes evident if we examine the prob-\nability density of the state. Taking the complex square of the wave function yields the probability density\n \nP1x2 = 0 wp1x20\n2\n \n \n = w*\np1x2wp1x2\n \n \n = A*e-i px>U Aei px>U \n \n = 0 A0\n2.\n \n \n(6.19)\nx\nRe \u000bp\u0007x\b\nx\nIm \u000bp\u0007x\b\n(a)\n(b)\nFIGURE 6.1 Momentum eigenstate. Both the (a) real and (b) imaginary parts of the wave \nfunction extend to {\u0005. A single wavelength characterizes the momentum eigenstate.\n",
    "6.1 Free Particle Eigenstates \n165\nAs shown in Fig. 6.2, the probability density of a momentum eigenstate is a constant independent of \nposition, extending to inﬁnity. This presents us with two problems. Conceptually, we expect a particle \nto be localized to a small region of space, not spread out over an inﬁnite region. Mathematically, we \ncannot normalize the momentum eigenstates because the integral of the probability density over all \nspace is inﬁnite. This is a new and quite serious problem. All previous basis states we have encoun-\ntered have been normalizable. This lack of normalizability is a pathology of all continuous bases—\nthis one being our ﬁrst example. Fortunately, there is a solution to this mathematical problem that \nalso solves our conceptual problem. By constructing superpositions of momentum eigenstates to make \nwave packets, we get wave functions that are normalizable and are localized to ﬁnite regions of space. \nBefore we construct wave packets, it is useful to discuss some of the mathematical properties of the \nmomentum eigenstates.\nWe expect a set of basis states to exhibit three important properties. The states should be: (1) nor-\nmalized, (2) orthogonal, and (3) complete. All the discrete basis sets we have encountered have satisﬁed \nthese conditions, which we express in Dirac notation as\n \n 8ai0 aj\u0002i9 = 0   orthogonality \n \n 8ai0 ai9 = 1   normalization \n \n a\ni\n0 ai98ai0 = 1   completeness, \n \n(6.20)\nassuming a set of discrete eigenstates 0 ai9. The orthogonality and normalization conditions are com-\nbined into one orthonormality equation by using the Kronecker delta:\n \n8ai0 aj9 = dij. \n(6.21)\nTo adapt this orthonormality equation to a continuous basis, we need to use the continuous analog \nof the discrete Kronecker delta, which is the Dirac delta function. The Dirac delta function, writ-\nten d1x - x02, is a function that is zero at every value of x, except at x = x0, where it is inﬁnite (not \nunity). This inﬁnity means that the Dirac delta function does not strictly represent the normalization \ncondition, but it is consistent with the inﬁnite norm we found for the momentum eigenstates above. \nThus, we expect that the “orthonormality” condition for a continuous basis set of momentum states is\n \n8p\u000e 0\n p\u00049 = d1 p\u000e - p\u00042 \n(6.22)\nx\n\u0002\u000bp(x)\u00022\nFIGURE 6.2 Position probability distribution for a momentum eigenstate.\n",
    "166 \nUnbound States\nin Dirac notation. Using the rules developed in Chapter 5 for translating bra-ket notation to wave \n function notation, we express the inner product in Eq. (6.22) as an overlap integral\n \nL\n\u0005\n- \u0005\nw*\np\u000e1x2wp\u00041x2dx = d1 p\u000e - p\u00042. \n(6.23)\nThe momentum eigenstates deﬁned in Eq. (6.14) satisfy this new form of the  orthonormality \n equation, as long as we deﬁne the normalization constant A for the momentum eigenstates as \n (Problem 6.7)\n \nA =  \n1\n22pU\n. \n(6.24)\nAlthough continuous basis sets, such as the momentum basis, do not strictly satisfy the normalization \ncondition required by quantum mechanics, it is still practical to use Eqs. (6.22) and (6.23) to “normalize” \na basis, and we refer to this process as Dirac normalization. We thus write the “normalized” momen-\ntum eigenstates as\n \nwp1x2 =  \n1\n22pU\n ei px>U    . \n(6.25)\nIt is worth thinking about dimensions at this point. With the normalization of the momentum eigen-\nstates in Eq. (6.25), we see that the dimensions of the left hand side of Eq. (6.23) are 3length4>3U4, \nwhich from Eq. (6.16) are equivalent to 1>3p4 or inverse momentum. Thus, the Dirac delta function \nhas dimensions of the inverse of its argument. This is another difference from the Kronecker delta that \nwe have to live with.\nThe completeness of a basis implies that any function (relevant to the problem at hand) can \nbe written as a superposition of the basis states. Completeness is difﬁcult to prove mathematically, so we \ngenerally just assume that it is satisﬁed. In the discrete basis case, the completeness condition (closure \nrelation) in Eq. (6.20) is a sum of the projection operators over the discrete basis set. To change to a con-\ntinuous basis, we change the sum over the discrete label to an integral over the continuous label. For the \nmomentum eigenstates, the completeness condition is\n \nL\n\u0005\n- \u0005\n0  p98p0 dp = 1, \n(6.26)\nwhere we understand that the right hand side is the identity operator. To demonstrate how complete-\nness allows us to express any general state as a superposition of the basis states, insert Eq. (6.26) into \nthe Dirac expression for a wave function\n \n c1x2 = 8x0 c9\n \n \n = 8x0 b\nL\n\u0005\n- \u0005\n0  p98p0 dpr 0 c9 \n \n =\n \nL\n\u0005\n- \u0005\n8x0  p98p0 c9dp.\n \n \n(6.27)\nThe ﬁrst term 8x0 p9 in the integrand is the projection of the momentum eigenstate 0  p9 onto the posi-\ntion basis, which is the wave function representation wp1x2 of the momentum eigenstate. The second \nterm 8p0 c9 in the integrand is the projection of the general state 0 c9 onto the momentum basis 0  p9 \n(i.e., the probability amplitude for the general state 0 c9 to have momentum p). Given the rules of Dirac \n",
    "6.1 Free Particle Eigenstates \n167\nnotation, you might expect the probability amplitude 8p0 c9 to be written as 8p0 c9 = c 1 p2. However, \nthere is risk of confusion here with the wave function c1x2 because c1 p2 and c1x2 are not the same \nmathematical function with different arguments, but rather are different mathematical functions. To \navoid this possible confusion, it is common to use a different symbol for the momentum probability \namplitude, such as\n \nf1 p2 = 8p0 c9, \n(6.28)\nalthough such notation brings its own confusion between the different Greek symbols. The function \nf1 p2 is known as the momentum space wave function. As in the position case, the probability ampli-\ntude f1 p2 = 8p0 c9 is a continuous function that is the collection of numbers that represents the quan-\ntum state vector in terms of the momentum eigenstates. The wave function c1x2 and the momentum \nspace wave function f1 p2 are both representations of the state 0 c9, but they are representing that \nstate in different bases. Which basis we should use is up to us and is generally a matter of convenience \ndecided by what we wish to calculate. Using this deﬁnition of the momentum space wave function, we \nwrite Eq. (6.27) as\n \nc1x2 =\n \nL\n\u0005\n- \u0005\nwp1x2f1 p2dp, \n(6.29)\nwhich, in words, says that a general state 0 c9 \u0003 c1x2 can be decomposed into an integral (i.e., super-\nposition) over all momentum eigenstates 0  p9 \u0003 wp1x2 with a proportionality coefﬁcient given by the \nprobability amplitude f1p2 = 8p0 c9 for the general state to be measured in that particular momentum \nbasis state.\nIf we put the explicit form of the momentum eigenstates wp1x2 into Eq. (6.29), then the superposi-\ntion becomes\n \nc1x2 =  \n1\n22pU L\n\u0005\n- \u0005\nf1 p2ei px>U dp   . \n(6.30)\nThis should look familiar! It is the Fourier transform of the function f1 p2. Thus, quantum mechani-\ncal superpositions behave much like classical wave superpositions. In both cases, the Fourier trans-\nform represents a superposition of sinusoidal waves that combine to make a wave packet. We thus \nexpect that the connection in the opposite direction (i.e., writing the momentum space wave function \nin terms of the position space wave function) would be an inverse Fourier transform. We can show that \nthis is so by using our prescription for writing a probability amplitude in wave function language as an \noverlap integral. The momentum space wave function f1 p2 is a probability amplitude f1 p2 = 8p0 c9, \nand the rule for converting a Dirac bra-ket projection to wave function overlap integral is to convert the \nket 0 c9 to a wave function c1x2, the bra 8p0  to a wave function conjugate w*\np1x2 = e-i px>U> 12pU, and \nthen integrate over all space. Thus, we get\n \nf1 p2 =  \n1\n22pU L\n\u0005\n- \u0005\nc1x2e-i px>U dx   , \n(6.31)\nwhich we recognize as an inverse Fourier transform. Thus, we see that the connection between the \nmomentum space wave function f1 p2 and the (position space) wave function c1x2 is the Fourier \ntransform. As we saw in the spins case, we are free to use whichever representation of a quantum state \nvector that we ﬁnd most convenient. The position and momentum representations are similarly equally \nvalid representations. We focus on the position representation because it is generally the most useful.\n",
    "168 \nUnbound States\n6.2 \u0002 WAVE PACKETS\nThe key result from the previous section is that Fourier superpositions of momentum eigenstates are \nrequired for proper representation of free particle states. Let’s ﬁrst consider a discrete Fourier series \nexample that illustrates many of the important features of wave packets, and then we’ll make a real \nwave packet using continuous Fourier transforms.\n6.2.1 \u0002 Discrete Superposition\nIn this example, we add just three momentum eigenstates together. We choose one “central” state \nwith momentum p0 to have twice the amplitude of two “side mode” states that are equally spaced at \np = p0 { dp about the central state, as shown in the momentum state distribution in Fig. 6.3. As the \ndashed line hints, we are using this three-mode superposition as a model of a continuous momentum \ndistribution characterized by a center momentum p0 and a momentum distribution width dp that we \nwill discuss in Section 6.2.2.\nA graphical representation of this three-state superposition of sinusoidal waves and the resultant \nwave is shown in Fig. 6.4. The different wavelengths of the three components lead to constructive and \ndestructive interference, as indicated in the plots. The resultant wave is localized to a region of space \nand hence is referred to as a wave packet. The wave packet shown in Fig. 6.4 has a characteristic \nwavelength determined by the central momentum, so it resembles a wave, but it also has a limited spa-\ntial extent, and so it also resembles a particle. In this case, we are using a discrete Fourier sum, so this \nlocalization is repeated periodically. For the more realistic continuum distribution, only one localized \nregion exists and a true wave packet is realized. The coexisting particle and wave characteristics of a \nwave packet are the essence of the wave-particle duality of quantum mechanics.\nTo understand the motion of the wave packet, we must study the time evolution. The wave func-\ntion at time t = 0 is given by the weighted superposition of the three momentum eigenstates\n \n c1x, 02 = a\nj\ncj wpj1x2\n \n \n c1x, 02 = a\nj\ncj \n1\n22pU\n ei pj\n x>U\n \n \n c1x, 02 =\n1\n22pU\n 31\n2 ei1 p0-dp2x>U + ei p0\n x>U + 1\n2 ei1 p0+dp2x>U4. \n \n(6.32)\np0 \n Δp\np0 \r Δp\np0\np\nΦ\u0007p\b\nFIGURE 6.3 Discrete momentum distribution used to model continuous distributions \nand to build a discrete wave packet.\n",
    "6.2 Wave Packets \n169\nThe time-dependent wave function representing this wave packet is obtained by following the Schrödinger \ntime-evolution recipe. Momentum eigenstates are also energy eigenstates of free particles, so the \nsuperposition is already written in the energy basis and we multiply each energy eigenstate by its own \nenergy-dependent phase factor:\n \nc1x, t2 = a\nj\ncj wpj1x2e-i Ej\n t>U. \n(6.33)\nThe energy of each momentum eigenstate is given by the free particle energy\n \nEj =\np2\nj\n2m, \n(6.34)\nwhich for the three states yields\n \n Ep0 =\np2\n0\n2m\n \n \n Ep0{dp = 1p0 { dp2\n2\n2m\n=\np2\n0 { 2p0dp + 1dp2\n2\n2m\n. \n \n(6.35)\nWe assume that the width of the momentum distribution is narrow enough that dp V p0 and so we \nneglect the small 1dp22 term in the energies. Hence, the time-evolved wave packet state is\n \nc1x, t2 =\n1\n22pU\n 31\n2\n ei1 p0-dp2x>U e-i1p2\n0 -2p0dp2t>2m  U + ei p0\n x>U e-i p2\n0\n t>2m  U + 1\n2\n ei1 p0+dp2x>U e-i1p2\n0 +2p0dp2t>2m  U4\n \nc1x, t2 =\n1\n22pU\n  ei p0\n x>U e-i p2\n0\n \n t>2m  U 31\n2\n e-idpx>U ei p0dpt>m  U + 1 + 1\n2\n eidpx>U e-i p0dpt>m  U4\n \nc1x, t2 =\n1\n22pU\n ei p0\n x>U e-i p2\n0\n \n t>2m U c 1 + cosadp\nU\n x - p0dp\nm U\n tb d , \n \n(6.36)\nDestructive Interference\nDestructive Interference\nConstructive Interference\n\r\n\r\n\b\nx\n0\nΔx\n\nΔx\nFIGURE 6.4 Discrete wave packet with three components.\n",
    "170 \nUnbound States\nwhich yields\n \nc1x, t2 =\n1\n22pU\n eip01x-p0t>2m2>UJ1 + cos adp\nU\n c x - p0\nm\n td b R. \n(6.37)\nThis wave packet contains the expected form f  1x { vt2 of a wave, but it has two such parts with \ndifferent arguments. The ﬁrst part of Eq. (6.37) (in curly brackets) is characterized by the momentum \np0 and hence wavelength l0 = h>p0 of the single harmonic wave. This part is called the carrier wave, \nand from its argument we ﬁnd that it moves at the phase velocity vph = p0>2m, as we discussed above. \nThe second part of the wave packet (in square brackets) is characterized by the momentum width dp \nand hence a wavelength lenv = h>dp that is much longer than l0 1because dp V p02. This second \npart is known as the envelope of the wave packet because it modulates the carrier wave, as shown in \nFig. 6.5. Because of the different arguments of the two parts, the envelope moves at a different  velocity \nvgp = p0>m from the carrier. This velocity is called the group velocity because it characterizes the \nvelocity of the group of waves together.\nThe different velocities are evident if the plot of the wave packet in Fig. 6.5 is animated \n( Problem 6.8). Several frames from such an animation are shown in Fig. 6.6, where you can see that \nthe velocity of the envelope—the group velocity—is twice the velocity of the wiggles within the \n envelope—the phase velocity. Notice that the group velocity is equal to the classical velocity of a par-\nticle with momentum p0. This is the sense in which this wave packet can properly represent the motion \nof a particle. This discrete superposition is a good starting point, but it still suffers from the pathologies \nof harmonic waves—it is not normalizable and it therefore cannot predict expectation values—so we \nmust use a continuous momentum distribution to model real experiments. Moreover, the “localiza-\ntion” of the discrete Fourier series superposition is repeated periodically, and so cannot represent a \nsingle particle.\nx\nΨ(x)\nEnvelope\nCarrier\nFIGURE 6.5 Wave packet showing the carrier wave and the modulation envelope.\n",
    "6.2 Wave Packets \n171\n6.2.2 \u0002 Continuous Superposition\nTo go from the discrete case to the continuous case, we change the superposition sum in Eq. (6.32) to \na superposition integral (i.e., we change the Fourier series to a Fourier integral or Fourier transform). \nWhile this may seem like a trivial extension, there are important differences. As we did in the dis-\ncrete case, we perform the expansion using the momentum eigenstate basis wp1x2 because these states \nare also energy eigenstates in the free particle example, which then sets us up to use the Schrödinger \ntime-evolution recipe. In the integral superposition, we specify the amplitudes of the momentum \neigenstate as a continuous distribution f1 p2 rather than specifying discrete amplitudes. Thus, we \nwrite the initial superposition state as\n \n c1x, 02 =\n \nL\n\u0005\n- \u0005\nf1 p2wp1x2dp\n \n \n =\n \nL\n\u0005\n- \u0005\nf1 p2 \n1\n22pU\n ei px>U dp, \n \n(6.38)\nwhere f1p2 is also called the momentum space wave function. The time-evolved state is found by fol-\nlowing the recipe for Schrödinger time evolution and including the energy dependent phase factors:\n \nc1x, t2 =\n \nL\n\u0005\n- \u0005\nf1 p2wp1x2e-iEp\n t>U dp. \n(6.39)\nFIGURE 6.6 Discrete wave packet animation with time increasing from top to bottom. \nOpen circles identify a point of constant phase, which moves at the phase velocity. Filled \ncircles identify the peak of the envelope, which moves at the group velocity.\n",
    "172 \nUnbound States\nPutting in the explicit momentum eigenstate wave functions and the expression for the free particle \nenergy results in\n \nc1x, t2 =  \n1\n22pU L\n\u0005\n- \u0005\nf1 p2ei px>U e-i p2t>2m U dp, \n(6.40)\nwhich simpliﬁes to\n \nc1 x , t2 =  \n1\n22pU L\n\u0005\n- \u0005\nf1 p2ei p1x  - pt>2m2>U dp. \n(6.41)\nThis is the time-dependent generalization of the Fourier transform in Eq. (6.30) for the case of a free \nparticle. The time-dependent generalization of the inverse Fourier transform in Eq. (6.31) is\n \nf1 p, t2 =  \n1\n22pU L\n\u0005\n- \u0005\nc1x, t2e-i px>U dx. \n(6.42)\nTo evaluate the Fourier integral in Eq. (6.41) and determine the wave function for any particular case, \nwe need to know the particular momentum distribution f1p2, which may be speciﬁed as an initial \ncondition, or can be determined from the initial wave function c1x, 02 via the Fourier transform in \nEq. (6.31) that relates the spatial and momentum space wave functions.\nAs an example, consider the case of a Gaussian momentum distribution. This is a very common \nexample because Gaussian functions are easy to integrate—you get another Gaussian in the Fourier \nspace. In addition, the Gaussian distribution is a very good representation of many real experimental \nsituations. The Gaussian function is one of the standard classical probability distributions and is com-\nmonly written as\n \nf 1z2 =  e-1z-m2\n2>2s2\ns22p\n, \n(6.43)\nwhere m is the mean value or average of the distribution and s is the standard deviation of the distribu-\ntion. Relating these deﬁnitions to the quantum mechanical quantities, the mean value is the expectation \nvalue 8z9 and the standard deviation is the uncertainty \u0006z. The probability distribution in Eq. (6.43) is \nnormalized to unity:\n \nL\n\u0005\n- \u0005\nf 1z2dz = 1. \n(6.44)\nNotice that the function f 1z2 is not squared in the normalization integral in Eq. (6.44), contrary to \nthe normalization of quantum mechanical wave functions to which you have become accustomed. In \nquantum mechanics, we have to square the wave function to get the probability density, which is then \nnormalized, analogous to Eq. (6.44). So, technically speaking, the phrase “normalize the quantum \nmechanical wave function” is not correct, because we actually normalize the probability distribution, \nnot the wave function. But that phrase is ingrained into all practicing physicists, so we are stuck with it.\nJust as we did in the discrete case, let’s assume that the momentum distribution is peaked at p0 and \nhas a width characterized by a parameter b. The Gaussian momentum space wave function is\n \nf1 p2 = ¢\n1\n2pb2 ≤\n1>4\n e-1p-p02\n2>4b2, \n(6.45)\n",
    "6.2 Wave Packets \n173\nwhere the scale factor ensures proper normalization. This momentum space wave function is shown in \nFig. 6.7, with the previous discrete case for comparison. The momentum probability distribution (per \nunit momentum) is the absolute square of the momentum space wave function:\n \nP1 p2 = 0 f1p20\n2 = e-1p-p02\n2>2b2\nb22p\n. \n(6.46)\nComparison of this quantum mechanical momentum probability distribution with the standard \n Gaussian probability function in Eq. (6.43) allows us to determine the momentum expectation value \n8p9 and momentum uncertainty \u0006p by inspection as\n \n 8p9 = p0 \n \n \u0006p = b . \n \n(6.47)\nThe time-evolved spatial wave function for this Gaussian wave packet is obtained by substituting \nEq. (6.45) into the Fourier transform in Eq. (6.41):\n \nc1x, t2 =\n1\n22pU L\n\u0005\n- \u0005\n¢\n1\n2pb2 ≤\n1>4\n e-1 p-p022>4b2 ei px>U e-i p2t>2m U dp. \n(6.48)\nThis integral can be performed using the standard Gaussian integral shown in Appendix F, Eq. (F.23): \n(Problem 6.9). The result is\n \nc1x, t2 =\n22b\n3Ug22p\n ei p01x-p0t>2m2>U e-1x-p0t>m2\n2b2>U2g, \n(6.49)\nwhere the new parameters are\n \n g = 1 +  it\nt\n \n t =  m U\n2b2 .  \n(6.50)\np\nΦ(p)\n2Β\np0 \u0004 Δp \np0 \u0006 Δp\np0\nFIGURE 6.7 Gaussian momentum space wave function.\n",
    "174 \nUnbound States\nIf we deﬁne\n \na =\nU\n2b , \n(6.51)\nthen we can express the wave function as\n \nc1x, t2 = a\n1\n2pa2b\n1>4 1\n1g\n  ei p01x-p0t>2m2>U e-1x-p0t>m2\n2>4a2g, \n(6.52)\nwhere a is useful later as a measure of the width in position space.\nJust as in Eq. (6.37) for the discrete momentum distribution, this wave packet has a carrier wave \npart (in curly brackets) that is characterized by p0 and propagates at the phase velocity p0>2m, and \nan envelope part (in square brackets) that is characterized by the momentum width b (through the a \nparameter) and propagates at the group velocity p0>m. As we expected, the envelope is a Gaussian \nfunction. To isolate the envelope propagation, calculate the spatial probability density by taking the \nsquare modulus of the wave function:\n \nP1x, t2 = 0 c1x, t20\n2 =  \n1\n22pa\u000f\n e-1x -p0t>m2\n2>2a2\u000f2, \n(6.53)\nwhere we have deﬁned a new parameter\n \n\u000f = 30 g0\n2 = B1 + t2\nt2 . \n(6.54)\nThe only velocity that appears in the probability density is the group velocity p0>m, which agrees \nwith our classical expectation that the particle propagates at this velocity. This Gaussian wave packet \nis shown in Fig. 6.8(a) and the probability density is shown in Fig. 6.8(b). This wave packet is truly \nlocalized; the probability density decays to zero away from the central peak in Fig. 6.8(b) with none \nof the secondary peaks that were evident in the discrete superposition in Fig. 6.4. The continuum of \nmomentum states used in this superposition ensures that the destructive interference of the constituent \nwaves away from the central peak is effective in truly localizing the wave/particle. This localization \nthrough interference means that this wave packet superposition is normalizable even though the indi-\nvidual waves used are not themselves normalizable.\nThe experimental parameters that one would like to measure in order to fully characterize a wave \npacket are the position and momentum. The expectation value of the position is, formally,\n \n8x9 =\n \nL\n\u0005\n- \u0005\nx  P1x, t2dx =\n \nL\n\u0005\n- \u0005\nx 0 c1x, t20\n2 dx, \n(6.55)\nx\nRe\tΨ\u0007x\b\n2\u0003x\nx\nP\u0007x\b\n2\u0003x\n(a)\n(b)\nFIGURE 6.8 Gaussian wave packet (a) wave function and (b) probability density.\n",
    "6.2 Wave Packets \n175\nbut it can also be obtained by inspection of the Gaussian probability density [compare Eq. (6.53) with \nEq. (6.43)]:\n \n8x9 =  p0\nm\n t. \n(6.56)\nThis result again shows that the wave packet moves with the group velocity p0>m.\nThe expectation value of the momentum can be calculated either with a spatial integral\n \n8p9 =\n \nL\n\u0005\n- \u0005\nc*1x, t2 pn  \n c1x, t2dx \n(6.57)\nor a momentum integral\n \n8p9 =\n \nL\n\u0005\n- \u0005\np P1p, t2dp =\n \nL\n\u0005\n- \u0005\np 0 f1p, t20\n2 dp. \n(6.58)\nEither way, we get the result found by inspection previously in Eq. (6.47):\n \n8p9 = p0. \n(6.59)\nThe uncertainties of position and momentum are (again by inspection)\n \n \u0006x = a\u000f =\nU\n2b\n B\n1 + a2b2t\nm U b\n2\n \n \n \u0006p = b.\n \n \n(6.60)\nThe wave packet momentum width remains constant, which is consistent with the conservation of \nmomentum. The position width grows in time because the different momentum components used to \nconstruct the wave packet all move with different phase velocities. The spatial spreading of the quan-\ntum mechanical wave packet agrees with our classical ideas about waves. It could be considered analo-\ngous to a short laser pulse propagating through glass with dispersion in the index of refraction such \nthat different colors in the pulse travel at different speeds. However, the wave packet spreading is not \nwhat we expect for a classical particle, and we have uncovered one of the counterintuitive realities of \nthe quantum world—quantum particles do not stay intact.\nAs we did for the discrete wave packet, we visualize the motion of the continuous Gaussian wave \npacket with frames of an animation in Fig. 6.9. Again, we note that the carrier wave moves at the phase \nvelocity, which in this case is half of the group velocity of the envelope motion. From previous study \nof optics or waves, you may recall that the formal deﬁnitions of the phase and group velocities that \nwork for any wave packet are\n \n vphase = v\nk\n \n \n vgroup = dv\ndk `\nk0\n, \n \n(6.61)\nwhere the derivative in the group velocity is evaluated at the peak of the distribution of wave vector \nstates comprising the group. Applying these wave relations to the quantum mechanical free particle, \nwe ﬁnd that the phase velocity of the wave is\n \nvphase = v\nk = U v\nUk = E\np =\np2>2m\np\n=\np\n2m = vclassical\n2\n, \n(6.62)\n",
    "176 \nUnbound States\nwhich is half the classical particle velocity. The group velocity is\n \nvgroup = dv\ndk `\nk0\n=\nd1U v2\nd1Uk2 `\nk0\n= dE\ndp `\np0\n=\nd1 p2>2m2\ndp\n`\np0\n= p0\nm = vclassical, \n(6.63)\nwhich is equal to the classical particle velocity. Both results agree with the results we obtained by \ninspection of the Gaussian wave packet for a free particle.\n6.3 \u0002 UNCERTAINTY PRINCIPLE\nThe Fourier connection between position space and momentum space is also important for under-\nstanding the Heisenberg uncertainty principle as it applies to position and momentum. We learned \nin Chapter 2 that spin projection measurements along different axes are incompatible, meaning that \nwe cannot simultaneously measure both observables. We saw that, in general, two observables cannot \nbe measured simultaneously if they do not commute. We expressed this incompatibility in terms of the \nproduct of the measurement uncertainties of the two observables\n \n\u0006A\u0006B Ú 1\n2 083A, B49 0 , \n(6.64)\nwhere the uncertainty is deﬁned as the standard deviation\n \n\u0006A = 481A - 8A9229 = 48A29 - 8A92. \n(6.65)\nWe can now ask whether position and momentum measurements are compatible. Because we \nknow how to represent the position and momentum operators, we can calculate their commutator to \nanswer this question. The answer is that position and momentum do not commute (Problem 6.6). Their \ncommutator is\n \n3xn,  pn4 = i U. \n(6.66)\nFIGURE 6.9 Gaussian wave packet animation with time increasing from top to bottom. \nOpen circles identify a point of constant phase, which moves at the phase velocity. Filled \ncircles identify the peak of the envelope, which moves at the group velocity.\n",
    "6.3 Uncertainty Principle \n177\nThus, the Heisenberg uncertainty principle as applied to position and momentum is\n \n\u0006x\u0006p Ú U\n2  . \n(6.67)\nThis condition limits the product of the uncertainties of position and momentum to a minimum value. \nThe Heisenberg uncertainty principle represents a tradeoff between our knowledge of position and \nour knowledge of momentum. The Fourier connection between position and momentum helps us to \nunderstand this limitation.\nConsider the Fourier wave packet constructed from discrete momentum components. The uncer-\ntainty in momentum \u0006p is approximately the spacing \u0010p of the side modes from the central mode, as \nshown in the momentum distribution of Fig. 6.3. We estimate the uncertainty in position \u0006x as the \nseparation \u0010x of the two destructive interference minima from the central maximum of the correspond-\ning spatial wave function in Fig. 6.4. The minima are located where the phases of the side mode waves \nare p out of phase with the central sinusoid. These phases are determined by the arguments of the \nei pj x>U terms in Eq. (6.32). If we assume that the wave packet maximum, where the three waves are in \nphase, is at x = 0, then the destructive interference minimum on the right is at x = dx, as indicated in \nFig. 6.4. To calculate \u0010x, set the phase difference between the upper side mode 1 p = p0 + dp2 and \nthe central mode 1p = p02 equal to p and solve:\n \n 1p0 + dp2dx\nU\n- p0dx\nU\n= p  \n \n dpdx\nU\n= p. \n \n(6.68)\nThe uncertainty product for this discrete wave packet is approximately\n \n\u0006x\u0006p \u0003 pU. \n(6.69)\nHence, there is an inverse relationship between the width \u0006x of the position distribution and the \nwidth \u0006p of the momentum distribution. A wave packet that is well localized in space 1small \u0006x2 \nrequires a broad distribution \u0006p of momentum states, while a broad spatial distribution requires a \nnarrow momentum distribution. While this wave packet of discrete momentum components (i.e., a \nFourier series) does not strictly obey Eq. (6.69) because the “localization” is repeated out to inﬁnity, \nthe inverse relation between the position and momentum widths is a hallmark of Fourier transforms of \ncontinuous distributions.\nWe learned in the last section that a Gaussian momentum distribution leads to a Gaussian posi-\ntion distribution because the Fourier transform of a Gaussian function is itself a Gaussian function. In \nFig. 6.10 we plot these Fourier transform pairs for a range of widths; the inverse relation between the \nposition and momentum spaces is graphically evident. Using the position and momentum uncertain-\nties in Eq. (6.60), we calculate the uncertainty product of a Gaussian wave packet:\n \n\u0006x\u0006p = U\n2\n D1 + a2b2t\nm U b\n2\n. \n(6.70)\nAt time t = 0 the Gaussian wave packet obeys the equality of the Heisenberg uncertainty relation \n\u0006x\u0006p = U>2. For this reason, a Gaussian wave function 1at t = 02 is a minimum uncertainty state. \nAs the wave packet evolves in time, it broadens in position space and the uncertainty product increases \n(Problem 6.12).\n",
    "178 \nUnbound States\n(a)\n\"Wave\"\nx\nRe\tΨ\u0007x\b\n2\u0003x\n2\u0003x\n2\u0003x\np0\np0\np0\np\nΦ\u0007p\b\n\"Wave/Particle\"\nx\nRe\tΨ\u0007x\b\np\nΦ\u0007p\b\n\"Particle\"\nx\nRe\tΨ\u0007x\b\np\nΦ\u0007p\b\n2\u0003p\n2\u0003p\n2\u0003p\n(b)\n(c)\nFIGURE 6.10 Gaussian wave packets with decreasing spatial widths and the \ncorresponding momentum space wave functions obtained by Fourier transform.\nThe wave packet in Fig. 6.10(a) extends spatially over many wavelengths, so the “wave” nature \nof the packet is evident. In contrast, the wave packet in Fig. 6.10(c) extends only over one wavelength \nand so is more representative of a well-localized “particle.” If we take this wave-particle duality to its \nlogical extremes, we get the states shown in Fig. 6.11. A pure “wave” has an inﬁnite spatial extent, \nwhich corresponds to an inﬁnitesimal momentum width, as shown in Fig. 6.11(a). The pure wave state \nis the momentum eigenstate wave function 0  p09 \u0003 wp01x2 = ei p0\n x>U> 12pU, and the corresponding \nmomentum space wave function must be a Dirac delta function because there is only one momentum \nvalue. This is consistent with the Fourier connection between position and momentum because the \nFourier transform of a pure sinusoid is a delta function:\n \n fp01p2 =\n1\n12pU L\n\u0005\n- \u0005\nwp01x2e-i px>U dx\n \n \n =\n1\n12pU L\n\u0005\n- \u0005\n \n1\n12pU ei p0\n x>U e-i p x>U dx \n \n =\n1\n2pU L\n\u0005\n- \u0005\nei 1p0\n -\n p2x>U dx\n \n \n = d1p - p02.\n \n \n(6.71)\n",
    "6.3 Uncertainty Principle \n179\nA pure “particle” state has an inﬁnitesimally narrow spatial extent, which corresponds to an inﬁ-\nnite momentum width, as shown in Fig. 6.11(b). This state represents a particle that is measured to be \nat a unique position, x0 for example. A state with a unique value of the position observable is a position \neigenstate 0\n x09. In analogy with the momentum space representation of the momentum eigenstate above, \nthe position representation (i.e., spatial wave function) of a position eigenstate is the Dirac delta function\n \n0\n x09 \u0003 wx01x2 = d1x - x02. \n(6.72)\nThis state satisﬁes the position eigenvalue equation\n \n xn 0 x09 = x00 x09\n \n \n xn d1x - x02 = x0 d1x - x02. \n \n(6.73)\nSo we have ﬁnally found the wave function for the position eigenstate we introduced in the last chap-\nter. The inﬁnite extent of the momentum space representation of this state is now clear, because the \nFourier transform of a delta function is a pure sinusoid:\n \n fx01 p2 =\n1\n12pU L\n\u0005\n- \u0005\nwx01x2e-i px>U dx\n \n \n =\n1\n12pU L\n\u0005\n- \u0005\nd1x - x02e-i px>U dx \n \n =\n1\n12pU e-i px0>U.\n \n \n(6.74)\nThe position eigenstates have the same pathologies as the momentum eigenstates—they cannot be \nnormalized and so they cannot truly represent physical states.\nx\nRe\t\u0005p0(x)\nRe\tΦx0(p)\nx\n\u0005x0(x)\np\nΦp0(p) \np\n(a)\n(b)\np0\nx0\nFIGURE 6.11 (a) Momentum eigenstate wave function and its corresponding delta-function \nmomentum distribution, and (b) position eigenstate wave function and its corresponding inﬁnite \nextent momentum distribution.\n",
    "180 \nUnbound States\nIn summary, the eigenstates of position and momentum in the two representations\n \n \n \nPosition space \nMomentum space \n \nPosition eigenstate \n0  x09 \u0003 d1x - x02 \n0  x09 \u0003\n1\n12pU e-i p0 x>U \n \nMomentum eigenstate 0  p09 \u0003\n1\n12pU ei p0 x>U 0  p09 \u0003 d1 p - p02  \n(6.75)\ndemonstrate an appealing parallel between position and momentum. This parallel is also evident in the \nposition and momentum operators. In the position representation, the position operator is simple mul-\ntiplication, while the momentum operator is a derivative with respect to position. Similar to the cor-\nrespondence of the wave functions in Eq. (6.75), it turns out that in the momentum representation, the \nmomentum operator is simple multiplication, while the position operator is a derivative with respect to \nmomentum:\n \nPosition space Momentum space \n \n xn \u0003 x\n \n xn \u0003 iU d\ndp\n  \n \n pn \u0003 -iU d\ndx\n \n pn \u0003 p     . \n(6.76)\nThe incompatibility of position and momentum measurements inherent in the Heisenberg uncer-\ntainty principle is in stark contrast to the classical notion that position and momentum are independent \nquantities that can each be measured with precision limited only by experimental technique. In quan-\ntum mechanics, position and momentum are complementary rather than independent quantities. The \nresult is that we cannot know the trajectory of a particle in quantum mechanics. We can make predic-\ntions of the probability that the particle is in a region of space, but we cannot know the trajectory as we \ndo in classical physics.\n6.3.1 \u0002 Energy Estimation\nWe can also use the uncertainty principle to estimate the minimum energy of a particle. If we know \nthat a particle is localized to a ﬁnite region \u0006x of space, then the uncertainty principle tells us that the \nmomentum distribution required to produce that localization must satisfy\n \n\u0006p Ú\n U\n2\u0006x . \n(6.77)\nIf the momentum distribution has this minimum width, then we can use this width as a rough estimate \nof the minimum momentum\n \npmin \u0005\nU\n2\u0006x . \n(6.78)\nIgnoring the potential energy for the moment, we can then estimate the minimum energy of the particle\n \n Emin =  p2\nmin\n2m\n \n \n Emin \u0005  \nU2\n8m1\u0006x2\n2 . \n \n(6.79)\nThis approach is a common “back-of-the-envelope” calculation used to get a rough estimate of bound-\nstate energies.\n",
    "6.4 Unbound States And Scattering \n181\nConsider a particle bound in a square well potential. The potential energy well by its nature con-\nﬁnes the particle to a spatial region \u0006x approximately the size L of the box. We then use the uncertainty \nprinciple to ﬁnd the corresponding uncertainty in the particle momentum:\n \n \u0006p\u0006x Ú  U\n2 \n \n \u0006p Ú  U\n2\u0006x \n \n \u0006p Ú  U\n2L.  \n \n(6.80)\nIf the particle momentum is uncertain to this degree, then the value of the particle momentum must be \nat least this big, and possibly much larger:\n \npmin =  U\n2L . \n(6.81)\nNow use this estimate of the minimum momentum to estimate the minimum energy that the bound \nparticle can have:\n \n Emin =  p2\nmin\n2m\n \n \n =  U2\n8mL2 . \n \n(6.82)\nCompare this with the ground-state energy in the inﬁnite well:\n \nE\u0005, n=1 =  p2U2\n2mL2 \u0002 5 U2\nmL2 . \n(6.83)\nWhile not a great match, the energy estimate from the Heisenberg uncertainty principle does predict \nthe correct dependence of the energy on the well size. As the well gets smaller the energy levels go up, \nwhich is a general feature of bound energy states. The proportionality depends on the well width and \nis 1>L2 for the square well.\nThe actual ground-state energy in the inﬁnite square well [Eq. (6.83)] is about 40 times larger than \nthe uncertainty principle estimate in Eq. (6.82). There are two reasons for this poor agreement. (1) We \noverestimated the position spread of the particle; a particle conﬁned to a well of size L has a position \nuncertainty less than L (Problem 6.20). (2) The minimum energy estimate comes from assuming that \nthe uncertainty product is a minimum \u0006x\u0006p = U>2, which is true only for Gaussian wave functions. \nBoth of these factors lead to an underestimate of the minimum momentum, which leads to an even big-\nger underestimate of the energy because it depends on the square of the momentum. This method of \nestimating energies with the Heisenberg uncertainty principle must be taken with a grain of salt, as this \nexample shows.\n6.4 \u0002 UNBOUND STATES AND SCATTERING\nWe have discussed bound states in potential wells and free particle states in ﬂat potentials. To com-\nplete our introduction to the quantum mechanics of particle motion, we now discuss unbound states \nin potential energy wells. Unbound states have an energy that is greater than the potential energy at \n",
    "182 \nUnbound States\nx\nE1\nE2\nE3\nE4\nE5\nE6\nEnergy\nV(x)\nBound states\nUnbound States\nFIGURE 6.12 Bound 1E 6 E1\u000522 and unbound 1E 7 E1\u000522 states in a generic potential energy well.\ninﬁnity, in contrast to bound states, which have an energy that is less than the potential energy at inﬁn-\nity, as illustrated in Fig. 6.12. Bound states must “ﬁt” into the potential well, which leads to energy \nquantization, while unbound states “lie” above the well with sinusoidal wave functions that extend to \ninﬁnity, “and beyond!” Unbound states are similar to free particle states in that there are not enough \nconstraints to fully determine the wave function, with the result that there is no energy quantization \nfor unbound states. However, the unbound states are not simply free particle states with a well-deﬁned \nmomentum. Unbound states are affected by the potential energy proﬁle, which causes the states to \n“scatter.” We often use the term scattering states in this context.\nTo begin our study of unbound states, we return to the ﬁnite square well potential. For the study of \nscattering states, it is more convenient to choose the zero of potential energy to be the energy at inﬁn-\nity, rather than the energy at the bottom of the well as we did for bound states. Hence, we deﬁne the \npotential energy shown in Fig. 6.13 as\n \nV1x2 = •\n  0,\n-V0,\n  0,    \n   x 6 -a\n-a 6 x 6 a\n   x 7 a.\n \n(6.84)\nWith this choice of potential energy origin, bound states have E 6 0 and scattering states have E 7 0. \nIt turns out that we are also able to use the solutions to this problem to study an inverted well (a barrier) \nby changing the sign of V0.\nWe follow the same approach we have used in all previous wave function problems—we ﬁrst \nsolve the energy eigenvalue equation. As in the previous well problems, we get separate equations in \nthe different regions:\n \n a-  U2\n2m\n d 2\ndx 2 - V0bwE1x2 = EwE1x2,   0 x 0 6 a  \n \n a-  U2\n2m\n d 2\ndx 2 + 0b wE1x2 = EwE1x2,   0 x 0 7 a. \n \n(6.85)\n",
    "6.4 Unbound States And Scattering \n183\nScattering states have E 7 0 and so we expect sinusoidal solutions in both regions. Hence, it is useful \nto deﬁne two wave vectors\n \n k1 = A\n2mE\nU2\n \n \n k2 = B\n2m1E + V02\nU2\n. \n \n(6.86)\nThese two parameters are used to rewrite the energy eigenvalue equations as\n \n \nd 2wE1x2\ndx2\n= -k2\n2wE1x2,   0 x0 6 a \n \n \nd 2wE1x2\ndx2\n= -k2\n1wE1x2,   0 x0 7 a. \n \n(6.87)\nThe solutions to these differential equations are sinusoids or complex exponentials. Which form \nwe choose to start with is a matter of convenience; the solution dictates the ﬁnal form. It turns out \nthat bound-state wave functions are real, as we found in Chapter 5, and unbound state wave func-\ntions are complex, so the complex exponentials are more convenient here. We write the general \nsolutions as\n \nwE1x2 = •\nAeik1x + Be-ik1x,\nCeik2x + De-ik2x,\nFeik1x + Ge-ik1x,   \n    x 6 -a\n -a 6 x 6 a\n    x 7 a .\n \n(6.88)\nIn principle, we should now proceed as we did in the bound-state problems earlier. That is, \nwe should impose the boundary conditions and solve for the allowed energies and wave function \nx\nV(x)\n\u0006a\n\u0006V0\n0\na\nFIGURE 6.13 Finite square potential energy well.\n",
    "184 \nUnbound States\namplitudes. However, that road quickly becomes a heavy slog. So it is instructive to focus on speciﬁc \nphysical problems of interest and consider what we can actually measure.\nFirst, observe that there are seven unknowns (coefﬁcients A, B, C, D, F, G, and energy E) in this \nproblem. To solve for all seven unknowns, we need seven equations, or seven pieces of information. \nWhen we impose the boundary conditions of wave function amplitude and derivative continuity at \nthe two sides of the well, we get four pieces of information. For bound-state systems, the remaining \nthree pieces of information come from the normalization condition, resulting in energy quantization. \nWe saw this explicitly in the discussion of numerical solutions of energy eigenvalue equations; only by \nchoosing the energy perfectly could we achieve a wave function that decayed to zero as it approached \ninﬁnity. Unbound or scattering states need not decay to zero at inﬁnity, so we cannot and do not need \nto impose the normalization condition. However, the absence of the normalization condition implies \nthat the energy is not quantized and any energy is allowed for a scattering state. So our ﬁrst conclusion \nis that scattering states have a continuous energy spectrum; therefore, we treat the energy E as an ini-\ntial condition rather than as an unknown.\nIn a typical scattering experiment, we shoot particles at each other and ask how their motion is \naffected by their interactions. We usually consider one particle as ﬁxed—the target—and the other \nas moving—the projectile. The potential energy well represents the interaction between them. The \nwave function we solve for then represents the motion of the projectile. In an experiment, projectile \nparticles originate from a source, which we assume is at negative inﬁnity. In the general solution then, \nthe Aeik1x term represents the incoming projectile particles, as illustrated in Fig. 6.14. These incoming \nprojectile particles can interact with the well (target) in two possible ways: they might reﬂect and head \nback to the left, which would be the Be-ik1x term, or they might continue to the right, which would be \nthe Feik1x term after passing the well region. In this scenario, there are no particles on the right side of \nthe barrier that are moving to the left—the Ge-ik1x term. That term could come about only if there were \na source of particles at positive inﬁnity headed back toward the origin, or if another potential energy \nchange occurred to the right of the well that could reﬂect the original particles back to the left. Hence, \nthe typical scattering experiment is consistent with setting G = 0. Using this viewpoint and treating \nthe energy E as an initial condition rather than as an unknown, we have now reduced the number of \nunknowns in the problem from seven to ﬁve.\nx\nE\na\n\u0002a\n\u0002V0\nE\nBe\u0002ik1 x\nDe\u0002ik2 x\nAeik1 x\nCeik2 x\nFeik1 x\nFIGURE 6.14 Waves incident upon, reﬂected from, and transmitted through \na square potential energy well.\n",
    "6.4 Unbound States And Scattering \n185\nUnfortunately, we still have one more unknown than we can solve for because we have only four \nequations or pieces of information from the boundary conditions. We get that one extra piece of infor-\nmation by using a new way to normalize the wave function. The coefﬁcient A represents the amplitude \nof the incoming wave, B the amplitude of the reﬂected wave, and F the amplitude of the transmitted \nwave, all of which are things we can measure. But we only expect our theory to predict the amplitudes \nof the reﬂected and transmitted waves. The amplitude of the incident wave is something we control \nin the experiment. Moreover, we expect that more incoming wave amplitude (input particle ﬂux) will \nlead to more reﬂected and transmitted wave amplitude (output particle ﬂux), so we really want to \npredict the ratios B>A and F>A of the reﬂected and transmitted waves, respectively, to the incoming \nwave. In this sense, we are normalizing our solutions to the amplitude of the incoming wave. In prac-\ntice, we divide the boundary condition equations by A, which effectively gives us four equations with \nfour unknowns. C and D represent the amplitudes of the wave function inside the potential well and \nare typically not amenable to measurement, so we try to eliminate those in favor of the measurables.\nIn light of this new way of approaching the problem, the general solution is\n \nwE1x2 = •\nAeik1x + Be-ik1x,\nCeik2x + De-ik2x,\nFeik1x,\n   \n    x 6 -a\n -a 6 x 6 a\n    x 7 a.\n \n(6.89)\nNow apply the boundary conditions of wave function amplitude and derivative continuity at the two \nsides of the well:\n \n wE1-a2:   Ae-ik1a + Beik1a = Ce-ik2a + Deik2a \n \n dwE1x2\ndx\n`\nx=-a\n:   ik1Ae-ik1a - ik1Beik1a = ik2Ce-ik2a - ik2Deik2a \n \n wE1a2:   Ceik2a + De-ik2a = Feik1a\n \n \n \ndwE1x2\ndx\n`\nx =a\n:   ik2Ceik2a - ik2De-ik2a = ik1Feik1a.\n \n(6.90)\nSolve the last two equations for C and D in terms of F and then substitute into the ﬁrst two equations \nto eliminate C and D, which are not so interesting. Then solve the ﬁrst two equations for the ratios B>A \nand F>A (Problem 6.24):\n \n F\nA =\ne-2ik1a\ncos12k2a2 - i \nk 2\n1 + k 2\n2\n2k1k 2\n sin12k2a2\n \n \n B\nA = i F\nA \nk 2\n2 - k 2\n1\n2k1k2\n sin12k2a2.\n \n(6.91)\nThe ratio F>A is the ratio of the amplitude of the transmitted wave to the amplitude of the incom-\ning wave. The absolute square of this ratio gives the relative probability T that an incident particle is \ntransmitted through the potential well, which we call the transmission coefﬁcient. The transmission \ncoefﬁcient for a ﬁnite square well is\n \nT = 0 F0 2\n0 A0 2 =\n1\n1 + 1k 2\n1 - k 2\n22\n2\n4k 2\n1k 2\n2\n sin212k2a2\n. \n(6.92)\n",
    "186 \nUnbound States\nExpressed in terms of the energy E and the potential well depth V0, the transmission coefﬁcient is\n \nT =\n1\n1 +\nV 2\n0\n4E1E + V02\n sin2 a2a\nU 42m1E + V02b\n. \n(6.93)\nThis is the probability that a particle with an incoming energy E is transmitted through the potential \nregion.\nThe reﬂection coefﬁcient R is the probability that an incident particle is reﬂected from the poten-\ntial well and is given by the absolute square of the ratio B>A of the amplitude of the reﬂected wave to \nthe amplitude of the incoming wave:\n \nR = 0 B0 2\n0 A0 2 =\n1\n1 +\n4k 2\n1k 2\n2\n1k 2\n1 - k 2\n22\n2 sin212k 2a2\n. \n(6.94)\nIn this ﬁnite square well problem, there is no absorption of particles by the well, so the reﬂection and \ntransmission coefﬁcients add up to unity:\n \nT + R = 1 \n(6.95)\nand the reﬂection coefﬁcient is simply R = 1 - T. In contrast to quantum mechanical particles, \n classical particles do not reﬂect from potential wells. They merely speed up and then slow down as \nthey traverse the well. The reﬂection of quantum mechanical particles is thus further evidence of the \nwave nature of particle motion. It is analogous to classical wave motion through different media. For \nexample, a light wave incident on a slab of glass is also partially reﬂected and partially transmitted.\nThe transmission and reﬂection coefﬁcients for a ﬁnite square well are plotted in Fig. 6.15 as \na function of the incident energy E. For large energy, the transmission goes to unity, which is to be \nexpected because the potential well becomes insigniﬁcant. The transmission is also unity for particular \nenergies, commonly called resonances. These resonances occur whenever the sine term in the trans-\nmission coefﬁcient is zero, which occurs if\n \n2k2a = np. \n(6.96)\n1\n2\nE/V0\n0.2\n0.4\n0.6\n0.8\n1.0\nT,R\nR\nT\nFIGURE 6.15 Reﬂection and transmission coefﬁcients for scattering from a ﬁnite square well. \nThe vertical lines indicate resonances where the transmission is unity.\n",
    "6.4 Unbound States And Scattering \n187\nThe reason for these resonances is evident if we rewrite this expression in terms of the wavelength \nl2 = 2p>k2 inside the potential well:\n \n 2a2p\nl2\nb  a = np \n \n 2a = n l2\n2 .\n \n(6.97)\nWhen the width of the potential well (2a) contains an integer number of half wavelengths, the trans-\nmission is unity and the reﬂection is zero. This effect is well known in physical optics, where light \nundergoes multiple reﬂections from the front and back surfaces of a glass slab, as shown in Fig. 6.16. \nForward-going waves all interfere constructively and backward-going waves all interfere destructively \nwhen the thickness of the glass slab contains an integer number of half wavelengths. In the optics case, \nthe changes in transmission and reﬂectivity that come from changing the wavelength (or the slab thick-\nness) are known as interference fringes. One of the most common manifestations of this effect is the \nappearance of colored bands in a thin ﬁlm of oil on water, as in the street after a rainstorm. In the optics \ncase, the transmission and reﬂection are found by explicitly adding up all the interfering waves shown \nin Fig. 6.16. In the quantum case, we solved the energy eigenvalue equation and imposed the boundary \nconditions to achieve the same result. In both cases, the waves look like those shown in Fig. 6.17.\n\u0002a\na\nx\nE,Ψ\nIncident\nReflected\nTotal\nFIGURE 6.17 Waves incident upon, reﬂected from, and transmitted through a ﬁnite square well. \nNote that there are two vertical axes, energy and wave function, with different zeroes.\nΛ1\nΛ1\nΛ2\nFIGURE 6.16 Optics interference analogy.\n",
    "188 \nUnbound States\n\u0002a\na\nx\nE\nE \u0003 V0\nV0\nFIGURE 6.18 A ﬁnite square barrier with the incident particle energy above \nthe barrier height.\nIf we write the resonance condition in terms of the energy, we get\n \na2a\nU b\n2\n2m1E + V02 = n2p2 \n \nE =  -V0 +\nn2p2\n U2\n2m12a2\n2 .\n \n(6.98)\nThus, the energies of the transmission resonances (with respect to the bottom of the well) correspond \nto the bound-state eigenenergies of the inﬁnite well. A similar effect is seen in atomic physics, where it \nis called the Ramsauer-Townsend effect.\nWe can use these same solutions to solve the problem of a barrier potential, as shown in Fig. 6.18, \nas long as the energy is above the barrier height. We simply change the well depth from V0 to \u0011V0 in \nall the formulae above. The results are the same; there are still resonances at the same energy levels. \nThe only difference is that now the wavelength in the potential region is longer rather than shorter than \nthe wavelength outside. This corresponds to the classical optics case where light from glass is incident \non a slab of air.\n6.5 \u0002 TUNNELING THROUGH BARRIERS\nIf the energy of the particle is below the barrier height, then the barrier region is classically forbid-\nden and a classical particle reﬂects perfectly from the barrier. In the quantum mechanical treatment \nthere is a possibility that the particle can penetrate the barrier and come out on the other side! This is \nbecause the quantum mechanical wave function penetrates into the classically forbidden region. This \nphenomenon is called quantum mechanical tunneling, and it is responsible for radioactive decay and \nthe current in high frequency semiconductor diodes, for example. Quantum tunneling has an optical \nanalogue where a light wave penetrates into air while being totally internally reﬂected from inside a \nglass prism. This penetrating wave is called an evanescent wave.\n",
    "6.5 Tunneling Through Barriers \n189\nA square potential energy barrier is shown in Fig. 6.19. The potential energy is described as\n \nV1x2 = •\n0,\nV0,\n0,   \n   x 6 -a\n-a 6 x 6 a\n   x 7 a.\n \n(6.99)\nIf the energy E of the incident particle beam is less than the well height V0, then the region \n-a 6 x 6 a is classically forbidden. As in the previous well problems, there are separate eigenvalue \nequations in the different regions:\n \n a-  U2\n2m d 2\ndx 2 + V0bwE1x2 = EwE1x2,   0 x 0 6 a  \n \n a-  U2\n2m d 2\ndx 2 + 0b wE1x2 = EwE1x2,   0 x 0 7 a. \n(6.100)\nThe energy E is less than the potential barrier height V0, so the interior solutions must be real expo-\nnentials and the exterior solutions must be complex exponentials. It is useful to deﬁne a wave vector k \noutside the well and a decay constant q inside the well:\n \n k = A\n2mE\nU2\n \n \n q = C\n2m1V0 - E2\nU2\n . \n(6.101)\nUse these two constants to rewrite the energy eigenvalue equations as\n \n \nd 2wE1x2\ndx 2\n= q2wE1x2,       0 x 0 6 a  \n \n \nd 2wE1x2\ndx 2\n= -k 2wE1x2,   0 x 0 7 a. \n(6.102)\n\u0002a\na\nx\nE\nE \u0004 V0\nV0\nFIGURE 6.19 A ﬁnite square barrier with the incident particle energy below the barrier height.\n",
    "190 \nUnbound States\nThe general solutions to these equations are\n \nwE1x2 = •\nAeikx + Be-ikx,\nCeqx + De-qx,\nFeikx,\n   \n   x 6 -a\n-a 6 x 6 a\n   x 7 a,\n \n(6.103)\nwhere we have again assumed that there are particles incident from the left, but not from the right. \nIt is important that the wave function in the classically forbidden region contains both the exponen-\ntially decreasing and the exponentially growing terms. The growing term cannot vanish as it did in the \ncase where the classically forbidden region extended to inﬁnity (Section 5.5). The boundary condition \nequations for continuity of the wave function and of the derivative of the wave function are\n \n w1-a2:   Ae-ika + Beika = Ce-qa + Deqa \n \n \ndw1x2\ndx\n`\nx=-a\n:   ikAe-ika - ikBeika = qCe-qa - qDeqa \n \n w1a2:   Ceqa + De-qa = Feika\n \n \n \ndw1x2\ndx\n`\nx =a\n:   qCeqa - qDe-qa = ikFeika.\n \n(6.104)\nAs before, we solve for the ratios of the amplitudes to get the transmission probability:\n \n T = 0 F0\n2\n0 A0 2 =\n1\n1 + 1k 2 + q 22\n2\n4k 2q 2\n sin h212qa2\n \n \n =\n1\n1 +\nV 2\n0\n4E1V0 - E2\n sin h2a2a\nU\n 42m1V0 - E2b\n. \n(6.105)\nThis transmission probability for quantum mechanical tunneling quantiﬁes the probability for a par-\nticle incident upon the barrier to penetrate the barrier and come out the other side. Remember that the \nclassical result would be zero—a classical particle only reﬂects from such a barrier.\nThe reﬂection coefﬁcient for the incident beam is\n \n R = 0 B0\n2\n0 A0 2 = 1 - T =\n1\n1 +\n4k 2q 2\n1k 2 + q 22\n2sin h212qa2\n \n \n =\n1\n1 +\n4E1V0 - E2\nV 2\n0 sin h2a2a\nU\n 42m1V0 - E2b\n.\n \n(6.106)\n",
    "6.5 Tunneling Through Barriers \n191\nThe reﬂection and transmission coefﬁcients are plotted in Fig. 6.20 for the tunneling situation \n1E>V0 6 12, along with the coefﬁcients for the “over the barrier” situation 1E>V0 7 12, using \nEqs. (6.93) and (6.94) with V0 replaced by \u0011V0. In the tunneling case, the transmission is nearly \nzero except near the top of the barrier, where the tunneling probability increases exponentially. As the \nenergy of the incident particle exceeds the barrier height, the transmission becomes large and exhibits \nthe same resonances seen in the ﬁnite well problem. For large energy, the transmission goes to unity, \nwhich is to be expected because the potential barrier becomes insigniﬁcant.\nThe wave function of a particle that tunnels through a barrier is shown in Fig. 6.21. On the left \nside of the potential barrier are the incident and transmitted oscillatory waves. On the right side is \nthe transmitted oscillatory wave. Inside the barrier there is an exponentially damped wave function \n(the evanescent wave of optics). The growing exponential term is part of the interior wave function \n[see Eq. (6.103)], but the decaying term dominates (Problem 6.32).\n\u0002a\na\nx\nE,Ψ\nIncident\nReflected\nTotal\nFIGURE 6.21 Wave function (real part) of a particle tunneling through a square barrier. \nNote that there are two vertical axes, energy and wave function, with different zeroes.\n1\n2\n3\nE /V0\n0.2\n0.4\n0.6\n0.8\n1.0\nT,R\nR\nR\nT\nT\nFIGURE 6.20 Reﬂection and transmission coefﬁcients for scattering from a square barrier.\n",
    "192 \nUnbound States\ntip\nair\nsample\nV0\nd\nV\u0007x\b\nx\nFIGURE 6.22 Schematic diagram of the scanning tunneling microscope, and the \nrepresentation in terms of a potential energy diagram.\nA beautiful example of quantum mechanical tunneling is the scanning tunneling microscope, \nwhich was invented by Gerd Binnig and Heinrich Rohrer in 1981 and earned them the Nobel Prize in \nphysics in 1986. This imaging device employs a small sharp conducting tip that is brought up close to \na sample, as shown in Fig. 6.22. The air (or vacuum) region between the tip and sample is a potential \nenergy barrier because the electrons inside the two materials have lower potential energy than they \nwould in the free space between them due to the work functions of the materials. The probability that \nan electron can tunnel from the tip to the sample (or vice versa) is given by Eq. (6.105) and can be \napproximated as (Problem 6.33)\n \nT \f e-2qd, \n(6.107)\nwhere d is the separation of the tip and sample. In the microscope, a small bias voltage is applied \nbetween the tip and sample to create a preferential direction for current ﬂow. The tip and sample do not \n“touch” so the current is due only to tunneling and is proportional to the tunneling probability:\n \nI = I0 e-2qd. \n(6.108)\nThe exponential dependence makes the current extremely sensitive to the tip-sample separation, which \nis typically in the nanometer range to produce measurable currents. As the tip is moved laterally above \nand parallel to the sample surface, the current provides a measure of the surface topology. A scanning \ntunneling microscope produces images with typical lateral resolution of 0.1 nm and depth resolution \nof 0.01 nm, sufﬁcient to image individual atoms on the surface. A Web image search of “scanning \ntunneling microscope” reveals many beautiful pictures of natural and man-made atomic scale objects.\n6.6 \u0002 ATOM INTERFEROMETRY\nMany of the examples we have discussed in the last two chapters have clearly demonstrated the inher-\nent wave nature of particle motion in quantum mechanics. So can some of the classical light experi-\nments like diffraction and interference be translated to electrons, or even to bigger particles like atoms \nand molecules? Yes! Electron diffraction experiments have been used for a long time and have played \nan important role in studying the atomic level structure of solid state crystals and DNA molecules. In \nrecent years, the advent of laser cooling and trapping of atoms (see Chapter 16) has made it possible to \n",
    "6.6 Atom Interferometry \n193\nperform interference experiments with atoms and molecules. This new ﬁeld of atom interferometry \nis leading to new ways to measure a variety of phenomena with unprecedented precision and to probe \nthe mysteries of quantum measurement theory.\nLet’s discuss how an atom interferometer works by starting with the canonical double-slit inter-\nference experiment, as depicted in Fig. 6.23. You may have already seen this experiment when you \nstudied optics, where it is commonly referred to as Young’s double-slit experiment. The beauty is that \nthe experiment can be performed with light or with particles such as electrons, neutrons, or atoms. \nMoreover, we can use it to discuss the wave-particle duality of quantum mechanics.\nLet’s ﬁrst explain how the double-slit experiment works with light and then extend that to other \nparticles. A source of light illuminates two narrow slits and the light passing through the slits lands on a \ndistant screen. Each slit by itself produces on the screen a diffraction pattern whose spatial extent depends \ninversely on the width of the slit. We assume that the slits are narrow enough that these two diffraction pat-\nterns overlap substantially. If both slits are open, the overlapping diffraction patterns exhibit an additional \ninterference pattern on the screen, within the overall single-slit diffraction pattern, as shown in Fig. 6.23. \nThese interference fringes are comfortably explained by using our notions about waves. The important \nwave idea is that the measured pattern of light cannot be explained by adding intensities, but rather we \nmust add amplitudes and then square the result to ﬁnd the total intensity, as discussed in Section 1.1.4. The \ntotal ﬁeld at the screen is thus the sum of the ﬁelds from each of the two slits:\n \n E1x2 = E11x2 + E21x2  \n \n = E0eikr1 + E0eikr2, \n(6.109)\nwhere the distances r1 and r2 depend on the transverse position x of the observation point, the wave \nvector k = 2p>l, and l is the wavelength of light. The intensity at the screen is proportional to the \ncomplex square of the electric ﬁeld\n \n I1x2 \f 0 E1x20\n2\n \n \n \f 0 E0eikr1 + E0eikr20\n2 \n \n = I00 eikr1 + eikr20\n2.\n \n(6.110)\nSource\nr1\nr2\nx\nFIGURE 6.23 Double-slit interference experiment and resulting interference intensity pattern \non the screen.\n",
    "194 \nUnbound States\nThe interference comes from the cross term in the complex square in Eq. (6.110):\n \n I1x2 = 2I011 + cos k1r2 - r122\n \n \n = 2I0 a1 + cos 2p 1r2 - r12\nl\nb. \n(6.111)\nAs you move the observation point up and down on the screen, the path length difference r2 - r1 \n varies, resulting in the sinusoidal intensity pattern characteristic of two interfering waves. The maxima \nin the interference pattern occur when the path length difference r2 - r1 is an integer multiple of the \n wavelength l.\nThis same wave-optics analysis applies to the wave function analysis of a quantum mechanics \nparticle, using the de Broglie wavelength to characterize the wave nature of the particle. A beam of \nparticles directed toward the double slits of Young’s experiment results in interference fringes at the \ndistant screen. The wave function at the screen resulting from equal contributions from the two slits is \nanalogous to the electric ﬁeld of the light above\n \nc = A1ei pr1>U + ei pr2>U2. \n(6.112)\nThe probability density for detecting a particle on the screen is\n \n P1x2 = 0 c1x20\n2 = 0 A0\n2\n 0 ei pr1>U + ei pr2>U0\n2 \n \n = 20 A0\n2\n a1 + cos  p\nU\n 1r2 - r12b\n \n,\n \n(6.113)\nwhich we rewrite in terms of the de Broglie wavelength using p = h>ldB:\n \nP1x2 = 20 A0\n2\n a1 + cos2p 1r2 - r12\nldB\nb. \n(6.114)\nThis has the same form as Eq. (6.111) and gives rise to the same interference pattern.\nYoung performed the original double-slit experiment with sunlight in 1801. Soon after de \n Broglie’s hypothesis in 1923 that matter can be described as a wave, diffraction experiments were \n performed with particles such as electrons, atoms, molecules, and neutrons to demonstrate matter \nwaves. Since then, Young’s double-slit interference experiment has been performed with electrons \n(1961), neutrons (1988), helium atoms (1991), and even with C60 buckyballs (1999). How about \n baseballs? Could we see interference fringes from something so large? Probably not. As we discussed \nin Section 4.2, a macroscopic object interacts strongly with the environment and its wave function \n suffers decoherence, which washes out the interference fringes.\nThe double-slit experiment is entirely consistent with the wave picture of light or matter, and so \nwould not appear to include any particle-like behavior. However, if we can control the source well \nenough to turn down the incident intensity so low that only one particle per second leaves the source, \nthen we can observe particle behavior with our own eyes. In the case of the light beam, the particles of \nlight are photons. Given that the screen is sensitive enough, the low intensity source produces individual \n",
    "6.6 Atom Interferometry \n195\nblips on the screen corresponding to the arrivals of the individual particles. At ﬁrst, these blips appear at \nseemingly random places on the screen, as shown in Fig. 6.24(a). However, as more blips are recorded \n[Figs. 6.24(b) and (c)] we begin to see that the density of blips coincides with the interference pattern \n[Fig. 6.24(d)] from the wave model, as described by Eq. (6.114). The individual blips are consistent with \nour notion of a particle and its spatial localization, but they are inconsistent with our notion of a wave \nbecause they do not individually exhibit the interference pattern predicted above. On the other hand, the \ninterference pattern that builds up after many particles is consistent with our wave interference model, \nbut is inconsistent with our idea that particles travel in straight lines such that each particle from the \nsource should go through one slit and arrive at the corresponding upper or lower spot on the screen.\nThus, we appear to arrive at a paradox. Some aspects of the experiment are consistent with a \nparticle model, while others are consistent with a wave model. The quantum mechanical resolution is \nto say that we use the wave model to predict the probabilities of detecting individual particles. This \nis consistent with the interpretation we used in the spins sections where the quantum state vector was \nused to predict the probability that a spin projection was measured to be up or down. So what we called \nthe light intensity in the classical wave description is now transformed into a probability of detecting \nphotons at particular places on the screen. Any given photon arrival occurs randomly on the screen and \nthe pattern builds up only after many arrivals. This is what we mean by wave-particle duality. (More \ncomplete discussions of this example can be found in Feynman and Cohen-Tannoudji et al.)\nIf you are not a little confused at this point, try this: What if you could measure which slit the par-\nticle went through? That is, which path did the particle take to arrive at the screen? Well, if you knew \nwhich slit the particle went though, then the wave description wouldn’t be right, because it requires \nthat the wave goes through both slits in order to deﬁne the path length difference in Eqs. (6.111) and \n(6.114). If the wave picture isn’t right, then the interference pattern shouldn’t be present. As it turns \nout, the interference pattern does indeed disappear if you know which slit the particle went through. \n(a)\n(b)\n(c)\n(d)\nFIGURE 6.24 A computer simulation of the arrival of particles at the detection screen in a double-slit \nexperiment, showing (a) random early arrivals, (b) and (c) the buildup of an interference pattern, and  \n(d) a plot of the predicted interference intensity distribution.\n",
    "196 \nUnbound States\nSource\nr2\nr1\nV2\nV1\nFIGURE 6.25 Double-slit atom interferometer for measuring potential energy differences.\nThe answer to this conundrum lies at the heart of quantum mechanical measurement theory. As hard as \nyou might try, you cannot measure, and therefore cannot know, which slit the “particle” goes through \nwithout disturbing it just a little bit. The simplest way to measure which slit the particle goes through \nis to watch, but you need some light to watch. If you see the particle, then at least one photon must \nhave scattered from the particle toward your eye, and the change in momentum of that photon in the \nscattering process will (through conservation of momentum) impart an equal and opposite change to \nthe particle’s momentum. This change is enough to alter the phase of the particle’s wave function and \ndestroy the interference fringes. In the early days of quantum mechanics, such “which path” experi-\nments were merely “thought” experiments or gedanken experiments because they were too hard to \nperform. However, in recent years careful experiments have demonstrated these effects beyond doubt.\nOne of the important features of an atom interferometer is its ability to measure extremely small \nchanges in potential energy. This ability arises from the dependence of the de Broglie wavelength of \nthe particle on the potential energy. If the potential energy varies, then the kinetic energy and hence the \nmomentum varies because the energy is conserved. The de Broglie wavelength depends on the particle \nmomentum, so a varying potential gives rises to a varying wavelength\n \n ldB = h\np\n \n \n =\nh\n22m1E - V2\n . \n(6.115)\nA measurement of the potential energy with an atom interferometer proceeds as shown in Fig. 6.25. \nDifferent regions of potential energy are placed behind slit 1 and behind slit 2. A difference in the two \npotential energies produces a phase shift between the two wave functions that interfere at the distant \nscreen. Hence, a measurement of the fringe shift in the interference pattern is a measurement of the \npotential energy difference. The different regions might, for example, have different electric ﬁelds, \nwhich produce different energies in atomic states (see Section 10.7.2). Or, if the atom interferometer \nis oriented vertically (or at an angle) instead of horizontally, then the two paths experience different \ngravitational potential energies. Recent experiments have been precise enough to test features of Ein-\nstein’s general theory of relativity. Atom interferometers can also measure rotation and acceleration, \nsimilar to ﬁber optic gyroscopes that are commonly used for navigation.\n",
    "Problems \n197\nSUMMARY\nIn this chapter, we learned about the unbound states of quantum particles. The momentum eigenstate \nwave functions are\n \n0  p9 \u0003 wp1x2 =\n1\n12pU ei px>U. \n(6.116)\nFor a free particle 3V1x2 = 04, the momentum eigenstates are also energy eigenstates with energy\n \nE = p2\n2m. \n(6.117)\nA free particle has a characteristic wavelength given by the de Broglie relation\n \nlde Broglie = h\np. \n(6.118)\nA more realistic representation of particle motion is obtained by superposing momentum \n eigenstates in a wave packet. The amplitude of each momentum component is f1 p2 and the resultant \nsuperposition is\n \nc1x2 =\n1\n12pU L\n\u0005\n- \u0005\nf1 p2ei px>U dp, \n(6.119)\nwhich has the form of a Fourier transform. The momentum amplitudes are related to the position space \nwave function through the inverse Fourier transform\n \nf1p2 =\n1\n12pU L\n\u0005\n- \u0005\nc1x2e-i px>U dx. \n(6.120)\nThe Heisenberg uncertainty relation between position and momentum is\n \n\u0006x\u0006p Ú U\n2 \n(6.121)\nand tells us that tight spatial localization requires a broad range of momenta, and a particle with a \nwell-deﬁned momentum is spread over a large spatial region. The Gaussian wave packet is the only \nwave packet that satisﬁes the equality of the uncertainty relation and so is referred to as a minimum \nuncertainty state.\nIf a potential energy is present, the unbound states are scattering states. A particle incident on \na potential well is partially transmitted and partially reﬂected, except at certain resonance energies \nwhere there is no reﬂection. A particle with energy below the height of a potential barrier can tunnel \nthrough the barrier, a phenomenon that is not observed classically.\nPROBLEMS\n 6.1 Calculate the de Broglie wavelengths of the following items:\na) an electron with a kinetic energy of 3 eV\nb) a proton with a kinetic energy of 7 MeV\nc) a buckyball 1C602 with a speed of 200 m>s\nd) an oxygen molecule at room temperature\n",
    "198 \nUnbound States\ne) a raindrop\nf) yourself walking to class\nIn which of the above cases might you expect quantum mechanics to play an important role  \nand why?\n 6.2 The wave function for a particle in one dimension is\n(i)  \nc1x2 = Ae-x 2>a2.\na) Normalize the wave function.\nb) Calculate the expectation value 8x9 of the position.\nc) Calculate the uncertainty \u0006x of the position.\nd) Calculate the probability that the particle is found in the region 0 6 x 6 a.\ne)  Plot the wave function and the probability density and indicate the results to (b), (c), and \n(d) on the plot.\nf) Calculate the expectation value 8p9 of the momentum.\ng) Calculate the uncertainty \u0006p of the momentum.\nh) Does this state satisfy the uncertainty principle?\nRepeat for other wave functions:\n(ii)  \nc1x2 = Axe-x 2>a2\n(iii)  \nc1x2 = A \n1\nx 2 + a2\n 6.3 A beam of particles is prepared in a momentum eigenstate 0  p09. The beam is directed to a \n shutter that is open for a ﬁnite time t.\na) Find the wave function of the system immediately after passing through the shutter.\nb) Find the momentum probability distribution of the beam after the shutter.\n 6.4 Calculate the momentum space wave function for a particle in an energy eigenstate of the \ninﬁnite square well. Plot the momentum probability densities for the n \u0003 1, 2, and 10 energy \neigenstates. Discuss your results.\n 6.5 Show that the momentum and Hamiltonian operators commute for a free particle. Do this two \nways, using both the differential form (position representation) of the operators and the abstract \nform.\n 6.6 Calculate the commutator of the position and momentum operators. Do this two ways, using \nboth the position representation of the operators and the momentum representation.\n 6.7 Show that the momentum eigenstates wp1x2 = Aei px>U satisfy the Dirac orthogonality condition \nin Eq. (6.23) and that the normalization constant is A = 1> 12pU. Use the Dirac orthogonality \ncondition to normalize the wave vector eigenstates wk1x2 = Aeikx\n  and explain why the result \ndiffers from that for the momentum eigenstates.\n 6.8 Use your favorite computational plotting tool to create and plot a wave packet comprising \nthree sinusoidal waves, as done in Section 6.2.1. Vary the separation dp of the side modes \nfrom the  central mode and notice the effect upon the spatial extent dx of the “localized” wave \npacket. Quantify the relationship between the momentum spread dp and the position spread dx. \n Animate your plots and distinguish the motion of the wave packet envelope and the motion of \nthe sinusoidal waves inside the envelope.\n 6.9 Perform the Gaussian integral in Eq. (6.48) and verify the result in Eq. (6.49).\n",
    "Problems \n199\n 6.10 Calculate the expectation values of position and momentum for a Gaussian wave packet by \ndirect integration and verify Eqs. (6.56) and (6.59).\n 6.11 Use your favorite computational plotting tool to create and plot a Gaussian wave packet. Vary \nthe width b of the momentum distribution and notice the effect upon the spatial extent \u0006x of \nthe wave packet. Quantify the relationship between the momentum spread and the position \nspread. Animate your plots and distinguish the motion of the wave packet envelope and the \nmotion of the sinusoidal waves inside the envelope.\n 6.12 Show that a propagating Gaussian wave packet broadens in position space but not in \n momentum space. Plot the position-momentum uncertainty product as a function of  \ntime and show that the Gaussian wave packet is a minimum uncertainty state. Discuss  \nyour results.\n 6.13 Discuss each step in the calculation of the phase and group velocities in Eqs. (6.62) and (6.63).\n 6.14 Consider a particle whose wave function is c1x2 = Asin1 p0  x>U2. Is this wave function an \neigenstate of momentum? Find the expectation value 8p9 of the momentum and the momentum \nprobability distribution. Calculate the uncertainty \u0006p of the momentum. What are the possible \nresults of a measurement of the momentum?\n 6.15 Use the uncertainty principle to estimate the ground state energy of a particle of mass m \n conﬁned to a box with a size of a. Calculate the energy in electron volts for an electron \n conﬁned in a box with a = 0.1 nm, which is roughly the size of an atom.\n 6.16 Use the uncertainty principle to estimate the ground-state energy of a particle of mass m bound \nin the harmonic oscillator potential V1x2 = 1\n2\n kx 2.\n 6.17 Use the uncertainty principle to estimate the ground-state energy of a particle of mass m bound \nin the potential V1x2 = a0 x0 .\n 6.18 Use the uncertainty principle to estimate the ground-state energy of a particle of mass m bound \nin the potential V1x2 = bx4.\n 6.19 Use the uncertainty principle to estimate the ground-state energy of the hydrogen atom.\n 6.20 Calculate the position uncertainty for a particle bound to an inﬁnite square well of width L \nif (a) the particle is in the ground state, and (b) if the probability density is uniform across \nthe well.\n 6.21 A beam of particles is described by the wave function\nc1x2 = Aei p0>Ux e-x2>4a2.\na) Calculate the expectation value 8p9 of the momentum by working in the position \nrepresentation.\nb) Calculate the expectation value 8p9 of the momentum by working in the momentum \nrepresentation.\n 6.22 A beam of particles is described by the wave function\nc1x2 = eAei p0 x>U1b - 0 x02,\n0,\n   0 x0 6 b\n0 x0 7 b.\na) Normalize the wave function. \nb) Plot the wave function.\nc) Calculate and plot the momentum probability distribution.\n",
    "200 \nUnbound States\n0\nV(x)\nV0\nx\nFIGURE 6.26 Step potential.\n 6.23 Some radioactive nuclei emit electrons (beta radiation), so you might speculate that electrons \ncan exist within a nucleus. Use the uncertainty principle to estimate the minimum kinetic \nenergy (beware of relativity) of an electron conﬁned within a nucleus of size 2 fm. Compare \nthat with the Coulomb potential energy of the electron and comment on the possibility of \n electron conﬁnement within the nucleus.\n 6.24 Solve the boundary condition equations (6.90) to ﬁnd the amplitudes for transmission and \nreﬂection in Eq. (6.91).\n 6.25 Electrons incident upon a ﬁnite square well of depth 12 eV are transmitted with unit probabil-\nity when their kinetic energy is 20 eV. What is the minimum width of the well? Assuming this \nminimum width, for what other kinetic energies are the electrons also transmitted completely? \nDoes this well have any bound states?\n 6.26 A ﬁnite square well of depth 8 eV has 5 bound states. Electrons incident upon the well are \ntransmitted with unit probability when their kinetic energy is 11 eV. What is the width of the \nwell? For what other kinetic energies are the electrons also transmitted completely?\n 6.27 A ﬁnite square well has depth 5 eV and width 0.5 nm. What are the bound-state energies of this \nwell? Find the kinetic energies of electrons incident upon the well that are transmitted with unit \nprobability.\n 6.28 A ﬁnite square barrier has height 5 eV and width 1 nm. Find the kinetic energies of electrons \nincident upon the well that are transmitted with unit probability.\n 6.29 Consider a potential energy step as shown in Fig. 6.26 with a beam of particles incident from \nthe left.\na) Calculate the reﬂection coefﬁcient for the case where the energy of the incident particles is \nless than the height of the potential energy step.\nb) Calculate the reﬂection coefﬁcient for the case where the energy of the incident particles is \ngreater than the height of the step.\nc) Plot your results as a function of the incident energy and comment.\n 6.30 Show that a double step potential can be designed such that particles of particular energies are \ntransmitted with unit probability. The optical analogue is an antireﬂection coating.\n 6.31 Calculate the probability of transmission of an electron with kinetic energy 5 eV through a \n barrier of height 10 eV and width 1 nm.\n",
    "Resources \n201\n 6.32 Consider a particle incident upon a potential energy barrier with a barrier height larger than the \nkinetic energy. Show that the growing exponential wave inside the barrier is always less than or \nequal to the decaying exponential term.\n 6.33 Show that the tunneling probability through a barrier of width d is proportional to e-2qd for \nqd W 1.\n 6.34 If the tunneling current in a scanning tunneling microscope is 1 nA at 1 nm tip-surface \n separation, how much current will ﬂow at tip-surface separations of 0.8 nm, 1.2 nm, or 2 nm? \nAssume that the work functions of the metals are 5 eV and that the bias voltage is minimal.\nRESOURCES\nActivities\nThe bulleted activity is available at\nwww.physics.oregonstate.edu/qmactivities\n•  Time Evolution of a Gaussian Wave Packet: Students predict and study the time evolution of a \nGaussian wave packet.\nQuantum Tunneling and Wave Packets: This simulation experiment from the PHET group at the \nUniversity of Colorado animates wave functions tunneling through barriers: \nhttp://phet.colorado.edu/en/simulation/quantum-tunneling\nFurther Reading\nInterference experiments with particles are discussed in these articles:\nA. Tonomura, J. Endo, T. Matsuda, T. Kawasaki, and H. Ezawa, “Demonstration of single- \nelectron buildup of an interference pattern,” Am. J. Phys. 57, 117–120 (1989).\nO. Nairz, M. Arndt, and A. Zeilinger, “Quantum interference experiments with large molecules,” \nAm. J. Phys. 71, 319–325 (2003).\nD. E. Pritchard, A. D. Cronin, S. Gupta, D. A. Kokorowski, “Atom optics: Old ideas, current \n technology, and new results,” Ann. Phys. (Leipzig) 10, 35–54 (2001).\nThe Nobel Prize for scanning tunneling microscopy is described here: \nnobelprize.org/nobel_prizes/physics/laureates/1986/\n",
    "C H A P T E R  \n7\nAngular Momentum\nIn the last two chapters, we learned the fundamentals of solving quantum mechanical problems with \nthe wave function approach. We studied particles bound in idealized square potential energy wells \nand free particles. We are now ready to attack the most important problem in the history of quan-\ntum mechanics—the hydrogen atom. The ability to solve this problem and compare it with precision \nexperiments has played a central role in making quantum mechanics the best proven theory in physics.\nThe hydrogen atom is the bound state of a positively charged proton and a negatively charged \nelectron that are attracted to each other by the Coulomb force. Classically, we expect the electron \n(me = 9.11 * 10-31 kg) to orbit around the more massive proton (mp = 1.67 * 10-27 kg), in the \nsame manner that the earth orbits around the sun, as depicted in Fig. 7.1(a). However, the uncertainty \nprinciple dictates that we cannot know the position of the electron well enough for Fig. 7.1(a) to be a \nvalid representation, but rather, the electron is represented by a probability cloud as in Fig. 7.1(b). By \nthe end of the next chapter, we will be able to predict the details of the many different possible shapes \nof the electron cloud.\nAs always in quantum mechanics, we begin by identifying the Hamiltonian of the system of inter-\nest because of its role in determining the dynamics of the system through the Schrödinger equation\n \niU d\ndt 0 c9 = H0 c9. \n(7.1)\nFIGURE 7.1 (a) A classical atom and (b) a quantum atom.\n",
    "7.1 Separating Center-of-Mass and Relative Motion \n203\nHsysΨsys(R,r) \u0004 EsysΨsys(R,r)\nΨsys(R,r) \u0004 ΨCM(R)Ψrel(r)\n7.30\n7.24, 7.27\n7.20\n7.21\nHCMΨCM(R)\u0004 ECMΨCM(R)\n7.24, 7.28\nHrelΨrel(r) \u0004 ErelΨrel(r)\nΨCM(X,Y,Z)\nr\nΘ\nΦ\nΨsys(R,r) \u0004 ΨCM(X,Y,Z)Ψrel(r,Θ,Φ)\nΨrel(r,Θ,Φ)\u0004R(r)Θ(Θ)Φ(Φ)\n8.69\nFIGURE 7.2 Flowchart for solving the hydrogen atom energy eigenvalue problem by reducing the \ntwo-body problem to a one-body problem and by separation of the spherical coordinate variables.  \nThe numbers in the corners of the boxes refer to the relevant equation numbers in the text.\nOnce we know the Hamiltonian, we ﬁnd the energy eigenstates by solving the energy eigenvalue equation\n \nH0 E9 = E0 E9. \n(7.2)\nThe energy eigenstates form the preferred basis for expanding any initial state and applying the \nSchrödinger time evolution recipe, so solving the energy eigenvalue equation is the primary task \nrequired to solve most quantum mechanical problems.\nCompared to the problems in the last two chapters, the hydrogen atom system presents us with \ntwo major complications: two particles and three dimensions. The goal of this chapter is to simplify \nboth these aspects of the problem. Analogous to the approach taken in classical mechanics, we reduce \nthe two-body problem to a ﬁctitious one-body problem and we separate the three spatial degrees of \nfreedom in a way that each spherical coordinate can be treated independently. A ﬂowchart depicting \nthese two simpliﬁcations is shown in Fig. 7.2. In this chapter, we perform all the steps of Fig. 7.2 except \nthe radial coordinate part. In particular, we focus on the two angular degrees of freedom because they \nrelate to the angular momentum, which is a conserved quantity. In the next chapter, we solve the radial \naspect of the problem for a 1>r Coulomb potential energy, which leads to the quantized energy levels \nof the hydrogen atom. The journey through the next two chapters requires some mathematics that may \nappear daunting; we provide the roadmaps in Figs. 7.2 and 7.6 so you can see the forest for the trees.\n",
    "204 \nAngular Momentum\nFor a three-dimensional system of two particles, the Hamiltonian is the sum of the kinetic energies \nof the two individual particles and the potential energy that describes the interaction between them:\n \nHsys =\np2\n1\n2m1\n+\np2\n2\n2m2\n+ V1r1, r22. \n(7.3)\nParticle 1 has mass m1, position r1, and momentum p1; particle 2 has mass m2, position r2, and \nmomentum p2, and the interaction of the two particles is characterized by the potential energy \nV1r1, r22. We assume that the potential energy depends only on the magnitude of the separation of \nthe two particles\n \nV1r1, r22 = V10 r1 - r202, \n(7.4)\nwhich we refer to as a central potential. In this chapter, we do not need to know the actual form of \nthe central potential. In fact, the quantum mechanical angular wave functions we ﬁnd in this chapter \nare valid for any central potential, which is a very powerful result. We introduce the Coulomb potential \nenergy for the hydrogen atom system in the next chapter.\n 7.1 \u0002 SEPARATING CENTER-OF-MASS AND RELATIVE MOTION\nIn classical mechanics, we simplify the motion of a system of particles by separating the motion of the \ncomposite system into the motion of the center of mass and the motion about the center of mass. We \ntake this same approach to simplify the quantum mechanical description of the hydrogen atom. We will \nwork this through in some detail because the procedure of separating the motion is very common and \nneeds to be understood, but, in fact, we will not pursue the motion of the center of mass beyond this \nsection. In the next section, we’ll begin the discussion of the motion about the center of mass, which is \nwhere many treatments of the hydrogen atom start.\nAs illustrated in Fig. 7.3, we deﬁne the center-of-mass coordinate position vector for this two-\nbody system as\n \nR = m1r1 + m2r2\nm1 + m2\n \n(7.5)\nand the relative position vector as\n \nr = r2 - r1. \n(7.6)\nIn classical mechanics, we typically use velocities, which are obtained by differentiation of position \nwith respect to time. In quantum mechanics, we use momentum as the preferred quantity, so the appro-\npriate quantities to separate the two-body motion are the momentum of the center of mass\n \nP = p1 + p2 \n(7.7)\nand the relative momentum\n \nprel = m1p2 - m2p1\nm1 + m2\n. \n(7.8)\n",
    "7.1 Separating Center-of-Mass and Relative Motion \n205\nThe relative momentum takes the simpler form that looks like a relative velocity\n \nprel\nm\n= p2\nm2\n- p1\nm1\n \n(7.9)\nif we deﬁne the reduced mass m:\n \n 1\nm =\n1\nm1\n+ 1\nm2\n \n \n m =\nm1m2\nm1 + m2\n . \n \n(7.10)\nWith the deﬁnitions in Eqs. (7.7) and (7.8), the two-body Hamiltonian in Eq. (7.3) becomes \n(Problem 7.1)\n \nHsys = P 2\n2M +\np 2\nrel\n2m + V1r2, \n(7.11)\nwhere the relative particle separation r is the magnitude 0 r2 - r10 . This procedure has separated the \nsystem Hamiltonian into two independent parts:\n \nHsys = HCM + Hrel, \n(7.12)\nwith a center-of-mass term\n \nHCM = P2\n2M \n(7.13)\nrepresenting the motion of a particle of mass M = m1 + m2 located at position R with momentum \nP = p1 + p2, and a relative term\n \nHrel =\np 2\nrel\n2m + V1r2 \n(7.14)\nx\ny\nr \u0004 r2 \u0002 r1\nz\nm2 (x2,y2,z2)\nm1 (x1,y1,z1)\nR\u000b\n\u000b\n\u000b\n\u000b\n\u000b\n\u000b\n(X,Y,Z)\nr1\nr2\nFIGURE 7.3 The center-of-mass and relative coordinates for a two-body system.\n",
    "206 \nAngular Momentum\nrepresenting the motion of a single ﬁctitious particle of mass m located at position r = r2 - r1 with \nmomentum prel subject to a potential energy V1r2 created by a force-center that is ﬁxed at the origin. \nNotice that the center-of-mass Hamiltonian HCM does not depend on the relative motion variables prel \nand r, and the relative Hamiltonian Hrel does not depend on the center-of-mass motion variables P \nand R; this is what we mean by “separable.” In contrast, Eq. (7.3) presents the same Hamiltonian in \nterms of p1 and r1 and p2 and r2, but the potential energy V contains both r1 and r2, so H is not sepa-\nrable in those coordinates. Notice also that the center-of-mass position vector R does not appear in \nthe Hamiltonian at all, which, classically, is a reﬂection of the fact that the momentum of the center \nof mass is conserved because there are no external forces. For the hydrogen atom system, the reduced \nmass is m = 0.9995me and the center of mass is located very near the proton.\nThe separation of the Hamiltonian into center-of-mass motion and relative motion can also be \ndone using the explicit position representation of the momentum operators as differentials. In the posi-\ntion representation, the one-dimensional momentum operator is\n \np \u0003 -i U d\ndx . \n(7.15)\nIn three dimensions, the momentum operator is cast in terms of the gradient operator \u0002:\n \np \u0003 -i U a 0\n0x\n in +\n0\n0y\n jn +\n0\n0z\n kn b = -i U\u0002. \n(7.16)\nFor a two-particle system, the momentum operators for the two particles are\n \n p1 \u0003 -i U ¢ 0\n0x1\n in +\n0\n0 y1\n jn +\n0\n0 z1\n kn ≤= -i U\u00021  \n \n p2 \u0003 -i U ¢ 0\n0x2\n in +\n0\n0 y2\n jn +\n0\n0z2\n kn ≤= -i U\u00022. \n \n(7.17)\nSubstituting these position representations into the Hamiltonian in Eq. (7.3) leads to the same separa-\ntion as in Eq. (7.11), where the center-of-mass momentum operator has the position representation \n(Problem 7.1)\n \nP \u0003 -i U a 0\n0X\n in +\n0\n0Y\n jn +\n0\n0Z\n kn b = -i U\u0002R. \n(7.18)\nX, Y, and Z are the Cartesian coordinates of the center-of-mass vector R, and \u0002R is the gradient opera-\ntor corresponding to the center-of-mass coordinates. The relative momentum operator has the position \nrepresentation\n \nprel \u0003 -i U a 0\n0x\n in +\n0\n0 y\n jn +\n0\n0z\n kn b = -i U\u0002r, \n(7.19)\nwhere x, y, and z are the Cartesian coordinates of the relative position vector r = r2 - r1 and \u0002r is the \ngradient operator corresponding to the relative coordinates.\nWith the Hamiltonian separated into center-of-mass motion and relative motion, we expect that \nthe quantum state vector can also be separated. This is not always the case, as we saw in the discussion \nof entanglement in Chapter 4, but it is a valid assumption for the hydrogen atom problem we want to \nsolve because the potential energy is a function only of the relative coordinate r. Hence, we write the \nwave function for the system as\n \ncsys1R, r2 = cCM1R2 crel1r2. \n(7.20)\n",
    "7.1 Separating Center-of-Mass and Relative Motion \n207\nThe energy eigenvalue equation for the system is\n \nHsys csys1R, r2 = Esys  csys1R, r2, \n(7.21)\nand substituting the separated Hamiltonian [Eq. (7.12)] and separated wave function [Eq. (7.20)] gives\n \n1HCM + Hrel2cCM1R2 crel1r2 = Esys  cCM1R2 crel1r2. \n(7.22)\nThe separate center-of-mass and relative Hamiltonians act only on their respective wave functions \nbecause the gradients \u0002R and \u0002r are independent, so Eq. (7.22) becomes\n \ncrel1r2HCM cCM1R2 + cCM1R2Hrel crel1r2 = Esys  cCM1R2 crel1r2. \n(7.23)\nWe assert that the separate center-of-mass and relative Hamiltonians satisfy their own energy eigen-\nvalue equations (Problem 7.2)\n \n HCM cCM1R2 = ECM cCM1R2 \n \n Hrel crel1r2 = Erel crel1r2 \n \n(7.24)\nand arrive at the energy eigenvalue equation for the system \n \nHsys cCM1R2 crel1r2 = 1ECM + Erel2cCM1R2 crel1r2, \n(7.25)\nwhich demonstrates that the system energy is the additive energy of the two parts\n \nEsys = ECM + Erel. \n(7.26)\nUsing the separate Hamiltonians in Eqs. (7.13) and (7.14), the separated energy eigenvalue \nequations are\n \nP 2\n2M\n cCM1R2 = ECM cCM1R2 \n(7.27)\nand\n \na\np 2\nrel\n2m + V1r2b  crel1r2 = Erel crel1r2. \n(7.28)\nThe center-of-mass energy eigenvalue equation (7.27) is the free particle eigenvalue equation we \nencountered in Chapter 6, while the relative motion energy eigenvalue equation (7.28) contains the \ninteraction potential and so has the interesting physics of the hydrogen atom. Using the position rep-\nresentation of the momentum operator in Eq. (7.18), the center-of-mass energy eigenvalue equation is\n \n-  U2\n2M\n a 0 2\n0X 2 + 0 2\n0Y 2 + 0 2\n0Z 2b cCM1X, Y, Z2 = ECM cCM1X, Y, Z2. \n(7.29)\nThe solution to Eq. (7.29) is the three-dimensional extension of the free-particle eigenstates we stud-\nied in Chapter 6\n \ncCM1X, Y, Z2 =\n1\n12pU2\n3>2 e i1PXX+PYY+PZZ2>U \n(7.30)\nwith energy eigenvalues\n \nECM =\n1\n2M AP 2\nX + P 2\nY + P 2\nZB. \n(7.31)\n",
    "208 \nAngular Momentum\nFor measurements of observables associated with the relative motion, the center-of-mass wave func-\ntion contributes only an overall phase to the system wave function and so has no effect on calculat-\ning probabilities of relative motion quantities. We can therefore leave the center-of-mass motion and \nconcentrate only on the relative motion dictated by the energy eigenvalue equation (7.28). That is the \nproblem we want to solve for the hydrogen atom. Remember that the angular momentum discusssion \nthat will follow in this chapter is valid for any central potential. In Chapter 8, we will insert the speciﬁc \nform of the potential for the hydrogen atom.\n7.2 \u0002 ENERGY EIGENVALUE EQUATION IN SPHERICAL COORDINATES\nThe relative motion Hamiltonian that governs the hydrogen atom is\n \nH = p 2\n2m + V1r2, \n(7.32)\nwhere we drop the “relative” subscripts because we are now focusing exclusively on the relative \nmotion and ignoring the center-of-mass motion. Using the position representation of the momentum \noperator from Eq. (7.19), the Hamiltonian is represented by\n \nH \u0003 -  U2\n2m\n \u00022 + V1r2 \n(7.33)\nand the energy eigenvalue equation is the differential equation\n \na-  U2\n2m\u00022 + V1r2b c1r2 = Ec1r2. \n(7.34)\nBecause the potential energy in Eq. (7.34) depends on the parameter r only, this problem is clearly \nasking for the use of spherical coordinates centered at the origin of the central potential. The system of \nspherical coordinates is shown in Fig. 7.4(a) and the relations between the spherical coordinates r, u, f \nand the Cartesian coordinates x, y, z are\n \n x = r sin u cos f \n \n y = r sin u sin f  \n \n(7.35)\n \n z = r cos u.\n \nThe differential volume element dV = dx dy dz expressed in spherical coordinates is\n \ndV = r2 sin u d u d f dr. \n(7.36)\nThis volume element is shown in Fig. 7.4(b), leading one to consider the grouping\n \ndV = 1r d u21r sin u d f21dr2. \n(7.37)\nHowever, for calculating the normalization of wave functions, we will group the terms as\n \ndV = 1sin u d u21d f21r2 dr2 \n(7.38)\n",
    "7.2 Energy Eigenvalue Equation in Spherical Coordinates \n209\nand normalize each coordinate piece of the wave function separately. It is also convenient to express \nthe volume element as\n \ndV = r2 dr d\t, \n(7.39)\nwhere\n \nd\t = sin u d u d f \n(7.40)\nis the differential solid angle element.\nIn spherical coordinates, the gradient operator is\n \n\u0002 = rn 0\n0r + un 1\nr\n  0\n0 u + fn \n1\nr sin u\n  0\n0 f \n(7.41)\nand the Laplacian operator \u00022 = \u0002~\u0002 is\n \n\u00022 = 1\nr2  0\n0r\n ar2\n 0\n0r b +\n1\nr2 sin u\n  0\n0 u\n asin u 0\n0 u b +\n1\nr2 sin2 u\n  0 2\n0 f2 . \n(7.42)\nUsing this spherical coordinate representation, the energy eigenvalue equation (7.34) becomes the dif-\nferential equation\n \n-  U2\n2m\n c 1\nr2  0\n0r\n ar2\n 0\n0r b +\n1\nr2 sin u\n  0\n0 u\n asin u 0\n0 u b +\n1\nr2 sin2 u\n  0 2\n0 f2 d c1r, u, f2 \n \n+ V1r2c1r, u, f2 = Ec1r, u, f2 .  \n(7.43)\nThis looks formidable, so it is worth remembering that this is just the position representation of the \nenergy eigenvalue equation\n \nH0 E9 = E0 E9. \n(7.44)\nSolving Eq. (7.43) for the energy E and the eigenstates 0 E9 \u0003 c1r, u, f2 is our primary task, but ﬁrst \nlet’s discuss the important role that angular momentum plays in this equation.\n(a)\nx\ny\nz\nr\u0002\nΘ\nΦ\ndΘ\ndΦ\ndr\nx\ny\nz\n(b)\nFIGURE 7.4 (a) Spherical coordinates and (b) the differential volume element.\n",
    "210 \nAngular Momentum\n7.3 \u0002 ANGULAR MOMENTUM\n 7.3.1 \u0002 Classical Angular Momentum\nThe classical angular momentum is deﬁned as\n \nL = r * p. \n(7.45)\nIn the case of central forces, the torque r * F is zero and angular momentum is a conserved quantity:\n \nt = d L\ndt = 0  1   L = constant. \n(7.46)\nA central force F1r2 depends only on the distance of the reduced mass from the center of force \n(i.e., the separation of the two particles) and not on the angular orientation of the system. Therefore, \nthe system is spherically symmetric; it is invariant (unchanged) under rotations. Noether’s theorem \nstates that whenever the laws of physics are invariant under a particular motion or other operation, \nthere will be a corresponding conserved quantity. In this case, the conservation of angular momentum \nis related to the invariance of the physical system under rotations.\n 7.3.2 \u0002 Quantum Mechanical Angular Momentum\nIn quantum mechanics, the Cartesian components of the angular momentum operator L = r * p in \nthe position representation are\n \n Lx = ypz - zpy \u0003 -i U ay 0\n0z - z 0\n0y b  \n \n Ly = zpx - xpz \u0003 -i U az 0\n0x - x 0\n0z b  \n(7.47)\n \n Lz = xpy - ypx \u0003 -i U ax 0\n0y - y 0\n0x b. \nPosition and momentum operators for a given axis do not commute 13x, px4 = iU, etc.2, whereas posi-\ntion and momentum operators for different axes do commute 13x, py4 = 0, etc.2. We can use these \ncommutators to calculate the commutators of the components of the angular momentum operator. For \nexample,\n \n 3Lx, Ly4 = 3ypz - zpy, zpx - xpz4\n \n \n = ypz  z px - ypz  x pz - z py \n z px + z py \n x pz - z px  ypz + z px  z py + x pz  ypz - x pz\n z py\n . \n(7.48)\nNow use the commutation relations to move commuting operators through each other (e.g., \nypz  z px = ypx pz z) and cancel terms:\n \n3Lx, Ly4 = ypx  pz\n z - x ypz  pz - zz px py + x py z pz - ypx  z pz + zz px py + x ypz  pz - x py  pz\n z \n \n= ypx pzz + x py  z pz - ypx  z pz - x py  pz\n z . \n(7.49)\nFinally, collect terms and use the commutator relation 3z, pz4 = iU :\n \n 3Lx, Ly4 = x py1z pz - pz\n z2 - ypx1z pz - pz\n z2 \n \n = x py3z, pz4 - ypx3z, pz4\n \n(7.50)\n \n = i U1xpy - ypx2\n \n \n = i ULz .\n \n",
    "7.3 Angular Momentum \n211\nCyclic permutations of this identity give the three commutation relations\n \n 3Lx, Ly4 = i ULz \n \n 3Ly, Lz4 = i ULx  \n \n(7.51)\n \n 3Lz, Lx4 = i ULy  . \nThese are exactly the same commutation relations that spin angular momentum obeys (Section 2.4)! \nSo orbital and spin angular momentum appear to have something in common, as you might expect. \nIndeed, this is why the physical property of spin angular momentum was given this name.\nWhen we studied spin, we found it useful to consider the S2 = S~S operator. The corresponding \noperator for orbital angular momentum is\n \nL 2 = L~L = L2\nx + L2\ny + L2\nz . \n(7.52)\nIn the spin case, the operator S2 commutes with all three component operators. Let’s try the same with \norbital angular momentum. For example,\n \n 3L 2, Lx4 = 3L2\nx + L2\ny + L2\nz , Lx4\n \n \n = 3L2\nx, Lx4 + 3L2\ny, Lx4 + 3L2\nz , Lx4  \n \n(7.53)\n \n = L2\ny Lx - Lx L2\ny + L2\nzLx - Lx L2\nz . \nAdd zero to this equation, but choose the terms that sum to zero cleverly so they help:\n \n 3L 2, Lx4 = Ly Ly Lx - Ly Lx Ly + Ly Lx Ly - Lx Ly Ly + Lz Lz Lx - Lz Lx Lz + Lz Lx Lz - Lx Lz Lz \n \n=0\n \n=0\n \n = Ly3Ly, Lx4 + 3Ly, Lx4Ly + Lz3Lz, Lx4 + 3Lz, Lx4Lz \n(7.54)\n \n = -i ULy Lz - i ULz Ly + i ULz Ly + i ULy Lz\n \n \n = 0.\n \nThe other two components also commute with L 2 (Problem 7.4):\n \n 3L 2, Lx4 = 0  \n \n 3L 2, Ly4 = 0  \n \n(7.55)\n \n 3L 2, Lz4 = 0  . \nSo orbital and spin angular momentum obey all the same commutation relations.\nThough we did not do it that way in Chapter 1, the eigenvalues and the eigenstates of spin angular \nmomentum can be derived solely from the commutation relations of the operators (see Section 11.3). \nThe spin eigenvalue equations are\n \n S20 sms9 = s1s + 12U20 sms9 \n \n(7.56)\n \n Sz0 sms9 = ms U0 sms9.\n \nThe states 0 sms9 are simultaneously eigenstates of S2 and Sz, which is possible because the two opera-\ntors commute with each other. Because orbital angular momentum obeys the same commutation rela-\ntions as spin, the eigenvalue equations for L 2 and Lz have the same form:\n \n L 20 /m/9 = /1/ + 12U20 /m/9 \n \n Lz0 /m/9 = m/ U0 /m/9\n \n \n(7.57)\n",
    "212 \nAngular Momentum\nand the states 0 /m/9 are simultaneously eigenstates of L 2 and Lz. Hence, we can draw on all the work \nwe did in the spins chapters to help us understand orbital angular momentum. The quantum number / \nis the orbital angular momentum quantum number and gives a measure of the “size” of the angular \nmomentum vector in that the magnitude is 2/1/ + 12U. The quantum number m/ is the orbital magnetic \nquantum number and indicates that the magnitude of the z-component of the angular momentum is m/ U.\nThere is one crucial difference between spin angular momentum and orbital angular momentum. \nIn the spin case, the allowed quantized values of the spin angular momentum quantum number s are \nthe integers and half integers:\n \ns = 0, 1\n2, 1, 3\n2, 2, 5\n2, 3, 7\n2, 4, ... . \n(7.58)\nIn Chapters 1–3 we studied spin-1/2 and spin-1 systems. In the case of orbital angular momentum, the \nquantum number / is allowed to take on only integer values\n \n/ = 0, 1, 2, 3, 4, ...  . \n(7.59)\nOther than this important distinction, spin and orbital angular momentum behave the same in quantum \nmechanical calculations of probabilities, expectation values, etc. The spin magnetic quantum number \nms spans the range from -s S +s in integer steps. The orbital magnetic quantum number m/ is similarly \nrestricted to the 2/ + 1 values\n \nm/ = -/, -/ + 1, ..., -1, 0, 1, ..., / - 1, /  . \n(7.60)\nIn the spin-1/2 system, we represent the spin operators as matrices:\n \n S2 \u0003 3\n4\n U2 a1\n0\n0\n1b  Sz \u0003 U\n2\n a1\n0\n0\n-1b  \n \n Sx \u0003 U\n2\n a0\n1\n1\n0b   Sy \u0003 U\n2\n a0\n-i\ni\n0 b  ,\n \n \n(7.61)\nwhere the basis states of the representation are the eigenstates of S2 and Sz as deﬁned in Eq. (7.56). For \norbital angular momentum, we also represent the operators as matrices, with the exception that only \ninteger values of / are allowed. For example, the matrix representations of the orbital angular momen-\ntum operators for / = 1 are\n \n L 2 \u0003 2U2 °\n1\n0\n0\n0\n1\n0\n0\n0\n1\n¢  L z \u0003 U\n  °\n1\n0\n0\n0\n0\n0\n0\n0\n-1\n¢\n \n \n Lx \u0003\nU\n22\n °\n0\n1\n0\n1\n0\n1\n0\n1\n0\n¢    Ly \u0003\nU\n22\n °\n0\n-i\n0\ni\n0\n-i\n0\ni\n0\n¢  , \n \n(7.62)\nwhere the basis states of the representation are the eigenstates of L 2 and Lz as deﬁned in Eq. (7.57). \nThese matrices are exactly the same as the spin-1 matrices we deﬁned in Chapter 2.7.\n",
    "7.3 Angular Momentum \n213\nExample 7.1 A particle with orbital angular momentum / = 1 is in the state\n \n0 c9 = 4\n1\n3 0119 + 4\n2\n3 0109. \n(7.63)\nFind the probability that a measurement of Lz yields the value U for this state and calculate the \nexpectation value of Lz.\nThe eigenstate of Lz with eigenvalue Lz = +U Aand eigenvalue L 2 = 2U2B is \n0 / = 1, m/ = 19 = 0 119, so the probability of measuring Lz = +U is\n \n PU = 08110 c90\n2\n \n \n = @  8110  A4\n1\n3 0119 + 4\n2\n3 0109B @\n2\n \n(7.64)\n \n = @4\n1\n3 8110119 + 4\n2\n3 8110109@\n2\n. \nThe states 0 /m/9 form an orthonormal basis, so 8110119 = 1 and 8110109 = 0, and the probability is\n \n PU = @4\n1\n3\n @\n2\n \n(7.65)\n \n = 1\n3.\n \nThe expectation value of Lz is\n \n8Lz9 = 8c0 Lz0 c9. \n(7.66)\nLet’s calculate this with matrices. Using the matrix (column) representation of 0 c9:\n \n0 c9 \u0003\n1\n23\n °\n1\n22\n0\n¢ , \n(7.67)\nwe get\n \n 8Lz9 =\n1\n23\n 11\n \n22\n02 U °\n1\n0\n0\n0\n0\n0\n0\n0\n-1\n¢ 1\n23\n °\n1\n22\n0\n¢\n \n(7.68)\n \n = U\n3\n 11\n \n22\n02°\n1\n0\n0\n¢\n \n \n = U\n3.\n \nThese calculations are no different than if this were a spin-1 problem.\nSo it looks like we can solve orbital angular momentum problems using our spin knowledge, and \nyou may well ask: Is that all there is to it? Yes and no! If you can solve a problem like Example 7.1 \nusing the bra-ket or matrix notation we developed in the spins chapters, then do that. But there are \nproblems where we need to do more.\n",
    "214 \nAngular Momentum\nIn Chapters 1–3 we never discussed a position representation of spin operators or eigenstates, \nbecause it is not possible to describe spin angular momentum using the wave function language we \ndeveloped in Chapter 5. In contrast, it is possible to represent orbital angular momentum operators and \neigenstates in the position representation. We have already presented the position representation of the \norbital angular momentum operators Lx, Ly, and Lz in Eq. (7.47), and the end result of this chapter is a \nposition representation of the angular momentum eigenstates 0 /m/9. In solving for the allowed spatial \nwave functions, we will prove that the orbital angular momentum is quantized according to Eqs. (7.59) \nand (7.60).\nArmed with wave functions detailing the spatial dependence of orbital angular momentum, we \nwill then be able to visualize the angular probability distribution of the electron around the proton \nin the hydrogen atom. We will be able to understand why two hydrogen atoms form a molecule and \nwhy the carbon bonds in a diamond lattice are oriented in such a way to make diamond so unique. For \nexample, Fig. 7.5 shows the angular orientation of the four tetrahedral bonds that one carbon atom \nmakes within the diamond lattice.\nTo see the importance of orbital angular momentum in solving the hydrogen atom energy eigen-\nvalue equation, we change the angular momentum operators in Eq. (7.47) to spherical coordinates. \nUsing the relations in Eq. (7.35), one can show that the angular momentum operator Lz has the spheri-\ncal coordinate representation (Problem 7.8)\n \nLz \u0003 -i U 0\n0 f \n(7.69)\nand depends on f alone. Likewise, we convert Lx and Ly to spherical coordinates (Problem 7.8) and \nobtain the operator L 2 = L~L = L 2\nx + L 2\ny + L 2\nz:\n \nL2 \u0003 -U2 c\n1\n sin u\n  0\n0 u\n asin u 0\n0 u b +\n1\nsin2u\n  0 2\n0 f2d  , \n(7.70)\nwhich depends on u and f, and not on r. We now have the expressions for the two operators L 2 and Lz \nthat we need to express the angular momentum eigenvalue equations (7.57) in the spherical coordinate \nrepresentation, which we do later in this chapter.\nNow compare the L 2 operator in Eq. (7.70) with the energy eigenvalue equation (7.43). You \nnotice that the L 2 operator is part of the differential operator in the energy eigenvalue equation. Hence, \nwe can rewrite the energy eigenvalue equation H0 c9 = E0 c9 with the L 2 operator\n \n-  U2\n2m\n c 1\nr2 0\n0r\n ar2\n 0\n0r b -\n1\nU2r2 L2 d c1r, u, f2 + V1r2c1r, u, f2 =  Ec1r, u, f2  . \n(7.71)\nAll of the angular part of the Hamiltonian is contained in the L 2 angular momentum operator. In this \nform, it is clear that the central force Hamiltonian commutes with the orbital angular momentum oper-\nators L 2 and Lz (Problem 7.9)\n \n 3H, L 24 = 0 \n \n 3H, Lz4 = 0, \n \n(7.72)\nwhich implies that we can ﬁnd simultaneous eigenstates of all three operators.\n",
    "7.4 Separation of Variables: Spherical Coordinates \n215\nFIGURE 7.5 Angular dependence of the four sp3 hybrid orbitals in a diamond lattice.\n7.4 \u0002 SEPARATION OF VARIABLES: SPHERICAL COORDINATES\nWe have already simpliﬁed the two-body nature of the hydrogen atom problem to an effective one-\nbody problem by separating the relative motion (interesting) from the center-of-mass motion (not so \ninteresting). We now proceed to simplify the three-dimensional aspect of the problem by separating \nthe three spherical coordinate dimensions from each other. To do this, we apply the standard tech-\nnique of separation of variables to the energy eigenvalue differential equation (7.71). This technique \nis reviewed in Appendix E, where six steps detail the process in its general form. The ﬂowchart in \nFig. 7.6 shows how the separation and recombination process will progress over the remainder of this \nchapter and through the next chapter.\nIn the ﬁrst instance, we apply the six steps of the separation of variables procedure to isolate the \nradial r dependence and the angular u, f dependence into two separate equations.\nStep 1: Write the partial differential equation in the appropriate coordinate system. We have done \nthis already in Eq. (7.71)\n \n-  U2\n2m\n c 1\nr2 0\n0r\n ar2\n 0\n0r b -\n1\nU2r2 L2 d c1r, u, f2 + V1r2c1r, u, f2 =  Ec1r, u, f2. (7.73)\nStep 2:  Assume that the solution c1r, u, f2 can be written as the product of functions, at least one of \nwhich depends on only one variable, in this case r. The other function(s) must not depend at \nall on this variable, that is, assume\n \nc1r, u, f2 = R1r2Y1u, f2. \n(7.74)\n \n Plug this assumed solution into the partial differential equation (7.73) from Step 1. Because \nof the special form of c, the partial derivatives each act on only one of the functions in c. Any \n",
    "216 \nAngular Momentum\npartial derivatives that act only on a function of a single variable may be rewritten as total \nderivatives, yielding\n \n-  U2\n2m\n c Y 1\nr2 d\ndr\n ar2 dR\ndr b -\n1\nU2r2 R1L2Y2d + V1r2RY = ERY. \n(7.75)\nY(Θ,Φ) \u0004 Θ(Θ)Φ(Φ)\nΦm(Φ)\nHΨnlm(r,Θ,Φ) \u0004 EnΨnlm(r,Θ,Φ)\nLzΦm(Φ) = mhΦm(Φ)\nB(m)\nA(l)\nHΨ(r,Θ,φ) \u0004 EΨ(r,Θ,Φ)\nΨ(r,Θ,Φ) \u0004 R(r)Y(Θ,Φ)\nΘl\nm(cosΘ)\nL2Yl\nm(Θ,Φ) \u0004 l(l+1)h2Yl\nm(Θ,Φ) \nYl\nm(Θ,Φ) \nΨnlm(r,Θ,Φ) \u0004 Rnl(r )Yl\nm(Θ,Φ)\nRnl(r)\n7.83\n7.100\n7.82\n7.79\n8.67\n7.156\n7.161\n8.69\n7.81\n7.74\n7.43\nd\ndΦ eqn \u0004 BΦ\nd\ndΘ eqn \u0004 AΘ\nd\ndr eqn \u0004 ER\nFIGURE 7.6 Flowchart of the separation of variables procedure applied to the hydrogen atom. \nThe numbers in the corners of the boxes refer to the relevant equation numbers in the text.\n",
    "7.4 Separation of Variables: Spherical Coordinates \n217\n \n Note that the orbital angular momentum operator L2 acts only on angular spatial functions \n[Eq. (7.70)].\n Step 3: Divide both sides of the equation by c = RY:\n \n-  U2\n2m\n c 1\nR 1\nr2 d\ndr\n ar2 dR\ndr b - 1\nY 1\nU2r2 1L2Y2d + V1r2 = E. \n(7.76)\n Step 4: Isolate all of the dependence on one coordinate on one side of the equation. To isolate the r \ndependence, we multiply Eq. (7.76) by r 2 to clear the r dependence from the denominator \nof the angular term (involving angular derivatives in L 2 and angular functions in Y). Further \nrearranging Eq. (7.76) to get all of the r dependence on the left-hand side, we obtain:\n \n1\nR1r2 d\ndr\n ar2 \ndR1r2\ndr\nb - 2m\nU2  1E - V1r22r2 = 1\nU2 \n1\nY1u, f2 L2Y1u, f2. \n(7.77)\n \nfunction of r only\n \nfunction of u, f only\n \n The left-hand side of Eq. (7.77) is a function of r only, while the right-hand side is a function \nof u, f only.\n Step 5: Now imagine changing the isolated variable r by a small amount. In principle, the left-hand \nside of Eq. (7.77) could change, but nothing on the right-hand side would. Therefore, if the \nequation is to be true for all values of r, the particular combination of r dependences on the \nleft-hand side must result in no overall dependence on r—the left-hand side must be a con-\nstant. We thus deﬁne a separation constant, which we call A in this case:\n \n1\nR1r2 d\ndr\n ar2 \ndR1r2\ndr\nb - 2m\nU2  1E - V1r22r2 = 1\nU2 \n1\nY1u, f2\n L2Y1u, f2 K A. \n(7.78)\n Step 6: Write each equation in standard form by multiplying each equation by its unknown function \nto clear it from the denominator. Rearranging Eq. (7.78) slightly, we obtain the radial and \nangular equations in the more standard forms:\n \nc -  U2\n2mr2 d\ndr ar2 d\ndr\n b + V1r2 + A U2\n2mr2 d R1r2 = ER1r2 \n(7.79)\n \nL2Y1u, f2 = A U2 Y1u, f2. \n(7.80)\nNotice that the only place that the central potential V1r2 enters the set of differential equations is in \nthe radial equation (7.79), which is not yet in the form of an eigenvalue equation because it contains \ntwo unknown constants, E and A. Equation (7.80) is an eigenvalue equation for the orbital angular \nmomentum operator L 2 with eigenvalue AU2. It has the same form as Eq. (7.57), so we fully expect \nthat the separation constant A = /1/ + 12, which we will prove shortly. The angular momentum \neigenvalue equation is independent of the central potential V1r2, so once we have solved for the \norbital angular momentum eigenstates, we will have solved that aspect of the problem for all central \npotentials. Only the radial equation need be solved again for different potentials.\nThe separation of variables procedure can be applied again to separate the u dependence from the \nf dependence in the angular equation (7.80). If we let\n \nY1u, f2 = \u00121u2\u00131f2, \n(7.81)\n",
    "218 \nAngular Momentum\nthen the separated equations are (Problem 7.10)\n \nc\n1\nsin u d\nd u\n asin u d\nd ub - B \n1\n sin2 u d \u00121u2 = -A \u00121u2 \n(7.82)\n \nd2\n \u00131f2\ndf2\n= -B \u0013(f), \n(7.83)\nwhere we have deﬁned the new separation constant as B. Equation (7.83) is an eigenvalue equation for \nthe operator d2>df2 with eigenvalue -B. Equation (7.82) is not yet in the form of an eigenvalue equa-\ntion because it contains two unknown constants A and B.\nWe started with a partial differential equation in three variables and we ended up with three ordi-\nnary differential equations by introducing two separation constants A and B. You should always get \none fewer separation constant than the number of variables you started with; each separation constant \nshould appear in two equations of the ﬁnal set.\nSo in turn we have identiﬁed a radial differential equation for R1r2, a polar angle differential \nequation for \u00121u2, and an azimuthal differential equation for \u00131f2. But note that the radial equation \ncontains the polar separation constant A and the polar equation contains the azimuthal separation \nconstant B. So we must solve the azimuthal equation ﬁrst, then the polar equation, and ﬁnally the \nradial equation. The azimuthal solution to Eq. (7.83) determines the constant B, which then goes \ninto Eq. (7.82) to determine the polar angle solution and the constant A. The combined azimuthal and \npolar solutions also satisfy the eigenvalue equation (7.80) for the orbital angular momentum operator L2. \nFinally, the constant A goes into the radial equation (7.79) and the energy eigenvalues are determined.\nRather than simply solving these mathematical equations, we will place each of these three \neigenvalue equations in some physical context by identifying situations that isolate the different equa-\ntions from the original energy eigenvalue equation H0 E9 = E0 E9. In this chapter, we focus on the two \nangular equations, which are independent of the central potential energy V1r2. In the next chapter, we \nsolve the radial equation for the special case of the hydrogen atom with the Coulomb potential energy \nfunction.\n7.5 \u0002 MOTION OF A PARTICLE ON A RING\nTo isolate the azimuthal eigenvalue problem in Eq. (7.83), we consider a system with no radial or \npolar angle dependence. This system comprises a particle of mass m conﬁned to move on a ring of \nconstant radius r0, as shown in Fig. 7.7. We assume that the ring lies in the x, y plane, so that in spheri-\ncal coordinates u = p>2. Thus, the motion takes place at constant r and constant u, with the azimuthal \nangle f as the sole degree of freedom. The wave function c is independent of r and u, so derivatives \nwith respect to those variables are zero. Hence, the energy eigenvalue equation [Eq. (7.43)] reduces to\n \n-U2\n2m  1\nr2\n0\n 0 2\n0 f2 c + V1r02c = Eringc, \n(7.84)\nwhich is the position representation of\n \nHring0 Ering9 = Ering0 Ering9. \n(7.85)\n",
    "7.5 Motion of a Particle on a Ring \n219\nFollowing our notation in the previous section, we call the wave function \u00131f2 and we change the \npartial derivative in Eq. (7.84) to a total derivative because there is only one variable. For this simpli-\nﬁed ring problem, the potential energy is a constant V1r02, which we choose to be zero, but we have to \nremember that we cannot make this choice when we are working on the full hydrogen atom problem. \nWe also identify mr2\n0 = I as the moment of inertia of a classical particle of mass m traveling in a ring \nabout the origin. With these choices, the energy eigenvalue equation becomes\n \n-  U2\n2I d2\ndf2 \u00131f2 = Ering\u00131f2. \n(7.86)\nThis is the same eigenvalue equation we found in Eq. (7.83) for the azimuthal function \u00131f2 as long \nas we identify the separation constant B as\n \nB = 2I\nU2 Ering \n(7.87)\nin this problem of a particle on a ring. Thus, this idealized particle-on-a-ring example has the same dif-\nferential equation, and hence the same wave function solutions, as the separated azimuthal equation in \nthe three-dimensional hydrogen atom problem.\nIf we compare the azimuthal differential equation (7.86) with the orbital angular momentum \noperator in Eq. (7.69), we note that the energy eigenvalue equation can be expressed as\n \nL2\nz\n2I\n \u00131f2 = Ering\u00131f2, \n(7.88)\nwhich again emphasizes the importance of angular momentum. This energy eigenvalue equation is \nwhat you would expect for a classical particle rotating in a circular path in the x, y plane with kinetic \nenergy T = Iv2>2 = L2\nz >2I and resultant Hamiltonian\n \nHring = T =\nL2\nz\n2I , \n(7.89)\nx\ny\nΦ\nr0\nΜ\nKnown quantities\nΜ, r0, I, \u0002\nParameters\nΦ\nUnknown quantities\nEring,\u0007Ψ\nFIGURE 7.7 Particle conﬁned to move on a ring.\n",
    "220 \nAngular Momentum\nassuming zero potential energy. We noted earlier that eigenstates of Lz obey an eigenvalue equation\n \nLz0 m9 = m U0 m9, \n(7.90)\nwhere we suppress the / quantum number (for the moment) because it is not applicable to this ideal-\nized one-dimensional particle-on-a-ring problem. The 0 m9 states are also eigenstates of L2\nz:\n \nL2\nz 0 m9 = m2U20 m9 \n(7.91)\nand hence of the Hamiltonian of the particle on a ring:\n \n Hring0 m9 = Ering0 m9  \n \n \nL2\nz\n2I 0 m9 = m2 U2\n2I 0 m9. \n \n(7.92)\nSo it looks like we already know the answer; that the energy eigenvalues are E = m2 U2>2I and the \nseparation constant is B = m2. However, we know the properties of the 0 m9 states in the abstract only; \nwe do not know their spatial representation. That comes from solving the differential equation (7.86), \nwhich is the position representation of the abstract equation (7.92). Let’s solve it and conﬁrm our \nexpectations about the energy eigenvalues.\n 7.5.1 \u0002 Azimuthal Solution\nThe azimuthal differential equation written in terms of the separation constant is\n \nd2\u00131f2\ndf2\n= -B\u00131f2. \n(7.93)\nThe solutions to this differential equation are the complex exponentials\n \n\u00131f2 = Ne{i2Bf, \n(7.94)\nwhere N is the normalization constant. Mathematically B could have any value, but the physics \nimposes some constraints.\nThere is no “boundary” on the ring, so we cannot impose boundary conditions like we did for the \npotential energy well problems in Chapter 5. However, there is one very important property of the wave \nfunction that we can invoke: it must be single-valued. The variable f is the azimuthal angle around the \nring, so that f + 2p is physically the same point as f. If we go once around the ring and return to our \nstarting point, the value of the wave function must remain the same. Therefore, the solutions must sat-\nisfy the periodicity condition \u00131f + 2p2 = \u00131f2. In order for the eigenstate wave function \u00131f2 \nto be periodic, the value of 1B must be real (complex 1B would result in real exponential solutions). \nFurthermore, the solutions must have the correct period, which requires that 1B be an integer:\n \nm = 0, {1, {2, ... . \n(7.95)\nSo we see that there are many solutions, each corresponding to a different integer (which can be zero, \npositive, or negative). We write the solutions as\n \n\u0013m1f2 = Neimf. \n(7.96)\n",
    "7.5 Motion of a Particle on a Ring \n221\nThe quantum number m is the orbital magnetic quantum number we introduced in Section 7.3. We \ndon’t use a subscript on m here because there is no need to distinguish it from spin for now.\nIf we operate on the eigenstate wave function \u0013m1f2 with the derivative form of the Lz operator, \nwe obtain\n \n Lz\u0013m1f2 = -i U 0\n0 f\n 1Neimf2  \n \n = -i U1im21Neimf2 \n \n = m U1Neimf2\n \n \n = m U\u0013m1f2.\n \n \n(7.97)\nAs expected, we have found that the energy eigenstates for the particle on a ring are the states 0 m9 that \nsatisfy the Lz eigenvalue equation (7.90).\nAs usual, we ﬁnd the normalization constant N in Eq. (7.94) by requiring that the probability of \nﬁnding the particle somewhere on the ring is unity:\n \n1 =\n \nL\n2p\n0\n\u0013*\nm1f2\u0013m1f2df =\n \nL\n2p\n0\nN*e-imf Neimf df = 2p0 N0\n2. \n(7.98)\nWe are free to choose the constant to be real and positive:\n \nN =\n1\n22p\n. \n(7.99)\nWe have thus found the position representation \u0013m1f2 = 8f0 m9 of the 0 m9 states:\n \n0 m9 \u0003 \u0013m1f2 =\n1\n22p\n eimf   . \n(7.100)\nThe eigenfunctions of the ring form an orthonormal set (Problem 7.11):\n \nL\n2p\n0\n\u0013*\nk1f2\u0013m1f2df = dkm. \n(7.101)\nTo reiterate, these functions are eigenstates of the ring Hamiltonian\n \n Hring0 m9 = Ering0 m9\n \n \n Hring\u0013m1f2 = Ering\u0013m1f2 \n \n(7.102)\nas well as eigenstates of the z-component of orbital angular momentum\n \n Lz0 m9 = m U0 m9\n \n \n Lz\u0013m1f2 = m U\u0013m1f2  . \n \n(7.103)\n",
    "222 \nAngular Momentum\nThe allowed values of the separation constant B are B = m2, so the possible energy eigenvalues \nusing Eq. (7.87) are\n \nE0m0 = m2 U2\n2I, \n(7.104)\nwhich is exactly what we expected from Eq. (7.92). The spectrum of allowed energies is shown in \nFig. 7.8. The eigenstates corresponding to + 0 m0  and - 0 m0  states have the same energy, so there are \ntwo energy states at every allowed energy except for the one corresponding to m = 0. Thus the \nparticle-on-a-ring system exhibits degeneracy, which we ﬁrst encountered in the free-particle system \nin Section 6.1.1. For the particle-on-a-ring system, all states are two-fold degenerate except for m = 0, \nwhich is nondegenerate. The {m degeneracy of the energy eigenstates corresponds to the angular \nmomentum states with Lz = +mU and Lz = -mU. That is, the two degenerate energy states represent \nstates with opposite components of the angular momentum along the z-axis. The energy is the same \nregardless of the direction of rotation, which is analogous to the free particle in one dimension where \nthe energy is independent of the direction of travel.\nThe particle on a ring is a one-dimensional system even though it exists in a two-dimensional \nspace. This is because there is only one degree of freedom f, similar to the particle-in-a-box system \nwe studied in Chapter 5, where the single degree of freedom was x. The solutions to both problems \nhave the same oscillatory form. As in the particle-in-a-box problem, the energy eigenvalues of the par-\nticle-on-a-ring system are discrete because of a boundary condition. The difference is that the bound-\nary condition appropriate to the ring problem is periodicity because f is a physical angle, rather than \nc1x2 = 0 at the boundaries, which is appropriate to an inﬁnite potential.\n0\n5\n10\n15\nE/E1\n\u0002m\u0002 =\u00074\n\u0002m\u0002 =\u00073\n\u0002m\u0002 =\u00072\n\u0002m\u0002 =\u00071\nm =\u00070\nFIGURE 7.8 Energy spectrum for a particle on a ring.\n",
    "7.5 Motion of a Particle on a Ring \n223\n 7.5.2 \u0002 Quantum Measurements on a Particle Conﬁned to a Ring\nMany of the aspects of quantum measurement applied to this new system are similar to the spin and \nparticle-in-a-box examples we studied previously (e.g., Examples 2.3, 5.5, and 7.1). However, the \ndegeneracy of energy levels presents a new aspect. Because the states 0 m9 and 0 -m9 have the same \nenergy, the probability of measuring the energy E0m0 is the sum\n \nPE0m0 = 08m0 c90\n2 + 08-m0 c90\n2, \n(7.105)\nexcept for the m = 0 state. On the other hand, the state 0 m9 uniquely speciﬁes the orbital angular \nmomentum component along the z-direction, so the probability of measuring the angular momentum \ncomponent is\n \nPLz=m U = 08m0 c90\n2. \n(7.106)\nExample 7.2 A particle on a ring is in the superposition state\n \n0 c9 =\n1\n17 10 09 + 20 19 + 0\n -19 + 0 292. \n(7.107)\nIf we measure the energy, what is the probability of measuring the value E1 = U2>2I and what is \nthe state of the system after measuring that value?\nThe probability of measuring the value E1 = U2>2I is obtained using Eq. (7.105):\n \n PE1 = 0810 c90\n2 + 08-10 c90\n2\n \n \n = @ H1@  1\n17  A @  0I + 2@  1I + @  -1I + @  2IB @\n2\n+ @ H-1@  1\n17  A @  0I + 2@  1I + @  -1I + @2IB @\n2\n \n \n = @  2\n17\n @\n2\n+ @  1\n17\n @\n2\n \n(7.108)\n \n = 5\n7.\n \nAfter the measurement, the new state vector is the normalized projection of the input state onto the \nkets corresponding to the result of the measurement (postulate 5, Chapter 2):\n \n@cafter E0m09 = 0 m98m0 + 0 -m98-m0\n2PE0m0\n0 c9, \n(7.109)\nwhich in this case is\n \n @cafter E19 = 0 19810 + 0  -198-10\n2PE1\n 1\n17 10 09 + 20 19 + 0  -19 + 0 292 \n(7.110)\n \n =\n1\n15 120 19 + 0  -192.\n \nUsing Stern-Gerlach analyzers, measurements of the angular momentum component Lz could \nbe made after the energy measurement, and would yield the results shown in Fig. 7.9 (Problem 7.12).\n",
    "224 \nAngular Momentum\n 7.5.3 \u0002 Superposition States\nThe eigenstate wave functions for the particle on a ring are complex, so we must plot both the real \nand imaginary components for a proper graphical representation of the wave function. Plots of three \n\u0013m1f2 eigenstates are shown in Fig. 7.10. The probability density of an eigenstate is\n \nPm1f2 = 0 \u0013m1f20\n2. \n(7.111)\nSubstituting in the eigenstate wave function from Eq. (7.100), we obtain\n \nPm1f2 = 2\n1\n12p eimf 2\n2\n=\n1\n2p , \n(7.112)\nwhich is a constant independent of the quantum number m. So there is no measurable spatial depen-\ndence of the 0 m9 eigenstates.\nHowever, there is spatial dependence in the probability density for superposition states. For \nexample, consider a state of the system with an initial wave function comprising two eigenstates:\n \nc1f, 02 = c1\u0013m11f2 + c2eiu\u0013m21f2. \n(7.113)\nm \u0004\u00070\nΠ\n2Π\nΦ\n\t(Φ)\n1\n2Π\n\n1\n\u000b\n2Π\n\u000b\nΦ\nm \u0004\u00071\nΠ\n2Π\n\t(Φ)\n1\n2Π\n\n1\n\u000b\n2Π\n\u000b\nΦ\nm \u0004\u00072\nΠ\n2Π\n\t(Φ)\n1\n2Π\n\n1\n\u000b\n2Π\n\u000b\nFIGURE 7.10 Eigenstate wave functions for a particle on a ring. The real part of the wave function is the solid \nline and the imaginary part is the dashed line.\n14\nH\n2\n1\n0\n\n1\n\n2\n2\nLz\nLz\n1\n0\n\n1\n\n2\n2\n1\n0\n\n1\n\n2\n58\n14\n14\nLz\n\u0002Ψ\u0003\nE2\nE1\nE0\nFIGURE 7.9 Energy measurement and orbital angular momentum component measurements.\n",
    "7.5 Motion of a Particle on a Ring \n225\nWe assume that this function is already properly normalized Aso that c2\n1 + c2\n2 = 1B, and we assume \nthat the constants c1 and c2 are real. An overall phase has no physical meaning (cannot be measured), \nso we can always choose one coefﬁcient to be real. Relative phases play a crucial role in measurement, \nso we have made the relative phase explicit by separating the phase eiu from the coefﬁcient of the sec-\nond term. Using the Schrödinger time-evolution recipe from Chapter 3, the initial state in Eq. (7.113) \nbecomes\n \n c1f, t2 = c1\u0013m11f2e-iE0m10t>U + c2eiu\u0013m21f2e-iE0m20t>U\n \n \n = c1 \n1\n12p eim1fe-iE0m10t>U + c2eiu\n1\n12p eim2fe-iE0m20t>U.\n \n \n(7.114)\nFor this state, the probability density for measuring the position of the particle on the ring is\n \n P1f, t2 = 0 c1f, t20\n2 = c*1f, t2c1f, t2\n \n =\n1\n2p\n 1c1e-im1fe+iE0m10t>U + c2e-iue-im2fe+iE0m20t>U21c1eim1fe-iE0m10t>U + c2eiueim2fe-iE0m20t>U2\n \n =\n1\n2p\n 3c2\n1 + c2\n2 + c1c21e-im1fe+iE 0\n  m10\n t>Ueiueim2fe-iE 0\n  m20\n t>U + eim1fe-iE 0\n  m10\n t>Ue-iue-im2fe+iE 0\n  m20\n t>U24 \n \n=\n1\n2p\n 31 + 2c1c2 cos 51m1 - m22f - u - 1E0m10 - E0m202t>U64. \n(7.115)\nThis probability density exhibits spatial dependence and time dependence in the form of a wave mov-\ning around the ring. There are four measurable properties of this probability density wave: the spatial \nfrequency, the temporal frequency, the amplitude, and the phase of the wave. These four quantities are \ndetermined by the factors 1m1 - m22, 1E0m10 - E0m202, c1c2, and u, respectively, in Eq. (7.115). Using \nthe measured values for these four quantities, the direction of the wave, and the normalization condi-\ntion c2\n1 + c2\n2 = 1 allows us to determine the ﬁve constants c1, c2, m1, m2, and u that specify the wave \nfunction superposition in Eq. (7.113) (Problem 7.17).\nExample 7.3 Calculate and plot the probability density for the initial superposition state\n \nc1f, 02 = 4\n1\n3 \u001331f2 + i 4\n2\n3 \u0013-11f2. \n(7.116)\nThe time-evolved wave function is\n \nc1f, t2 =\n1\n12p\n  4\n1\n3\n ei3f e-i9Ut>2I + i \n1\n12p\n  4\n2\n3\n e-if e-iUt>2I \n(7.117)\nand the probability density is\n \n P1f, t2 =\n1\n2p\n c 1 + 222\n3  cos a4f - p\n2 - 8U\n2I\n tb d  \n(7.118)\n \n =\n1\n2p\n c 1 + 222\n3   sin a4f - 4U\nI\n tb d .\n \n",
    "226 \nAngular Momentum\nThe probability density varies around the ring and at t = 0 is a maximum where  sin 4f = +1, or \nf = p>8, 5p>8, 9p>8, and 13p>8. The spatial dependence of the probability density is plotted in \nFig. 7.11 in three different graphical representations. The traditional plot in Fig. 7.11(a) is similar \nto the particle-in-a-box plots and conveys the idea of a varying density, but the single dimension \nfails to make it clear that the left and right ends are connected on the ring and must have the same \ndensity. The plot in Fig. 7.11(b) makes the connection between f = 0 and f = 2p clear by \nplotting the probability density using grayscale (color) as a parameter along the ring. The plot in \nFig. 7.11(c) combines the ideas of the previous two plots by using both the vertical scale and gray-\nscale to represent the probability density. Because the probability density varies with time, each of \nthe plots in Fig. 7.11 moves (toward increasing f in this example) when they are animated. (See the \nactivity on a particle conﬁned to a ring.)\nFIGURE 7.11 Probability density of a superposition state for a particle on a ring displayed as \n(a) a linear plot, (b) grayscale around the ring, and (c) height and grayscale around the ring.\n",
    "7.6 Motion on a Sphere \n227\nFIGURE 7.12 Particle conﬁned to move on the surface of a sphere.\nWe have now completed our investigation of the particle on a ring. We have identiﬁed the Hamil-\ntonian, found the energy spectrum, found the position representation of the eigenstates, and studied the \nprobability distributions, including the time dependence. These eigenstates are the same ones we will \nuse as the azimuthal part of the three-dimensional wave function to solve the hydrogen atom problem.\n7.6 \u0002 MOTION ON A SPHERE\nWe have now solved for the azimuthal part of the hydrogen atom wave function, so we turn our atten-\ntion to the polar angle part of the wave function. This is best done in the context of a system that \ninvolves both angular variables u and f, so that we ﬁnd the solutions \u00121u2 to Eq. (7.82) and then com-\nbine them with the azimuthal states \u0013m1f2 to form the solutions Y1u, f2 to the angular momentum \neigenvalue equation (7.80). The system we choose to discuss angular wave functions is that of a par-\nticle of mass m conﬁned to the surface of a sphere of radius r0, as shown in Fig. 7.12, which is a natural \nextension of the ring problem. The results of this analysis yield predictions that can be successfully \ncompared with experiments on molecules and nuclei that rotate more than they vibrate. For this reason, \nthe problem of a mass conﬁned to a sphere is often called the rigid rotor problem. Furthermore, the \nsolutions Y1u, f2 that we ﬁnd, called spherical harmonics, occur whenever one solves a partial dif-\nferential equation that involves spherical symmetry.\nFor a particle conﬁned to a sphere, the wave function c is independent of r, so derivatives with \nrespect to r are zero and the energy eigenvalue equation (7.43) reduces to\n \n-  U2\n2mr 2\n0\n c\n1\n sin u 0\n0 u\n asin u 0\n0 u b +\n1\nsin2 u 0 2\n0 f 2 d c + V1r02c = Esphere c, \n(7.119)\nwhich is the position representation of\n \nHsphere0 Esphere9 = Esphere0 Esphere9. \n(7.120)\n",
    "228 \nAngular Momentum\nFollowing our previous notation, we call the wave function Y1u, f2 = \u00121u2\u00131f2. For this simpliﬁed \nsphere problem, we choose the potential energy V1r02 to be zero, as in the ring problem. We identify \nmr 2\n0 = I as the moment of inertia of a classical particle of mass m moving on a sphere. With these \nchanges, the energy eigenvalue equation is\n \n-  U2\n2I\n c\n1\n sin u 0\n0 u\n asin u 0\n0 u b +\n1\nsin2 u 0 2\n0 f2 d Y1u, f2 = EsphereY1u, f2. \n(7.121)\nUsing Eq. (7.70), we identify the angular differential operator as the position representation of the \nangular momentum operator L2 and write the energy eigenvalue equation in operator form:\n \nL2\n2I\n Y1u, f2 = EsphereY1u, f2. \n(7.122)\nThis eigenvalue equation appears similar to the ring problem but is actually very different, because \nnow the particle can move anywhere on the sphere and so the angular momentum is no longer con-\nﬁned to the z-direction. Equation (7.122) is the same eigenvalue equation we obtained in Eq. (7.80) \nthrough separation of variables for the angular function Y1u, f2 = \u00121u2\u00131f2, as long as we identify \nthe separation constant A as\n \nA = 2I\nU2 Esphere. \n(7.123)\nAs noted above, we expect that the separation constant A is equal to /1/ + 12 because the L2 oper-\nator obeys the eigenvalue equation (7.57). Now that we know that this sphere problem is equiva-\nlent to the angular momentum eigenvalue equation, we proceed to solve for the polar angle function \n\u00121u2 that we identiﬁed in the differential equation (7.82). We have already solved for the azimuthal \nangle wave function \u0013m1f2, so at the end we combine \u00121u2 and \u0013m1f2 to yield the eigenstates \nY1u, f2 = \u00121u2\u0013m1f2 for the particle on the sphere. In due course, we’ll ﬁnd that the \u00121u2 eigen-\nstates have their own quantum numbers, and so we’ll label the polar angle states as \u0012m\n/ 1u2 and the \nspherical harmonics as Y m\n/ 1u, f2 (the m label is a superscipt, not an exponent).\n 7.6.1 \u0002 Series Solution of Legendre’s Equation\nThe polar angle equation (7.82) is our ﬁrst encounter with a differential equation that requires a \nsophisticated solution method. The next two sections detail the series solution method and arrive at \nthe Legendre and associated Legendre functions that solve the polar angle equation. If you are already \nexperienced with this method and are knowledgeable about the Legendre functions, you may safely \nskip these two sections.\nThe solutions \u0013m1f2 to the f equation (7.83) that we found in the ring problem told us the pos-\nsible values of the separation constant B = m2, where m is any integer. We now substitute these \nknown values into the polar angle differential equation (7.82). The u equation becomes an eigenvalue \nequation for the unknown function \u00121u2 and the separation constant A:\n \nc\n1\nsin u d\ndu\n asin u d\ndub -\nm2\nsin2 u d \u00121u2 = -A\u00121u2. \n(7.124)\nTo solve this differential equation, we start with a change of independent variable z = cos u, where \nz is the rectangular coordinate for the particle, assuming a unit sphere. We also introduce a new function\n \nP1z2 = \u00121u2. \n(7.125)\n",
    "7.6 Motion on a Sphere \n229\nThis step is not mathematically necessary but resolves the difference between the required normaliza-\ntion properties of quantum mechanial wave functions [\u0012(u)] and the standard normalization used for \nthe solutions [P(z)] to Eq. (7.124). As u ranges from 0 to p, z ranges from 1 to -1. Using the chain \nrule for derivatives and  sin u = 21 - z2, the differential term becomes\n \nd\ndu = dz\ndu\n  d\ndz = -sin u d\ndz = - 21 - z2 d\ndz . \n(7.126)\nNotice, particularly, the last equality: we are trying to change variables from u to z, so it is important to \nmake sure we change all the u’s to z’s. Multiplying by sin u, we obtain:\n \n sin u d\ndu = -11 - z22 d\ndz . \n(7.127)\nBe careful ﬁnding the second derivative; it involves a product rule:\n \n 1\nsin u d\ndu\n asin u d\ndub = d\ndz\n a11 - z22 d\ndzb\n \n \n = 11 - z22 d 2\ndz2 - 2z d\ndz\n .\n \n \n(7.128)\nInserting Eq. (7.128) into Eq. (7.124), we obtain a standard form of the associated Legendre \nequation:\n \na11 - z22 d 2\ndz2 - 2z d\ndz + A -\nm2\n11 - z22b P1z2 = 0. \n(7.129)\nOnce we solve this equation for the eigenfunctions P1z2, we substitute z = cos u everywhere to ﬁnd \nthe quantum mechanical eigenfunctions \u00121u2 of the original equation (7.124).\nIt is easiest to begin the solution of Eq. (7.129) with the m = 0 case, which corresponds to the \nsimplest possible f dependence: \u001301f2 = 1> 12p. Setting m = 0 in equation (7.129) gives us the \nspecial case known as Legendre’s equation:\n \n¢11 - z22 d 2\ndz2 - 2z d\ndz + A≤ P1z2 = 0. \n(7.130)\nBy dividing this equation by 11 - z22, we express it as\n \na d 2\ndz2 -\n2z\n11 - z22\n d\ndz +\nA\n11 - z22\nb P1z2 = 0, \n(7.131)\nwhich emphasizes the mathematical singularities at z = {1.\nWe use the series method to ﬁnd a solution of Legendre’s equation; that is, we assume that the \nsolution can be written as a series\n \nP1z2 = a\n\u0005\nn=0\na n zn \n(7.132)\n",
    "230 \nAngular Momentum\nand solve for the coefﬁcients an. The differentials\n \n dP\ndz = a\n\u0005\nn=0\na n nz n-1\n \n(7.133)\n \n d 2P\ndz2 = a\n\u0005\nn=0\na n n1n - 12z n-2 \n(7.134)\nsubstituted into Eq. (7.130) yield\n \n 0 = a\n\u0005\nn=0\na n n1n - 12z n-2 - z 2 a\n\u0005\nn=0\na n n1n - 12z n-2 - 2z a\n\u0005\nn=0\na n nz n-1 + Aa\n\u0005\nn=0\na n z n \n \n = a\n\u0005\nn=0\na n n1n - 12z n-2 - a\n\u0005\nn=0\na n n1n - 12z n - 2a\n\u0005\nn=0\na n nz n + Aa\n\u0005\nn=0\na n z n.\n \n(7.135)\nTo combine the sums, we must collect terms of the same powers. To do this, we note that the ﬁrst two \nterms of the ﬁrst sum are zero:\n \na01021-12z-2 + a1112102z-1 = 0 + 0, \n(7.136)\nso we shift the dummy variable n S n + 2 in the ﬁrst sum, giving\n \n a\n\u0005\nn=0\nan n1n - 12z n-2 =\na\n\u0005\nn=-2\nan+21n + 221n + 12z n \n \n = a\n\u0005\nn=0\nan+21n + 221n + 12z n.\n \n \n(7.137)\nNow all the sums in Eq. (7.135) have the same power and we group the sums together to yield\n \na\n\u0005\nn=0\n3an+21n + 221n + 12 - an n1n - 12 - 2an n + Aan4z n = 0. \n(7.138)\nNow comes the magic part. Because Eq. (7.138) is true for all values of z, the coefﬁcient of zn for \neach term in the sum must separately be zero:\n \nan+21n + 221n + 12 - an n1n - 12 - 2an n + Aan = 0. \n(7.139)\nTherefore, we can solve Eq. (7.139) for the recurrence relation, giving the later coefﬁcient an+2 in \nterms of the earlier coefﬁcient an:\n \nan+2 =\nn1n + 12 - A\n1n + 221n + 12\n an. \n(7.140)\nPlugging successive even values of n into the recurrence relation Eq. (7.140) allows us to ﬁnd a2, a4, \netc. in terms of the arbitrary constant a0, and successive odd values of n allow us to ﬁnd a3, a5, etc. in \nterms of the arbitrary constant a1. Thus, for the second-order differential equation (7.130), we obtain \ntwo solutions as expected. The coefﬁcient a0 becomes the normalization constant for a solution with \n",
    "7.6 Motion on a Sphere \n231\nonly even powers of z, and a1 becomes the normalization constant for a solution with only odd powers \nof z. For example, some even coefﬁcients are\n \n a2 = -  A\n2\n a0\n \n \n a4 = 6 - A\n12\n a2 = - a6 - A\n12\nba A\n2 b a0 \n \n(7.141)\nand some odd coefﬁcients are\n \n a3 = 2 - A\n6\n a1\n \n \n a5 = 12 - A\n20\n a3 = a 12 - A\n20\nba2 - A\n6\nb a1 \n(7.142)\nso that\n \nP1z2 = a0 c z0 - aA\n2 b z 2 + ...d + a1c z1 + a2 - A\n6\nb z 3 + ...d . \n(7.143)\nWe seek solutions that are normalizable, so we must address the convergence of the series solu-\ntion. Note that for large n, the recurrence relation gives\n \nan+2\nan\n\u0002 1, \n(7.144)\nwhich implies that the series solution we have assumed does not converge at the end points where \nz = {1. This is to be expected because the coefﬁcients of Eq. (7.131) are singular at z = {1, which \ncorrespond to the north and south poles u = 0, p. But there is nothing special about the physics at \nthese points, only the choice of coordinates is special here. This is an important example of a problem \nwhere the choice of coordinates for a partial differential equation ends up imposing boundary con-\nditions on the ordinary differential equation which comes from it. To ensure convergence, we thus \nrequire that the series not be inﬁnite, but rather that it terminate at some ﬁnite power nmax. Inspection \nof the recurrence relation in Eq. (7.140) tells us that the series terminates if we choose\n \nA = nmax1nmax + 12, \n(7.145)\nwhere nmax is a non-negative integer. When we started this problem, we expected the separation con-\nstant to be A = /1/ + 12 and we have found just that, as long as we identify the termination index \nnmax with the orbital angular momentum quantum number /. We have now succeeded in ﬁnding the \nquantization condition for orbital angular momentum, and it is just as we expected from our work \nwith spin angular momentum. But we came to it from a very different perspective, which is one of the \nbeautiful aspects of physics. We have now found that the orbital angular momentum quantum number \n/ must be a non-negative integer:\n \n/ = 0, 1, 2, 3, 4, ...  . \n(7.146)\nThe solutions to Eq. (7.130) for these special values of A are polynomials of degree /, denoted P/1z2, \nand are called Legendre polynomials.\nThe Legendre polynomials can also be calculated using Rodrigues’ formula:\n \nP/1z2 =\n1\n2//! d /\ndz/ 1z 2 - 12/. \n(7.147)\n",
    "232 \nAngular Momentum\nThe ﬁrst few Legendre polynomials are shown in Table 7.1 and are plotted in Fig. 7.13. There are \nseveral useful patterns to the Legendre polynomials:\n• The overall coefﬁcient for each solution is conventionally chosen so that P/112 = 1. As \ndiscussed in the next section, this is an inconvenient convention that we are stuck with.\n• P/1z2 is a polynomial of degree /.\n• Each P/1z2 contains only odd or only even powers of z, depending on whether / is even \nor odd. Therefore, each P/1z2 is either an even or an odd function.\n• Because the differential operator in Eq. (7.130) is Hermitian, we are guaranteed that \nthe Legendre polynomials are orthogonal for different values of / ( just as with Fourier \nseries), that is,\n \nL\n1\n-1\nP*\nk 1z2P/1z2dz =\n2\n2/ + 1 dk/. \n(7.148)\nNote that the Legendre polynomials are not normalized to unity, rather the “squared norm” of P/ is \n2>12/ + 12.\nNotice that when we substitute the separation constant A = /1/ + 12 back into the original dif-\nferential equation (7.130)\n \n11 - z 22 d 2P\ndz 2 - 2z dP\ndz + /1/ + 12P = 0, \n(7.149)\nTable 7.1 Legendre Polynomials\nP01z2 = 1\nP11z2 = z\nP21z2 = 1\n2 13z  2 - 12\nP31z2 = 1\n2 15z  3 - 3z2\nP41z2 = 1\n8 135z  4 - 30z  2 + 32\nP51z2 = 1\n8 163z  5 - 70z  3 + 15z2\n\n1\n1\nz\n\n1\n\n0.5\n0.5\nPl(z)\n1\nl\u00040\nl\u00041\nl\u00042\nl\u00043\nl\u00044\nFIGURE 7.13 Legendre polynomials.\n",
    "7.6 Motion on a Sphere \n233\nthe result is a different equation for different values of /. For a given value of /, you should expect \ntwo solutions of Eq. (7.149), but we have only given one. The “other” solution for each value of / is \nnot regular (i.e., it blows up) at z = {1. In cases where the separation constant A does not have the \nspecial value /1/ + 12 for non-negative integer values of /, it turns out that both solutions blow up. \nWe discard these irregular solutions as unphysical for the problem we are solving.\n 7.6.2 \u0002 Associated Legendre Functions\nWe now return to Eq. (7.129) to consider the cases with m \u0002 0. We need a slightly more sophisticated \nversion of the series technique from the m = 0 case, and we do not detail this here. We again ﬁnd solu-\ntions that are regular at z = {1 whenever we choose A = /1/ + 12 for / \u0002 50, 1, 2, 3, ...6. With \nthese values for A, we obtain the standard form of the associated Legendre equation, namely\n \na11 - z 22 d 2\ndz 2 - 2z d\ndz + /1/ + 12 -\nm2\n11 - z 22b P1z2 = 0. \n(7.150)\nSolutions of this equation that are regular at z = {1 are called associated Legendre functions, and \nare calculated from the Legendre functions by differentiation:\n \n P m\n/ 1z2 = P -m\n/ 1z2 = 11 - z 22\nm>2\n d m\ndz m P/1z2  \n \n =\n1\n2//!\n 11 - z 22\nm>2\n d m+/\ndz m+/ 1z 2 - 12/,\n \n \n(7.151)\nwhere m Ú 0. In Eq. (7.151), the integer m is a superscript label—not an exponent—on the associated \nLegendre function Pm\n/ 1z2, but m is an exponent on the right hand side of the equation. The associated \nLegendre equation (7.150) is independent of the sign of the integer m, so\n \nP-m\n/ 1z2 = Pm\n/ 1z2. \n(7.152)\nThe Legendre function P/1z2 is a polynomial of order /, so the mth derivative in Eq. (7.151), and hence \nthe associated Legendre function Pm\n/ 1z2, vanishes if m 7 /. In the ring problem, we learned that m \nmust be an integer, but there was no limit on the possible values of those integers. Now we have dis-\ncovered an additional constraint on the magnetic quantum number for the sphere problem\n \nm = -/, -/ + 1, ..., -1, 0, 1, ..., / - 1, /  . \n(7.153)\nAgain, this is consistent with our expectations from the spin problem.\nIt is more useful for us to express the Legendre polynomials and the associated Legendre functions \nin terms of the polar angle u rather than the variable z, so we substitute z = cos u into the functions. \nThe Legendre polynomial P/1cos u2 is a polynomial in cos u, while the associated Legendre function \nP m\n/ 1cos u2 is a polynomial in cos u times a factor of sinm u because of the additional term\n \n11 - z 22\nm>2 = 1sin2 u2\nm>2 =  sinm u \n(7.154)\n",
    "234 \nAngular Momentum\nin Eq. (7.151). Some of the associated Legendre functions are shown in Table 7.2 and are plotted in \nFig. 7.14. The plots in Fig. 7.14 are polar plots where the “radius” r at each angle u is the absolute \nvalue of the function P m\n/ 1cos u2, as illustrated further in Fig. 7.15. The associated Legendre functions \nare deﬁned over the interval 0 … u … p, but the convention is to plot the functions reﬂected in the \nz-axis in anticipation of their application to the full three-dimensional hydrogen atom.\nSome useful properties of the associated Legendre functions are:\n• P m\n/ 1z2 = 0 if 0m0 7 /\n• P -m\n/ 1z2 = P m\n/ 1z2\n• P m\n/ 1{12 = 0 for m \u0002 0 Acf. factor of 11 - z 22\nm>2B\n• P m\n/ 1-z2 = 1-12\n/-m P m\n/ 1z2 (behavior under parity)\n• \nL\n1\n-1\nP m\n/ 1z2P m\nq1z2dz =\n2\n12/ + 12 1/ + m2!\n1/ - m2! d/q.\nP0\n0\nP1\n0\nP1\n1\nP2\n0\nP2\n1\nP2\n2\nP3\n0\nP3\n1\nP3\n2\nP3\n3\nFIGURE 7.14 Polar plots of associated Legendre functions.\n",
    "7.6 Motion on a Sphere \n235\n\n1\n\n0.5\n0.5\n1\n\n0.5\n0.5\nz\nr \u0004 Pl\nm(Θ)\u0002\nΘ\nP1\n1 (Θ)\n\u0002\nFIGURE 7.15 Polar plot of an associated Legendre function.\nTable 7.2 Associated Legendre Functions\nP 0\n0 = 1 \nP 0\n1 = cos u \nP 0\n3 = 1\n2 15 cos3 u - 3 cos u2\nP 1\n1 = sin u \nP 1\n3 = 3\n2 sin u15 cos2 u - 12\nP 0\n2 = 1\n2 13 cos2 u - 12 \nP 2\n3 = 15 sin2 u cos u\nP 1\n2 = 3 sin u cos u \nP 3\n3 = 15 sin3 u\nP 2\n2 = 3 sin2 u\nThe last property shows that for each given value of m, the associated Legendre functions form an \northogonal basis on the interval -1 … z … 1. Any function on this interval can be expanded in terms \nof any one of these bases. The associated Legendre functions are not normalized to unity, but by multi-\nplying by the appropriate factor we construct the eigenstates \u0012 m\n/ 1u2 that solve the eigenvalue equation \n(7.124) and are normalized to unity over the interval 0 … u … p:\n \nL\np\n0\n\u0012 m\n/ 1u2\u0012 m\nq1u2sin u du = d/q. \n(7.155)\nThese eigenstates are\n \n\u0012m\n/ 1u2 = 1-12\nm\n 12/ + 12\n2\n 1/ - m2!\n1/ + m2!\n P m\n/ 1cos u2, m Ú 0, \n(7.156)\nwith the negative m states deﬁned by\n \n\u0012 -m\n/ 1u2 = 1-12\nm \u0012 m\n/ 1u2, m Ú 0. \n(7.157)\n",
    "236 \nAngular Momentum\n 7.6.3 \u0002 Energy Eigenvalues of a Rigid Rotor\nWe now know the separation constant A in Eq. (7.124), which determines the energy of the parti-\ncle bound to the sphere through Eq. (7.123). Substituting A = /1/ + 12 into Eq. (7.123) gives the \nallowed energy eigenvalues\n \nE/ = U2\n2I\n  /1/ + 12. \n(7.158)\nThe energy is independent of the magnetic quantum number m, so each energy level is degenerate, \nwith 12/ + 12 possible m states for a given /. The free particle and the particle on a ring both exhib-\nited degeneracy because the kinetic energy was independent of the direction of the motion. Similarly, \nthe rotational kinetic energy of the particle on a sphere is independent of the orientation of the angular \nmomentum. The spectrum of energy levels is shown in Fig. 7.16. The selection rule for transitions \nbetween these levels is \u0006/ = {1, yielding the emission lines in Fig. 7.16. The transition energies are\n \n \u0006E = E/+1 - E/\n \n \n = U2\n2I\n  1/ + 121/ + 22 - U2\n2I\n /1/ + 12 \n \n = U2\n2I\n  21/ + 12\n \n \n(7.159)\n \n = U2\n2I\n  52, 4, 6, 8, 10, ...6.\n \n0\n1\n2\n3\n4\n5\nEnergy (\u00022/2l\u0007)\nE (\u00022/2l\u0007)\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n22\n24\n26\n28\n30\n0\n2\n4\n6\n8\n10\nl\nFIGURE 7.16 Energy spectrum and transitions of a rigid rotor.\n",
    "7.6 Motion on a Sphere \n237\nz\nm1\nm2\nr1\n\u0002\nr2\n\u0002\nL\u0002\nFIGURE 7.17 A diatomic molecule is the simplest example of a rigid rotor. The two-atom \nsystem rotates around an axis perpendicular to the symmetry axis of the molecule.\nA physical example of this particle-on-a-sphere model is the rigid rotor. The simplest rigid rotor is \na diatomic molecule, as illustrated in Fig. 7.17. The two atoms with a separation r0 have a moment \nof inertia about the center of mass of I = mr2\n0, just as we have assumed in our particle-on-a-sphere \nmodel. Molecular spectroscopists call the energy U2>2I the rotational constant of the molecule.\nFor example, consider the diatomic molecule hydrogen chloride HCl. The equilibrium bond \nlength is r0 = 0.127 nm, which gives a rotational constant\n \nU2\n2I `\nHCl\n= 1.32 meV = 10.7 cm-1. \n(7.160)\nThe experimentally measured value is 10.4 cm-1. That seems close, but is in fact a clue that something \nis missing from the model. It turns out that the coupling of the vibrational motion (Chapter 9) to the \nrotational motion changes the energy levels of a real molecule. Reﬁning simple models leads to better \nunderstanding; our job here is to gain basic understanding.\n 7.6.4 \u0002 Spherical Harmonics\nWe have in hand the eigenfunctions of the two angular equations, so we can construct the energy \neigenstates of the particle on the sphere. The normalized solutions of the f equation (7.83) that satisfy \nperiodic boundary conditions are the \u0013m1f2 states in Eq. (7.100) with the restriction that the magnetic \nquantum number m be an integer. The normalized solutions of the u equation (7.82) that are regular at \nthe poles are the \u0012 m\n/ 1u2 states in Eq. (7.156) with the restriction that / = 0, 1, 2, ... and m = -/, ..., / \nin integer steps. The product \u0012 m\n/ 1u2\u0013m1f2 of the two solutions yields the function Y m\n/ 1u, f2 that we \nassumed when we applied the separation of variables procedure to the angular equation (7.80). These \nangular functions are the spherical harmonics\n \nY m\n/ 1u, f2 = 1-12\n1m+ 0m02>2\nC\n12/ + 12\n4p\n \n1/ - 0 m02!\n1/ + 0 m02! P m\n/ 1cos u2eimf, \n(7.161)\nthe ﬁrst few of which are listed in Table 7.3. The somewhat peculiar choice of sign is conventional and \ngives the useful result\n \nY -m\n/ 1u, f2 = 1-12\nm Y m*\n/ 1u, f2. \n(7.162)\n",
    "238 \nAngular Momentum\nLet’s now discuss the important properties of the spherical harmonics.\n• Orthonormality\nThe spherical harmonics are orthonormal on the unit sphere\n \n8/1m10 /2m29 =\n \nL\n2p\n0\nL\np\n0\nY m1*\n/1 1u, f2 Y m2\n/2 1u, f2 sin u du df = d/1/2dm1m2  , \n(7.163)\nwhich means that two wave functions must have the same angular momentum (/1 = /2) and the \nsame z-component (m1 = m2) or else the overlap integral is zero. The / orthogonality comes from the \nassociated Legendre u functions and the m orthogonality comes from the complex exponential f func-\ntions. The orthonormality condition is also written compactly as an integral over the full solid angle\n \nL\nY m1*\n/1 1u, f2Y m2\n/2 1u, f2d\t = d/1/2dm1m2 \n(7.164)\nfor those common occasions when there is no need to consider separate angular integrals.\n• Completeness\nThe spherical harmonics are complete in the sense that any sufﬁciently smooth function c1u, f2 \non the unit sphere can be expanded in a Laplace series as\n \nc1u, f2 = a\n\u0005\n/=0\n a\n/\nm=-/\nc/mY m\n/ 1u, f2. \n(7.165)\nThe c/m expansion coefﬁcients are found by projecting the superposition wave function onto the \n0 /m9 eigenstates:\n \nc/m = 8/m0 c9 =\n \nL\n2p\n0\nL\np\n0\nY m*\n/ 1u, f2c1u, f2sin u du df. \n(7.166)\nTable 7.3 Spherical Harmonics\n/ \nm \nY  m\n/ 1u, f2\n0 \n0 \nY  0\n0 = 4\n1\n4p\n1 \n0 \nY  0\n1 = 4\n3\n4p cos u\n \n{1 \nY {1\n1\n= <4\n3\n8p sin ue{if\n2 \n0 \nY  0\n2 = 4\n5\n16p 13 cos2 u - 12\n \n{1 \nY {1\n2\n= <4\n15\n8p sin u cos ue{if\n \n{2 \nY {2\n2\n= 4\n15\n32p sin2 ue{i2f\n3 \n0 \nY  0\n3 = 4\n7\n16p 15 cos3 u - 3 cos u2\n \n{1 \nY {1\n3\n= <4\n21\n64p sin u15 cos 2 u - 12e{i f\n \n{2 \nY {2\n3\n= 4\n105\n32p sin2 u cos ue{i2f\n \n{3 \nY {3\n3\n= 4\n35\n64p sin3 ue{i3f\n",
    "7.6 Motion on a Sphere \n239\n• Parity\nThe behavior of the spherical harmonics under the parity operation r S -r is determined by the \nangular momentum quantum number /. Spherical harmonics with even / have even parity and \nthose with odd / have odd parity:\n \nY m\n/ 1p - u, f + p2 = 1-12\n/\n Y m\n/ 1u, f2. \n(7.167)\nTo summarize, we have found that the spherical harmonics Y m\n/ 1u, f2 are eigenstates of the Ham-\niltonian for the particle on a sphere [Eq. (7.121)]. Because the Hamiltonian for this problem is pro-\nportional to the L2 orbital angular momentum operator [Eq. (7.122)], the spherical harmonics are also \neigenstates of L2 [Eq. (7.80)]. The spherical harmonics contain the \u0013m1f2 eigenstates, so they are also \neigenstates of the Lz operator (Problem 7.24). These three eigenvalue equations are\n \nHsphereY m\n/ 1u, f2 = U2\n2I\n /1/ + 12Y m\n/ 1u, f2\n \nL 2Y m\n/ 1u, f2 = /1/ + 12U2Y m\n/ 1u, f2 \n.\n \n(7.168)\n \nLzY m\n/ 1u, f2 = m UY m\n/ 1u, f2\nThese three operators share eigenstates because they commute with each other (Problem 7.9).\nFor a particle on a sphere, the measurement probabilities are complicated by the degeneracy, \njust as we saw in the particle on a ring [Eq. (7.105)]. For a state 0 c9, the probability of measuring the \nenergy E/ is a sum over all the degenerate states:\n \nPE/ =\na\n/\nm=-/\n08/m0 c90\n2. \n(7.169)\nThe probability of measuring the L2 angular momentum observable to be /1/ + 12U2 is also given by \nEq. (7.169) because the energy eigenstates and the L2 eigenstates exhibit the same degeneracy. The \nprobability of measuring the Lz angular momentum observable to be m U is the sum over all the / states \nfor which that value of m is allowed:\n \nPLz=m U = a\n\u0005\n/=m\n08/m0 c90\n2. \n(7.170)\nLet’s practice using the spherical harmonics.\nExample 7.4 A particle on a sphere is in the state\n \nc1u, f2 = 4\n15\n16p sin 2 u cos f. \n(7.171)\nWhat are the probabilities of energy (H) and angular momentum AL2 and LzB measurements?\nThis wave function looks almost like a spherical harmonic eigenstate, so we try to do this \nproblem by inspection. Using trigonometric identities, rewrite the wave function as\n \n c1u, f2 = 4\n15\n16p 12 sin u cos u2aeif + e-if\n2\nb\n \n \n = 4\n15\n16p sin u cos ueif + 4\n15\n16p sin u cos ue-if\n \n(7.172)\n \n = -  1\n12 A-4\n15\n8p sin u cos ueifB +\n1\n12 A4\n15\n8p sin u cos ue-ifB, \n",
    "240 \nAngular Momentum\nwhich we recognize from Table 7.3 of spherical harmonics as the superposition\n \nc1u, f2 = -  1\n12 Y 1\n11u, f2 +\n1\n12 Y \n -1\n1 1u, f2. \n(7.173)\nIn Dirac notation, this state is\n \n0 c9 = -  1\n12 0 119 +\n1\n12 0 1, -19. \n(7.174)\nSo, without doing any integrals, we obtain the expansion coefﬁcients\n \nc/m = 8/m0 c9 = - 1\n12 d/1dm1 +\n1\n12 d/1dm,-1 \n(7.175)\nand the energy measurement probabilities\n \n PE/ =\na\n/\nm=-/\n08/m0 c90\n2\n \n \n = A-  1\n12 d/1B\n2\n+ A 1\n12 d/1B\n2\n \n \n(7.176)\n \n = d/1.\n \nThe probability of measuring the energy to be E1 = U2>I is 100%, as is the probability for measur-\ning L2 = 2U2.\nThe probability of measuring Lz = U is\n \n PL z=\n U = a\n\u0005\n/=m\n08/10 c90\n2 \n \n = A-  1\n12B\n2\n \n \n(7.177)\n \n = 1\n2.\n \nSimilarly PL z=-U = 1>2.\nSolution by inspection is nice when it works, but sometimes we must bite the bullet and integrate, \nas we’ll see in the example in the next section.\n 7.6.5 \u0002 Visualization of Spherical Harmonics\nVisualization of spherical harmonics is a challenge because of the two-dimensional structure of the \nwave functions and the fact that they are represented by complex numbers. To overcome the com-\nplex problem, it is common to plot the complex square, which is the probability density, or to plot \nthe absolute value. In either case, the azimuthal dependence vanishes as we saw with the ring prob-\nlem earlier. A two-dimensional polar plot, like we used for the Legendre polynomials, is therefore \nsufﬁcient to display the polar angle dependence, as shown in Fig. 7.18(a). To convey the uniform \nazimuthal dependence, one should visualize the polar plot as rotated around the vertical z-axis, as \ndisplayed in the three-dimensional polar plot in Fig. 7.18(b). In this plot, the “radius” at each angle \nu, f is the complex square of the spherical harmonic function. In the ring case, we also displayed the \nprobability density as a grayscale on the ring itself, which suggests plotting the spherical harmonic \n",
    "7.6 Motion on a Sphere \n241\nFIGURE 7.18 Spherical harmonic 0 Y 1\n31u, f20\n2 displayed as (a) a two-dimensional polar plot, \n(b) a three-dimensional polar plot, (c) grayscale on a sphere, (d) grayscale on a ﬂat rectangular  \nprojection, and (e) grayscale on a ﬂat Mollweide projection.\nprobability density as grayscale (or color) on the sphere, as shown in Fig. 7.18(c). The grayscale \nsphere can then also be projected onto a ﬂat surface, as mapmakers do, yielding the two-dimensional \nrepresentations in Figs. 7.18(d) and (e). Note that these plots do not yet give the three-dimensional \nelectron probability density because the  spherical harmonics are not functions of the radius r. We \nstill have to learn about the radial wave function in the next chapter.\nThe three-dimensional polar plots for the ﬁrst four sets of spherical harmonics are shown in \nFig. 7.19. The standard convention is to label the spherical harmonics, or orbitals, with a letter \n corresponding to the value of the orbital angular momentum quantum number /:\n \n / = 0   1   2   3   4   5   6   7  ...  \n \n(7.178)\n \n letter = s   p   d    f    g    h    i    k  ... . \nThe plots in Fig. 7.19 show angular momentum eigenstate wave functions. In many cases, such as the \ncarbon atom in Fig. 7.5, the actual orbitals are superpositions, or hybrids, of the angular momentum \neigenstates.\n",
    "242 \nAngular Momentum\nExample 7.5 Given the angular wave function for a particle on a sphere\n \nc1u, f2 = 4\n60060\n139301p a 1\n4 + cos3 2 u + sin2 fb, \n(7.179)\ngenerate the histogram of possible energy measurements.\nTo ﬁnd the probabilities of energy measurements\n \nPE/ =\na\n/\nm=-/\n08/m0 c90\n2, \n(7.180)\nFIGURE 7.19 Three-dimensional polar plots of some spherical harmonics.\n",
    "7.6 Motion on a Sphere \n243\nwe must ﬁnd the overlap integrals\n \nc/m = 8/m0 c9 =\n \nL\n2p\n0\nL\np\n0\nY m*\n/ 1u, f2c1u, f2sin u d u d f. \n(7.181)\nThis wave function looks like it could be a ﬁnite sum of spherical harmonics, but the wild nor-\nmalization constant is a clue that an inﬁnite sum is required. You could try to calculate the c/m \n coefﬁcients by hand, but this problem is a good chance to explore the power of mathematical pack-\nages such as Mathematica, Maple, or Matlab. Mathematica, for example, has the spherical harmon-\nics built into its system and the overlap integral requires one command line\n \nTable3Integrate3Conjugate3SphericalHarmonicY3l,m,u,f44 \n \nc3u,f4Sin3u4,5u,0,p6,5f,0,2p64,5l,0,76,5m,-l,l64, \n(7.182)\nwhich generates a table of the c/m coefﬁcients for / = 0 S 7 and m = -/ S /, assuming c1u, f2 \nhas been deﬁned previously. A subset of the results is presented in Table 7.4. The last column of \nthe table is the probability of measuring the energy E/. From the explicit square roots in the results, \nit is evident that Mathematica does the integral analytically, not numerically. The results also indi-\ncate the symmetries of the wave function. Only m = -2, 0, 2 states contribute nonzero terms to \nthe expansion because of the symmetry of the azimuthal dependence of the wave function:\n \n  sin2 f = 31eif - e-if2>2i4\n2\n \n \n = 1\n4 1ei 2f + e-i 2f - 22. \n \n(7.183)\nFor m = 0, the coefﬁcients beyond / = 6 are zero because the polar angle term cos3 2 u has no \ncos/ u or  sin/ u terms beyond / = 6. The m = {2 coefﬁcients extend to / = \u0005.\nTable 7.4 Coefﬁcients of Spherical Harmonic Expansion\nc/m\nm =\na\n/\nm=-/\n0 c/m0\n2\n-3\n-2\n-1\n0\n1\n2\n3\n0\n1\n2\n3\nO = 4\n5\n6\n7\n0\n0\n0\n0\n0\n-5 4\n1001\n278602\n0\n4\n3003\n278602\n0\n-  13\n2  4\n11\n139301\n0\n0\n0\n0\n0\n0\n0\n0\n69 4\n429\n4875535\n0\n80 4\n143\n2925321\n0\n-128 4\n39\n53630885\n0\n512 4\n5\n32178531\n0\n0\n0\n0\n0\n0\n0\n0\n5 4\n1001\n278602\n0\n4\n3003\n278602\n0\n 13\n2  4\n11\n139301\n0\n0\n0\n0\n0\n0\n2042469\n4875535\n0\n1440725\n2925321\n0\n1795131\n5360885\n0\n3050869\n64357062\n0\n",
    "244 \nAngular Momentum\nA partial histogram of energy measurement probabilities is shown in Fig. 7.20. The energy \nprobabilities for the states up to / = 6 shown in Table 7.4 and Fig. 7.20 sum to 0.9923, so we \nexpect that the ﬁnite spherical harmonic expansion\n \ncfinite1u, f2 = a\n6\n/=0\n a\n/\nm=-/\nc/mY m\n/ 1u, f2 \n(7.184)\nshould be a good approximation to the actual wave function. The original wave function and the \nﬁnite spherical harmonic expansion are shown in Fig. 7.21. The match between the two is good, \nexcept at the endpoints u = 0,p, which is a phenomenon similar to that seen in Fourier series \nexpansions. Note that this wave function exhibits azimuthal dependence because it is a superposi-\ntion of different m states.\nFIGURE 7.21 (a) Original wave function and (b) 6-term spherical harmonic expansion.\nE0 E1 E2\nE3\nE4\nE5\nE6\nE\n0.5\nPEi\n\u0002\u0004E2\u0002Ψ\u0003\u00022\n\u0002\u0004E4\u0002Ψ\u0003\u00022\n\u0002\u0004E6\u0002Ψ\u0003\u00022\n\u0002\u0004E0\u0002Ψ\u0003\u00022\nFIGURE 7.20 Histogram of energy measurements.\n",
    "Problems \n245\nSUMMARY\nIn this chapter, we introduced the idea of orbital angular momentum and illustrated its importance in \nsolving the three-dimensional differential equation that is the energy eigenvalue equation for the hydro-\ngen atom. By separating variables in the eigenvalue equation H0 E9 = E0 E9, we isolated the differential \nequations for the angular variables u and f from the differential equation for the radial variable r. Only \nthe radial differential equation includes the potential energy, so the solutions to the angular equations are \nvalid for all central potentials. The f equation yielded the azimuthal wave functions\n \n\u0013m1f2 =\n1\n22p\n eimf \n(7.185)\nand the u equation yielded the polar wave functions\n \n\u0012 m\n/ 1u2 = C\n12/ + 12\n2\n \n1/ - 0 m02!\n1/ + 0 m02! P m\n/ 1cos u2. \n(7.186)\nThe products of these two are the total angular wave functions, which are the spherical harmonics\n \n0 /m9 \u0003 Y m\n/ 1u, f2 = 1-121m+ 0  m 02>2\nC\n12/ + 12\n4p\n \n1/ - 0 m02!\n1/ + 0 m02! P m\n/ 1cos u2eimf. \n(7.187)\nThe spherical harmonics are eigenstates of the angular momentum operators L2 and Lz. In Dirac nota-\ntion, the eigenvalue equations are\n \n L 20 /m9 = /1/ + 12U20 /m9 \n \n Lz0 /m9 = m U0 /m9.\n \n(7.188)\nIn wave function notation, the eigenvalue equations are\n \n L 2Y m\n/ 1u, f2 = /1/ + 12U2Y m\n/ 1u, f2 \n \n LzY m\n/ 1u, f2 = m UY m\n/ 1u, f2.\n \n \n(7.189)\nThe limitations on the quantum numbers m and / arise from requiring the wave function to be periodic \nin f and ﬁnite at u = 0, p, respectively. The quantum numbers m and / must be integers with the \nlimitations\n \n m = -/, -/ + 1, ... 0, ..., / - 1, / \n \n / = 0, 1, 2, 3, ...\u0005.\n \n(7.190)\nPROBLEMS\n 7.1 Show that the two-body Hamiltonian in Eq. (7.3) can be separated into center-of-mass and \nrelative Hamiltonians, as in Eq. (7.11). Do this in two ways: (a) with momentum operators in \nthe abstract, and (b) momentum operators in the position representation.\n 7.2 Use the separation of variables procedure in Appendix E to separate the two-body energy eigen-\nvalue equation into the center-of-mass and relative energy eigenvalue equations in Eq. (7.24).\n 7.3 Use the separation of variables procedure in Appendix E to separate equation Eq. (7.29) into \nthree ordinary differential equations for each Cartesian coordinate.\n",
    "246 \nAngular Momentum\n 7.4 Verify the angular momentum commutation relations in Eqs. (7.51) and (7.55).\n 7.5 An angular momentum system with / = 1 is prepared in the state\n0 c9 =\n2\n129 0 119 + i 3\n129 0 109 -\n4\n129 0 1, -19.\na) What are the possible results of a measurement of the angular momentum component Lz, \nand with what probabilities would they occur?\nb) What are the possible results of a measurement of the angular momentum component Lx, \nand with what probabilities would they occur?\nc) Plot histograms of the predicted measurement results from parts (a) and (b).\n 7.6 An angular momentum system with / = 1 is prepared in the state\n0 c9 =\n1\n114 0 119 -\n3\n114 0 109 + i 2\n114 0 1, -19.\na) What are the possible results of a measurement of the angular momentum component Lz, \nand with what probabilities would they occur?\nb) Suppose that the Lz measurement on the system yields the result Lz = -U. Subsequent to \nthat result, a second measurement is performed to measure the angular momentum com-\nponent Lx. What are the possible results of that measurement, and with what probabilities \nwould they occur?\nc) Draw a schematic diagram depicting the successive measurements in parts (a) and (b).\n 7.7 An angular momentum system is prepared in the state\n0 c9 =\n1\n110 0 119 -\n2\n110 0 109 + i 2\n110 0 229 + i 1\n110 0 209.\na) What are the possible results of a measurement of the angular momentum observable L2, \nand with what probabilities would they occur?\nb) What are the possible results of a measurement of the angular momentum component Lz, \nand with what probabilities would they occur?\nc) Plot histograms of the predicted measurement results from parts (a) and (b).\n 7.8 Using Eqs. (7.35) and (7.47), show that the orbital angular momentum operators Lx, Ly, and Lz \nare represented in spherical coordinates as\n \n Lx \u0003 i U asin f 0\n0 u + cos f cot u 0\n0 fb\n \n \n Ly \u0003 i U a-cos f 0\n0 u + sin f cot u 0\n0 fb \n \n Lz \u0003 -i U 0\n0 f\n \n \n and verify that the operator L 2 = L~L = L 2\nx + L 2\ny + L 2\nz is represented in spherical coordi-\nnates as in Eq. (7.70).\n 7.9 Verify that the angular momentum operators L 2 and Lz commute with the central force \nHamiltonian.\n 7.10 Use the separation of variables procedure in Appendix E on the angular equation (7.80) to obtain \nEq. (7.82) and Eq. (7.83) for the polar and azimuthal angles.\n",
    "Problems \n247\n 7.11 Show by direct integration that the azimuthal eigenstates \u0013m1f2 are orthonormal.\n 7.12 Consider the particle-on-a-ring state in Example 7.2. What are the possible values of a mea-\nsurement of the observable Lz? Calculate the measurement probabilities and show that they \nagree with the results indicated in Fig. 7.9.\n 7.13 Consider the normalized state 0 c9 for a quantum mechanical particle of mass m constrained to \nmove on a circle of radius r0, given by:\n0 c9 = 23\n2 0 39 + i\n2 0 -29.\na) What is the probability that a measurement of Lz will yield 2U? 3U?\nb) What is the probability that a measurement of the energy yields E = 2U2>I?\nc) What is the expectation value of Lz in this state?\nd) What is the expectation value of the energy in this state?\n 7.14 A particle on a ring is in the normalized state\n0 c9 =\n1\n115\n 1 0 09 + i0 19 - 2i0 29 + 30 -292.\na) What are the possible results of an energy measurement and what are the corresponding \nprobabilities? Calculate the expectation value of the energy.\nb) What are the possible results of an Lz measurement and what are the corresponding prob-\nabilities? Calculate the expectation value of Lz.\n 7.15 Consider the normalized state 0 c9 for a quantum mechanical particle of mass m constrained to \nmove on a circle of radius r0, given by\n0 c9 \u0003\nN\n2 + cos 13f2 ,\n \n where N is the normalization constant.\na) Find the normalization constant N.\nb) Plot the wave function.\nc) What is the expectation value of Lz in this state?\n 7.16 A particle on a ring is prepared in the initial state\n0 c9 = 4\n1\n5 0 29 - i  4\n4\n5\n 0 -19.\n \n Find the probability density as a function of time.\n 7.17 The time-dependent probability density for a particle on a ring is measured to be\nP1f, t2 =\n1\n2p\n c 1 - 22223\n13\n sin a3f + 3U\n2I\n tb d .\n \n Determine the initial state of the particle.\n 7.18 Calculate the moment of inertia of a diatomic molecule, as depicted in Fig. 7.17. Express the \nmoment two ways: (1) in terms of the individual masses m1 and m2 and the coordinates r1 and r2, \nand (2) in terms of the reduced mass m and the atom-atom separation r0.\n 7.19 Calculate the rotational constant for the hydrogen iodide (HI) molecule.\n",
    "248 \nAngular Momentum\n 7.20 In each of the following sums, shift the dummy index n S n + 2. Don’t forget to shift the lim-\nits of the sum as well. Then write out all of the terms in the sum (if the sum has a ﬁnite number \nof terms) or the ﬁrst ﬁve terms in the sum (if the sum has an inﬁnite number of terms) and con-\nvince yourself that the two different expressions for each sum are the same:\na) a\n3\nn=0\nn\nb) a\n5\nn=1\neinf\nc) a\n\u0005\nn=0\nan n1n - 12zn-2\n 7.21 Use Rodrigues’ formula, by hand, to generate the ﬁrst ﬁve Legendre polynomials. Show by \ndirect integration that P21cos u2 is orthogonal to P41cos u2, and that P21cos u2 is normalized \naccording to Eq. (7.148).\n 7.22 Generate the associated Legendre functions P 1\n21z2 and P 3\n31z2 by hand. Express each function \nboth as a function of the argument z and as a function of u.\n 7.23 Use the deﬁnitions in Eqs. (7.151) and (7.161) to generate the spherical harmonics Y 0\n11u, f2 \nand Y \n -2\n2 1u, f2. Ensure that they are normalized and orthogonal by direct integration.\n 7.24 Verify that the spherical harmonics are eigenstates of the orbital angular momentum component \noperator Lz by direct application of the position representation of Lz. What are the eigenvalues?\n 7.25 Verify that the spherical harmonics are eigenstates of the orbital angular momentum operator \nL2. What are the eigenvalues?\n 7.26 Consider the new operators L+ and L- deﬁned by L{ = Lx { iLy. Use the results of Problem 7.8 \nto show that the position representations of these operators in spherical coordinates are\nL{ = U e{i f a{ 0\n0 u + i cot u 0\n0 f b.\n \n Act with these new operators on all the / = 1 spherical harmonic wave functions and sum-\nmarize your results in Dirac notation. Based on your results, postulate the names of these new \noperators. This is a preview of Chapter 11.\n 7.27 Express the / = 1 spherical harmonics in Cartesian coordinates. Combine the m = {1 func-\ntions in two possible ways to make real functions that closely resemble the m = 0 function.\n 7.28 Use your favorite tool (e.g., Maple, Mathematica, Matlab, pencil) to generate the Legendre \npolynomial expansion of the function f 1z2 = sin 1pz2. How many terms do you need to \ninclude in a partial sum to get a “good” approximation to f 1z2 for -1 6 z 6 1? What do you \nmean by a “good” approximation? How about the interval -2 6 z 6 2? How good is your \napproximation then? Discuss your answers. Answer the same set of questions for the function \ng1z2 = sin 13pz2.\n 7.29 Consider the normalized state of a particle on a sphere given by:\n0 c9 =\n1\n12 0 1, -19 +\n1\n13 0 109 +\ni\n16 0 009.\na) What is the probability that a measurement of Lz will yield 2U? -U? 0 U?\nb) What is the expectation value of Lz in this state?\nc) What is the expectation value of L 2 in this state?\n",
    "Resources \n249\nd) What is the expectation value of the energy in this state?\ne) What is the expectation value of Ly in this state?\n 7.30 A particle conﬁned to the surface of a sphere is in the state\nc1u, f2 = μ\nN ap2\n4 - u2b,\n   0 6 u 6 p\n2\n0,\n     p\n2 6 u 6 p,\n \n where the normalization constant is\nN =\n1\nB\np5\n8 + 2p3 - 24p2 + 48p\n.\na) Find the coefﬁcients for the 0 /m9 = 0 009, 0 1, -19, 0 109, and 0 119 terms in a spherical \nharmonics expansion of c1u, f2.\nb) What is the probability that a measurement of the square of the total angular momentum \nwill yield 2U2? 0 U2?\nc) What is the probability that the particle can be found in the region 0 6 u 6 p>6 and \n0 6 f 6 p>6? Repeat the question for the region 5p>6 6 u 6 p and 0 6 f 6 p>6. \nPlot your approximation from part (a) above on and check to see if your answers seem \nreasonable. (The activity on linear combinations of spherical harmonics has a Maple  \nworksheet ylmcombo.mws for plotting.)\nRESOURCES\nActivities\nThese activities are available at\nwww.physics.oregonstate.edu/qmactivities\nEigenstates of a Particle Conﬁned to a Ring: Students investigate eigenstates of a quantum particle \nconﬁned to a ring.\nGuessing the Legendre Polynomial Expansion of a Function: Students try to ﬁt a given function \nwith a linear combination of Legendre polynomials using the guess and check method.\nFinding Legendre Coefﬁcients: Students use Maple to ﬁnd the ﬁrst few coefﬁcients of a Legendre \nseries to approximate a function.\nParticle Conﬁned to a Ring: Students visualize linear combinations of eigenstates and study anima-\ntions of time evolution of the probability density.\nParticle Conﬁned to a Sphere: Students visualize the spherical harmonics.\nLinear Combinations of Spherical Harmonics: Students visualize states that are made up of linear \ncombinations of spherical harmonics.\n",
    "C H A P T E R \n8\nHydrogen Atom\nThe angular wave functions we found in the last chapter are independent of the particular form of \nthe central potential that binds the system. The remaining radial part of the wave function, however, \ndepends critically on the central potential you choose. The radial part of the problem determines the \nallowed energies of the system and hence the spectroscopic ﬁngerprint of the system that we observe \nin experiments. In this chapter, we solve for the quantized energies and the radial wave functions of \nthe bound states of the hydrogen atom, which is the simplest atomic system, comprising one electron \nbound to one proton in the nucleus. The electron and proton are bound together by the Coulomb poten-\ntial, which underlies the bonding in all atoms, molecules, liquids, and solids.\n8.1 \u0002 THE RADIAL EIGENVALUE EQUATION\nIn Chapter 7, we separated the three-dimensional energy eigenvalue equation into differential equa-\ntions for each of the spherical coordinates r, u, and f. We solved the f eigenvalue equation (7.83) and \nfound the azimuthal eigenstates \u0013m1f2 and eigenvalues m, which determined the separation constant \nB = m2. We then used the separation constant B to make the u differential equation (7.82) into an \neigenvalue equation and solved for the polar eigenstates \u0012 m\n/ 1u2 and the eigenvalues /1/ + 12, which \ndetermined the separation constant A = /1/ + 12. We now use the separation constant A to make the \nradial differential equation (7.79) into an eigenvalue equation for the energy E:\n \nc-  U2\n2mr 2 d\ndr\n ar 2 d\ndrb + V1r2 + /1/ + 12 U2\n2mr 2d R1r2 = ER1r2. \n(8.1)\nSolving this differential equation will give us the radial eigenstates R1r2 and the allowed ener-\ngies E. We then combine the three separated eigenstates into the three-dimensional eigenstate \nc1r, u, f2 = R1r2Y m\n/ 1u, f2, where the spherical harmonics Y m\n/ 1u, f2 = \u0012 m\n/ 1u2\u0013m1f2 are the prod-\nucts of the azimuthal and polar eigenstates that we found in Chapter 7.\nBefore we begin the solution, notice that the radial eigenvalue equation (8.1) resembles a one-\ndimensional eigenvalue equation with an effective potential energy Veff:\n \nVeff 1r2 = V1r2 +\nU2/1/ + 12\n2mr 2\n. \n(8.2)\nThe term U2/1/ + 12>2mr 2 in the effective potential energy is called the centrifugal barrier. It \nbehaves like a repulsive potential, and it increases with / in exact analogy with classical mechanics. In \nthis viewpoint, the effective potential energy that determines the radial motion of the electron is differ-\nent for each state with a different angular momentum quantum number /, as shown in Fig. 8.1.\n \n",
    "8.1 The Radial Eigenvalue Equation \n251\nFor the hydrogen atom, the Coulomb potential energy is responsible for attracting the electron to \nthe proton. This Coulomb potential energy is\n \nV1r2 = -  Ze2\n4pe0r , \n(8.3)\nwhere we assume that the nucleus has a charge +Ze so that our solution applies to the general case of a \nhydrogenic atom: H, He+, Li++, etc. With this choice of V1r2, the radial differential equation is\n \nd 2R\ndr 2 + 2\nr dR\ndr + 2m\nU2  cE +\nZe2\n4pe0r -\nU2/1/ + 12\n2mr 2\nd R = 0. \n(8.4)\nThe potential energy at r = \u0005 is V1\u00052 = 0, so bound states have energy E 6 0 while unbound \nstates have energy E 7 0.\nIt is convenient at this point to rewrite the radial differential equation in terms of dimensionless \nenergy and position parameters. The angular differential equations in Chapter 7 were treated similarly \nbecause the separation constants A and B were dimensionless. We deﬁne a characteristic length scale \nof the hydrogenic atom as a, such that the dimensionless radius is\n \nr = r\na. \n(8.5)\nWithout knowing what this scale is yet, we write the differential equation for R1r2 as\n \n1\na2 d 2R\ndr2 + 1\na2 2\nr dR\ndr + 2m\nU2  cE +\nZ e2\n4pe0 ar - U2/1/ + 12\n2ma2r2\nd R = 0. \n(8.6)\nMultiplying Eq. (8.6) by a2, we obtain\n \nd 2R\ndr2 + 2\nr dR\ndr + c\n2ma2\nU2\n E + a mZ e2\n4pe0\n U2b 2a\nr -\n/1/ + 12\nr2\nd R = 0. \n(8.7)\n2\n4\n6\n8\n10\nr\na0\n\n8\n\n6\n\n4\n\n2\n0\n2\n4\nVeff(r) (eV)\n\u0002 \u0004\u00070\n\u0002 \u0004\u00071\n\u0002 \u0004\u00072\nFIGURE 8.1 The effective potential for different values of the angular momentum quantum number /.\n",
    "252 \nHydrogen Atom\nThe terms inside the square brackets of Eq. (8.7) are now dimensionless, so we identify the hydrogen \ncharacteristic length scale as\n \na = 4pe0  U2\nm Z e2  \n(8.8)\nand the characteristic energy scale as U2>2ma2. We deﬁne a dimensionless energy parameter as\n \n-g2 =\nE\na U2\n2ma2b\n   , \n(8.9)\nwhere we assume that E 6 0 because we are seeking bound-state solutions. Using U2>2ma2 as the \nenergy scale is reasonable in light of the ground state energy being E1 = p2\n U2>2ma2 for a particle in \na box of size a. With the dimensionless length and energy parameters, the radial differential equation \nbecomes\n \nd 2R\ndr2 + 2\nr dR\ndr + c-g2 + 2\nr -\n/1/ + 12\nr2\nd R = 0. \n(8.10)\nIn this dimensionless form, the eigenvalue we are seeking is g2 and the eigenfunction is R1r2.\n8.2 \u0002 SOLVING THE RADIAL EQUATION\n8.2.1 \u0002 Asymptotic Solutions to the Radial Equation\nTo solve the radial eigenvalue equation (8.10), it is instructive to ﬁrst get some clues about the form \nof the solution by looking at the limiting behavior of the solutions for large and small r (i.e., large \nand small r). For large r, the terms in Eq. (8.10) involving r-1 and r-2 can be neglected, so Eq. (8.10) \nbecomes approximately\n \nd 2R\ndr2 - g2R = 0. \n(8.11)\nThis equation has the familiar exponential solutions R1r2 = e{gr, where the { symbol is required \nbecause Eq. (8.11) involves the second derivative of R1r2. We eliminate one of these signs by not-\ning that the solution e+gr blows up as r goes to inﬁnity. We want solutions for the wave functions to \nyield reasonably behaved probability densities (that is, they must be ﬁnite everywhere), and we must \ntherefore discard any solution that leads to an inﬁnite probability. Our solution for the radial wave \nfunction in this limit then becomes:\n \nR1r2\u0003\n e-  gr 1large r2. \n(8.12)\nNow let’s look at the behavior of the solutions when r is small. Now the r-2 term dominates and \nwe neglect the other terms in the square brackets in Eq. (8.10). In this case, we obtain the approximate \nequation\n \nd 2R\ndr2 + 2\nr dR\ndr - /1/ + 12\nr2\n R = 0. \n(8.13)\n",
    "8.2 Solving the Radial Equation \n253\nWe see by inspection that a solution of the form R1r2 = rq satisﬁes Eq. (8.13). For this choice of \nR1r2, each term in Eq. (8.13) is proportional to rq-2, and the three terms sum to zero for all values of \nr when\n \nq1q - 12rq-2 + 2\nr\n qrq -1 - /1/ + 12\nr2\n rq = 0, \n(8.14)\nwhich leads to\n \nq1q + 12 - /1/ + 12 = 0. \n(8.15)\nThis quadratic equation for q yields two solutions: q = / and q = -/ - 1. For small r, the solution \nr-/-1 blows up, so we discard this solution. We then have the limiting form\n \nR1r2\u0007r/  1small r2. \n(8.16)\nCombining Eqs. (8.12) and (8.16), we expect the radial solution to look something like \nR1r2\u0007r/e-  gr. We have not violated the proper behavior at the limits by combining these two  solutions; \nR1r2 remains well-behaved for r = 0 and r S \u0005. What else do we need to complete the solution? We \nneed to know the radial dependence at intermediate r, so let’s try an additional function H1r2 that is well-\nbehaved by remaining ﬁnite at r = 0 Aor blowing up more slowly than r-l B and as r S \u0005 (or blowing \nup more slowly than egr). We therefore seek solutions to the radial equation of the form\n \nR1r2 = r/e-  gr H1r2, \n(8.17)\nand our next goal is to determine the function H1r2.\n8.2.2 \u0002 Series Solution to the Radial Equation\nWe substitute the trial function R1r2 = r/e-  gr H1r2 into the radial differential equation (8.10) in \norder to ﬁnd the differential equation for the new function H1r2. Immediately, we ﬁnd that we need \nthe ﬁrst two derivatives of R1r2:\n \ndR\ndr = r/-1e-  gr 3/H1r2 - grH1r2 + rH\u00041r24, \n(8.18)\nwhere H\u00041r2 = dH>dr, and\n \nd 2R\ndr2 = r/-1e-  gr 312 - 2g - 2g/2 H 1r2 + 12 + 2/ - 2gr2  H\u00041r2 + rH\u000e1r24. \n(8.19)\nNow we substitute Eqs. (8.18) and (8.19) into Eq. (8.10) and collect terms to obtain the differential \nequation for H1r2:\n \nr d 2H\ndr2 + 21/ + 1 - gr2 dH\ndr + 211 - g - g/2  H 1r2 = 0. \n(8.20)\nJust as we did with the u differential equation in Chapter 7 [Eq. (7.132)], we use a power series \nexpansion to solve the radial equation (8.20). We assume that H1r2 has the form\n \nH1r2 = a\n\u0005\nj=0\ncj\n r j, \n(8.21)\n",
    "254 \nHydrogen Atom\nand now our job is to ﬁnd the cj coefﬁcients. The derivatives of H1r2 in this series form that we need are\n \n dH\ndr = a\n\u0005\nj =  0\n jcj r j -1 = a\n\u0005\nj =  0\n1 j + 12cj+1 r j \n(8.22)\n \n d 2H\ndr2 = a\n\u0005\nj=0\n j1 j + 12cj+1 r j-1,\n \nwhere we have shifted indices in the ﬁrst equation, as we did in the angular solutions in Section 7.6.1. \nSubstituting Eq. (8.22) into Eq. (8.20), we obtain\n \n a\n\u0005\nj=0\n j1 j + 12cj+1 r j + 21/ + 12 a\n\u0005\nj=0\n1 j + 12cj +1 r j\n \n \n -2ga\n\u0005\nj=0\n jcj r j + 211 - g - g/2 a\n\u0005\nj=0\n cj r j = 0. \n(8.23)\nIn order for all terms of the series in Eq. (8.23) to sum to zero for any and all values of r, the coef-\nﬁcient of each power of r must be zero, just as for the Legendre equation solution. The coefﬁcient of \nthe general term r j is\n \nj1 j + 12cj +1 + 21/ + 121 j + 12cj +1 - 2gjcj + 211 - g - g/2cj = 0, \n(8.24)\nwhich leads to the recurrence relation\n \ncj +1 =\n2g11 + j + /2 - 2\n1 j + 121 j + 2/ + 22\n cj. \n(8.25)\nThe recurrence relation shows us that the starting coefﬁcient c0 determines all of the remaining expan-\nsion coefﬁcients in the function H1r2. The normalization requirement determines c0, as you have \nprobably already realized, and we’ll return to this point in Section 8.4.\nIn our study of the polar angle wave functions \u00121u2, we found that we had to force the series to \nterminate to prevent the wave function from becoming inﬁnite. So far, we have assumed that the series \nexpansion of H1r2 includes an inﬁnite number of terms 1 j S \u00052. We have forced the asymptotic \nforms of R1r2 to remain ﬁnite, so let’s see how the new part of the solution, H1r2, behaves for large \nvalues of j and how that affects the radial function R1r2 = r/e-gr H1r2.\nFor large j, the recurrence relation in Eq. (8.25) is\n \ncj +1 \u0002 2gj\nj 2  cj = 2g\nj\n cj. \n(8.26)\nThis is exactly the same recurrence relation we ﬁnd for the exponential function! The series expansion \nof the exponential function\n \nea\n x = a\n\u0005\nn=0\n a n\nn!\n x n = 1 + a\n1!\n x + a2\n2!\n x 2 + a3\n3!\n x 3 + ... \n(8.27)\nhas a recurrence relation cj+1 = 1a>1 j + 122cj \u0002 1a>j2cj for large j. Hence, the large j limit in \nEq. (8.26) implies that for large r,\n \nH1r2 \u0002 e2gr, \n(8.28)\n",
    "8.2 Solving the Radial Equation \n255\nwhich leads to an asymptotic radial function\n \nR1r2 \u0002 r/e -gre2gr = r/egr. \n(8.29)\nThis asymptotic behavior has the same exponential pathology that we rejected in arriving at Eq. (8.12), \nso we must reject it once again. We do that by forcing the series expansion of H1r2 to terminate at a \nﬁnite value of j, just as we did for the Legendre polynomials.\nHence, the requirement that the wave function be normalizable leads us to deﬁne a value jmax \nsuch that the numerator of the recurrence relation, Eq. (8.25), goes to zero and terminates the series:\n \n2g11 + jmax + /2 - 2 = 0. \n(8.30)\nBecause j and / are integers, 11 + jmax + /2 is also an integer, which we denote as n:\n \nn = jmax + / + 1. \n(8.31)\nThis new integer is the principal quantum number of the hydrogen atom. The deﬁnition of the prin-\ncipal quantum number in Eq. (8.31) leads us to three important conclusions.\n• The integers j and / both start at 0 (make sure you know why), so the principal quantum number \nn starts at 1 and continues to inﬁnity because / can go to inﬁnity:\n \n n = 1, 2, 3, ... \u0005  . \n(8.32)\n• The dimensionless energy parameter g has discrete values! We learn this by substituting the \nnew quantum number n into Eq. (8.30) and solving:\n \ng = 1\nn. \n(8.33)\nFurthermore, the energy itself takes on only discrete values, and we ﬁnd those values by \nsubstituting Eq. (8.33) into the deﬁnition of g in Eq. (8.9). We also need the length scale in \nEq. (8.8) and arrive at\n \n-  1\nn2 =\nE\na U2\n2ma2b\n=\nE\na U2\n2mb\n a4pe0\n U2\nm Z e2 b\n2\n. \n(8.34)\nSo the requirement that the radial wave function be well behaved has led us to the quanti-\nzation condition on the allowed energies of the hydrogen atom. Solving Eq. (8.34) for the \nallowed energy yields\n \nEn = -  1\n2n2 a Z e2\n4pe0\nb\n2m\nU2 ,   n = 1, 2, 3, ...   , \n(8.35)\nwhich relates the hydrogen energy to the newly deﬁned principal quantum number n. We’ll \nsay more about the energy spectrum in the next section.\n• The angular momentum quantum number / is limited to a ﬁnite set of values for every n. We \nlearn this by solving Eq. (8.31) for /:\n \n/ = n - jmax - 1. \n(8.36)\n",
    "256 \nHydrogen Atom\nThe polar angle eigenstate solution in Chapter 7 told us that the angular momentum quantum \nnumber / had a range from 0 to inﬁnity. The lower limit of 0 is consistent with Eq. (8.36), in \nwhich case n = jmax + 1. However, the upper limit of inﬁnity is consistent with Eq. (8.36) \nonly for the special case of n = \u0005. For ﬁnite values of n, / cannot exceed n - jmax - 1, \nwhich is largest for the case of jmax = 0, implying that /max = n - 1. Thus, the radial \neigenvalue solution places a new limit on the allowed values of the angular momentum quan-\ntum number / that came from the polar eigenvalue equation:\n \n/ = 0, 1, 2, ... n - 1  . \n(8.37)\nWe now know all the quantum numbers for the hydrogen atom, so let’s take a moment to summa-\nrize our journey. We solved the f eigenvalue equation and found that the magnetic quantum number \nm was any integer from negative inﬁnity to positive inﬁnity. We then solved the u eigenvalue equation \nand found that the angular momentum quantum number / was an integer from 0 to inﬁnity, but that \nthe absolute value of the magnetic quantum number m could be no larger than /. Finally, we have now \nsolved the r eigenvalue equation and found that the principal quantum number n is an integer from 1 to \ninﬁnity, but the angular momentum quantum number / can be no larger than n - 1. In summary, the \nhydrogen atom quantum numbers are\n \n n = 1, 2, 3, ...\u0005\n \n \n / = 0, 1, 2, ..., n - 1\n \n \n(8.38)\n \n m = -/, -/ + 1, ... 0, ..., / - 1, /  . \n8.3 \u0002  HYDROGEN ENERGIES AND SPECTRUM\nThe solution to the radial eigenvalue equation has now given us the quantized energy eigenvalues of \nthe hydrogenic atom:\n \nEn = -  1\n2n2 a Ze2\n4pe0\nb\n2\n m\nU2 ,   n = 1, 2, 3, ...  . \n(8.39)\nThe principal quantum number n ranges from 1 to inﬁnity and is sometimes referred to as the shell \nnumber. The quantized energies are less than zero because the zero of potential energy is taken to be \nwhere the electron and nucleus are separated to inﬁnity—also called the ionization limit. Note that E \ndepends only on n and not on /, even though the radial wave function Rn/1r2 depends on both n and / \nthrough the jmax  in Eq. (8.31).\nIt is common to express the hydrogen energy in different forms that are more instructive than the \njumble of constants in Eq. (8.39). To simplify our discussion, we focus on the hydrogen atom itself \nand set Z = 1. We also follow the convention of using the electron mass me rather than the reduced \nmass m at this stage, and then using the correct reduced mass in later calculations. With these simpliﬁ-\ncations and a few rearrangements of constants, the hydrogen energy levels are\n \nEn = -  1\n2n2 mec2 a\ne2\n4pe0\n Ucb\n2\n. \n(8.40)\n",
    "8.3 Hydrogen Energies and Spectrum \n257\nThis form is useful because it contains the electron rest mass energy Erest = me\n c2 and a collection of \nfundamental constants, which must be dimensionless. The dimensionless constant inside the parenthe-\nses is the ﬁne structure constant\n \na =\ne 2\n4pe0\n U c , \n(8.41)\nso named because of its role in the ﬁne structure of the hydrogen spectra that we’ll study in Chapter 12. \nMore important, the ﬁne structure constant is a measure of the fundamental strength of the electromag-\nnetic interaction, and is also called the electromagnetic coupling constant. In terms of the ﬁne structure \nconstant, the hydrogen energy levels take on the simple form\n \nEn = -  1\nn2 1\n2\n a2me\n c2  . \n(8.42)\nThe ﬁne structure constant has the approximate value\n \na =\n1\n137\n   . \n(8.43)\nThe electron rest mass energy has the approximate value\n \nme\n c2 = 511 keV  . \n(8.44)\nAt this level of precision, the hydrogen energy levels are\n \nEn = -  1\nn2 13.6 eV  . \n(8.45)\nYou should commit the three numerical values in Eqs. (8.43), (8.44), and (8.45) to memory.\nAnother common and convenient form of the hydrogen energy level formula is obtained by using \nthe length scale we deﬁned in Eq. (8.8). In the case of hydrogen, the nuclear charge is Z = 1, and \nusing the electron mass rather than the reduced mass, we deﬁne the quantity\n \na 0 = 4pe0\n U2\nmee2  \n(8.46)\nas the Bohr radius, with the approximate value\n \na 0 = 0.0529 nm = 0.529 A \n\b  . \n(8.47)\nIn terms of the Bohr radius, the hydrogen energy levels are\n \nEn = -  1\n2 n2 a\n1\n4pe0\n e 2\na 0\nb  , \n(8.48)\nwhich emphasizes the Coulomb binding of the atom.\n",
    "258 \nHydrogen Atom\nThe spectrum of hydrogen energy states is shown in Fig. 8.2. There are several noteworthy fea-\ntures of the hydrogen energies:\n• There are an inﬁnite number of bound states in the hydrogen atom because the Coulomb \npotential energy falls off slowly for r S \u0005. In contrast, a three-dimensional ﬁnite square \nwell has a ﬁnite number of bound states, similar to the one-dimensional case.\n• The hydrogen energy levels are degenerate with respect to the / and m quantum numbers \nbecause the energy depends on n only. For each energy level En, there are n possible / states \nranging from / = 0  to  / = n - 1 in unit steps. For each of those / states, there are 2/ + 1 \npossible m states ranging from m = -/ to m = +/ in unit steps. The total number of states \nat each energy level En is the sum of these possibilities:\n \na\nn -1\n/=0\n12/ + 12 = 2a\nn-1\n/=0\n/ + a\nn-1\n/=0\n1 = 2 n1n - 12\n2\n+ n = n2. \n(8.49)\n1\n2\n3\n4\n5\nn\nLyman\nBalmer\nPaschen\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\nE\nf\nΛ\n\r\nEnergy (eV)\nFIGURE 8.2 Hydrogen energy levels and emission spectrum.\n",
    "8.3 Hydrogen Energies and Spectrum \n259\nWhen we include the two spin possibilities of the electron—spin up and spin down along the \nz-axis—then there are 2n2 possible states per energy level. The m degeneracy is a result of the \nspherical symmetry of the hydrogen atom and is removed if we break this symmetry, for exam-\nple by applying an electric or magnetic ﬁeld in a given direction (Chapter 10). The / degeneracy \nis a result of a special symmetry of the 1>r Coulomb potential and is removed when we account \nfor non-Coulomb interactions in the atom (Chapter 12).\n• The results we have obtained for the hydrogen energy levels are the same as those obtained \nwith the semi-classical Bohr model. That is a bit surprising because the Bohr model used \nsome incorrect physics. Because of this equality of results, the energy levels we have derived \nhere are often still referred to as the Bohr energies.\n• The Bohr energies require corrections due to relativity and internal magnetic ﬁelds \nthat change the energies at the level of about 1 part in 104, and considering that today’s \nspectroscopic techniques permit a precision of 1 part in 1014, 1 part in 104 is huge! This \nmeans that hydrogen is a wonderful playground to test reﬁnements of the simplest \nmodels. We will study some of these effects in Chapter 12.\nHydrogen atoms absorb or emit light when electrons make transitions between energy levels. \nWhen an electron transitions from a higher-lying to a lower-lying level, a photon is emitted. Some of \nthese emission lines are shown in Fig. 8.2. Transitions to the n = 1 ground state comprise the Lyman \nseries, with the lowest energy transition 1n = 2 S 12 referred to as the Lyman-A line or L a , the next \none Lb , etc. Transitions from higher levels down to the n = 2 level comprise the Balmer series and \ntransitions down to the n = 3 level comprise the Paschen series. The wavelengths of some of these \ntransitions are listed in Table 5.1. Transitions to higher-lying levels require the absorption of light.\nWhether the photon is emitted or absorbed, its energy matches the energy difference between the \ntwo atomic states involved:\n \nEphoton = \u0006Efi = 0 Ef - Ei0 = 1\n2\n me\n c2 a\ne 2\n4pe0\n Ucb\n2\n2 1\nn2\ni\n- 1\nn2\nf\n2 . \n(8.50)\nThe energy of the photon is related to its wavelength via\n \nEphoton = U v = hf = hc\nl , \n(8.51)\nso the wavelength of the photon obeys the relation\n \n1\nl = R\u0005 2 1\nn2\ni\n- 1\nn2\nf\n2 , \n(8.52)\nwhere we deﬁne the Rydberg constant as\n \nR\u0005 =\nme\n4pU3c\n a e2\n4pe0\nb\n2\n  . \n(8.53)\nThe Rydberg constant was discovered empirically in the nineteenth century through experimental \nmeasurements of the spectrum of hydrogen. The subscript \u0005 refers to our use of the electron mass \nin Eq. (8.53) as opposed to the reduced mass, which must be done to get accurate results. If we use \nthe reduced mass for hydrogen in Eq. (8.53), then the result is referred to as RH. RH and R\u0005 differ \nby 5 parts in 104 (huge!), so in precision measurement it’s important to be clear which is being used. \nToday the Rydberg constant is the second most precisely measured fundamental constant (the g-factor \nof the electron being the most precise). The latest measured value is\n \nR\u0005 = 109 737.315 685 271732 cm-1. \n(8.54)\n",
    "260 \nHydrogen Atom\nIt is also common to use the term Rydberg (without the word “constant”) in reference to the energy \ninstead of the inverse wavelength. For example, one often writes the hydrogen energies in the form\n \nEn = -  1\nn2 Ryd  , \n(8.55)\nwhere one Rydberg (Ryd) is equal to 13.6 eV.\nNot all transitions between states are allowed in the hydrogen atom. As we discussed in Chapter 3, \nthe probability of a transition is proportional to the matrix element of the light interaction between \nthe two states: 8cnf    /f  mf 0 Vint 0 cni /i\n mi9 [Eq. (3.109)]. The general properties of these matrix elements \ndetermine the selection rules that tell us which transitions are allowed and which are forbidden. For \nthe electromagnetic interaction that characterizes the emission and absorption of light, the selection \nrules for transitions in the hydrogen atoms are\n \n \u0006/ = /f - /i = {1\n \n \u0006m = mf - mi = 0, {1  .   \n (8.56)\nThese selection rules are primarily due to the conservation of angular momentum. The photon has \nspin angular momentum 1, so when an atom absorbs or emits light, the atom must change its angular \nmomentum by one unit. Some of the allowed transitions in hydrogen are shown in Fig. 8.3 where the \ndifferent angular momentum states s, p, d, etc. are identiﬁed in order to emphasize the \u0006/ = {1 tran-\nsitions. We will study these transitions and selection rules further in Chapter 14.\n1s\n2s\n2p\n3s\n3p\n3d\n4s\n4p\n4d\n4f\n\r\nEnergy (eV)\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\nFIGURE 8.3 Transitions between states in hydrogen, emphasizing the \u0006/ = {1 selection rule.\n",
    "8.4 The Radial Wave Functions \n261\n8.4 \u0002 THE RADIAL WAVE FUNCTIONS\nLet’s now return to the radial wave function solution R1r2 = r/e-gr H1r2 [Eq. (8.17)]. We have \ndetermined that g = 1>n, established that n and / are restricted integers, and found the recurrence \nrelation for the coefﬁcients in the series H1r2. The next thing to do is to put the dimensions back into \nthe problem. In terms of the Bohr radius a0, the length scale parameter a is\n \na = 4pe0\n U2\nme\n Ze2 = a0\nZ  , \n(8.57)\nand we have continued the convention of using the electron mass me rather than the reduced mass m. \nThe dimensionless radial position r is then\n \nr = r\na = Zr\na0\n. \n(8.58)\nThe radial wave function with the dimensions back in place is\n \nRn/1r2 = ¢Zr\na0\n≤\n/\n e-Zr\u0006na0 H ¢Zr\na0\n≤. \n(8.59)\nWe label the radial wave functions as Rn/ using the two quantum numbers n and / that affect the radial \ndependence. Now we’re ready to use our knowledge of the allowed quantum numbers and the recur-\nrence relation to ﬁnd the polynomial H1Zr>a02 for each state. The polynomial terminates at the value\n \njmax = n - / - 1. \n(8.60)\nLet’s look at solutions for a few particular values of n and /, and then we’ll discuss the general results \nfor the radial wave function.\nThe ground state of hydrogen has the principal quantum number n = 1 and the angular momen-\ntum quantum number / = 0, so Eq. (8.60) tells us that the polynomial terminates at jmax = 0. That’s \nthe simplest polynomial possible! Hence, we have H1Zr>a02 = c0 and the radial wave function is\n \nR101r2 = c0 e-Zr>a0. \n(8.61)\nThe constant c0 is determined from the normalization requirement (Problem 8.1).\nThe ﬁrst excited state of hydrogen has n = 2 and two possible values for /: / = 0 and / = 1. \nFor the 2s state 1/ = 02, Eq. (8.60) tells us that the polynomial terminates at jmax = 1. The polyno-\nmial is therefore H1Zr>a02 = c0 + c11Zr>a02. The coefﬁcients c0 and c1 are related by the recurrence \nrelation Eq. (8.25):\n \nc1 = -  1\n2\n c0 \n(8.62)\nso that H1Zr>a02 = c011 - Zr>2a02. The radial wave function is therefore\n \nR201r2 = c0 e-Zr>2a0 11 - Zr>2a02. \n(8.63)\nAgain, the constant c0 is determined from the normalization requirement, and it must be emphasized \nthat the coefﬁcients for different sets of quantum numbers n and / are not related to each other.\nFor the 2p state 1/ = 12, the polynomial terminates at jmax = 0, so H1Zr>a02 = c0 . The radial \nwave function is therefore\n \nR211r2 = c0 re-Zr>2a0. \n(8.64)\n",
    "262 \nHydrogen Atom\nContinuing this procedure results in the complete set of radial wave functions, some of which are \nshown in Table 8.1 and illustrated graphically in Fig. 8.4.\nIt turns out that the radial wave functions can also be written in terms of a common set of functions \nknown as the associated Laguerre polynomials L p\nq1x2, which are deﬁned as\n \nL p\nq1x2 = d p\ndx p Lq1x2. \n(8.65)\nTable 8.1 Radial Wave Functions of Hydrogenic Atoms\nR101r2 = 2a Z\na0\nb\n3>2\ne-Zr>a0\nR201r2 = 2a Z\n2a0\nb\n3>2\n c1 - Zr\n2a0\nd e-Zr>2a0\n R211r2 =\n1\n13\n a Z\n2a0\nb\n3>2\n Zr\na0\n e-Zr>2a0\n R301r2 = 2a Z\n3a0\nb\n3>2\n £1 - 2Zr\n3a0\n+ 2\n27 aZr\na0\nb\n2\n§  e-Zr>3a0\nR311r2 = 412\n9\n a Z\n3a0\nb\n3>2\n Zr\na0\n a1 - Zr\n6a0\nb e-Zr>3a0\n  R321r2 = 212\n2715\n a Z\n3a0\nb\n3>2\n aZr\na0\nb\n2\n e-Zr>3a0\n1\n2\n3\n4\n0\n0.5\n1.\n1.5\nR(r)\nR(r)\nR(r)\nR(r)\nR(r)\nR(r)\n1s\n2\n4\n6\n8\n10\n\n0.2\n0\n0.2\n0.4\n0.6\n2s\n2\n4\n6\n8\n10\nr\na0\n0\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\n2p\n5\n10\n15\n\n0.1\n0\n0.1\n0.2\n0.3\n3s\n5\n10\n15\n\n0.04\n\n0.02\n0\n0.02\n0.04\n0.06\n3p\n5\n10\n15\n0\n0.01\n0.02\n0.03\n0.04\n3d\nr\na0\nr\na0\nr\na0\nr\na0\nr\na0\nFIGURE 8.4 Radial wave functions for hydrogen energy eigenstates.\n",
    "8.5 The Full Hydrogen Wave Functions \n263\nThe ordinary Laguerre polynomials Lq1x2 are deﬁned as \n \nLq1x2 = e x\n d q\ndx q 1x qe-x2. \n(8.66)\nThe Laguerre polynomials Lq1x2 are of degree q, so the associated Laguerre polynomials L p\nq1x2 are of \ndegree q - p. Using these deﬁntions, the radial wave functions are\n \nRn/1r2 = - b a 2Z\nna0\nb\n3\n 1n - / - 12!\n2n31n + /2!43 r\n1>2\n e-Zr/na0 a2Zr\nna0\nb\n/\nL n+/\n2/+112Zr>na02  . \n(8.67)\nThe \nassociated \nLaguerre \npolynomial \nL 2/+1\nn+/ 12Zr>na02 \nis \na \npolynomial \nof \ndegree \n1n + /2 - 12/ + 12 = n - / - 1, as expected from the value of jmax  given by Eq. (8.60). Be \naware that there are differing deﬁnitions of the Laguerre polynomials, so the expression for the radial \nwave function may look different in other texts.\nIn Chapter 7, we normalized each of the angular wave functions separately, and we do the same \nhere with the radial function. This isn’t mathematically or physically necessary; it’s just a convenient \nway to do it. The radial normalization condition is\n \nL\n\u0005\n0\nr 2 dr 3Rn/1r24\n2 = 1, \n(8.68)\nwhich includes the r 2 term we discussed in Eq. (7.38). The normalization condition in Eq. (8.68) is \nwhat we need to ﬁnd the c0 coefﬁcients in Eqs. (8.61), (8.63), and (8.64) and was used to normalize the \nradial wave functions in Eq. (8.67).\n8.5 \u0002 THE FULL HYDROGEN WAVE FUNCTIONS\nFinally, we’re ﬁnished! We’ve solved each of the separated differential equations, we’ve found the \nthree quantum numbers n, /, and m for the hydrogen atom, and we’ve found the allowed energies. \nWe’re now ready to recombine the three separated parts of the wave function to form the full three-\ndimensional energy eigenstate wave functions of the hydrogen atom\n \n0 n/m9 \u0003 cn/m1r, u, f2 = Rn/1r2Y /\nm1u, f2  . \n(8.69)\nThe full eigenstates for the ﬁrst few energy levels of a hydrogenic atom are given in Table 8.2; the \nradial part comes from Eq. (8.67) and the angular part from Eq. (7.161). These states are also eigen-\nstates of the angular momentum operators L2 and Lz. They can be eigenstates of H, L2, and Lz simul-\ntaneously because these three operators commute with each other. The three eigenvalue equations are:\n \n H cn/m1r, u, f2 = -  13.6 eV\nn2\n cn/m1r, u, f2\n \n L2 cn/m1r, u, f2 = /1/ + 12U2 cn/m1r, u, f2   \n(8.70)\n \n Lz cn/m1r, u, f2 = m U cn/m1r, u, f2     . \n",
    "264 \nHydrogen Atom\nThe normalization condition for the full wave function is the three-dimensional  integral\n \n1 = 8n/m0 n/m9 =\n \nL\n0 cn/m1r, u, f20\n2 dV \n \n=\n \nL\n\u0005\n0\nL\n2p\n0\nL\np\n0\n0 Rn/1r20\n2 0 Y m\n/ 1u, f20\n2 r 2 sin u d u df dr. \n(8.71)\nIt is instructive to rewrite Eq. (8.71) to emphasize our choice to normalize the radial and angular parts \nof the wave function independently:\n \n1 = 8n/m0 n/m9 = b\nL\n\u0005\n0\nr 20 Rn/1r20\n2 dr r b\nL\n2p\n0\nL\np\n0\n@ Y m\n/ 1u, f2@\n2\n sin u d u dfr. \n(8.72)\n \n=1\n \n=1\nWe could break this down further into u and f pieces, but that step is not generally necessary. Note \nagain that the r 2 part of the differential volume element goes with the radial integral.\nTable 8.2 Energy Eigenstate Wave Functions of Hydrogenic Atoms\nc1001r, u, f2 =\n1\n1p\n a Z\na0\nb\n3>2\n e -Zr>a0\nc2001r, u, f2 =\n1\n1p\n a Z\n2a0\nb\n3>2\n c1 - Zr\n2a0\nd e -Zr>2a0\nc2101r, u, f2 =\n1\n21p\n a Z\n2a0\nb\n3>2\n Zr\na0\n e -Zr>2a0 cos u\nc21,{11r, u, f2 = <  \n1\n212p\n a Z\n2a0\nb\n3>2\n Zr\na0\n e -Zr>2a0 sin ue{if\nc3001r, u, f2 =\n1\n1p\n a Z\n3a0\nb\n3>2\n £ 1 - 2Zr\n3a0\n+ 2\n27 aZr\na0\nb\n2\n§  e -Zr>3a0\nc3101r, u, f2 =\n212\n313p\n a Z\n3a0\nb\n3>2\n Zr\na0\n a1 - Zr\n6a0\nb e -Zr>3a0 cos u\nc31,{11r, u, f2 = <  \n2\n313p\n a Z\n3a0\nb\n3>2\n Zr\na0\n a1 - Zr\n6a0\nb e -Zr>3a0 sin ue{if\nc3201r, u, f2 =\n1\n2712p\n a Z\n3a0\nb\n3>2\n aZr\na0\nb\n2\n e -Zr>3a0 13 cos2\n u - 12\nc32,{11r, u, f2 = <  13\n271p\n a Z\n3a0\nb\n3>2\n aZr\na0\nb\n2\n e -Zr>3a0 sin u cos ue{if\nc32,{21r, u, f2 =\n13\n541p\n a Z\n3a0\nb\n3>2\n aZr\na0\nb\n2\n e -Zr>3a0 sin2\n ue{i2f\n",
    "8.5 The Full Hydrogen Wave Functions \n265\nThe probability density is the absolute square of the wave function, so for an energy eigenstate\n \n P1r, u, f2 = 0 cn/m1r, u, f20\n2\n \n \n = 0 Rn/1r2Y m\n/ 1u, f20\n2. \n(8.73)\nMultiplying the probability density by the inﬁnitesimal volume element dV = r 2 dr sin u du df gives \nthe probability of measuring the electron to be within that volume element:\n \n P1r, u, f2dV = 0 cn/m1r, u, f20\n2 r 2 dr sin u du df\n \n \n = 0 Rn/1r2Y m\n/ 1u, f20\n2\n r 2dr sin u du df. \n(8.74)\nTo calculate the probability of ﬁnding the electron within some ﬁnite volume, we integrate Eq. (8.74) \nover that region.\nBecause the probability density is three dimensional, it is difﬁcult to represent graphically on a \nﬂat piece of paper. We needed three dimensions to properly visualize the two-dimensional spherical \nharmonic probability densities, so we would need four dimensions to visualize the three-dimensional \natomic probability density. A variety of different visualization schemes are possible, many aided by \nthe power of modern computers.\nLet’s start with the ground state of the hydrogen atom. The wave function is\n \nc1001r, u, f2 =\n1\n4pa 3\n0\n e-r>a0 \n(8.75)\nand the probability density is\n \nP1001r, u, f2 = 0 c1001r, u, f20\n2 =\n1\npa 3\n0\n e-2r>a0. \n(8.76)\nThe dimensions of the probability density are 1>length3 as you would expect for a three-dimensional \ndensity. For the hydrogen ground state, the probability density is independent of the angles u and f,\nwhich means that the electron cloud around the nucleus is spherically symmetric. The three-dimen-\nsional electron probability distribution of the 1s state is illustrated in Fig. 8.5. In Fig. 8.5(a) the three \naxes represent physical space and the value of the probability density is represented by a grayscale \n(white is high, black is low). Just three parallel planes are shown, allowing us to “peek” at the distribu-\ntion. In Fig. 8.5(b), the grayscale density plot in the x-z plane 1y = 02 is shown. On a computer, you \ncan animate the motion of the slicing planes in Fig. 8.5(a) to visualize the full electron cloud, and you \ncan also use color while you’re at it (see the activity on hydrogen probability densities). Figure 8.6(a) \nrepresents the 1s probability density in the x-z plane using height above the plane as the indicator of \nprobability density, and Fig. 8.6(b) shows the probability density in a one-dimensional plot as a function \nof r, the distance from the nucleus. All of these representations demonstrate that the probability density \nfor measuring the electron position in the 1s state is largest at the origin.\nGrayscale density plots in the x-z plane for the eigenstates in the ﬁrst three energy levels of the \nhydrogen atom are shown in Fig. 8.7. The density plots for negative values of m are indistinguishable \nfrom those for positive m, so they are not included. In the grayscale plots in Fig. 8.7, we plot the abso-\nlute value of the wave function, which is the square root of the probability density, to provide a better \nvisual representation of the electron distribution. The spatial scales are different for each value of n. \nEach plot has a range of -3n2a0 to +3n2a0 .\n",
    "266 \nHydrogen Atom\nHere are some important features of the radial wave functions and the probability densities.\n• All the radial functions have an r/ dependence, so the wave function vanishes at the origin \nexcept for the s states 1/ = 02. This is caused by the centrifugal barrier that “repels” the \nelectron from the nucleus for / Ú 1, as we saw in the effective potential in Fig. 8.1. For s \nstates, the probability density at the origin is\n \n Pns10, u, f2 = 0 cn 0010, u, f20\n2 = 0 Rn 0102Y 0\n01u, f20\n2 =\n1\n4p 0 Rn 01020\n2 \n \n = 1\np a Z\nna0\nb\n3\n.\n \n(8.77)\n \nThis nonzero probability density is important because it means that the electron has some ﬁnite \nprobability of being inside the nucleus, which affects the real energy levels when we consider \nthe nucleus not to be a point particle, as well as some other effects we address in Chapter 12.\nFIGURE 8.6 Probability density of the ground state of hydrogen (a) represented as the height \nabove the x-z plane and (b) plotted as a function of radius.\nFIGURE 8.5 (a) Two-dimensional slices of the three-dimensional electron distribution of the ground \nstate of hydrogen. In each slice, the probability density is represented by grayscale (black = 0, \nwhite = maximum). (b) The particular two-dimensional probability density slice at y = 0.\n",
    "8.5 The Full Hydrogen Wave Functions \n267\n• Each radial wave function Rn/ (r) has n - / - 1 nodes and n - / antinodes. The particle-\nin-a-box energy eigenstates also have more nodes as the energy increases. The hydrogen \nradial functions for a given n have fewer nodes for higher / states, but the angular wave func-\ntions compensate for that by having more nodes.\n• The full wave function has parity (-1)/ (recall that the parity operation is r S -r). The par-\nity of the wave function derives from the parity of the spherical harmonics, which we noted in \nEq. (7.167). The parity is important later in calculating matrix elements.\n• The probability densities are independent of the azimuthal angle f, which we have already \nseen in Chapter 7 from the nature of the spherical harmonics.\nThe probability plots we have shown are informative, but ultimately we need to calculate prob-\nabilities or expectation values to compare with experiments. These are often done with computers, but \nyou need to know what to tell the computer to do. Let’s work an example that is analytically tractable.\nExample 8.1 Find the probability that the electron in the ground state of hydrogen is measured to \nbe within one Bohr radius of the nucleus and calculate the expectation value of the radial position r.\nThe probability is the integral of the probability density over a sphere of radius a0, so we limit \nthe r integral to r 6 a0 and integrate over the full range of u and f:\n \n Pr6a0 =\nL\nsphere r6a0\nP1r, u, f2dV \n \n=\nL\na0\n0\nL\n2p\n0\nL\np\n0\nP1r, u, f2  r 2 sin u du df dr \n(8.78)\n \n=\nL\na0\n0\nL\n2p\n0\nL\np\n0\n0 Rn/1r2Y m\n/ 1u, f20\n2\n r 2 sin u du df dr. \nFIGURE 8.7 Grayscale density plots in the x-z plane of the absolute value of the \nwave function for hydrogen energy eigenstates 0 n/m9 denoted by the labels above \neach plot. The spatial range of each plot is -3n2a0 to +3n2a0.\n",
    "268 \nHydrogen Atom\nWe separate the radial and angular integrals\n \nPr6a0 = e\nL\na0\n0\nr 20 Rn/ 1r20\n2\n drf e\nL\n2p\n0\nL\np\n0\n0 Y m\n/ 1u, f20\n2\n sin u du dff. \n(8.79)\nThe angular integral is unity because the spherical harmonics are normalized (See! The separate \nnormalization is useful!), leaving\n \nPr6a0 =\n \nL\na0\n0\nr 20 Rn/1r20\n2\n dr. \n(8.80)\nNow we put in the radial ground state wave function to get\n \nPr6a0 =\n \nL\na0\n0\nr 2 4Z 3\na 3\n0\n e-2Zr>a0 dr. \n(8.81)\nSubstituting x = 2Zr>a0 and integrating gives\n Pr6a0 = 1\n2 L\n2Z\n0\nx2e-x dx = 1\n2\n 1-x2 - 2x - 22e-x 2\n2Z\n0\n= 1\n2\n 31-4Z2 - 4Z - 22e-2Z + 24. (8.82)\nFor the hydrogen case, Z = 1, and the probability is\n \n Pr6a0 = 31 - 12 + 2 + 12e-24 = 1 - 5e-2 \n \n = 0.323.\n \n(8.83)\nIn a set of radial position measurements, 32% of the results will be within one Bohr radius of the \nnucleus.\nThe expectation value of the radius is\n \n 8r9 = 8n/m0 r0 n/m9 = 81000 r0 1009\n \n \n =\n \nL\n r0 cn/m1r, u, f20\n2\n dV\n \n(8.84)\n \n =\n \nL\n\u0005\n0\nL\n2p\n0\nL\np\n0\nr0 Rn/1r2Y m\n/ 1u, f20\n2\n r 2 sin u du df dr. \nAgain, we separate the radial and angular integrals\n \n8r9 = b\nL\n\u0005\n0\nr 30 Rn/1r20\n2\n dr r b\nL\n2p\n0\nL\np\n0\n@ Y m\n/ 1u, f2@\n2\n sin u du dfr. \n(8.85)\nThe angular integral is unity and we get\n \n8r9 =\n \nL\n\u0005\n0\nr 30 Rn/1r20\n2\n dr. \n(8.86)\nSubstituting in the radial ground state wave function, we get\n \n8r9 =\n \nL\n\u0005\n0\nr 3 4Z 3\na 3\n0\n e-2Zr>a0 dr. \n(8.87)\n",
    "8.5 The Full Hydrogen Wave Functions \n269\nSubstituting x = 2Zr>a0 and integrating gives\n \n 8r9 = a0\n4Z L\n\u0005\n0\nx 3e-x dx = a0\n4Z\n 1-x 3 - 3x 2 - 6x - 62e-x `\n\u0005\n0\n \n \n = 3a0\n2Z\n .\n \n(8.88)\nFor the hydrogen atom, the mean value of the radius is 3a0>2. The integrand r 20 Rn/1r20\n2 of the \nintegral in Eq. (8.80) is plotted in Fig. 8.8. The hatched area under the curve represents the prob-\nability we calculated above that the electron is measured to be in the region 0 … r … a0. The arrow \nindicates the expectation value of the radius, which is beyond the peak because the integrand is \nnot symmetric.\nExpectation values of the radial position are useful for many calculations we will do later. We \nquote here without proof the expectation values 8n/m0 r k0 n/m9 for different powers:\n \n 8r9 = a0\n2Z\n 33n2 - /1/ + 124\n \n \n 8r 29 =\na 2\n0\n n2\n2Z 2  35n2 + 1 - 3/1/ + 124 \n \n h1\nr i =\nZ\na0\n n2\n \n(8.89)\n \n h 1\nr 2 i =\nZ 2\na 2\n0\n n31/ + 1\n22\n \n \n h 1\nr 3 i =\nZ 3\na 3\n0\n n3/1/ + 1\n221/ + 12\n .\n \nThe result in Example 8.1 agrees with the general expression in the ﬁrst equation above.\n1\n2\n3\n4\nr\na0\n0.1\n0.2\n0.3\n0.4\n0.5\nr 2R2(r)\n1s\n\u0004r\u0003\nFIGURE 8.8 Radial probability integrand for the hydrogen 1s ground state. The hatched \nregion indicates the probability Pr…a0 and the arrow indicates the expectation value 8r9.\n",
    "270 \nHydrogen Atom\n8.6 \u0002 SUPERPOSITION STATES\nHaving solved the energy eigenvalue equation for the hydrogen atom and found the allowed energies \nand allowed wave functions, we can now use them to ﬁnd the time evolution of the atom with arbitary \ninitial conditons using the Schrödinger time-evolution recipe we developed in Chapter 3. If the atom \nstarts in one of the energy eigenstates, then the time evolution of the system is\n \n0 c1t29 \u0003 c1r, u, f, t2 = Rn/1r2Y m\n/ 1u, f2e-iEnt>U, \n(8.90)\nwhere En are the energy eigenvalues given in Eq. (8.39). The wave function acquires an overall time-\ndependent phase factor, but that does not affect any measurements we make on the system, so this is a \nstationary state, as we have seen in previous chapters.\nMore interesting time-dependent behavior occurs if the system starts in a superposition of energy \neigenstates. In this case, the time evolution of the wave function is\n \n0 c1t29 \u0003 c1r, u, f, t2 = a\nn,/,m\ncn/m Rn/1r2Y m\n/ 1u, f2e-iEnt>U, \n(8.91)\nwhere the expansion coefﬁcients are obtained from the projections of the initial state 0 c1t = 029 onto \nthe energy eigenstates\n \ncn/m = 8n/m0 c1029 =\n \nL\n\u0005\n0\nr 2 dr\nL\np\n0\nsin u du\nL\n2p\n0\ndf R*\nn/ 1r2 Y m*\n/ 1u, f2 c1r, u, f, 02. \n(8.92)\nExample 8.2 Find the time evolution of an equal superposition of the 1s ground state and the \n2p01m = 02 excited state:\n \n0 c1029 =\n1\n12 0 1009 +\n1\n12 0 2109. \n(8.93)\nThese states are both energy eigenstates, so the time evolution is obtained by application of the \nSchrödinger recipe:\n \nc1r, u, f, t2 =\n1\n12 c1001r, u, f2e-iE1t>U +\n1\n12 c2101r, u, f2e-iE2t>U \n \n=\n1\n42pa 3\n0\n e-r>a0\n e-iE1t>U +\n1\n4pa 3\n0\n r cos u\n8a0\n e-r>2a0\n e-iE2t>U \n(8.94)\n \n=\n1\n42pa 3\n0\n e-iE1t>U ae-r>a0 + r cos u\n412a0\n e-r>2a0\n e-iv21tb, \nwhere the Bohr frequency is v21 = 1E2 - E12>U. Noting that z = r cos u, we rewrite the wave \nfunction as\n \nc1r, u, f, t2 =\n1\n42pa 3\n0\n e-iE1t>U ae-r>a0 +\nz\n412a0\n e-r>2a0\n e-iv21tb, \n(8.95)\nwhich emphasizes the z-dependence of the state. The probability amplitude (absolute value of \nthe wave function) is displayed in Fig. 8.9(a) at time t = 0. The electron cloud is displaced in the \npositive z-direction, but as time evolves, animation of Fig. 8.9(a) shows that the cloud moves up \nand down along z. This is a model of the oscillating electric dipole moment that is responsible for \nthe radiation that the atom emits at the Bohr frequency (Problem 8.13).\n",
    "Example 8.3 Find the time evolution of an equal superposition of the 2s excited state and the\n2p0 1m = 02 excited state:\n \n0 c1029 =\n1\n12 0 2009 +\n1\n12 0 2109. \n(8.96)\nThe time-evolved state is\n \n c1r, u, f, t2 =\n1\n12 c2001r, u, f2e-iE2t>U +\n1\n12 c2101r, u, f2e-iE2t>U\n \n \n =\n1\n24pa 3\n0\n a1 -\nr\n2a0\nb e-r>2a0 e-iE2t>U +\n1\n4pa 3\n0\n r cos u\n8a0\n e-r>2a0 e-iE2t>U \n(8.97)\n \n =\n1\n24pa 3\n0\n e-iE2t>U a a1 -\nr\n2a0\nb e-r>a0 +\nz\n4a0\n e-r>2a0b.\n \nIn this case, the two states are degenerate in energy and there is no relative time-dependent phase \nfactor. The probability amplitude (absolute value of the wave function) is displayed in Fig. 8.9(b) \nat time t = 0. The electron cloud is displaced in the negative z-direction in this case because of the \ndifferent radial wave function for the 2s state, and as time evolves, the cloud does not move. This \nis a model of a static electric dipole moment that we will use again when we study the response of \nthe atom to an applied electric ﬁeld—the Stark effect—in Chapter 10. Such an s-p superposition is \na hybrid orbital that can be used to explain molecular bonding. Two atoms with displaced electron \nclouds facing each other reduce the electrostatic repulsion of the positively charged nuclei and \nstabilize the system.\nFIGURE 8.9 Probability amplitude (wave function) densities for (a) 1s-2p and \n(b) 2s-2p superposition states.\n8.6 Superposition States \n271\n",
    " SUMMARY \nThe radial part of the energy eigenvalue equation contains the crucial physics of the Coulomb interac-\ntion that determines the energies of the bound hydrogen atom. Solving the radial differential equation \nyields the quantization condition on the energy. The new quantum number is the principal quantum \nnumber n = 1, 2, 3, ... . The resultant energies of the hydrogen atom states are\n \nEn = -  1\nn2 13.6 eV. \n(8.98)\nThe length scale of the hydrogen atom is set by the Bohr radius\n \na0 = 0.0529 nm. \n(8.99)\nThe radial wave functions Rn/1r2 combine with the spherical harmonics from Chapter 7 to give \nthe full three-dimensional wave functions of the hydrogen atom\n \n0 n/m9 \u0003 cn/m1r, u, f2 = Rn/1r2Y m\n/ 1u, f2. \n(8.100)\nThe allowed values of the three quantum numbers are\n \nn = 1, 2, 3, ...\u0005\n \n/ = 0, 1, 2, ..., n - 1 \n(8.101)\n \nm = -/, -/ + 1, ...0, ..., / - 1, /.\nThe hydrogen atom states cn/m1r, u, f2 are simultaneously eigenstates of the Hamiltonian H, and the \nangular momentum operators L2 and Lz:\n \n Hcn/m1r, u, f2 = En cn/m1r, u, f2\n \n \n L2\n cn/m1r, u, f2 = /1/ + 12U2 cn/m1r, u, f2 \n \n(8.102)\n \n Lz \n cn/m1r, u, f2 = m U cn/m1r, u, f2.\n \n PROBLEMS \n \n8.1 Calculate the coefﬁcient c0 that normalizes the radial wave function R101r2 in Eq. (8.61) and \nconﬁrm the wave function shown in Table 8.1.\n \n8.2 Use the recurrence relation for the radial wave function to construct the n = 3 radial states of \nhydrogen. Calculate the normalization constant for the R321r2 state.\n \n8.3 Use the deﬁnition of the radial wave function in terms of the associated Laguerre polynomials \n[Eq. (8.67)] to construct the radial wave function R421r2.\n \n8.4 Show that the wave functions representing the 01009 and 0 2109 states are orthogonal.\n \n8.5 By direct application of the differential operators, verify that the state 0 3219 \u0003 c3211r, u, f2 is \nan eigenstate of H, L2, and Lz and determine the corresponding eigenvalues.\n \n8.6 Calculate the probability that the electron is measured to be within one Bohr radius of the \nnucleus for the n = 2 states of hydrogen. Discuss the differences between the results for the \n/ = 0 and / = 1 states.\n272 \nHydrogen Atom\n",
    " \n8.7 Calculate the probability that the electron is measured to be in the classically forbidden region \nfor the n = 2 states of hydrogen. Discuss the differences between the results for the / = 0 and \n/ = 1 states.\n \n8.8 Calculate by direct integration the expectation values 8r 29 and 81>r9 of the radial position for \nthe ground state of hydrogen. Compare your results to the quoted expressions in Eq. (8.89) \nand discuss your results. Did you expect that 81>r9 \u0002 1>8r9? Use your result for 81>r9 to \nﬁnd the expectation value of the kinetic energy of the ground state of hydrogen and discuss \nyour result.\n \n8.9 Calculate by direct integration the expectation value of the radial position for each of the \nn = 3 states of hydrogen. Compare your results to the quoted expression in Eq. (8.89) and \ndiscuss your results.\n 8.10 Calculate the probability that the electron in the ground state of a hydrogenic atom of nuclear \ncharge Z is measured to be inside the nucleus. A nucleus with A nucleons (Z protons and \nA-Z neutrons) has an approximate radius of r \u0002 11.2 * 10-15 m2A1>3. Calculate the prob-\nabilities for hydrogen and uranium-238.\n 8.11 Tritium is an isotope of hydrogen, with a nucleus comprising one proton and two neutrons. The \ntritium nucleus (triton) is radioactive, decaying by beta (electron) emission to the helium-3 \nnucleus comprising two protons and one neutron. An electron is initially in the ground state of \na tritium atom. After the instantaneous beta decay, what is the probability that the electron is in \nthe ground state of the new atom?\n 8.12 Find the ground state energy, the effective Bohr radius [using Eq. (8.8)], and the Lyman-alpha \nwavelength of the following hydrogenic systems:\na) deuterium: electron and nucleus with one proton and one neutron\nb) positive helium ion: 4He+\nc) positronium: electron 1q = -e,  m = me2 and positron 1q = +e,  m = me2\nd) muonium: electron and antimuon 1q = +e,   m = mm \u0002 207me2\ne) muonic hydrogen: muon and proton\nf ) hydrogen-like uranium: 235U 91+\n 8.13 Consider the one-dimensional probability density  P1z2 along the z-axis obtained by integrating \nover a plane perpendicular to the z-axis, either in Cartesian coordinates\n \nP1z2 =\n \nL\n\u0005\n- \u0005 L\n\u0005\n- \u0005\n0 cn/m1x, y, z2 0\n2\n dx dy \n \n or in cylindrical coordinates\n \nP1z2 =\n \nL\n2p\n0\nL\n\u0005\n0\n0 cn/m1r, f, z20\n2\n r d r df. \n \n Calculate this probability density for the superposition states 0 c19 = 10 1009 + 021092> 12 \nand 0 c29 = 10 2009 + 0 21092> 12. Use these probability densities to ﬁnd the expectation \nvalue of the electric dipole moment d = qr and verify that the moments for these two states \nare oppositely oriented as indicated by Fig. 8.9. Plot and animate the probability densities to \nverify that one state is oscillating and one state is static.\n Problems \n273\n",
    "274 \nHydrogen Atom\n 8.14 A hydrogen atom is initially in the superposition state\n \n0 c1029 =\n1\n114 0 2119 -\n2\n114 0 32, -19 +\n3i\n114 0 4229. \na) What are the possible results of a measurement of the energy and with what probabilities \nwould they occur? Plot a histogram of the measurement results. Calculate the expectation \nvalue of the energy. \nb) What are the possible results of a measurement of the angular momentum operator L2 and \nwith what probabilities would they occur? Plot a histogram of the measurement results. \nCalculate the expectation value of L2.\nc) What are the possible results of a measurement of the angular momentum component oper-\nator Lz and with what probabilities would they occur? Plot a histogram of the measurement \nresults. Calculate the expectation value of Lz.\nd) How do the answers to (a), (b), and (c) depend upon time?\n 8.15 Consider a particle of mass m bound in an inﬁnite square potential energy well in three \n dimensions:\n \nV1z2 = e\n 0,\n\u0005,   0 6 x 6 L, 0 6 y 6 L, 0 6 z 6 L\nother wise.\n \n Use separation of variables in Cartesian coordinates to ﬁnd the energy eigenvalues and eigen-\nstates of this particle in a cubical box. Find the degeneracy of the ﬁrst 6 energy levels.\nRESOURCES \nActivities\nThese activities are available at\nwww.physics.oregonstate.edu/qmactivities\nRadial Wavefunctions: Students visualize the radial part of the probability density of the hydrogen \natom.\nHydrogen Probability Densities: Students visualize the probability density of the electron in the \nhydrogen atom.\nFurther Reading \nHigh resolution spectroscopy of the hydrogen atom is discussed in this article:\nT. W. Hänsch, A. L. Schawlow, and G. W. Series, “The spectrum of atomic hydrogen,” \nScientiﬁc American, 240(3), 94–110 (1979).\n",
    " \n275\nC H A P T E R \n9\nHarmonic Oscillator \nIn the last four chapters, you have learned the tools for analyzing the motion of particles in quantum \nmechanics. You applied these tools to three important problems: (1) a particle bound in an inﬁnite \nsquare potential energy well in one dimension, (2) a free particle in one dimension, and (3) the hydro-\ngen atom in three dimensions. In this chapter we will solve another system with bound states in a one-\ndimensional potential energy well: the harmonic oscillator. This system resembles the inﬁnite square \nwell or particle-in-a-box system—the harmonic oscillator box just has a different shape. To solve the \nharmonic oscillator problem, we introduce a new method and some new tools in the process. Then we \nuse the solutions to the harmonic oscillator problem as a means to review the fundamental tools and \nconcepts of quantum mechanics. \n9.1 \u0002 CLASSICAL HARMONIC OSCILLATOR\nLet’s ﬁrst review the classical harmonic oscillator before we study the quantum mechanical case. A \nprototypical classical harmonic oscillator system is a mass m connected to a spring that is ﬁxed to a \nwall at its other end. The spring force is governed by Hooke’s law, which says that the force F is a \nrestoring force and is proportional to the displacement x of the mass from equilibrium:\n \nF = -kx , \n(9.1)\nwhere k is the spring constant. This linear restoring force is derivable from the quadratic potential \nenergy function V1x2 = 1\n2 kx 2 .\nThe beauty of the mass-on-a-spring system is that it is a model for many other systems in nature \nthat behave as harmonic oscillators. To see why this is so, consider the generic potential energy curve \nshown in Fig. 9.1. We are typically interested in ﬁnding the motion in the ground state or other low \nenergy states of the system. As the dashed line suggests, near the minimum at x0 of the potential \nenergy function that governs the system, the potential energy has the shape of a parabola, (i.e., it looks \nlike a harmonic oscillator). This parabolic shape is also evident if we consider a Taylor series expan-\nsion of the function about the minimum:\n \nV1x - x02 = V1x02 + 1x - x02 dV\ndx\n `\nx=x0\n+ 1\n2\n 1x - x02\n2 d 2V\ndx 2 `\nx=x0\n+ ... . \n(9.2)\nThe leading term in Eq. (9.2) is the quadratic term because the ﬁrst two terms are zero: (1) the \npotential energy offset V1x02  can be deﬁned to be to zero because a constant potential energy \ndoes not affect the motion, and (2) the linear term is zero because the potential derivative \n",
    "276 \nHarmonic Oscillator\n(i.e., slope) is zero at the minimum. Hence the motion of the system is that of a harmonic oscilla-\ntor in the vicinity of the potential energy minimum, and we identify the spring constant k as the \nsecond derivative of the potential energy evaluated at the minimum x0. If the motion takes the \nsystem too far from the minimum, the shape may deviate slightly from a parabola, and the motion \nwill be altered, but we still ﬁnd it useful to start by considering the motion as harmonic and then \nasking how that motion is perturbed. For these reasons, you will study harmonic oscillators as \nlong as you do physics.\nThe motion of the classical harmonic oscillator is solved by using Newton’s second law:\n \n F = ma\n \n -kx = m d 2x\ndt 2 .  \n(9.3)\nIt is convenient to deﬁne a new constant\n \nv = B\nk\nm \n(9.4)\nand rewrite the equation of motion as\n \nd 2x\ndt 2 = -v2 x1t2. \n(9.5)\nThis is a standard differential equation that you have likely encountered many times before. The \nsolution is the sinusoidal function\n \nx1t2 = A cos 1vt + f2, \n(9.6)\nwhere the amplitude A and phase constant f are determined by the initial state of the motion of the \nsystem. The motion is characterized by a single angular frequency (i.e., a single harmonic—hence the \nname) given by v.\n0\nx0\nx\nV(x)\nFIGURE 9.1 A general potential energy function (solid) \nis approximated by a quadratic harmonic  potential (dashed) \nin the vicinity of the potential minimum.\n",
    "9.2 Quantum Mechanical Harmonic Oscillator \n277\n9.2 \u0002 QUANTUM MECHANICAL HARMONIC OSCILLATOR\nThe procedure for ﬁnding the quantum mechanical Hamiltonian of any system is to ﬁrst ﬁnd the clas-\nsical energy and then rewrite that in terms of quantum mechanical operators. The potential energy of \nthe harmonic oscillator is\n \nV1x2 = 1\n2\n kx2. \n(9.7)\nThe total mechanical energy of the system is the sum of kinetic and potential energies:\n \nE = p2\n2m + 1\n2\n kx2. \n(9.8)\nThe oscillator frequency v plays an important role in quantum mechanics, so it is common to rewrite \nthe potential energy using v in place of k. From Eq. (9.4) we have k = mv2, so that the quantum \nmechanical Hamiltonian for the harmonic oscillator is\n \nH = pn  2\n2m + 1\n2 mv2xn  2   . \n(9.9)\nWe denote the operators xn and pn with carets to distinguish them from the variables x and p, but we \noften don’t use the caret notation if there is no ambiguity.\nAs always, our goal when presented with a new potential energy system is to solve the energy \neigenvalue equation H0 E9 = E0 E9 to ﬁnd the allowed energies in the system. Then we use the energy \neigenstates as the preferred basis to apply the recipe for Schrödinger time evolution. In the previous \npotential energy well problems, the square wells and the hydrogen atom, we expressed the energy \neigenvalue equation H0 E9 = E0 E9 as a differential equation in the wave function picture (i.e., the posi-\ntion representation). For the harmonic oscillator, the energy eigenvalue differential equation is\n \n-  U2\n2m \nd 2wE1x2\ndx 2\n+ 1\n2\n mv2x 2wE1x2 = EwE1x2. \n(9.10)\nWe can solve Eq. (9.10) using a power series solution, similar to the approach taken in the hydrogen \natom solutions in Chapters 7 and 8. Rather than do that here, we present a new method of solution that \nis more elegant and is known as the operator method or the algebraic method. Of course, we get the \nsame results either way.\nIf you haven’t seen it before, the operator method for solving the quantum mechanical harmonic \noscillator problem appears to be magic. We arrive at the solution by deﬁning some new quantities that \nyou would not imagine would be useful and by using minimal information about what how the opera-\ntors xn and pn behave. This operator method is also useful in describing angular momentum, and it is the \nbasis of quantum ﬁeld theory.\nTo make this discussion of the operator solution to the harmonic oscillator problem clearer, let’s \ngo ahead and present the energy spectrum answer to the problem. As we discussed in Chapter 5, the \nsolutions to bound state problems in different quantum mechanical systems share many features. The \nbound states in a potential energy well are discrete, with the ground state near, but not at, the bottom \nof the well. The positions of the energy levels depend upon the shape of the well. In the case of the \ninﬁnite square well that we studied in Chapter 5, the energy levels scale with n2, where n is the quan-\ntum number labeling the energy levels n = 1, 2, 3, ... . Hence the energy level spacing in the inﬁnite \nsquare well increases as n increases, as shown in Fig. 9.2(a). The hydrogen atom that we studied in \n",
    "278 \nHarmonic Oscillator\nChapter 8 has energy levels that scale as 1>n2 and so they get closer together as n increases, as shown \nin Fig. 9.2(b). The harmonic oscillator has a special potential energy well shape that gives rise to \nenergy levels that scale linearly with n and hence are evenly spaced, as shown in Fig. 9.2(c). The \nenergy eigenvalues of the harmonic oscillator are\n \nEn = U v1n + 1\n22,  n = 0, 1, 2, 3, ... . \n(9.11)\nThe convention is to label the ground state of the harmonic oscillator as n = 0, rather than n = 1 as \nin most other bound state problems. In Dirac notation, the energy eigenstates 0 n9 are labeled with the \nquantum number n and satisfy the energy eigenvalue equation\n \nH@ n9 = En@ n9 = U v1n + 1\n22@ n9. \n(9.12)\nIn the operator method of solving the harmonic oscillator problem, we deﬁne two new operators \nand use the properties of these operators to derive the energy eigenvalues given in Eq. (9.11). The new \noperators are the raising and lowering operators, a- and a, respectively, and they act to change the \nlabels n on the eigenstates. These new operators are built from the position and momentum opera-\ntors, xn and pn, that comprise the Hamiltonian in Eq. (9.9), and they simplify many of the calculations \nrequired in the harmonic oscillator problem.\nTo see where these new operators come from and why they are useful, ﬁrst note that the har-\nmonic oscillator Hamiltonian in Eq. (9.9) is a sum of squares. If it were a difference of squares, then \nwe could factor it as a product of the sum and difference Ci.e., u2 - v2 = 1u - v21u + v2D. But as a \nsum, we can still factor it if we use complex numbers, which we know are used quite often in quan-\ntum mechanics:\n \nu2 + v2 = 1u - iv21u + iv2. \n(9.13)\n(a)\nx\nE4\u000416E1\nE3\u00049E1\nE2\u00044E1\nE1\u0004E1\nE1\u0004E1\n(b)\nr\nE\n(c)\nx\nE\nE2\u0004\nE1\n4\nE3\u0004\nE1\n9\nE4\u0004\nE1\n16\nE3\u00047 \u0002Ω\n2\nE2\u00045 \u0002Ω\n2\nE1\u00043 \u0002Ω\n2\nE0\u0004 \u0002Ω\n2\nFIGURE 9.2 Spectra of energy eigenstates in (a) the inﬁnite square well, (b) the hydrogen atom, \nand (c) the harmonic oscillator well.\n",
    "9.2 Quantum Mechanical Harmonic Oscillator \n279\nSo let’s factor the Hamiltonian in the manner of Eq. (9.13), and while we’re at it, let’s make our life \neasier by using dimensionless quantities. We know that Planck’s constant times frequency has dimen-\nsions of energy, so we start by factoring out an energy term U v from the Hamiltonian\n \n H = 1\n2 mv2cxn2 +\npn2\nm2v2d\n \n = U v e mv\n2U\n cxn2 +\npn2\nm2v2d f \n(9.14)\nsuch that the expression inside the curly brackets is dimensionless. We now deﬁne a new dimension-\nless operator, called the lowering operator, to help us factor the Hamiltonian\n \na = A\nmv\n2U\n axn + i pn\nmvb. \n(9.15)\nNote that the lowering operator is not Hermitian, because it is not equal to its Hermitian conjugate\n \n a- = A\nmv\n2U\n axn - - i pn -\nmvb \n \n = A\nmv\n2U\n axn - i pn\nmvb, \n(9.16)\nwhich is the raising operator. Recall that xn and pn are Hermitian, xn = xn - and pn = pn -, because they \nrepresent physical observables. Because the raising and lowering operators are not Hermitian, they do \nnot correspond to measurable observables. Nonetheless, they are very useful.\nOur attempt to factor the Hamiltonian is complicated by the fact that quantum mechanical opera-\ntors do not in general commute with each other. In Eq. (9.13), we implicitly assumed that u and v com-\nmute with each other, so that the cross terms  -ivu and iuv cancel. However, the quantum mechanical \noperators xn and pn that we use to deﬁne the raising and lowering operators do not commute with each \nother. As a result, we must take care in ﬁnding the product of the two new operators:\n \n a-a = mv\n2 U\n axn - i pn\nmvbaxn + i pn\nmvb\n \n = mv\n2 U\n axn 2 +\npn 2\nm2v2 +\ni\nmv\n 3xnpn - pnxn4b \n(9.17)\n \n = mv\n2 U\n axn 2 +\npn 2\nm2v2 +\ni\nmv3xn, pn4b.\nHence, the product a-a of the raising and lowering operators gives us what we want—the term in the \ncurly brackets in Eq. (9.14)—but with an extra additive term proportional to the commutator of xn and pn.\nRecall [Problem 5.1 and Eq. (6.66)] that the commutator of xn and pn is\n \n3xn, pn4 = i U. \n(9.18)\n",
    "280 \nHarmonic Oscillator\nSubstituting into Eq. (9.17), we obtain\n \na-a = mv\n2 U\n axn  2 +\npn2\nm2v2b - 1\n2, \n(9.19)\nso that the Hamiltonian written in terms of these new operators is\n \nH = U v1a-a + 1\n22  . \n(9.20)\nWe need one more thing before we proceed. Go back to Eq. (9.17) and note that if we had reversed \nthe order of a- and a, then we would have obtained a similar result with one difference: the commutator\nwould be reversed in sign (Problem 9.1). Thus the reverse product of the raising and lowering \noperators is\n \naa- = mv\n2U\n axn  2 +\npn2\nm2v2b + 1\n2. \n(9.21)\nIf we now subtract Eq. (9.19) from Eq. (9.21), we ﬁnd the commutator of the two new operators:\n \n3a, a-4 = aa- - a-a = 1  . \n(9.22)\nThis commutator equation deﬁnes the algebra of these new operators and provides the key to ﬁnding \nthe eigenvalue spectrum.\nArmed with the commutator relation in Eq. (9.22), we can now demonstrate that the new opera-\ntors a- and a do act to raise and lower, respectively, the energy eigenstates, as we said at the beginning. \nTo see how the raising and lowering operators act on energy eigenstates, we ﬁrst calculate the com-\nmutator of the lowering operator with the Hamiltonian:\n \n  3H, a4 = Ha - aH\n \n = U v1a-a + 1\n22a - a U v1a-a + 1\n22 \n(9.23)\n \n = U v1a-aa - aa-a2.\nNow use the commutator of the raising and lowering operators to obtain\n \n 3H, a4 = U v1a-aa - 1a-a + 12a2 \n \n = -U v a .\n \n(9.24)\nLikewise, you can show that the commutator of the raising operator with the Hamiltonian is\n \n3H, a-4 = +U va-. \n(9.25)\nTo show that the lowering operator deserves its name, act with a on an energy eigenstate 0 E9, \nwhere we assume that 0 E9 is a normalized energy eigenstate that satisﬁes the energy eigenvalue equa-\ntion H0 E9 = E0 E9, but we don’t yet know the eigenvalue E. To learn about the energy of the new ket \na0 E9, consider what happens when the Hamiltonian H acts on a0 E9:\n \nH1a0 E92 = Ha0 E9. \n(9.26)\n",
    "9.2 Quantum Mechanical Harmonic Oscillator \n281\nThe commutator in Eq. (9.24) tells us that Ha = aH - U va, so Eq. (9.26) becomes\n \n H1a0 E92 = 1a H - U va20 E9\n \n = a H0 E9 - U va0 E9. \n(9.27)\nNow use the energy eigenvalue equation H0 E9 = E0 E9 to obtain\n \n H1a0 E92 = 1a E0 E9 - U va0 E2\n \n = 1E - U v21a0 E92.  \n(9.28)\nThis looks like algebraic gymnastics, but there is something useful buried here! Equation (9.28) tells \nus that when the new ket a0 E9 is acted on by the Hamiltonian H, the result is the same ket a0 E9 multi-\nplied by the factor 1E - U v2, which means that the new ket a0 E9 is also an eigenstate of H, but with \nan energy eigenvalue 1E - U v2 that is smaller than the eigenvalue E of the original ket 0 E9 by one \nquantum of energy U v. The eigenvalue equation for this new state is\n \nH0 E - U v9 = 1E - U v2 0 E - U v9. \n(9.29)\nSo a has earned the name “lowering operator.” The only tricky point is that the state a0 E9 may not be \nnormalized (in fact it is not), assuming that the eigenstates 0 E9 are normalized, so we cannot say that \na0 E9 is equal to 0 E - U v9, merely that they are proportional.\nThe result is that we have now learned what happens when the operator a acts on an eigenstate \n0 E9 of H: it produces another eigenstate of H with the eigenvalue lowered by one quantum U v. Like-\nwise, one can show that the action of a- on an eigenstate of H produces an eigenstate with the eigen-\nvalue raised by one quantum of energy (Problem 9.2). Now you see why we call the operators a and \na- lowering and raising operators. We also refer to these operators collectively as ladder operators \nbecause they take us up and down a ladder of energy eigenstates, as depicted schematically in Fig. 9.3. \nWe don’t yet know where the rungs of the ladder are (i.e., what the energy eigenvalues are) or whether \nthere are many interleaved ladders. But the importance of the ladder operators is that if we can ﬁnd just \none eigenstate 0 E9, then the ladder operators can be used to ﬁnd other eigenstates of the system, with \neach level separated by the energy quantum U v.\nFrom the discussion so far and from the schematic in Fig. 9.3, you probably have the impression \nthat the ladder of energy states goes up and down symmetrically. But the commutator in Eq. (9.22) \nalready gives us a hint that there is a built-in asymmetry in the ladder, which we can use to ﬁnd the \nenergy spectrum. Because a and a- do not commute, we have aa- \u0002 a-a, which we can express \nabstractly as\n \n1down21up2 \u0002 1up21down2. \n(9.30)\nBut that is not how you might expect ladder operators to behave. If you stand on a rung and go up then \ndown, you are in the same position as if you had gone down then up. The asymmetry of the harmonic \noscillator is also evident if we note that the potential energy well that deﬁnes the harmonic oscillator \nhas a minimum level at V = 0 from which it goes only upward. A classical particle cannot have a \ntotal energy below the potential energy minimum because kinetic energy cannot be negative. Though \nquantum mechanics does allow for negative kinetic energies (in the classically forbidden regions), it is \nalso true in quantum mechanics that the total energy of a particle cannot be below the potential energy \nminimum (quantum mechanics may be weird, but it is not that weird). So we conclude that the ladder \nof energy states in Fig. 9.3 must not go below E = 0.\n",
    "282 \nHarmonic Oscillator\nIf the ladder of energy states in Fig. 9.3 cannot go below zero, then there must be a lowest energy \nstate @ Elowest9. But how can that be consistent with the idea of the ladder operators? Wouldn’t the low-\nering operator take that “lowest” state to a state with lower energy, below zero? Not if we don’t let it! \nWe do that by requiring that when we operate on the lowest possible energy state with the lowering \noperator we get zero:\n \na @ Elowest9 = 0. \n(9.31)\nWe refer to this as the ladder termination condition. We now use this condition to ﬁnd the energy of \nthat lowest state. Act with the Hamiltonian H on the lowest state:\n \nH 0 Elowest9 = U v1a-a + 1\n220 Elowest9 \n(9.32)\nand note that the ladder termination condition in Eq. (9.31) means that the ﬁrst term on the right-hand \nside of Eq. (9.32) becomes zero, giving\n \nH 0 Elowest9 = 1\n2\n U v0 Elowest9. \n(9.33)\nThis is nothing but the energy eigenvalue equation H 0 E9 = E 0 E9 for the lowest state, so the energy is\n \nElowest = 1\n2 U\n v. \n(9.34)\nWow! This operator gymnastics has led us to the ground state energy of the quantum mechanical \nharmonic oscillator, using just the form of the Hamiltonian and the commutator of position and \nmomentum. Note that the ground state does not have zero energy, in contrast to the classical harmonic \noscillator. Rather, the quantum mechanical ground state has a zero-point energy of U v>2. This is not \nsurprising if we recall the other potential well systems we have studied such as the square well poten-\ntial, where the ground state is not at the bottom of the well. The zero-point energy is also consistent \nwith the uncertainty principle in that we expect there to be residual energy associated with the spread \nin momentum.\nTo generate the next energy eigenstate up the ladder of states, we act with the raising operator on \nthe ground state 0 Elowest9, which produces a new energy eigenstate with the energy increased by one \na\u0002\na\u0002\na\u0002\na\u0002\na\na\na\na\nE \u000f\u00072\u0002Ω\nE \n\u00072\u0002Ω\nE \u000f\u00071\u0002Ω\nE \n\u00071\u0002Ω\nE\nFIGURE 9.3 Part of the ladder of energy eigenstates, with the action of the raising \nand lowering  operators shown.\n",
    "9.2 Quantum Mechanical Harmonic Oscillator \n283\nquantum U v. We repeat the action of the raising operator to generate the complete ladder of energy \nvalues, as shown in Fig. 9.4:\n \nE = 1\n2\n U\n v, 3\n2\n U\n v, 5\n2\n U\n v, 7\n2  U\n v, ... . \n(9.35)\nWe write the energy spectrum compactly as\n \nEn = U v1n + 1\n22,  n = 0, 1, 2, 3, ...  , \n(9.36)\nwhich is the result we quoted at the beginning. The quantum number n is used to label the energy \neigenstates 0 En9 = 0 n9. These states satisfy the energy eigenvalue equation\n \nH 0 n9 = En0 n9 = 1n + 1\n22 U\n v0 n9, \n(9.37)\nare normalized to unity\n \n8n0 n9 = 1, \n(9.38)\nand are orthogonal to each other\n \n8m0 n9 = dmn. \n(9.39)\nThus we have found the complete spectrum of energy eigenstates of the harmonic oscillator, using \nminimal information about the operator properties.\nAs shown in Fig. 9.4, the energy eigenstates are evenly spaced by the energy quantum U v. The \nselection rule for the quantum mechanical harmonic oscillator (coming soon in Section 9.8) restricts \ntransitions to those between adjacent energy states, so the uniform spacing implies that a spectroscopy \nexperiment would yield only one possible value for an energy difference, no matter which levels were \ninvolved. This is similar to the classical case where there is only one frequency that characterizes a \nharmonic oscillator. \nIn addition to the ladder operators, it is useful to deﬁne one more new operator that will help us \n“count” energy quanta. The energy eigenvalue equation for the harmonic oscillator\n \n H 0 n9 = En\n 0 n9\n \n \n U v1a-a + 1\n22@ n9 = U v1n + 1\n22@ n9 \n(9.40)\ncan be simpliﬁed to obtain a new eigenvalue equation\n \na-a 0 n9 = n0 n9. \n(9.41)\nThis equation suggests that we deﬁne the operator a-a as the number operator N:\n \nN = a-a. \n(9.42)\nThe number operator N is dimensionless and obeys the eigenvalue equation\n \nN0 n9 = n0 n9. \n(9.43)\nThe eigenvalues of the number operator are the same integers n that we use to label the energy eigen-\nstates 0 n9. We can write the harmonic oscillator Hamiltonian in terms of the number operator:\n \nH = U v1N + 1\n22. \n(9.44)\n",
    "284 \nHarmonic Oscillator\nThe number operator is Hermitian, even though the ladder operators that comprise it are not. The \neigenvalues of the number operator represent the number of energy quanta U v there are in the system \nabove the ground state.\nThe mathematics of the quantum mechanical harmonic oscillator system can be applied to other \nquantum mechanical systems, even though they do not appear to be harmonic oscillators. All that is \nrequired is that the Hamiltonian be the sum of squares of operators. For example, the Hamiltonian \nrepresenting the electromagnetic ﬁeld energy can be written as the sum of squares of operators \nrepresenting the electric and magnetic ﬁelds (see any E&M text). Hence, when we apply quantum \nmechanics to the electromagnetic ﬁeld, the energy eigenstate 0 n9 represents a state of the system with \nn photons (particles or quanta of light), each with an energy U v. The ground state 0 09 represents the \nstate of the system with no photons, also known as the vacuum. Thus the zero-point energy represents \nthe electromagnetic energy of the vacuum state, which is a bit surprising because we usually associate \nthe vacuum with the absence of all “stuff.” Even though spectroscopic measurements determine only \nenergy differences, there are observable effects of this zero-point energy in the Lamb shift, which we \nwill learn about in Chapter 12. Because the raising and lowering operators change the number of pho-\ntons in the system, they are often referred to as creation and annihilation operators, respectively.\n9.3 \u0002 WAVE FUNCTIONS\nThough we have solved the energy eigenvalue equation, we are not quite done. We don’t yet know the \nspatial wave functions corresponding to the energy eigenstates. That is to say, we know that the states \n0 n9 are the energy eigenstates, but we don’t know their spatial representation 0 n9 \u0003 wn1x2 = 8x0 n9.\nAs we did for the particle in a box and the hydrogen atom, we could solve the differential equation \nform of the energy eigenvalue equation, which in this case is\n \n-  U2\n2m d 2wn1x2\ndx2\n+ 1\n2\n  mv2x2wn1x2 = Enwn1x2. \n(9.45)\nAs we mentioned earlier, this can be solved with a power series technique that would yield the ener-\ngies En and the states wn1x2. Rather, let’s continue our operator approach to ﬁnd the wave functions.\nWe said before that if we know one of the harmonic oscillator eigenstates, then we can use the \nladder operators to generate the other energy eigenstates. We used this idea to discover the spectrum \na\u0002\na\u0002\na\u0002\na\u0002\na\na\na\na\n\u00020\u0003\n\u00021\u0003\n\u00022\u0003\n\u00023\u0003\n\u00024\u0003\nE3 \u0004\n\u0002Ω\n\u0002Ω\n7\n2\nE2 \u0004 5\n2\nE1 \u0004\n\u0002Ω\n\u0002Ω\n3\n2\nE0 \u0004 1\n2\nE4 \u0004\n\u0002Ω\n9\n2\nFIGURE 9.4 The ladder of harmonic oscillator states has its lowest rung at n = 0.\n",
    "9.3 Wave Functions \n285\nof energy levels by noting that the ground state is unique in that there are no states below it, which led \nus to the ladder termination equation\n \na 0 09 = 0. \n(9.46)\nLet’s now use this same termination condition to ﬁnd the wave function representing the ground state, \nand then use the raising operator to generate all the other wave functions. In the x–representation, the \nladder termination equation is\n \n a w01x2 = 0  \n \n A mv\n2U  axn + i pn\nmvb w01x2 = 0  \n \n(9.47)\n \n A\nmv\n2U  ax +\nU\nmv\n  d\ndxb w01x2 = 0, \nwith the result\n \nd\ndx\n w01x2 = -  mv\nU\n xw01x2. \n(9.48)\nWe now have a ﬁrst-order differential equation for the ground state wave function. This equation tells \nus that we want a function whose derivative is equal to the function itself times a constant and x. We \nknow that the derivative of the exponential function ex is itself, so to get the extra factor of x we need \nan x2 in the exponent. To get the multiplicative factor correct, the function must be e-mvx2>2U. The prop-\nerly normalized solution to Eq. (9.48) is (Problem 9.3)\n \nw01x2 = amv\np Ub\n1>4\ne-mvx2>2U, \n(9.49)\nwhich is a Gaussian function. This ground state wave function is plotted in Fig. 9.5(a). The wave func-\ntion has a single antinode as we expect for the ground state. A classical particle with the same energy \n(U v>2) has classical turning points at {x0 where the energy is all potential energy:\n \n 1\n2 U v = 1\n2 mv2x2\n0\n \n \n x0 = B\nU\nmv.\n \n(9.50)\n(a)\n\n4 \n2\n2\n4\nx/x0\nΨ(x)\nn \u0004\u00070\n(b)\nx/x0\nΨ(x)\n\n4 \n2\n2\n4\nn \u0004\u00071\n(c)\nx/x0\nΨ(x)\n\n4 \n2\n2\n4\nn \u0004\u00072\nFIGURE 9.5 Energy eigenstate wave functions for the ﬁrst three states of the harmonic oscillator. \nThe dashed lines enclose the classically allowed region.\n",
    "286 \nHarmonic Oscillator\nFrom the plot in Fig. 9.5(a), you see that there is a ﬁnite probability that the particle is in the classically \nforbidden region beyond {x0 (Problem 9.4).\nTo ﬁnd the other energy eigenstates we act on the ground state with the raising operator. But \nwe have already mentioned that the ladder operators do not preserve the normalization of the energy \neigenstates, so we must determine the proper scaling factor. Let’s ﬁrst look at the lowering operator. \nConsider the norm of the state a0 n9. The rules of Hermitian conjugation allows us to write the norm as\n \n@  a 0 n9@\n2\n= 18n0 a-21a0 n92 = 8n0 a-a0 n9. \n(9.51)\nThe product a-a is the number operator N, so we get\n \n @  a 0 n9@\n2\n= 8n0 N0 n9 = 8n0 n0 n9 = n8n0 n9 \n \n = n,\n \n(9.52)\nwhere we have used the normalization 18n0 n9 = 12 of the energy/number eigenstates 0 n9. Let c be \nthe proportionality factor between the state a0 n9 and the eigenstate 0 n - 19:\n \na 0 n9 = c0 n - 19. \n(9.53)\nBecause both 0 n9 and 0 n - 19 are normalized to unity, we can use Eq. (9.52) to ﬁnd the constant c:\n \n @  a 0 n9@\n2\n= @  c0 n - 19@\n2\n \n \n n = 0 c0\n2.\n \n(9.54)\nBy convention, we choose the proportionality constant to be real and positive (an overall phase is not \nmeasurable) and obtain\n \na 0 n9 = 1n0 n - 19  . \n(9.55)\nLikewise you can show that the raising operator equation is (Problem 9.5)\n \na-0 n9 = 2n + 10 n + 19  . \n(9.56)\nA simple mnemonic to remember which operator gives which factor (n or n + 1) in Eqs. (9.55) and \n(9.56) is that the index under the square root is the larger value of the two eigenstates on the two sides \nof the equations. The different scale factors in Eqs. (9.55) and (9.56) are a reﬂection of the asymmetry \nof the raising and lowering operations that is embodied in the commutator relation in Eq. (9.22).\nTo generate states above the ground state we use Eq. (9.56) to formulate the raising operator \nequation\n \n0 n + 19 =\n1\n2n + 1\n a-0 n9. \n(9.57)\nApply Eq. (9.57) to the ground state and the resulting states to obtain\n \n0 19 =\n1\n21\n a-0 09\n \n0 29 =\n1\n22\n a-0 19 =\n1\n22 # 1\n 1a-2\n20 09\n \n(9.58)\n \n0 39 =\n1\n23\n a-0 29 =\n1\n23 # 2 # 1\n 1a-2\n30 09\n",
    "9.3 Wave Functions \n287\nand generalize to ﬁnd\n \n0 n9 =\n1\n2n!\n 1a-2\nn0 09. \n(9.59)\nProjected onto the spatial basis, this general result is\n \nwn(x) =\n1\n1n!\n c A\nmv\n2U\n ax -\nU\nmv d\ndxb d\nn\n w0 1x2. \n(9.60)\nExample 9.1 Use the eigenstate generating expression in Eq. (9.60) to determine the ﬁrst excited \nstate of the harmonic oscillator.\nTake Eq. (9.60) and set n = 1, which means that the raising operator acts only one time to \nyield the ﬁrst eigenstate above the ground state:\n \n w11x2 =\n1\n21!\n c A\nmv\n2 U  ax -\nU\nmv d\ndxb d w01x2\n \n = c A\nmv\n2 U  ax -\nU\nmv d\ndxb d amv\np U b\n1>4\ne-mvx2>2U \n \n = amv\np U b\n1>4\nA\nmv\n2 U  c ax -\nU\nmv a- mv\nU\n xbb d e-mvx2>2U\n \n(9.61)\n \n = amv\np U b\n1>4\nA\nmv\n2 U\n 12 x2 e-mvx2>2U. \nThis result is already normalized. Note that the wave function dimensions are 1/2length to ensure \nthat the normalization condition is dimensionless.\nThe general wave function expression in Eq. (9.60) can be difﬁcult to use in practice because it \nrequires n derivatives. When we apply the raising operator to the Gaussian function in w01x2 n times, \nwe obtain the Gaussian function multiplied by a polynomial of order n. The resultant polynomials are \nHermite polynomials. To simplify the general wave function expression, it is common to write the \nharmonic oscillator wave functions in terms of a dimensionless variable\n \nj K A\nmv\nU\n x. \n(9.62)\nIn this case, the ground state and the general states are written as\n \nw0 1x2 = amv\np U b\n1>4\ne-  j2>2 \n(9.63)\nand\n \nwn (x) = amv\np U b\n1>4\n1\n22nn!\n Hn (j) e-  j2>2. \n(9.64)\n",
    "288 \nHarmonic Oscillator\nThe ﬁrst several Hermite polynomials Hn 1j2 are:\n \n H0 1j2 = 1\n \n \n H1 1j2 = 2j\n \n \n H2 1j2 = 4j2 - 2\n \n \n(9.65)\n \n H3 1j2 = 8j3 - 12j\n \n \n H4 1j2 = 16j4 - 48j2 + 12. \nYou can easily verify that for n = 1, Eq. (9.64) agrees with the result we found in Example 9.1.\nThe ﬁrst three harmonic oscillator energy eigenstate wave functions are plotted in Fig. 9.5. As \nwe expected, the harmonic oscillator energy eigenstates are similar in many ways to the energy eigen-\nstates of the other bound state systems we have studied—the inﬁnite and ﬁnite square wells and the \nhydrogen atom. On a superﬁcial level, we can consider each of these bound state systems as a particle-\nin-a-box system—the boxes just have different shapes. Common features of these energy eigenstates \nare (1) the wave functions are oscillatory inside the well and exponential decaying outside the well, \nwhere the edge of the well is deﬁned by the classical turning points; (2) the wave functions of sym-\nmetric wells are alternately even and odd with respect to inversion about the center of the well, reﬂect-\ning the spatial symmetry of the well; and (3) the number of nodes and antinodes in the wave function \nincreases with energy.\nAs we have done in the previous bound state problems, we combine the schematic diagrams \ndepicting (i) the potential energy well, (ii) the energy spectrum, and (iii) the energy eigenstates in a \nsingle uniﬁed diagram, shown in Fig. 9.6(a). This single diagram is commonly used to represent the \npotential energy well problem and its quantum mechanical solution. In this uniﬁed schematic diagram, \nthe vertical scale measures energy (i and ii) or wave function (iii), and the zero of each wave function \nis placed at the corresponding energy level of that state in the well.\nThe spatial probability density is given by the absolute square of the wave function\n \nPn1x2 = 0 wn1x20\n2. \n(9.66)\n(a)\n(b)\nn=0\nn=1\nn=2\nn=3\nx\nE,Ψ\nx\nE,\u0002Ψ\u00022\nFIGURE 9.6 Energy eigenstate (a) wave functions and \n(b) probability densities of the harmonic oscillator.\n",
    "9.4 Dirac Notation \n289\nIn Fig. 9.6(b) we plot the probability densities of the ﬁrst four energy eigenstates in a uniﬁed diagram. \nThe ground state probability density is largest at the center of the well, but the location of the prob-\nability density maximum gets increasingly close to the classical turning points as the energy level \nincreases. The probability density for a large value of the quantum number n is shown in Fig. 9.7. For \nsuch a high energy state, the probability density is similar, at least when locally averaged, to the prob-\nability distribution of a classical harmonic oscillator.\nLet’s summarize how the harmonic oscillator illustrates the ﬁrst three basic postulates of quan-\ntum mechanics. The ﬁrst postulate tells us that quantum states are represented by kets, such as the \nenergy eigenstates 0 n9 \u0003 wn1x2. The second postulate tells us that observables are represented by \noperators, such as the Hamiltonian H, the position xn and the momentum pn. The third postulate tells us \nthat the eigenvalues of an operator are the only possible results of measurements, such as the energies \nEn = 1n + 1>22U v.\n9.4 \u0002 DIRAC NOTATION\nLet’s use the harmonic oscillator problem as a framework for reviewing Dirac notation. We use the \nDirac kets 0 n9 to represent the energy eigenstates. Recall that the labeling of the kets does not affect \nthe properties of the kets, so we are free to use whatever labeling is most convenient. The convention \nis to be as brief as possible without losing speciﬁcity. We label the harmonic oscillator energy eigen-\nstates states with the energy eigenvalue index n alone, but it is also common for w or c to be used as a \nlabel with the eigenvalue index as a subscript. Or one could use the energy value itself. These are all \nequally valid notations:\n \n 0 n9 = 0 wn9 = 0 En9 = @1n + 1\n22U\n v9 \n \n 0 09 = 0 w09 = 0 E09 = @ 1\n2  U\n v9.\n \n(9.67)\n\nxcl\nxcl\nx\n\u0002\u0010n(x)\u00022\nFIGURE 9.7 Quantum mechanical probability density for the n = 30 state. The \nclassical probability distribution (thin line) peaks at the classical turning points.\n",
    "290 \nHarmonic Oscillator\nIn Section 9.3, we found the energy eigenstate wave functions wn1x2. The connection between wave \nfunctions and abstract kets is expressed as\n \nwn1x2 = 8x0 n9. \n(9.68)\nIn words, Eq. (9.68) says that the wave function wn1x2 is the projection of the abstract ket 0 n9 onto the \nposition eigenstates 0 x9. Or using the representation notation\n \n0 n9 \u0003 fn1x2, \n(9.69)\nwe say that wn1x2 is the representation of the quantum state 0 n9 in the position representation.\nThe energy eigenstates of the harmonic oscillator obey the three important properties that we \nhave discussed previously: normalization, orthogonality, and completeness. The normalization condi-\ntion is expressed in wave function notation as\n \nL\n\u0005\n- \u0005\n0 wn1x2 0\n2dx = 1 \n(9.70)\nor in Dirac notation as\n \n8n0 n9 = 1. \n(9.71)\nThe connection between the normalization condition in the position representation [Eq. (9.70)] and \nin abstract Dirac notation [Eq. (9.71)] is evident if we use the completeness relation for the position \neigenstates, which form a complete continuous basis:\n \nL\n\u0005\n- \u0005\n0 x98x0 dx = 1. \n(9.72)\nBecause the right hand side of Eq. (9.72) is the unity operator, it can be inserted into an expression \nwithout altering the value of the expression. Inserting Eq. (9.72) into Eq. (9.71) yields\n \n 1 = 8n0 n9\n \n \n = 8n0 e\nL\n\u0005\n- \u0005\n0 x98x0 dx f 0 n9 \n \n =\nL\n\u0005\n- \u0005\n8n0 x98x0 n9dx\n \n \n(9.73)\n \n =\nL\n\u0005\n- \u0005\nw*\nn 1x2wn 1x2 dx\n \n \n =\nL\n\u0005\n- \u0005\n0 wn1x2 0\n2\n dx , \nwhich shows that Eq. (9.70) and Eq. (9.71) are equivalent.\nThe energy eigenstates of the harmonic oscillator are orthogonal because they are the eigenvec-\ntors of an Hermitian operator. The orthogonality condition is expressed in wave function notation as\n \nL\n\u0005\n- \u0005\nw*\nm1x2wn1x2dx = dmn  \n(9.74)\n",
    "9.4 Dirac Notation \n291\nor in Dirac notation as\n \n8m0 n9 = dmn\n . \n(9.75)\nBy using a Kronecker delta, the orthogonality condition also includes the normalization, so Eqs. (9.74) \nand (9.75) are called the orthonormality condition. You can check that the harmonic oscillator energy \neigenstate wave functions are orthogonal by doing the explicit integrals in Eq. (9.74) (Problem 9.7).\nThe harmonic oscillator eigenstates form a complete discrete basis, which is expressed in terms \nof the closure relation\n \na\n\u0005\nn=0\n0 n98n0 = 1, \n(9.76)\nwhere the right hand side is the unity operator. Completeness of the energy basis means that any \narbitrary state vector 0 c9 can be written in terms of the energy eigenstates, either in wave function \nnotation\n \n0 c9 \u0003\n c1x2 = a\n\u0005\nn=0\ncn wn 1x2 \n(9.77)\nor in Dirac notation\n \n0 c9 = a\n\u0005\nn=0\ncn0 n9. \n(9.78)\nTo ﬁnd the value of a particular expansion coefﬁcient, we use the closure relation in Eq. (9.76) to \nrewrite the state 0 c9 in terms of the energy eigenstates:\n \n 0 c9 = 10 c9\n \n \n = e a\n\u0005\nn=0\n0 n98n0 f 0 c9 \n(9.79)\n \n = a\n\u0005\nn=0\n0 n98n0 c9.\n \nBy comparing Eqs. (9.78) and (9.79), we conclude that the expansion coefﬁcient cn is the projection of \nthe wave function 0 c9 onto the particular basis state 0 n9, which in Dirac notation is\n \ncn = 8n0 c9 \n(9.80)\nand in wave function notation is\n \ncn =\nL\n\u0005\n- \u0005\nw*\nn 1x2c1x2dx. \n(9.81)\nThe normalization requirement on the general state 0 c9 \u0003  c1x2 in wave function notation is\n \nL\n\u0005\n- \u0005\n0 c1x2 0\n2dx = 1 \n(9.82)\n",
    "292 \nHarmonic Oscillator\nor in Dirac notation is\n \n8c0 c9 = 1. \n(9.83)\nWe can also use the energy eigenstate closure relation Eq. (9.76) to write the normalization require-\nment in terms of the eigenstate expansion\n \n 1 = 8c0 c9 = 8c0 e a\n\u0005\nn=0\n0 n98n0 f 0 c9\n \n = a\n\u0005\nn=0\n8c0 n98n0 c9 = a\n\u0005\nn=0\n08n0 c9 0\n2 \n(9.84)\n \n = a\n\u0005\nn=0\n0 cn0\n2.\nThe square of each expansion coefﬁcient is the probability that the state 0 c9is measured to be in state \n0 n9, that is, to have energy eigenvalue En:\n \nPEn = 08n0 c9 0\n2 = 0 cn0\n2. \n(9.85)\nThus the requirement that the state be normalized is a requirement that the total probability sum to \nunity, (i.e., there is unit probability that some value of energy is measured).\nExample 9.2 A quantum mechanical harmonic oscillator is in the state\n \n0 c9 = 12\n4 0 09 + i 2\n4 0 19 - i 1\n4 0 29 + 3\n4 eip>30 39. \n(9.86)\nWhat are the possible results of an energy measurement and with what probabilities do they occur? \nFind the expectation value of the energy.\nThe possible results of an energy measurement are the energy eigenvalues En = 1n + 1\n22U v. \nFor this superposition of four energy eigenstates, the probabilities calculated from Eq. (9.85) are \nzero except for the four energies E0, E1, E2, and E3. These probabilities are\n \nPE0 = 0800 c9 0\n2 = @H0@A12\n4 @0I + i 2\n4@1I - i 1\n4@2I + 3\n4 eip>3@3IB @\n2\n= @12\n4 @\n2\n=\n2\n16\n \nPE1 = 0810 c9 0\n2 = @H1@A12\n4 @0I + i 2\n4@1I - i 1\n4@2I + 3\n4 eip>3@3IB @\n2\n= @ i 2\n4@\n2 =\n4\n16\n \n(9.87)\n \nPE2 = 0820 c9 0\n2 = @H2@A12\n4 @0I + i 2\n4@1I - i 1\n4@2I + 3\n4 eip>3@3IB @\n2\n= @ -i 1\n4@\n2\n=\n1\n16\n \nPE3 = 0830 c9 0\n2 = @H3@A12\n4 @0I + i 2\n4@1I - i 1\n4@2I + 3\n4 eip>3@3IB @\n2\n= @3\n4 eip>3@\n2\n=\n9\n16.\nThe expectation value of the energy is\n \n 8E9 = a\n\u0005\nn=0\nEnPEn = 11\n2 U v2 2\n16 + 13\n2 U v2 4\n16 + 15\n2 U v2 1\n16 + 17\n2 U v2 9\n16\n \n= 41\n16 U v _ 2.56 U v.\n  \n(9.88)\n",
    "9.5 Matrix Representations \n293\nThe expectation value can also be calculated as 8E9 = 8c0 H0 c9, with the same result. A his-\ntogram of the energy measurements is shown in Fig. 9.8.\nLet’s continue the summary of how the harmonic oscillator illustrates the basic postulates of \nquantum mechanics. The fourth postulate tells us that the probability of a measurement is the complex \nsquare of the projection onto the measured eigenstate, such as the energy probability PEn = 08n0 c9 0\n2 \nor the position probability density P1x2 = 0 c1x2 0\n2. The ﬁfth postulate tells us that the quantum state \nvector after a measurement is the measured eigenstate, such as 0 c9 collapsing to 0 n9 after the energy \nEn is measured. The sixth postulate tells us how to ﬁnd the time evolution of states, which we’ll \naddress in Section 9.8.\n9.5 \u0002 MATRIX REPRESENTATIONS\nSo far we have presented the operators and states of the harmonic oscillator in abstract Dirac nota-\ntion and in wave function or position representation. However, we found a matrix representation to \nbe useful previously, for example in the discussion of spin states. Can we use a matrix representation \nfor the harmonic oscillator case? It turns out that we can. A matrix representation is a collection of \nnumbers that represents states and operators in terms of a chosen basis set. So we must ﬁrst choose \na basis for the matrix representation. We have just solved for the energy basis states of the harmonic \noscillator, so that choice seems reasonable, especially in light of the importance of the energy basis in \nthe Schrödinger time evolution recipe. So how do we ﬁnd the numbers we need to represent states and \noperators as matrices in the energy representation? We do it by inspection!\nWe learned in Section 2.1 that an operator is always diagonal in its own basis, and eigenvec-\ntors are unit vectors in their own basis. So the Hamiltonian is diagonal in the energy basis and the \nenergy eigenstates are unit vectors in the energy basis. The diagonal elements of the Hamiltonian \nare the energy eigenvalues, so by inspection of our energy result in Eq. (9.36), we ﬁnd the Ham-\niltonian matrix\n \nH \u0003 •\n1\n2 U v\n0\n0\n0\ng\n0\n3\n2 U v\n0\n0\ng\n0\n0\n5\n2 U v\n0\ng\n0\n0\n0\n7\n2 U v\ng\nf\nf\nf\nf\nf\nμ,  \n(9.89)\nFIGURE 9.8 Histogram of energy measurements.\nE\n0.5\n1.0\nP\nPE0\nPE1\nPE2\nPE3\n",
    "294 \nHarmonic Oscillator\nwhere we use the convention of ordering the rows and columns starting with the ground state energy. \nThere are an inﬁnite number of energy eigenstates, so the matrix representation of the Hamiltonian is \ninﬁnite, but discrete. In this matrix representation, the energy basis states are the unit vectors\n \n0 09 \u0003 •\n1\n0\n0\n0\nf\nμ, 0 19 \u0003 •\n0\n1\n0\n0\nf\nμ, 0 29 \u0003 •\n0\n0\n1\n0\nf\n μ, g. \n(9.90)\nThat’s all there is to it!\nFinding the matrix representation of other states and operators takes more work, but not too much. \nWe already found the expansion coefﬁcients cn = 8n0 c9 required to represent an arbitrary state 0 c9 \nin terms of the energy eigenstates in Eq. (9.80), now we just order them in a column vector:\n \n0 c9 \u0003 •\nc0\nc1\nc2\nc3\nf\nμ  . \n(9.91)\nThe matrix representation of other operators requires us to know how they act upon the energy eigen-\nstates. For the ladder operators, we learned this in Eqs. (9.55) and (9.56):\n \n a0 n9 = 1n0 n - 19\n \n a-0 n9 = 2n + 10 n + 19.\n \n(9.92)\nTo ﬁnd the individual matrix elements of the ladder operators, project each of these equations onto a \ndifferent eigenstate to obtain\n \n 8m0 a0 n9 = 8m0 1n0 n - 19  8m0 a-0 n9 = 8m0 2n + 10 n + 19 \n \n = 1n dm, n-1  \n = 2n + 1dm, n+1 .\n \n(9.93)\nBecause the ladder operators take one state to an adjacent state, the matrix elements connect only adja-\ncent states, as the Kronecker deltas indicate. Hence, the matrices for the ladder operators are\n \na \u0003 •\n0\n21\n0\n0\ng\n0\n0\n22\n0\ng\n0\n0\n0\n23\ng\n0\n0\n0\n0\ng\nf\nf\nf\nf\nf\nμ       a- \u0003 •\n0\n0\n0\n0\ng\n21\n0\n0\n0\ng\n0\n22\n0\n0\ng\n0\n0\n23\n0\ng\nf\nf\nf\nf\nf\nμ.  (9.94)\nNote that these operators are dimensionless, as expected. They are each nondiagonal and they are not \nHermitian. However, they are Hermitian conjugates of each other, as required by their deﬁnitions.\n",
    "9.5 Matrix Representations \n295\nThe ladder operators were deﬁned in Eqs. (9.15) and (9.16) in terms of the position and momen-\ntum operators. Hence, the position and momentum operators are related to the ladder operators by\n \nxn = B\nU\n2mv 1a- + a2  \n \npn = iB\nUmv\n2  1a- - a2\n \n(9.95)\nand their matrix representations are\n \nxn \u0003 B\nU\n2mv •\n0\n11\n0\n0\ng\n11\n0\n12\n0\ng\n0\n12\n0\n13\ng\n0\n0\n13\n0\ng\nf\nf\nf\nf\nf\nμ  pn \u0003 B\nUmv\n2  •\n0\n-i11\n0\n0\ng\ni11\n0\n-i12\n0\ng\n0\ni12\n0\n-i13 g\n0\n0\ni13\n0\ng\nf\nf\nf\nf\nf\nμ.\n \n(9.96)\nThese matrices are Hermitian, as they must be because position and momentum are observables. The \nposition and momentum matrix elements connect only adjacent states, but in this case, states above and \nbelow. This is important in determining the selection rules for transitions, as discussed in Section 9.8. \nThe position and momentum matrices are both nondiagonal in the energy basis, so they do not com-\nmute with the Hamiltonian.\nExample 9.3 Find the expectation value of position in the ground state of the harmonic oscillator.\nThere are three ways to calculate this.\n(1) The expectation value of position is the matrix element\n \n8xn9 = 8c0 xn 0 c9. \n \n(9.97)\nThe expectation value of position in the ground state is the speciﬁc matrix element\n \n8xn9 = 800 xn 0 09 = xn00 \n(9.98)\nwhich is zero by inspection of the position matrix in Eq. (9.96). \n(2) We can also calculate the expectation value using explicit Dirac notation and the ladder \noperators [Eq. (9.95)]:\n \n 8xn9 = 800 xn 0 09\n \n = B\nU\n2mv\n 8001a- + a20 09.\n \n(9.99)\nThe raising operator acting on the state 0 09 produces the state 0 19 in the ﬁrst term and the lowering \noperator acting on the state 0 09 yields the value 0 in the second term. The result\n \n 8xn9 = B\nU\n2mv\n 3800 19 + 04 \n(9.100)\n \n = 0\n \n is again zero.\n",
    "296 \nHarmonic Oscillator\n(3) Finally, we can calculate the expectation value in the position representation by doing an \nintegral\n \n 8xn9 = 800 xn 0 09\n \n \n =\nL\n\u0005\n- \u0005\nw*\n01x2xw01x2dx \n(9.101)\n \n =\nL\n\u0005\n- \u0005\nx0 w01x2 0\n2dx.\n \nThis integral is zero because the probability density is spatially symmetric (even) about the origin \nand the function x is antisymmetric (odd) about the origin, yielding an antisymmetric (odd) inte-\ngrand. The integral of an antisymmetric (odd) integrand over a symmetric (even) interval is zero.\nThis particular calculation is simple using any of these methods. More detailed calculations, such \nas the expectation value of the square of the position are most easily done using the operator method \nin Eq. (9.99) (Problem 9.9).\n9.6 \u0002 MOMENTUM SPACE WAVE FUNCTION\nThe matrices for position and momentum in Eq. (9.96) have the same form, with different constants \nto get the dimensions correct. This suggests some symmetry between the position and momentum \nrepresentation that does not exist in the other bound state problems we have solved. To explore \nthis symmetry, let’s ﬁnd the momentum space representation (see Section 6.1.2) of the energy \neigenstates 0 n9 \u0003 fn1p2 = 8p0 n9. There are three ways we can ﬁnd the momentum space wave \nfunctions.\n(1) We can take the same operator approach we used above to ﬁnd the position representation \nwave functions. We start with the ladder termination equation\n \na0 09 = 0 \n(9.102)\nand express this in the momentum representation, where the position operator is a derivative with \nrespect to momentum and the momentum operator is a multiplicative factor:\n \n a f01p2 = 0  \n \n A\nmv\n2U\n axn + i pn\nmvbf01p2 = 0  \n(9.103)\n \n A\nmv\n2U\n aiU d\ndp +\ni\nmv\n pbf01p2 = 0. \nThis leads to a differential equation\n \nd\ndp\n f01p2 = -  1\nmvU\n pf01p2 \n(9.104)\n",
    "9.6 Momentum Space Wave Function \n297\nthat has the same form as the differential equation for the ground state wave function in the position \nrepresentation [see Eq. (9.48)]. It is a ﬁrst-order differential equation whose solution is a Gaussian \nfunction [see Eq. (9.49)], but in this case, momentum is the argument of the function. Hence, the prop-\nerly normalized ground state energy eigenstate in the momentum representation is (Problem 9.18):\n \nf01p2 = a\n1\npmvUb\n1>4\ne-  p2>2mvU. \n(9.105)\nThe excited states can be found with the raising operator approach that we used in the position rep-\nresentation. The momentum representation result includes the same Hermite polynomials as in the \nposition representation:\n \nfn1p2 = a\n1\npmvUb\n1>4\n1\n22nn!\n Hna\np\n2mvU\nbe-  p2>2mvU. \n(9.106)\n(2) We can also go back to the energy eigenvalue equation and express it in the momentum repre-\nsentation. In this case, we get a second-order differential equation in momentum space\n \n H0 n9 = En0 n9\n \n \n 1\n2m\n 3 pn 2 + m2v2xn 24@ n9 = En0 n9\n \n \n 1\n2m\n c  p2 - m2v2U2 d 2\ndp2dfn1p2 = Enfn1p2\n \n(9.107)\n \n -  mv2U2\n2\n \nd\n 2fn1p2\ndp2\n+\n1\n2m\n p2fn1p2 = Enfn1p2. \nOnce again this differential equation has the same form as the spatial differential equation [see \nEq. (9.45)] and leads to the momentum space solutions in Eq. (9.106) with the same functional \ndependence as the position representation solutions [Eq. (9.64)] with momentum as the argument \nrather than position.\n(3) We can transform the position representation solutions to the momentum representation using \nthe Fourier transform. In Chapter 6, we found that the position representation wave function and the \nmomentum representation wave function are connected by the Fourier transform\n \nf1p2 =\n1\n22pU L\n\u0005\n- \u0005\nc1x2e-ipx>Udx. \n(9.108)\nBecause the Fourier transform of a Gaussian function is another Gaussian function, the ground state \nmomentum space wave function is the Gaussian function in Eq. (9.105). The Fourier transform of an \nHermite polynomial times a Gaussian function is also an Hermite polynomial times a Gaussian func-\ntion, so the excited states are given by Eq. (9.106).\nThus we ﬁnd the interesting result that the momentum space wave functions representing the \nenergy eigenstates have the same functional dependence on momentum as the position representation \nwave functions have on position. This similarity is visible in the momentum space probability density \nfor one particular energy eigenstate shown in Fig. 9.9. In this case, the limits {pn = {212n + 12mvU \nrepresent the limits of the classical momentum for a particle with energy En = 1n + 1>22U v. Note \nthe similarity with Fig. 9.7.\n",
    "298 \nHarmonic Oscillator\n9.7 \u0002 THE UNCERTAINTY PRINCIPLE\nThe Heisenberg uncertainty principle places a lower limit on the product of the uncertainties of posi-\ntion and momentum\n \n\u0006x\u0006p Ú U\n2, \n(9.109)\nwhere the quantum mechanical uncertainties are deﬁned as the standard deviations\n \n \u0006x = 481x - 8x9229 = 48x29 - 8x92\n \n \u0006p = 481p - 8p9229 = 48p29 - 8p92. \n \n(9.110)\nNow that we know the position and momentum probability distributions, these uncertainties are \nstraightforward to calculate by integration or by operator methods. For the ground state of the har-\nmonic oscillator, these uncertainties can be found by inspection because the Gaussian functional form \nof the ground state wave function is a standard probability function.\nThe standard way of writing a Gaussian function for use in probability analysis is\n \nf 1x2 =\n1\n22ps\n e-  1x-x22>2s2. \n(9.111)\nwhere x is the mean or average of the distribution and s is the standard deviation of the distribution. \nFor the harmonic oscillator ground state, the spatial probability density distribution is\n \nP01x2 = 0 w01x2 0\n2 = A\nmv\npU\n e-  mvx 2>U. \n(9.112)\n\npn\npn\np\n\u0002Φn(p)\u00022\nFIGURE 9.9 Momentum space probability density for the n = 30 harmonic \noscillator state.\n",
    "9.7 The Uncertainty Principle \n299\nThis is identical to the standard form in Eq. (9.111). By comparing the quantum mechanical probabil-\nity density in Eq. (9.112) and the standard probability expression in Eq. (9.111), we ﬁnd by inspection \nthat the mean and standard deviation are\n \n x = 0\n \n \n s = B\nU\n2mv.\n \n(9.113)\nThe mean or average is what we call the expectation value 8x9 in quantum mechanics, and the stan-\ndard deviation is the quantum mechanical uncertainty \u0006x. Hence, we have the results for the ground \nstate\n \n 8x9 = 0\n \n \n \u0006x = B\nU\n2mv.\n \n(9.114)\nWe already found that the expectation value 8x9 is zero in Example 9.3, and now we have found the \nuncertainty \u0006x by inspection.\nThe momentum probability density distribution also has a Gaussian form for the harmonic oscil-\nlator ground state\n \nP01p2 = \u0004f01p2 0\n2 = A\n1\npmvU\n e-  p2>mvU. \n(9.115)\nIf we also compare this to the standard Gaussian function, we ﬁnd by inspection that the expectation \nvalue 8p9 and the uncertainty \u0006p are \n \n 8p9 = 0\n \n \u0006p = B\nmvU\n2\n.\n \n(9.116)\nWe expect the expectation value 8p9 to be zero, based upon inspection of the momentum matrix in \nEq. (9.96).\nWe can now check that the uncertainty principle is obeyed. Using the results in Eqs. (9.114) and \n(9.116), we obtain\n \n\u0006x\u0006p = B\nU\n2mvB\nmvU\n2\n= U\n2.  \n(9.117)\nNot only is the uncertainty principle obeyed, but the uncertainty product has its minimum value, so we \nrefer to the harmonic oscillator ground state as a minimum uncertainty state. We found in Chapter 6 \nthat the Gaussian wave packet for a free particle is also a minimum uncertainty state. However, the \nfree particle wave packet evolves with time in a way that causes it to spread out in space, and so it \nis only a minimum uncertainty state at one time. The harmonic oscillator ground state is an energy \neigenstate and so its time evolution produces only a multiplicative overall phase factor, which does not \nchange the probability density in position or momentum space. Hence, the harmonic oscillator ground \nstate remains a minimum uncertainty state for all time. The shape of the harmonic oscillator potential \nenergy well is just right to counter the spreading of the wave packet.\n",
    "300 \nHarmonic Oscillator\n9.8 \u0002 TIME DEPENDENCE\nNow let’s study some examples of time dependence in the harmonic oscillator. These examples dem-\nonstrate the manifestation of the sixth postulate regarding Schrödinger time evolution. They also illus-\ntrate the power of the operator approach for the harmonic oscillator, in contrast with the wave function \napproach. A general state of the system is expressed as a superposition of energy eigenstates\n \n0 c1029 = a\nq\nn=0\ncn0 n9. \n(9.118)\nIn the energy basis, the Schrödinger time evolution recipe tells us that the time dependence is found by \nmultiplying each energy eigenstate coefﬁcient by an energy dependent phase factor, giving:\n \n 0 c1t29 = a\nq\nn=0\ncne-  i En t>U0 n9 \n \n = a\nq\nn=0\ncne-  i 1n+ 1\n22vt0 n9  \n(9.119)\n \n = e-  i vt>2 a\nq\nn=0\ncne-invt0 n9.\n \nThus we see that each successive term acquires an additional relative phase of e-ivt from the \nSchrödinger time evolution.\nExample 9.4 A harmonic oscillator system starts in an equal superposition of the ground state \nand the ﬁrst excited state\n \n0 c1029 =\n1\n12 0 09 +\n1\n12 0 19. \n(9.120)\nFind the probability as a function of time of measuring the system to have energy U v>2, the prob-\nability density as a function of time, and the expectation value of position.\nThe time-evolved state function is found from the Schrödinger recipe:\n \n0 c1t29 = e-  i vt>2 C 1\n12@0I +\n1\n12e-  ivt @1ID, \n(9.121)\nwhere we factor out the common phase because only the relative phase is important. The probability \nof ﬁnding the oscillator in the ground state is\n \nP0 = @80@ c1t29@\n2 = @H0@e-  i vt>2 C 1\n12@0I +\n1\n12 e-  ivt@1ID@\n2\n \n= @e-  i vt>2 1\n12 H0@0I + e-  i vt>2 1\n12 e-  ivt H0@1I@\n2\n \n(9.122)\n \n= @ e-  i vt>2 1\n12 @\n2\n \n \n = 1\n2. \nThis probability is time independent, as is the probability of making any particular measurement of \nthe energy. This is why we refer to energy states as stationary states.\n",
    "9.8 Time Dependence \n301\nThe spatial probability density of this two-state superposition is\n \nP1x, t2 = 08x0 c1t29 0\n2 = 0 c1x, t2 0\n2 = @Hx@e-  i vt>2 C 1\n12@0I +\n1\n12 e-  ivt @1ID@\n2\n \n \n= 1\n2 0 w01x2 + e-  ivtw11x2 0\n2 \n(9.123)\n \n= 1\n23 0 w01x2 0\n2 + 0 w11x2 0\n2 + w01x2w*\n11x2  e+ivt + w*\n01x2w11x2e-ivt4.\nIf the position is measured, then the result is time dependent because the position operator does \nnot commute with the Hamiltonian. For the harmonic oscillator, the wave functions are real and \nEq. (9.123) simpliﬁes to\n \nP1x, t2 = 1\n2\n 3w2\n01x2 + w2\n11x2 + 2w01x2w11x2cos vt4. \n(9.124)\nThis probability density oscillates with time, as depicted in the animation frames shown in \nFig. 9.10, where the constant t is the oscillation period t = 2p/v of the harmonic oscillator (see \nthe activity on time evolution of harmonic oscillator states). The probability distribution of this \nsuperposition sloshes back and forth in the well.\nWe calculate the expectation value of the position using the raising and lowering operators \n[see Eq. (9.99)]\n \n 8xn9 = 8c1t2 0 xn 0 c1t29 = B\nU\n2mv\n 8c1t2 0 a- + a0 c1t29\n \n = B\nU\n2mv\n  C 1\n12 H0@ +\n1\n12 e+ivt H1@D Aa- + aBC 1\n12@0I +\n1\n12 e-ivt@1ID\n \n = 1\n2B\nU\n2mv\n 380@1a- + a2@19e-ivt + 81@1a- + a2@ 09e+ivt4\n \n = 1\n2B\nU\n2mv\n 3180@  a-@ 19 + 80@  a @ 192e-ivt + 181@  a-@ 09 + 81@  a @ 092e+ivt4. \n(9.125)\nSuperposition of n\u00040 and n\u00041 states\nt/Τ \u0004 0.0\nt/Τ \u0004 0.1\nt/Τ \u0004 0.2\nt/Τ \u0004 0.3\nt/Τ \u0004 0.4\nt/Τ \u0004 0.5\nFIGURE 9.10 Time dependence of the spatial probability density for the superposition state com-\nposed of equal probabilities of n \u0003 0 and 1 states. The frames represent half of the oscillation period t.\n",
    "302 \nHarmonic Oscillator\nEach matrix element is found using the ladder operator matrix elements in Eq. (9.93) or by inspec-\ntion of the matrix in Eq. (9.94), yielding\n \n 8xn9 = 1\n2B\nU\n2mv CAH0@2I 22 + H0@0I 21Be -ivt + AH1@1I 21 + 0Be +ivtD \n \n = 1\n2B\nU\n2mv\n 3e -ivt + e +ivt4\n \n(9.126)\n \n = B\nU\n2mv cos vt.\nHence, the expectation value of position oscillates with time, which is evident in the animation \nframes shown in Fig. 9.10. This calculation of matrix elements was simpliﬁed greatly by using the \nladder operators. If we were to use wave functions, then we would need to calculate spatial inte-\ngrals. The moral of the story is: use the ladder operators wherever you can and do not do an integral \nif you don’t have to.\nNow consider a general two-state superposition, such as\n \n0 c1029 = cm0 m9 + cn0 n9. \n(9.127)\nIn this case, the expectation value of x is equal to zero if the states 0 m9 and 0 n9 that comprise the \nsuperposition are not adjacent energy states because the xn matrix [Eq. (9.96)] only connects adjacent \nstates. This means that a measurement of 8x9 can oscillate only at the frequency v, not at 2v, 3v, etc. \nThis result is similar to the classical oscillator where only a single harmonic is observed. Thus it is true \nin both quantum mechanics and classical mechanics that a linear oscillator has no higher harmonics. \nWe need nonlinearity in the restoring force to achieve anharmonicity and to observe other frequencies.\nNote, however, that the probability density does exhibit higher harmonics. For example, the \nprobability density of the state 0 c9 =\n1\n12 1 0 09 + 0 292  oscillates with time at the frequency 2v, but \nit does so in a manner that preserves the zero value of 8x9. As shown in the animation frames in \nFig. 9.11(a), the probability distribution “breathes” symmetrically such that 8x9 = 0. For the state \n0 c9 =\n1\n12 1 0 09 + 0 392, the probability distribution [Fig. 9.11(b)] has two lobes that pass through each \nother at frequency 3v, while preserving 8x9 = 0.\nThe presence of only a single Bohr frequency in the expectation value of the position is related to \nthe selection rule for transitions between energy levels. We know from Chapter 3 that the probability \nfor a system to make a transition is proportional to the matrix element of the interaction between the \ntwo states. Assuming that the bound particle has a charge q, then the relevant electric dipole  interaction \nis governed by the matrix element 8ni0 qxn 0 nf9 of the electric dipole operator dn = qxn. Because the \nmatrix for position connects only adjacent states, the matrix elements are\n \n8ni0 qxn 0 nf9 \f dni, nf {1 \n(9.128)\nand the selection rule for harmonic oscillator transitions is\n \n\u0006n = nf - ni = {1. \n(9.129)\nNow consider measurements of the momentum of a superposition state of the harmonic oscillator. \nThe similarity of the position and momentum operators means that the momentum probability distri-\nbution and the expectation value of momentum8p9show similar results to those for position obtained \n",
    "9.8 Time Dependence \n303\nabove (Problem 9.11). In particular, the expectation values of position and momentum follow Ehren-\nfest’s theorem (see Chapter 6), which tells us that expectation values obey classical laws. The classical \nrelation between position and momentum is p = mv = mdx>dt, so the quantum mechanical superpo-\nsition states obey the relation:\n \n8p9 = m d8x9\ndt . \n(9.130)\nThough the superposition state presented in Fig. 9.10 exhibits the classical behavior of Eq. (9.130), \nthe time evolution does not really “look” classical. Classically, we expect to see a well-localized “par-\nticle” oscillate between the turning points. We saw in Fig. 9.7 that higher energy states exhibit more \nclassical behavior, so we might ask if the time evolution would appear more classical if the states \n0 m9 and 0 n9 that comprise the superposition were higher in energy. An example of this is shown in \nFig. 9.12. The wave is more localized but now exhibits interference fringes that would not be expected \nfor a classical particle. One way to make the time evolution appear classical is to build a superposition \nstate known as a coherent state.\nA coherent state is a wave packet that moves within the quadratic harmonic oscillator potential \nin such a way that it retains its shape, unlike the two-state superpositions in Figs. 9.10, 9.11, and 9.12. \nWave packets in free space distort as they propagate, so this is a new phenomenon. The coherent state \nis an inﬁnite superposition of harmonic oscillator energy eigenstates with a particular choice ofampli-\ntudes and phases (hence the name coherent). The form of these coefﬁcients is not so important for now, \nbut what is interesting is that the wave function of the coherent state is identical to the ground state \nGaussian wave function, except that it is not centered at the origin. As shown in Fig. 9.13(a), this dis-\nplaced Gaussian state oscillates about the origin and does not change its shape (see the activity on time \nevolution of harmonic oscillator states). Because the ground state has a minimum uncertainty product, \nthe coherent states also minimize the uncertainty product and do so even as they move. Figure 9.13(b) \nshows that if we choose a displaced Gaussian wave packet with the wrong width it does not move with-\nout distortion. It remains a Gaussian, but changes it size (it breathes as it moves).\nSuperposition of n=0 and n=2 states\nSuperposition of n=0 and n=3 states\n(a)\n(b)\nt/Τ \u0004 0.0\nt/Τ \u0004 0.0\nt/Τ \u0004 0.0333\nt/Τ \u0004 0.05\nt/Τ \u0004 0.0667\nt/Τ \u0004 0.1\nt/Τ \u0004 0.2\nt/Τ \u0004 0.1\nt/Τ \u0004 0.133\nt/Τ \u0004 0.15\nt/Τ \u0004 0.167\nt/Τ \u0004 0.25\nFIGURE 9.11 Time dependence of the spatial probability density for the superposition states \ncomposed of equal probabilities of (a) the n = 0 and 2 states, and (b) the n = 0 and 3 states.\n",
    "304 \nHarmonic Oscillator\nCoherent State Superposition\nt/Τ\u0007= 0.0\nt/Τ\u0007= 0.1\nt/Τ\u0007= 0.2\nt/Τ\u0007= 0.3\nt/Τ\u0007= 0.4\nt/Τ\u0007= 0.5\nt/Τ\u0007= 0.0\nt/Τ\u0007= 0.1\nt/Τ\u0007= 0.2\nt/Τ\u0007= 0.3\nt/Τ\u0007= 0.4\nt/Τ\u0007= 0.5\nDisplaced Gaussian Superposition\n(a)\n(b)\nFIGURE 9.13 Time dependence of the spatial probability density \n(a) for a coherent state and (b) for a displaced Gaussian state that is  \nnot the ground state.\nSuperposition of n=19 and n=20 states\nt/Τ \u0004 0.0\nt/Τ \u0004 0.1\nt/Τ \u0004 0.2\nt/Τ \u0004 0.3\nt/Τ \u0004 0.4\nt/Τ \u0004 0.5\nFIGURE 9.12 Time dependence of the spatial probability density \nfor the superposition state composed of equal probabilities of the  \nn \u0003 19 and 20 states.\n",
    "9.9 Molecular Vibrations \n305\n9.9 \u0002 MOLECULAR VIBRATIONS\nOne of the most common applications of the quantum mechanical harmonic oscillator is found in the \nvibrations of the nuclei of molecules. In a diatomic molecule, the Coulomb attraction between the nuclei \nand the electrons is balanced by the Coulomb repulsion between the nuclei in a way that results in the \npotential energy diagram shown in Fig. 9.14. This diagram shows the Coulomb potential energy of a \ndiatomic molecule as a function of the nuclear separation for a given electron conﬁguration (in this case, \nthe ground state). The minimum of the potential energy -De occurs at the bond length R0 of the diatomic \nmolecule, and the zero of potential energy represents the separation of the two atoms to inﬁnite separa-\ntion, (i.e., the dissociation of the molecule). This potential energy curve determines the motion of the \nnuclei with respect to each other. Because this curve resembles a parabola near the minimum energy, the \nmotion of the nuclei resembles the motion of a quantum mechanical harmonic oscillator.\nThe harmonic oscillator potential energy that approximates the molecular potential energy is\n \nVHO1R2 =  -De + 1\n2 mv21R - R02\n2, \n(9.131)\nwhere m is the reduced mass of the two nuclei. However, the molecular potential energy curve resem-\nbles a parabola only near the minimum, as shown in Fig. 9.15. As the energy level approaches the \ndissociation limit, the difference between the parabolic harmonic oscillator potential and the true \nmolecular potential becomes quite dramatic. A better approximation to the molecular potential is \ngiven by the Morse potential\n \nVM1R2 = De1e -2a1R-R02 - 2e -a1R-R022, \n(9.132)\nwhere the constant a is\n \na = vA\nm\n2De\n. \n(9.133)\nR0\nR\n0\n−De\nV(R)\nFIGURE 9.14 Potential energy of a diatomic molecule as a function of the nuclear separation.\n",
    "306 \nHarmonic Oscillator\nThe energy levels shown in Fig. 9.15 are the solutions to the motion of the nuclei in the Morse poten-\ntial and show a marked deviation from the levels of an ideal harmonic oscillator. The Morse energy \nlevels become closer together near the top of the well, in contrast to the uniform spacing of the har-\nmonic oscillator. This difference has a clear signature in the spectra of molecular vibrations. An ideal \nharmonic oscillator has transitions only between adjacent energy levels 1\u0006n = {1 selection rule from \nSection 9.8), and all those possible transitions have the same energy difference U v. The transitions in a \nMorse oscillator exhibit a progression from the n = 0 4 1 transition at energy U v to smaller energies \nas we progress up the potential well. In addition, a Morse oscillator has allowed transitions between \nnonadjacent states 1\u0006n = {2, 3, ...2 at higher energies near to multiples of the harmonic energy U v. \nThe \u0006n = {1 selection rule is not obeyed because of the anharmonicity of the well, though these tran-\nsitions are typically weaker than the \u0006n = {1 transitions.\nThe spectra observed in molecules are further complicated by the rotation of the molecule that we \nstudied in Chapter 7, and by transitions between different electronic levels, similar to the transitions in \nthe hydrogen atom we studied in Chapter 8. The transitions due to changes in electronic, vibrational, \nand rotational levels are each characterized by a different energy scale. Electronic transitions are typi-\ncally in the 1–5 eV range, vibrational transitions are typically 500-5000 cm\u00111 (0.06–0.6 eV ), and \nrotational transitions are typically 0.2–60 cm\u00111 (0.02–7 meV ). Thus rotational transitions represent \nﬁner structure compared to vibrational transitions, and vibrational transitions represent ﬁner structure \ncompared to electronic transitions. A schematic of these different energy scales is shown in Fig. 9.16.\nWe discussed the hydrogen chloride molecule in Chapter 7 and noted that the rotational spec-\ntrum was affected by the vibrational motion. We can now explain this using Fig. 9.15. As a mol-\necule vibrates, it occupies higher lying vibrational levels within the potential energy well shown in \nFig. 9.15. Because the Morse potential is asymmetric, the average value of the nuclear separation \n(the “bond length”) deviates from the equilibrium value R0, with the deviation growing as the energy \nincreases. The deviation is always positive in the Morse potential, which implies that the moment of \ninertia of the diatomic molecule I = mR2 increases and the rotational constant U2>2I decreases. This \nnegative shift of the rotational constant explains the discrepancy between the calculated and observed \n spectra that we noted in Chapter 7. Note that this rotation-vibration coupling is present even in the \nn = 0 vibrational ground state, where one might be tempted to think that the molecule is not vibrat-\ning. This effect is another example of the effect of the zero-point energy of the harmonic oscillator.\nR0\nR\n0\n−De\nE\nn = 2\nn = 1\nn = 0\nn =\u00073\nFIGURE 9.15 Vibrational energy states of the nuclear motion in the molecular potential. The dashed \nline is the approximate harmonic potential and the solid line is the more accurate Morse potential.\n",
    "Summary \n307\nSUMMARY\nWe solved the quantum mechanical harmonic oscillator problem using an operator approach. We \ndeﬁned the lowering and raising operators\n \na = A\nmv\n2U\n axn + i pn\nmvb \n(9.134)\nand\n \na- = A\nmv\n2U\n axn - i pn\nmvb , \n(9.135)\nrespectively. Using these operators, we expressed the Hamiltonian as\n \nH = pn  2\n2m + 1\n2mv2xn  2 = U v1a-a + 1\n22. \n(9.136)\nWe solved the energy eigenvalue problem to ﬁnd\n \nEn = U v1n + 1\n22,  n = 0, 1, 2, 3, ... . \n(9.137)\nWe used the quantum mechanical harmonic oscillator to review the fundamental ideas of quan-\ntum mechanics. Table 9.1 summarizes the manifestations of the quantum mechanical postulates in the \ndifferent systems we have studied to this point.\nR\nE\nelectronic transition\nro-vibrational transition\nrotational transition\n FIGURE 9.16 Transitions in a diatomic molecule.\n",
    "308 \nHarmonic Oscillator\nPROBLEMS\n 9.1 Show that the product of a and a-, in that order, is given by Eq. (9.21).\n 9.2 Show that the action of a- on an eigenstate of H produces an eigenstate with the eigenvalue \nraised by one quantum.\n 9.3 Normalize the wave function e-mvx2>2U to get the correct ground state of the harmonic  oscillator, \nas given in Eq. (9.49).\n 9.4 Calculate the probability that a particle in the ground state of the harmonic oscillator is found \nin the classically forbidden region.\n 9.5 Show that the proper scale factor of the raising operation yields Eq. (9.56): \na-0 n9 = 1n + 10 n + 19.\n 9.6 Show that the raising and lowering operators would commute with each other if their action \non energy eigenstates preserved normalization. That is, assume a0 n9 = 0 n - 19 and \na-0 n9 = 0 n + 19, and use that information to show that a and a- commute.\n 9.7 Show by direct integration that the ground and ﬁrst excited states of the harmonic oscillator are \northogonal to each other.\n 9.8 Show that the spatial probability density of a classical harmonic oscillator is\n \nP1x2 =\n1\np2x2\n0 - x2 ,\n \n where x0 is the classical turning point (see Fig. 9.7).\n 9.9 a)  For the ground state of the harmonic oscillator, calculate 8x9, 8p9, 8x29, and 8p29 by \nexplicit spatial integration.\nb)  Calculate 8x9, 8p9, 8x29, and 8p29 for all the energy eigenstates 0 n9 of the harmonic \noscillator without doing integration (i.e., use the operators a and a†).\nc)  Check that the uncertainty principle is obeyed in both the above cases.\nTable 9.1 Manifestations of Quantum Mechanical Postulates\nPostulates\nSpin 1/2\nHydrogen atom\nHarmonic Oscillator\n1)  State deﬁned by \nket\n\u0004 +9, \u0004 -9\ncnlm1r,u,f2 = R nl 1r2Y m\nl 1u,f2\n\u0004 n9, wn1x2, fn 1p2\n2)  Observables as \noperators\nSz, S2, H\nH, L2, Lz\nH, xn, pn\n3)  Measure eigen-\nvalues\nSz = {U\n /2\nEn= -Z2R\nn2\nL2 = / (/ + 1)U2, Lz = mU\nEn = U v1n + 1\n22\n4)  Probability  \n(density)\n\u00048+ \u0004 c9\u0004\n2\n\u00048nlm \u0004 c9\u0004\n2,     \u0004 cnlm1r, u, f2 \u0004\n2\n\u00048n \u0004 c9\u0004\n2,    \u0004wn1x2\u0004\n2\n5) State reduction\n\u0004 c9 S \u0004 +9\n\u0004 c9 S \u0004 nlm9,     \u0004 c9 S \u0004 En9\n\u0004 c9 S \u0004 n9\n6)  Schrödinger time \nevolution\nLarmor  \nprecession\nDipole oscillation\nSuperposition oscillation\n",
    "Problems \n309\n 9.10 Discuss and show explicitly how Eq. (9.93) is used to ﬁnd the matrix representations of the \nladder operators.\n 9.11 A particle in the harmonic oscillator potential has the initial state\n \n0 c 1t = 029 = A3@ 09 + 2eip/2@ 194.\na) Find the normalization constant A.\nb) Find the time-evolved state 0 c(t)9.\nc) Calculate 8x9 and 8p9 as functions of time and verify that Ehrenfest’s theorem\n[Eq. (9.130)] is obeyed.\n 9.12 A particle is in the ground state of the harmonic oscillator potential V11x2 = 1\n2 mv2\n1 x2  when the \npotential suddenly changes to V21x2 = 1\n2 mv2\n2 x2  without initially changing the wave function.\na) What is the probability that a measurement of the particle energy yields the result 1\n2 U v2?\nb) Evaluate the result in (a) for the case v2 = 1.7v1.\n 9.13 Find the allowed energy levels of a particle of mass m moving in the one-dimensional potential \nenergy well\n \nV1x2 = e\n1\n2 mv2x2,     x 6 0\n  \u0005  ,     x 7 0.\n \n (Hint: The answer requires a qualitative argument rather than a calculation.)\n 9.14 A particle in the harmonic oscillator potential has the initial state\n \nc1x, 02 = A c 1 - 3A\nmv\nU\n x + 2 mv\nU\n x2 d e-mvx2>2U\n \n where A is the normalization constant.\na) Calculate the expectation value of the energy.\nb) At a later time T, the wave function is\n \nc1x, T2 = Bc 3 - 3iA\nmv\nU\n x - 2 mv\nU\n x2 d e-mvx2>2U\n \n for some constant B. What is the smallest possible value of T ?\n 9.15 A measurement of the energy of a harmonic oscillator system yields the results U v>2 and \n3U v>2 with equal probability. A measurement of the position (actually measurements on an \nensemble of identically prepared systems) yields the result 8x9 = - 1U>2mv sin vt. Calculate \nthe expectation value of the momentum.\n 9.16 A particle is in the harmonic oscillator potential V1x2 = 1\n2 mv2x2 and the energy is  measured. \n \n The probability that the energy measurement yields 3\n2 U v is 36% and the probability that \nthe energy measurement yields 5\n2 U v is 64%. The expectation value of the position 8x9 is a \n minimum at time t \u0003 0.\na) Find the time-dependent wave function.\nb) Calculate the expectation value 8p9 of the momentum for this particle, as a function of time.\nc) Calculate the expectation value 8E9 of the energy.\n",
    "310 \nHarmonic Oscillator\n 9.17 A particle in the harmonic oscillator potential V1x2 = 1\n2 mv2x2 starts out in the state\n \nc1x, 02 = A3w01x2 + 2w11x2 + 2w21x24,\n \n where wn1x2 are the normalized eigenfunctions of the Hamiltonian.\na) If you measure the energy of this particle, what values might you get, and with what \n probabilities?\nb) Calculate the expectation value 8p9 of the momentum for this particle, as a function of time.\nc) What is the expectation value 8E9 of the energy?\nd) What is the standard deviation \u0006E of the energy?\n 9.18 Solve the differential equation (9.104) and show that the properly normalized momentum \nspace wave function of the ground state of the harmonic oscillator is given by Eq. (9.105).\n 9.19 Find the momentum representation of the ground state of the harmonic oscillator using the \nFourier transform in Eq. (9.108).\n 9.20 Use your favorite software package to study the coherent states of the harmonic oscillator. \nAssume that the system has the initial wave function\n \nc1x, 02 = w01x - x02,\n \n where x0 is a constant representing the displacement of the Gaussian ground state waveform \nfrom the origin.\na) Plot the wave function, choosing x0 = 1U>mv\nb) Calculate the overlap integrals in cn = 8n0 c1029 necessary to express the initial wave \nfunction in terms of energy eigenstates. Do this for the ﬁrst 10 energy levels. Compare your \nresults to the expression\n \ncn =\nan\n2n!\n e-  a2>2,\n  \nwhere a = x01mv>2U. Check whether the 10 terms in the expansion are enough to prop-\nerly represent the wave function. Explain.\nc) Calculate the expectation value of the energy.\nd) Construct the time-dependent wave function. Animate the wave function and describe its \ntime evolution.\ne) Repeat the above for x0 = 41U>mv. You may need more that 10 terms!\n 9.21 Show that the Morse potential reduces to a parabolic potential for small displacements from \nthe equilibrium bond length. Find the cubic correction term to the harmonic potential that is \nincluded in the Morse potential.\n 9.22 Imagine a quantum system with an energy spectrum En = n3 U v for n = 1, 2, 3, ... . By \ninspection, write down the matrix representation of the Hamiltonian and the energy eigen-\nstates. Write down the matrix representing the operator A in this system that is deﬁned by \nA0 n9 = 3n20 n + 29.\n",
    "Resources \n311\nRESOURCES\nActivities\nThese activities are available at\nwww.physics.oregonstate.edu/qmactivities\nHarmonic Oscillator Basis States: Students express the normalization, orthogonality, and complete-\nness conditions for harmonic oscillator states in Dirac notation and in wave function notation.\nTime Evolution of Harmonic Oscillator States: Students animate wave functions consisting of lin-\near combinations of eigenstates.\nFurther Reading\nMore details on treating light as a harmonic oscillator and coherent states of light can be found in \nthese texts:\nMark Fox, Quantum Optics: An Introduction, Oxford: Oxford University Press, 2006.\nChristopher Gerry and Peter Knight, Introductory Quantum Optics, Cambridge: Cambridge \nUniversity Press, 2005.\nRodney Loudon, Quantum Theory of Light, Oxford: Oxford University Press, 2000.\n",
    "C H A P T E R \n10\nPerturbation Theory\nThe quantum mechanics you have studied so far has entailed solving a few carefully chosen prob-\nlems exactly. Unfortunately, those problems represent a small fraction of the realistic problems that \nnature presents to us, and in most cases those exactly solvable problems are only approximations to \nreal problems. Now we must learn to solve more realistic problems that do not admit exact solutions. \nThe approach we take to solving these realistic problems is to make them look like problems we have \nalready solved exactly, with an additional part that represents the new, more realistic aspect of the \nproblem. We assume that this new part, the perturbation, is small so that we can use approximations \nto ﬁnd the corrections to the exact solutions. Our focus is to discover how energies and eigenstates are \naffected by small additional terms in the Hamiltonian. To guide us, we will take some exactly solv-\nable problems, solve them, and then expand the solutions. We will compare these results with the new \nperturbation methods that we learn.\nWe had a sneak peek at how a perturbation affects a system in Chapter 5 when we studied the \nasymmetric square well. We saw that an additional potential energy “shelf” in the inﬁnite square well \nchanged the energy levels, as shown in Fig. 10.1. For small values V0 of the potential energy shelf, \nthe energy of the ground state is shifted by an amount that is linearly dependent on V0, but as the \nperturbation increases, the energy begins to change quadratically. This linear-to-quadratic behavior \nis a common feature of perturbation theory and will be evident as we proceed. Our goal is to produce \nplots like Fig. 10.1(b) that demonstrate how energy levels shift when a perturbation is applied to a \nsystem.\n0\nL/2\nL\nx\nV0\nV(x)\n(a)\n(b)\n\u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003\n0\n0.5\n1.0\nV0/E1\n0\n0.2\n0.4\n\u0012E/E1\n\r\nFIGURE 10.1 (a) Asymmetric square well and (b) energy shift of the ground state \nas a function of the perturbation V0, where the points are from the exact calculation in \nSection 5.9, and the straight line is from the perturbation calculation in Example 10.2.\n",
    "10.1  Spin-1/2 Example \n313\n10.1 \u0002 SPIN-1/2 EXAMPLE\nTo get a feel for what perturbation theory is and how it works, let’s go back to our old standby—the \nspin-1/2 problem. The usual Hamiltonian of a spin-1/2 system is the potential energy of the spin mag-\nnetic moment in an applied magnetic ﬁeld. For an applied magnetic ﬁeld in the z-direction B = B0zn , \nthe Hamiltonian is\n \nH0 = -M~B = v0Sz \u0003 U\n2\n ¢v0\n0\n0\n-v0 ≤, \n(10.1)\nwhere we have deﬁned the Larmor frequency v0 = eB0>me as we did in Chapter 3. The subscript zero \non the Hamiltonian in Eq. (10.1) indicates that this is the zeroth-order Hamiltonian (i.e., the Hamil-\ntonian before we apply a perturbation). The energy eigenstates of the zeroth-order Hamiltonian are the \nspin up and down states 0 {9 and the energy eigenvalues are\n \nE  (0)\n{ = {U v0\n2 , \n(10.2)\nwhere the superscript zero on the energy (not an exponent) denotes the order of the solution. The \nenergy spectrum of the zeroth-order energy eigenstates is shown in Fig. 10.2.\nThe goal of perturbation theory is to ﬁnd the higher-order corrections to the energy eigenvalues \nand eigenstates caused by the application of a perturbation to the system. For this spin-1/2 system, \nwe will solve the problem exactly and then expand the solutions to discover how perturbation series \nbehave. Our exact solution should contain the zeroth-order solutions we already know [Eq. (10.2)] and \nsmall corrections.\nThe simplest way to perturb this spin system is to change the magnetic ﬁeld. Any general change \nto the magnetic ﬁeld can be decomposed into an additional component along the original ﬁeld in the \nz-direction, and an additional component perpendicular to that, as shown in Fig. 10.3(a). We write \nthe new total ﬁeld as B = B0\n zn + B1zn + B2xn and characterize the two additional ﬁeld components \nby their respective Larmor frequencies v1 = eB1>me and v 2 = eB2>me. With this notation, the new \nHamiltonian is\n \nH = -M~B = v0 Sz + v1Sz + v2 Sx \u0003 U\n2\n av0+v1\nv2\nv2\n-v0 -v1\nb.  \n(10.3)\n\n0.50\n\n0.25\n0.00\n0.25\n0.50\nE/\u0002Ω0\n\u0002\u000f\u0003\n\u0002Ω0\nE\u000f\u0004 \u0002Ω0\n2\n\nE\n\u0004\n\u0002Ω0\n2\n\u0002\n\u0003\nFIGURE 10.2 Energy levels of a spin-1/2 particle in a uniform magnetic ﬁeld.\n",
    "314 \nPerturbation Theory\nIt is useful to separate the new Hamiltonian into the zeroth-order Hamiltonian H0 and the perturbation \nHamiltonian that we denote H\u0004:\n \nH = H0 + H\u0004. \n(10.4)\nThe zeroth-order Hamiltonian is given by Eq. (10.1) and the perturbation Hamiltonian is\n \nH\u0004 \u0003 U\n2\n ¢v1\nv2\nv2\n-v1\n≤. \n(10.5)\nThe perturbation Hamiltonian has terms along the diagonal and terms off the diagonal. These diagonal \nand off-diagonal terms play important roles in perturbation theory.\nWe now solve for the energy eigenvalues and eigenstates of the new Hamiltonian in Eq. (10.3) \nexactly by diagonalizing the matrix. But we have already done this in Chapter 2 for the general spin-1/2 \ncase of a magnetic ﬁeld at an angle \u0007 to the z-axis. We found there that the Hamiltonian is proportional \nto the spin component Sn along the new magnetic ﬁeld direction nn, and can be expressed in terms of \nthe angle \u0007 of the new ﬁeld as\n \n H = vnew Sn \u0003 U vnew\n2\n ¢cos u\nsin u\nsin u\n-cos u≤,  \n(10.6)\nwhere\n \ntan u =\nv2\nv0 + v1\n, \n(10.7)\nas shown in Fig. 10.3 (b). The new Larmor frequency vnew obeys the Pythagorean equation\n \nvnew = 4(v0 + v1)2 + v2\n2 \n(10.8)\ncorresponding to the total ﬁeld, as suggested by Fig. 10.3(b). The eigenstates of this new Hamiltonian \nare the spin states 0 {9n aligned along or against the new ﬁeld and the eigenenergies are (Problem 10.1)\n \nEnew = { U\n2\n vnew = { U\n2\n 4(v0 + v1)2 + v2\n2. \n(10.9)\nThis is the exact solution, which we now expand in a power series.\nB2\nB1\nB0\nBnew\nΩnew\nΘ\nΩ2\nΩ1\nΩ0 Θ\n(a)\n(b)\nFIGURE 10.3 (a) Perturbing magnetic ﬁelds and (b) the resultant Larmor frequencies.\n",
    "10.1  Spin-1/2 Example \n315\nThe basic idea of perturbation theory is to assume that the new terms in the Hamiltonian are \nsmall compared to the original Hamiltonian, (i.e., the perturbation Hamiltonian H\u0004 is much smaller \nthan the zeroth-order Hamiltonian H0). In this spin-1/2 example, that would imply that the added \nﬁelds B1 and B2 are small compared to the original ﬁeld B0, which means that the new Larmor fre-\nquencies v1 and v2 are small compared to the original Larmor frequency v0. Hence, we treat the \nratios v1>v0 and v2>v0 of the new to old Larmor frequencies as small dimensionless parameters and \nrewrite the energy in Eq. (10.9) as\n \n Enew = {U v0\n2\n B a1 + v1\nv0\nb\n2\n+\nv2\n2\nv2\n0\n \n \n = {U v0\n2\n B\n1 + 2v1\nv0\n+\nv2\n1\nv2\n0\n+\nv2\n2\nv2\n0\n. \n(10.10)\nSo far this is still the exact solution. Now we expand the exact energy to second order in a power \nseries in the small parameters v1>v0 and v2>v0, so as to reach some general conclusions about per-\nturbation theory:\n \n Enew = {U v0\n2\n c1 + 2v1\nv0\n+\nv2\n1\nv2\n0\n+\nv2\n2\nv2\n0\nd\n1>2\n \n \n = {U v0\n2\n c1 + v1\nv0\n+\nv2\n1\n2v2\n0\n+\nv2\n2\n2v2\n0\n- 1\n8\n a2v1\nv0\n+\nv2\n1\nv2\n0\n+\nv2\n2\nv2\n0\nb\n2\n+ ...d \n(10.11)\n \n = {U v0\n2\n c1 + v1\nv0\n+\nv2\n1\n2v2\n0\n+\nv2\n2\n2v2\n0\n-\nv2\n1\n2v2\n0\n+ ...d\n \n \n \u0002 {U v0\n2\n c1 + v1\nv0\n+\nv2\n2\n2v2\n0\nd .\n \nWe now conclude that the two energies of the perturbed system, to second order in the small quantities \ncharacterizing the perturbation, are\n \nE + \u0002 + U v0\n2\n+ U v1\n2\n+\nU v2\n2\n4v0\n \n \nE - \u0002 -  U v0\n2\n- U v1\n2\n-\nU v2\n2\n4v0\n. \n(10.12)\nIn both energies, we identify the ﬁrst term as the zeroth-order energy E (0)\n{  given by Eq. (10.2), and \nwe note two additional terms. The ﬁrst is linear or ﬁrst order in the perturbation and is equal to the \ncorresponding diagonal term {U v1>2 in the perturbation Hamiltonian [Eq. (10.5)]. The second addi-\ntional term is quadratic or second order in the perturbation and is proportional to the square of the \noff-diagonal term U v2>2 in the perturbation Hamiltonian. This general pattern of corrections is char-\nacteristic of perturbation theory, so we denote perturbed energies as the series\n \nEn = E (0)\nn\n+ E (1)\nn\n+ E (2)\nn\n+ ..., \n(10.13)\nwhere the superscript indicates the order of the perturbation. We found in this spin-1/2 example that \nthe linear corrections arose from the diagonal terms in the perturbation Hamiltonian and the quadratic \n",
    "316 \nPerturbation Theory\nterms arose from the off-diagonal terms, another characteristic pattern of general perturbation theory. \nIn Eq. (10.12), the second-order energy correction due to the off-diagonal terms has a factor of v0 in \nthe denominator, and it will diverge if the energy splitting U v0 is zero, (i.e., if the original levels are \ndegenerate in energy). This divergence violates the assumption that the perturbation corrections are \nsmall, which creates a problem that we will address in Section 10.5.\nIn addition to these features of the perturbed energies, we can also draw some conclusions about \nthe perturbation corrections to the eigenstates from our knowledge of the exact eigenstate solutions. \nThe eigenstates of the full Hamiltonian in Eq. (10.6) are the spin up and down eigenstates 0 {9n along \nthe direction nn :\n \n 0  +9n =  cos u\n2\n 0  +9 + sin u\n2\n 0  -9\n \n \n 0  -9n = -sin u\n2\n 0  +9 + cos u\n2\n 0  -9. \n(10.14)\nFrom Fig. 10.3(a), it is evident that the angle \u0007 is small for small perturbing magnetic ﬁelds, so we can \nalso use \u0007 as a small parameter for a series expansion:\n \n 0  +9n \u0002 a1 - u2\n8 b 0  +9 + u\n2\n 0  -9 = 0  +9 + u\n2\n 0  -9 - u2\n8\n 0  +9\n \n \n 0  -9n \u0002 - u\n2\n 0  +9 + a1 - u2\n8 b 0  -9 = 0  -9 - u\n2\n 0  +9 - u2\n8\n 0  -9. \n(10.15)\nTo second order in the angle \u0007, the new eigenstates have two correction terms: a ﬁrst-order term that is \northogonal to the original state, and a second-order term that is parallel (in a Hilbert space sense, not a \ngeometric sense) to the original state. If we neglect the parallel terms (we’ll see in Section 10.3.2 why \nwe do this), we get:\n \n 0  +9n \u0002 0  +9 + u\n2\n 0  -9  \n \n 0  -9n \u0002 0  -9 - u\n2\n 0  +9. \n(10.16)\nUsing the schematic in Fig. 10.3(b), we express the small angle u in terms of the Larmor frequencies. \nTo ﬁrst order [consistent with neglecting the second-order parallel terms in Eq. (10.15)], we obtain\n \nu \u0002\nv 2\n4(v0 + v1)2 + v2\n2\n\u0002 v 2\nv0\n. \n(10.17)\nThus, we arrive at the perturbation series expansion for the perturbed states, to ﬁrst order\n \n0  +9n \u0002 0  +9 + v2\n2v0\n 0  -9  \n \n0  -9n \u0002 0  -9 - v2\n2v0\n 0  +9. \n(10.18)\nWe conclude that the ﬁrst-order eigenstate correction depends only on the off-diagonal matrix ele-\nment, not on the diagonal elements. Note that the coefﬁcient of the original state remains one, which \n",
    "10.2 General Two-Level Example \n317\nmakes it appear that the state is no longer normalized. But if we check the normalization of the per-\nturbed state:\n \n n8  + @  +  9n = c8 + @ + v2\n2v0\n 8 -  @ d c @   +  9 + v2\n2v0\n @  -  9d  \n \n = 1 + a v2\n2v0\nb\n2\n \n(10.19)\n \n \u0002 1,\n \nwe see that it is normalized to ﬁrst order in the small perturbation parameters.\n 10.2 \u0002 GENERAL TWO-LEVEL EXAMPLE\nContinuing our introduction to perturbation theory, we consider a general two-level system that we \nsolve exactly to learn how the solutions depend on the perturbation and to practice the notation we will \nuse later. This example repeats the calculation of Section 10.1 with general notation rather than con-\nsidering a speciﬁc physical system. At each step in this section, refer back to Section 10.1 to identify \nwhat each term is in the speciﬁc case of a perturbing magnetic ﬁeld applied to a spin-1/2 system.\nIn the general two-level case, we assume a zeroth-order Hamiltonian of the form\n \nH0 \u0003 ¢E  (0)\n1\n0\n0\nE  (0)\n2\n≤ \n(10.20)\nand a general perturbation\n \nH\u0004 \u0003 ¢H =\n11\nH =\n12\nH =\n21\nH =\n22\n≤, \n(10.21)\nwhere the matrix elements of the perturbation Hamiltonian are written as\n \nH =\nij = 8i (0)0\n H\u00040  j (0)9. \n(10.22)\nNote that we use the energy eigenvectors of the zeroth-order Hamiltonian as the basis vectors for \nmatrix representation of the operators. It is useful to parameterize the strength of the perturbation \nwith a dimensionless quantity l that allows us to keep track of the order of the perturbation in a power \nseries solution (note that this l is not the l we often use as a placeholder when ﬁnding eigenvalues). In \nthe end we will set l equal to one, so it is used solely to keep track of the different orders in the power \nseries. Using this parameter, the full Hamiltonian is\n \nH = H0 + lH\u0004 \u0003 ¢E (0)\n1\n+ lH =\n11\nlH =\n12\nlH =\n21\nE (0)\n2\n+ lH =\n22\n≤. \n(10.23)\nNow let’s ﬁnd the exact eigenvalues of this Hamiltonian. To reduce the clutter in the algebra, \nredeﬁne the matrix values:\n \n¢E (0)\n1\n+ lH =\n11\nlH =\n12\nlH =\n21\nE (0)\n2\n+ lH =\n22\n≤K ¢ a\nc\nc*\nb≤, \n(10.24)\n",
    "318 \nPerturbation Theory\nnoting that because H\u0004 is Hermitian, H =\n12 = H =\n21\n*. Using the symbol E for the energy eigenvalue, we \ndiagonalize the Hamiltonian by ﬁnding the characteristic equation:\n \n ` a - E\nc\nc*\nb - E ` = 0 \n \n (a - E )(b - E ) - 0 c0\n2 = 0 \n(10.25)\n \n E 2 - E (a + b) + a b - 0 c0\n2 = 0 \nand solving to obtain\n \n E = 1\n2 (a + b){4\n1\n4 (a + b)\n2 - a b + 0 c0\n2 \n \n = 1\n2 (a + b){4\n1\n4 (a - b)2 + 0 c0\n2.\n \n(10.26)\nWe are considering the case where the perturbation is small  [i.e., c V (a - b)], so we factor and use \nthe binomial expansion:\n \n E = 1\n2 (a + b){1\n2 (a - b)c1 +\n40 c0\n2\n(a - b)2d\n1\n2\n \n \n \u0002 1\n2 (a + b){1\n2 (a - b)c1 +\n20 c0\n2\n(a - b)2d . \n(10.27)\nThis yields the two energies\n \n E1 \u0002 a +\n0 c0\n2\n(a - b)   \n \n E2 \u0002 b -\n0 c0\n2\n(a - b) . \n(10.28)\nNow rewrite these solutions in terms of the original parameters\n \n E1 = E (0)\n1\n+ lH =\n11 +\nl20 H =\n12 0\n2\n1E (0)\n1\n+ lH =\n11 - E (0)\n2\n- lH =\n222\n \n \n E2 = E (0)\n2\n+ lH =\n22 -\nl20 H =\n12 0\n2\n1E (0)\n1\n+ lH =\n11 - E (0)\n2\n- lH =\n222\n \n(10.29)\nand expand Eq. (10.29) to second order in the expansion parameter l :\n \n E1 \u0002 E (0)\n1\n+ lH =\n11 +\nl20 H =\n12 0\n2\n1E (0)\n1\n- E (0)\n2 2\n \n \n E2 \u0002 E (0)\n2\n+ lH =\n22 +\nl20 H =\n21 0\n2\n1E (0)\n2\n- E (0)\n1 2\n, \n(10.30)\n",
    "10.3 Nondegenerate Perturbation Theory \n319\nwhere we have written the results in a way to make it clear that the two energies have the same form \n(they are equivalent if we swap indices 1 4 2). We have left the expansion parameter in for now to \nmake the order of the expansion clear, but imagine it set to unity, as we will do later. The general \nconclusion is that an energy level En has a ﬁrst-order correction that is the matrix element H =\nnn of the \nperturbation in the state in question and a second-order correction that depends on the square of the \ncoupling H =\nnk to other states and inversely on the energy difference E (0)\nn\n- E (0)\nk  between states. This is \nthe same form that we saw in the spin example above and also what we will see as we develop pertur-\nbation theory in general. Note again that degeneracy of the two states (E (0)\nn\n- E (0)\nk\n= 0) creates prob-\nlems. For this reason we will study nondegenerate and degenerate perturbation theories separately.\n 10.3 \u0002 NONDEGENERATE PERTURBATION THEORY\nIn the examples above, we solved the problems exactly, even the “perturbed problems,” to ﬁnd the \nnew energy eigenvalues and eigenstates, and then we approximated these exact solutions to draw \nsome conclusions about the general behavior of perturbed energies and states. Now we tackle per-\nturbed problems that are not exactly solvable, but we assume that the nonperturbed part of the problem \nis exactly solvable. The exactly solvable part of the problem is called the zeroth-order problem and has \nan energy eigenvalue equation\n \nH0 @  n(0)9 = E (0)\nn @  n(0)9, \n(10.31)\nwhere we use a subscript on the energy to denote the quantum number and superscripts (not powers) \non the energy and eigenstates to denote the order of the solution. Now suppose that this system is per-\nturbed by the addition of a new term in the Hamiltonian that we call H\u0004. The new perturbed problem \nhas an energy eigenvalue equation\n \n(H0 + H\u0004) 0  n 9 = En0  n 9, \n(10.32)\nwhere En are the new energies and 0  n9 are the new eigenstates that we seek. As discussed in the pre-\nvious section, we parameterize the strength of the perturbation with a dimensionless quantity l and \nrewrite the energy eigenvalue equation as\n \n(H0 + lH\u0004) 0  n9 = En0  n9. \n(10.33)\nThe l parameter allows us to keep track of the order of the perturbation in a power series solution. In \nthe end we set it equal to one, so it is here solely to keep track of the different orders in the power series.\nThe essence of the perturbation technique is to assume that we can write the new eigenvalues and \neigenstates as power series expansions with ever-decreasing terms such that the series converge. There \nare some important examples where this does not work (e.g., superconductivity and quantum chromo-\ndynamics), but it does work in many cases. We use the dimensionless parameter l as the expansion \nparameter and write the desired eigenvalues and eigenstates as\n \n En = E (0)\nn\n+ lE (1)\nn\n+ l2E (2)\nn\n+ l3E (3)\nn\n+ ...  \n \n 0  n 9 = @  n(0)9 + l @  n(1)9 + l2 @  n(2)9 + l3 @  n(3)9 + ... . \n(10.34)\nTo ﬁnd the new solutions, substitute these series into the eigenvalue equation Eq. (10.33) and collect \nterms of the same order or power of the parameter l on each side of the equation. For the eigenvalue \nequation to hold for any value of l, the coefﬁcients of like orders on the two sides of the equation must \n",
    "320 \nPerturbation Theory\nbe equal, and we can isolate an equation for each order in the expansion parameter. The result is the \nfollowing set of equations: (Problem 10.2)\n \n\u00141l02:  1H0 - E (0)\nn 2@  n(0)9 = 0\n \n(10.35)\n \n\u00141l12:  1H0 - E (0)\nn 2@  n(1)9 = 1E (1)\nn\n- H\u00042@  n(0)9\n \n(10.36)\n \n\u00141l22:  1H0 - E (0)\nn 2@  n(2)9 = 1E (1)\nn\n- H\u00042@  n(1)9 + E (2)\nn @  n(0)9\n \n(10.37)\n \n\u00141l32:  1H0 - E (0)\nn 2@  n(3)9 = 1E (1)\nn\n- H\u00042@  n(2)9 + E (2)\nn @  n(1)9 + E (3)\nn @  n(0)9 \n(10.38)\nand so on. At this point, the parameter l has done its work and is not needed any more.\nEquation (10.35) is zeroth order in the expansion parameter and is simply the original eigenvalue \nequation [Eq. (10.31)]. That’s why it’s called the zeroth-order equation. We assume that the zeroth-\norder energies E (0)\nn  and the zeroth order eigenstates 0  n(0)9 have been solved for and are known.\n 10.3.1 \u0002 First-Order Energy Correction\nEquation (10.36) is the ﬁrst-order equation and contains the ﬁrst-order corrections E (1)\nn  and @  n(1)9 \nto the eigenvalues and eigenstates, respectively, as unknowns. For a system with N energy levels \n(i.e., N is the dimension of the Hilbert space), Eq. (10.36) represents N equations for n = 1, 2, ... N \nto be solved for each energy and each eigenstate. Moreover, for any given n, Eq. (10.36) is really a \nsystem of N equations because the Hamiltonian operators are represented by N * N matrices. To see \nour way through this morass of N2 equations, it is helpful to examine the full matrix representation of \nEq. (10.36) for one particular choice of n and then generalize from that result.\nOf course, to use matrices, we must choose a basis for representation. Given that we have solved \nonly the zeroth-order problem at this stage, the basis of zeroth-order energy eigenstates @  n(0)9 is the \nmost obvious basis at our disposal. So, we express each part of Eq. (10.36) in this basis. The matrices \nrepresenting the Hamiltonians H0 and H\u0004 do not depend on the choice of the state n and are\n \n H0 \u0003 •\nE (0)\n1\n0\n0\n0\ng\n0\nE (0)\n2\n0\n0\ng\n0\n0\nE (0)\n3\n0\ng\n0\n0\n0\nE (0)\n4\ng\nf\nf\nf\nf\nf\nμ  \n \n H\u0004 \u0003 •\nH =\n11\nH =\n12\nH =\n13\nH =\n14\ng\nH =\n21\nH =\n22\nH =\n23\nH =\n24\ng\nH =\n31\nH =\n32\nH =\n33\nH =\n34\ng\nH =\n41\nH =\n42\nH =\n43\nH =\n44\ng\nf\nf\nf\nf\nf\nμ  , \n(10.39)\nwhere the matrix elements of the perturbation Hamiltonian are deﬁned in the zeroth-order basis\n \nH =\nij = 8i (0) @H\u0004@\n  j (0)9. \n(10.40)\nAll of the elements of these two matrices are known quantities.\n",
    "10.3 Nondegenerate Perturbation Theory \n321\nThe other terms in Eq. (10.36) do depend on the choice of the state n. Let’s choose n = 3 for this \nexample, which means that the zeroth-order energy eigenstate 0  3(0)9 is\n \n0 3(0)9 \u0003 •\n0\n0\n1\n0\nf\nμ . \n(10.41)\nThe ﬁrst-order correction to the eigenstate @ 3(1)9 is not yet known, and we characterize it in terms of \na set of yet-to-be-found ﬁrst-order coefﬁcients c (1)\n3m = 8m(0)@ 3(1)9 in the zeroth-order basis\n \n0 3(1)9 \u0003 ß\nc(1)\n31\nc(1)\n32\nc(1)\n33\nc(1)\n34\nf\n∑. \n(10.42)\nThe other two ingredients in Eq. (10.36) are the known zeroth-order energy E (0)\n3  and the unknown \nﬁrst-order energy correction E (1)\n3 . We are now ready to construct the matrix form of Eq. (10.36) and \nsolve for the unknowns for this choice of n: the ﬁrst-order energy correction E (1)\n3  and the set of coef-\nﬁcients c(1)\n3m that determine the ﬁrst-order correction @ 3(1)9 to the eigenstate.\nFor the choice n = 3, the left-hand side of Eq. (10.36) is\n \n1H0 - E (0)\n3 2@ 3(1)9 \u0003 ß\nE (0)\n1\n- E (0)\n3\n0\n0\n0\ng\n0\nE (0)\n2\n- E (0)\n3\n0\n0\ng\n0\n0\n0\n0\ng\n0\n0\n0\nE (0)\n4\n- E (0)\n3\ng\nf\nf\nf\nf\nf\n∑ß\nc(1)\n31\nc(1)\n32\nc(1)\n33\nc(1)\n34\nf\n∑ \n \n\u0003 ß\n1E (0)\n1\n- E (0)\n3 2c(1)\n31\n1E (0)\n2\n- E (0)\n3 2c(1)\n32\n0\n1E (0)\n4\n- E (0)\n3 2c(1)\n34\nf\n∑ . \n(10.43)\n",
    "322 \nPerturbation Theory\nIn the matrix and the resultant vector, we have boxed the zero element that arises from subtracting the \nzeroth-order energy E (0)\n3  from the zeroth-order Hamiltonian in order to highlight the importance of \nthat element in the solution. The right-hand side of Eq. (10.36) is\n \n 1E (1)\n3\n- H =20 3(0)9 \u0003 ¶\nE (1)\n3\n- H =\n11\n-H =\n12\n-H =\n13\n-H =\n14\ng\n-H =\n21\nE (1)\n3\n- H =\n22\n-H =\n23\n-H =\n24\ng\n-H =\n31\n-H =\n32\nE 3\n(1) - H =\n33\n-H =\n34\ng\n-H =\n41\n-H =\n42\n-H =\n43\nE (1)\n3\n- H =\n44\ng\nf\nf\nf\nf\nf\n∂ ¶\n0\n0\n1\n0\nf\n∂\n \n\u0003 ¶\n-H =\n13\n-H =\n23\nE 3\n(1) - H =\n33\n-H =\n43\nf\n∂. \n(10.44)\nAgain we have boxed in the diagonal matrix element and the resultant vector component correspond-\ning to the state 0 3(0)9 that we are solving for. Equating the two sides of Eq. (10.36) gives\n \n¶\n1E (0)\n1\n- E (0)\n3 2c (1)\n31\n1E (0)\n2\n- E (0)\n3 2c (1)\n32\n0\n1E (0)\n4\n- E (0)\n3 2c (1)\n34\nf\n∂= ¶\n-H =\n13\n-H =\n23\nE 3\n(1) - H =\n33\n-H =\n43\nf\n∂ . \n(10.45)\nAs promised, we have N equations containing the unknown energy correction E\n (1)\n3  and the unknown \neigenstate correction @ 3(1)9 represented by the coefﬁcients c\n (1)\n3m. The third row of Eq. (10.45), which \nwe have been highlighting all along, yields the solution for the ﬁrst-order energy correction to the \nn = 3 state\n \nE (1)\n3\n- H =\n33 = 0 \n \nE (1)\n3\n= H =\n33.\n \n(10.46)\nWe conclude that the ﬁrst-order energy correction is the diagonal matrix element of the perturbation \nfor the state in question, which agrees with the results in Sections 10.1 and 10.2. The diagonal matrix \nelement of the perturbation is also what we call the expectation value of the perturbation. Note that \nno other states affect the energy correction of this state and the unperturbed states are used to ﬁnd the \nexpectation value of the perturbation; there is no need to know the correction to the state in order to \nﬁnd the ﬁrst-order correction to the energy. Having solved the ﬁrst-order perturbation equation for \n",
    "10.3 Nondegenerate Perturbation Theory \n323\nthis speciﬁc case of n = 3, we can now infer the result for the general n. The general result of nonde-\ngenerate ﬁrst-order perturbation theory is:\n \nE\n (1)\nn\n= H =\nnm = 8n(0)@H\u0004@ n(0)9  . \n(10.47)\nThe ﬁrst-order correction to the energy is the expectation value of the perturbation in the unper-\nturbed state. In wave function notation, the expectation value is expressed as an integral\n \nE (1)\nn\n= H =\nnn =\nL\nw(0)*\nn 1r2H\u0004w(0)\nn 1r2dV   , \n(10.48)\nwhere w(0)\nn (r) are the energy eigenstates of the zeroth-order Hamiltonian.\nThat’s the result. Now let’s use it.\nExample 10.1 The sodium nucleus has spin 3/2 and a magnetic moment MNa = (gNa\n e>2mp)S, \nwhere the gyromagnetic ratio is gNa = 1.48. The sodium nucleus is placed in a constant magnetic \nﬁeld in the z-direction B0 = B0\n zn. An additional, perturbative magnetic ﬁeld B\u0004 = B1zn  is applied to \nthe system. Find the ﬁrst-order energy shifts due to the perturbation.\nThis problem is a variation on the spin-1/2 example in Section 10.1. The zeroth-order Ham-\niltonian H0 is determined by the potential energy of the nuclear magnetic moment in the constant \nﬁeld B0 = B0\n zn :\n \nH0 = -M~B0 = v0Sz \u0003 §\n3\n2 U v0\n0\n0\n0\n0\n1\n2 U v0\n0\n0\n0\n0\n-  1\n2 U v0\n0\n0\n0\n0\n-  3\n2 U v0\n¥ , \n(10.49)\nwhere we have deﬁned the Larmor frequency v0 = gNa eB0>2mp. The zeroth-order energies are \nE (0)\n1\n= 3\n2 U v0 , E (0)\n2\n= 1\n2 U v0 , E (0)\n3\n= -  1\n2 U v0 , and E (0)\n4\n= -  3\n2 U v0 , labeled in order of decreasing \nenergy. The zeroth-order energy eigenstates are the eigenstates of the spin component operator Sz : \n@ 1(0)9= @ 3\n29, @ 2(0)9= @ 1\n29, @ 3(0)9= @ -1\n2 9, and @ 4(0)9= @ -3\n2 9, which are labeled with the magnetic quantum \nnumber m.\nThe perturbation Hamiltonian H\u0004 is determined by the ﬁeld B\u0004 = B1zn  and is characterized by \na different Larmor frequency v1 = gNa eB1>2mp :\n \nH\u0004 = -M~B\u0004 = v1Sz \u0003 §\n3\n2 U v1\n0\n0\n0\n0\n1\n2 U v1\n0\n0\n0\n0\n-  1\n2 U v1\n0\n0\n0\n0\n-  3\n2 U v1\n¥ . \n(10.50)\nSo far, all these are quantities known from the statement of the problem. Perturbation theory tells \nus that the ﬁrst-order correction to the energy is the expectation value of the perturbation in the \nunperturbed state:\n \nE (1)\nn\n= H =\nnn = 8n(0)@H\u0004@ n(0)9. \n(10.51)\n",
    "324 \nPerturbation Theory\nThese are the diagonal elements of the matrix representing H\u0004 in the basis of zeroth-order energy \neigenstates. The matrix in Eq. (10.50) thus yields the ﬁrst-order energy shifts due to the  perturbation:\n \n E\n (1)\n1\n= 3\n2 U v1\n \n \n E\n (1)\n2\n= 1\n2 U v1\n \n \n E\n (1)\n3\n= -  1\n2 U v1  \n \n E\n (1)\n4\n= -  3\n2 U v1. \n(10.52)\nThese energy shifts add to the zeroth-order energies to produce the results shown in Fig. 10.4 as \na function of the perturbing ﬁeld. The new energies exhibit the linear dependence we expect for \nﬁrst-order corrections. The constant B0 ﬁeld is assumed to be 2.35 Tesla, which is a standard ﬁeld \nin nuclear magnetic resonance spectroscopy because it produces a 100 MHz resonance for hydro-\ngen nuclei. For sodium nuclei, the resonance in this ﬁeld is 26.5 MHz (v0>2p), indicated by the \ntransition arrows in Fig. 10.4. As the perturbing ﬁeld increases, the resonance shifts in frequency. \nWhen the perturbing ﬁeld is produced by the local chemical environment of the nucleus, the res-\nonance shift is called a chemical shift. This technique is commonly used to identify  chemical \nmicrostructure.\n10.3.2 \u0002 First-Order State Vector Correction\nNow that we have the ﬁrst-order energy correction, we proceed to ﬁnd the ﬁrst-order correction to the \nenergy eigenstates. The other rows (nonhighlighted) of Eq. (10.45) yield equations of identical form \nfor the coefﬁcients c\n (1)\n3m that determine the ﬁrst-order correction to the state vector:\n \n1E\n (0)\nm\n- E\n (0)\n3 2c(1)\n3m = -H =\nm3 , \nm \u0002 3 \n \nc\n (1)\n3m =\nH =\nm3\nE\n (0)\n3\n- E\n (0)\nm\n \n, \nm \u0002 3. \n(10.53)\n0.5\n1.0\n1.5\n2.0\nB1(Tesla)\n\n40\n\n20\n20\n40\nE/h(MHz)\nm \u0004\u0007 3\n2\nm \u0004\u0007 1\n2\nm \u0004\u0007–3\n2\nm \u0004\u0007–1\n2\nFIGURE 10.4 First-order corrected energies of a sodium nucleus in a perturbing \nmagnetic ﬁeld that is parallel to the constant zeroth-order ﬁeld.\n",
    "10.3 Nondegenerate Perturbation Theory \n325\nEquation (10.53) determines all the coefﬁcients for m \u0002 3; however, there is no information about \nthe coefﬁcient c (1)\n33. This is not surprising. In solving energy eigenvalue equations before, we have \nalways found that one eigenstate coefﬁcient is undetermined by the equations. In those problems, we \nused the normalization requirement to determine the last coefﬁcient. We do the same here.\nUsing Eq. (10.34) (with l = 1) and Eq. (10.42), we write the corrected eigenstate to ﬁrst order as\n \n 0 39 = @ 3(0)9 + @ 3(1)9\n \n \n = @ 3(0)9 + a\nN\nm=1\nc\n (1)\n3m@ m(0)9. \n(10.54)\nSeparating out the undetermined coefﬁcient c (1)\n33, we obtain\n \n 0 39 = @ 3(0)9 + c (1)\n33 @ 3(0)9 + a\nm\u00023\nc (1)\n3m@ m(0)9 \n \n = 11 + c (1)\n33 2@ 3(0)9 + a\nm\u00023\nc (1)\n3m@ m(0)9.  \n(10.55)\nNow normalize this state to determine c (1)\n33 [all the other coefﬁcients are already speciﬁed by Eq. (10.53)]\n \n830 39 = e11 + c\n (1)*\n33 283(0)@ + a\nk\u00023\nc\n (1)*\n3k 8k(0)@ f e11 + c\n (1)\n33 2@ 3(0)9 + a\nm\u00023\nc\n (1)\n3m@ m(0)9f  \n \n= 11 + c\n (1)*\n33 211 + c\n (1)\n33 2 + a\nm\u00023\n@ c\n (1)\n3m @\n2 \n(10.56)\n \n= 1 + c\n (1)\n33 + c\n (1)*\n33\n+ @ c\n (1)\n33 @\n2 + a\nm\u00023\n@ c\n (1)\n3m @\n2,\nwhere we used the orthogonality 8k(0)@ m(0)9 = dkm of the zeroth-order states. We are working in the \nﬁrst-order perturbation approximation, so we must drop the second-order terms in the normalization \nequation for consistency, giving\n \n830 39 = 1 + c\n (1)\n33 + c\n (1)*\n33 . \n(10.57)\nUsing our freedom to choose the overall phase of the state vector, we choose c\n (1)\n33  to be real and con-\nclude that\n \nc\n (1)\n33 = 0. \n(10.58)\nHence, there is no component of the zeroth-order eigenstate @ 3(0)9 in the ﬁrst-order eigenstate correc-\ntion @ 3(1)9, which is the same conclusion we reached in the spin example in Eq. (10.15). This conclu-\nsion can be understood with an analogy between quantum state vectors and spatial vectors. Because all \nquantum state vectors must be normalized (in order to interpret the projections as probability ampli-\ntudes), all that really matters about a quantum state vector is its direction. Thus, as we look for changes \nin the vector, we must focus only on changes in direction. Figure 10.5 shows an analogy with spatial \nvectors, whereby we see that for small changes in direction, the change can be considered to be per-\npendicular to the original vector, and hence have no component along the  original vector.\n",
    "326 \nPerturbation Theory\nFor this n = 3 state, the ﬁrst-order eigenstate correction is\n \n@ 3(1)9 \u0003 ©\nH =\n13\nE\n (0)\n3\n- E\n (0)\n1\nH =\n23\nE\n (0)\n3\n- E\n (0)\n2\n0\nH =\n43\nE\n (0)\n3\n- E\n (0)\n4\nf\nπ, \n(10.59)\nand the corrected eigenstate to ﬁrst order is\n \n0 39 = @ 3(0)9 + @ 3(1)9 \u0003 ©\nH =\n13\nE\n (0)\n3\n- E\n (0)\n1\nH =\n23\nE\n (0)\n3\n- E\n (0)\n2\n1\nH =\n43\nE\n (0)\n3\n- E\n (0)\n4\nf\nπ. \n(10.60)\nFor the perturbation approach to be valid, we must have the new correction terms be small, which \nimplies that the matrix elements of the perturbation Hamiltonian are smaller than the unperturbed \nenergy differences. The absolute energies are not important because we can always shift the \nenergy axis.\n\u0002n(0)\u0003\n\u0002n(1)\u0003\n\u0002n\u0003\nFIGURE 10.5 Perturbed state corrections are orthogonal to the original state.\n",
    "10.3 Nondegenerate Perturbation Theory \n327\nFor a general state, the coefﬁcients for the eigenstate correction in Eqs. (10.53) and (10.58) \n generalize to\n \n c\n (1)\nnm =\nH =\nmn\nE\n (0)\nn\n- E\n (0)\nm\n , \nm \u0002 n \n \n c\n (1)\nnn = 0 .\n \n(10.61)\nWe conclude that the ﬁrst-order correction to the eigenstate is\n \n0 n(1)9 = a\nm\u0002n\n8m(0)0 H\u00040 n(0)9\n1E\n (0)\nn\n- E\n (0)\nm 2\n0 m(0)9    . \n(10.62)\nNote that the expansion coefﬁcients c(1)\nnm do not enter into the solution for the energy correction. This \nfeature is true in general for perturbation theory. We need the eigenstate correction only when solving \nfor the next order of the energy correction.\nTo illustrate the perturbation theory approach in the general case, let’s repeat the matrix calcula-\ntion we have just completed using Dirac bra-ket notation for the ﬁrst-order energy correction. We start \nwith the ﬁrst-order equation:\n \n1H0 - E\n (0)\nn 2@ n(1)9 = 1E (1)\nn\n- H\u00042@ n(0)9. \n(10.63)\nWe saw in Eq. (10.45) that the solution for the energy correction came from isolating the row for the \nstate of interest. In bra-ket notation, that means that we want to project Eq. (10.63) onto the zeroth-\norder nth eigenstate (as a bra):\n \n8n(0)@1H0 - E\n (0)\nn 2@ n(1)9 = 8n(0)@1E\n (1)\nn\n- H\u00042@ n(0)9.  \n(10.64)\nThe Hamiltonian H0 is Hermitian, so it can act backwards on the bra 8n(0)0 and give the energy \n18n(0)@H0 = 8n(0)@E\n (0)\nn 2, yielding zero on the left-hand side:\n \n 8n(0)@1E\n (0)\nn\n- E\n (0)\nn 2@ n(1)9 = E\n (1)\nn 8n(0)@ n(0)9 - 8n(0)@H\u0004@ n(0)9  \n \n 0 = E\n (1)\nn 8n(0)@ n(0)9 - 8n(0)@H\u0004@ n(0)9.  \n(10.65)\nSolving for the energy correction gives\n \nE\n (1)\nn\n= 8n(0)@  H\u0004@ n(0)9,  \n(10.66)\nwhich is the same as we obtained with the matrix approach above.\nIn summary, the new energy eigenvalues and eigenstates to ﬁrst order in the perturbation are\n \nEn = E\n (0)\nn\n+ 8n(0)@  H\u0004@ n(0)9\n \n \n0 n9 = @ n(0)9 + a\nm\u0002n\n8m(0)@H\u0004@ n(0)9\n1E\n (0)\nn\n- E\n (0)\nm 2\n@ m(0)9  . \n(10.67)\n",
    "328 \nPerturbation Theory\nThe eigenvalue and eigenstate corrections are independent of each other at this order. The corrections \nat this order will affect the corrections at the next order—the ﬁrst-order eigenstate corrections lead to \nsecond-order eigenvalue corrections, etc. The matrix elements of the perturbation Hamiltonian must \nbe smaller than the unperturbed energy differences for the perturbation approach to be valid.\nExample 10.2 An inﬁnite square well, shown in Fig. 10.6(a), is perturbed by an additional poten-\ntial energy term, with the resultant well shown in Fig. 10.6(b). Find the ﬁrst-order corrections to \nthe energies.\nWe solved the zeroth-order Hamiltonian for the inﬁnite square well in Chapter 5. The zeroth-\norder eigenvalues and eigenstates are\n \n E\n (0)\nn\n= n2 p2U2\n2mL2\n \n \n 0 n(0)9 \u0003 w(0)\nn (x) = A\n2\nL sin anpx\nL b. \n(10.68)\nIn the perturbed case, a potential energy shelf of value V0  is added to the right half of the well. \nThis is the asymmetric square well problem that we solved exactly in Section 5.9 as a sneak peek \nat perturbation theory. Now we solve it using our new perturbation theory tools and compare to the \nexact result. The shift in energy to ﬁrst order is the expectation value of the perturbation Hamilto-\nnian in the unperturbed eigenstates:\n \nE\n (1)\nn\n= 8n(0)@ H\u0004@ n(0)9. \n(10.69)\nThe perturbation Hamiltonian H\u0004 = V(x) has spatial dependence in this problem, and we must \nuse the wave function form of the ﬁrst-order energy correction in Eq. (10.48) involving a spatial \n integral—we cannot simply use bra-ket notation. More formally, we do not know how this new \nperturbation acts on kets. The ﬁrst-order energy correction is\n \nE\n (1)\nn\n=\nL\n\u0005\n- \u0005\nw(0)*\nn (x)V(x)w(0)\nn (x)dx. \n(10.70)\n0\nL\nx\n(a)\n(b)\n0\nL\nx\nV(x)\nV(x)\nV0\nL/2\nL/2\n\u0005\n\u0005\nFIGURE 10.6 (a) An inﬁnite square potential energy well perturbed by \n(b) a potential energy shelf in the right half of the well.\n",
    "10.4  Second-Order Nondegenerate Perturbation Theory \n329\nThe perturbation is different in the two halves of the well, so we break the integral into two pieces, \nwith the perturbation Hamiltonian equal to zero in the left half and V0 in the right half:\n \n E\n (1)\nn\n=\nL\nL>2\n0\nw(0)*\nn (x) 0  w(0)\nn (x)dx +\nL\nL\nL>2\n w(0)*\nn (x)V0 w(0)\nn (x)dx \n \n = V0 L\nL\nL>2\n@ w(0)\nn (x)@\n2dx.\n \n(10.71)\nThe remaining spatial integral is the integral of the probability density over the right half of the \nwell. All the energy eigenstate probability densities are symmetric about the middle of the well, so \nthe integral is 1/2, yielding (Problem 10.4)\n \nE\n (1)\nn\n= V0\n2\n \n(10.72)\nfor all states. This is the result we included as the perturbation theory prediction of the ground state \nenergy in Fig. 10.1, which shows that the ﬁrst-order perturbation correction is not exact. This per-\nturbation involves higher-order terms because the off-diagonal matrix elements are not zero, lead-\ning to eigenstate corrections [Eq. (10.62)] (Problem 10.5), which lead to second-order eigenvalue \ncorrections as we’ll see in the next section.\n10.4 \u0002 SECOND-ORDER NONDEGENERATE PERTURBATION THEORY\nNow let’s proceed and ﬁnd the second-order energy correction in nondegenerate perturbation theory. \nThe second-order perturbation equation (10.37) is\n \n1H0 - E (0)\nn 2@ n(2)9 = 1E (1)\nn\n- H\u00042@ n(1)9 + E (2)\nn @ n(0)9.  \n(10.73)\nFirst, we’ll use the matrix approach to solve this. The matrices representing the Hamiltonians H0 and \nH\u0004 are again given by Eq. (10.39). The ﬁrst-order correction to the eigenstate @ 3(1)9 is now known, and \nthe second-order correction @ 3(2)9 is unknown, which we again characterize in terms of a set of coef-\nﬁcients c(2)\n3m in the zeroth-order basis. The corrections to the unperturbed n = 3 state are\n \n@ 3(1)9 \u0003 ß\nc(1)\n31\nc(1)\n32\n0\nc(1)\n34\nf\n∑,  @ 3(2)9 \u0003 ß\nc(2)\n31\nc(2)\n32\nc(2)\n33\nc(2)\n34\nf\n∑. \n(10.74)\n",
    "330 \nPerturbation Theory\nThe left-hand side of Eq. (10.73) is\n \n 1H0 - E (0)\n3 2@ 3(2)9 \u0003 ß\nE (0)\n1\n- E (0)\n3\n0\n0\n0\ng\n0\nE (0)\n2\n- E (0)\n3\n0\n0\ng\n0\n0\n0\n0\ng\n0\n0\n0\nE (0)\n4\n- E (0)\n3\ng\nf\nf\nf\nf\nf\n∑ ß\nc(2)\n31\nc(2)\n32\nc(2)\n33\nc(2)\n34\nf\n∑\n \n \u0003 ß\n1E (0)\n1\n- E (0)\n3 2c(2)\n31\n1E (0)\n2\n- E (0)\n3 2c(2)\n32\n0\n1E (0)\n4\n- E (0)\n3 2c(2)\n34\nf\n∑,\n \n \n(10.75)\nwhere we have again boxed the zero matrix element and the resultant vector component to highlight \ntheir importance in the solution. The ﬁrst term on the right-hand side of Eq. (10.73) is\n \n1E (1)\n3\n- H\u00042@ 3(1)9 \u0003 ß\nE (1)\n3\n- H =\n11\n-H =\n12\n-H =\n13\n-H =\n14\ng\n-H =\n21\nE (1)\n3\n- H =\n22\n-H =\n23\n-H =\n24\ng\n-H =\n31\n-H =\n32\nE (1)\n3\n- H =\n33\n-H =\n34\ng\n-H =\n41\n-H =\n42\n-H =\n43\nE (1)\n3\n- H =\n44\ng\nf\nf\nf\nf\nf\n∑ ß\nc(1)\n31\nc(1)\n32\n0\nc(1)\n34\nf\n∑ \n \n\u0003 ß\n1E (1)\n3\n- H =\n112c(1)\n31 - H =\n12c(1)\n32 - H =\n14c(1)\n34 - g\n-H =\n21c(1)\n31 + 1E (1)\n3\n- H =\n222c(1)\n32 - H =\n24c(1)\n34 - g\n-H =\n31c(1)\n31 - H =\n32c(1)\n32 - H =\n34c(1)\n34 - g\n-H =\n41c(1)\n31 - H =\n42c(1)\n32 + 1E (1)\n3\n- H =\n442c(1)\n34\nf\n∑.   \n(10.76)\n",
    "10.4  Second-Order Nondegenerate Perturbation Theory \n331\nAgain we have boxed in the diagonal matrix element and the resultant vector component correspond-\ning to the state 0 3(0)9 that we are solving for. The second term on the right-hand side of Eq. (10.73) is\n \nE (2)\n3  0 3(0)9 \u0003 E (2)\n3  ¶\n0\n0\n1\n0\nf\n∂= ¶\n0\n0\nE (2)\n3\n0\nf\n∂. \n(10.77)\nEquating the two sides of Eq. (10.73) gives\n \nß\n1E (0)\n1\n- E (0)\n3 2c(2)\n31\n1E (0)\n2\n- E (0)\n3 2c(2)\n32\n0\n1E 4\n(0) - E (0)\n3 2c(2)\n34\nf\n∑= ß\n1E (1)\n3\n- H =\n112c(1)\n31 - H =\n12c(1)\n32 - H =\n14c(1)\n34 - c\n-H =\n21c(1)\n31 + 1E (1)\n3\n- H =\n222c(1)\n32 - H =\n24c(1)\n34 - c\n-H =\n31c(1)\n31 - H =\n32c(1)\n32 - H =\n34c(1)\n34 - c\n-H =\n41c(1)\n31 - H =\n42c(1)\n32 + 1E (1)\n3\n- H =\n442c(1)\n34\nf\n∑+ ¶\n0\n0\nE (2)\n3\n0\nf\n∂. (10.78)\nAs in the ﬁrst-order calculation, the highlighted elements yield the solution for the second-order \nenergy correction\n \n E (2)\n3\n= H =\n31c(1)\n31 + H =\n32c(1)\n32 + H =\n34c(1)\n34 + ... \n \n = a\nm\u00023\nH =\n3mc(1)\n3m.\n \n(10.79)\nAs we said earlier, the second-order energy correction depends on the ﬁrst-order state vector correc-\ntion through the coefﬁcients c(1)\n3m. We substitute for the coefﬁcients from Eq. (10.53) to get\n \n E (2)\n3\n= a\nm\u00023\nH =\n3m \nH =\nm3\nE (0)\n3\n- E (0)\nm\n \n \n = a\nm\u0002383(0)@H\u0004@ m(0)9 8m(0)0 H\u00040 3(0)9\nAE (0)\n3\n- E (0)\nm B\n \n(10.80)\n \n = a\nm\u00023\n083(0)0 H\u00040 m(0)9 0\n2\nAE (0)\n3\n- E (0)\nm B\n  .\n \nHaving solved for the speciﬁc n = 3 case, we now generalize this result to\n \nE (2)\nn\n= a\nm\u0002n\n08n(0)0 H\u00040 m(0)9 0\n2\n1E (0)\nn\n- E (0)\nm 2\n  . \n(10.81)\n",
    "332 \nPerturbation Theory\nThe second-order energy correction is proportional to the squares of matrix elements connecting states \nand inversely proportional to the energy differences between those states. Hence, states that are nearby \nin energy generally have a stronger inﬂuence on the perturbation, and because the connecting matrix \nelements are squared, the sign of the energy shift is determined solely by the sign of the energy dif-\nference in the denominator. Energy levels m that lie above the state n give a negative contribution \nand hence push the energy level n down, away from the states m. Energy levels m that lie below \nthe state n give a positive contribution and hence push the energy level n up, also away from the \nstates m. The take-away message is that in second order, energy levels tend to repel each other, as \nshown in Fig. 10.7. This result has the quadratic form we expected from our examples on two-level \nproblems at the beginning of the chapter.\nExample 10.3 A sodium nucleus in a constant magnetic ﬁeld in the z-direction B0 = B0zn is sub-\nject to a perturbation caused by an additional magnetic ﬁeld B\u0004 = B2xn applied to the system. Find \nthe second-order energy shifts due to the perturbation and the state vectors correct to ﬁrst order.\nThis problem is a variation on the spin-3/2 problem in Example 10.1 and also parallels the \nspin-1/2 problem in Section 10.1. Here, the perturbing ﬁeld is perpendicular to, rather than paral-\nlel to, the zeroth-order ﬁeld B0 = B0zn. As in Example 10.1, the zeroth-order Hamiltonian H0 is \ndetermined by the potential energy of the nuclear magnetic moment in the uniform ﬁeld B0 = B0zn :\n \nH0 = -M~B0 = v0Sz \u0003 •\n3\n2 U v0\n0\n0\n0\n0\n1\n2 U v0\n0\n0\n0\n0\n-  1\n2 U v0\n0\n0\n0\n0\n-  3\n2 U v0\nμ, \n(10.82)\nwhere we have deﬁned the Larmor frequency v0 = gNaeB0>2mp. The zeroth-order energies are \nE (0)\n1\n= 3\n2 U v0 , E (0)\n2\n= 1\n2 U v0 , E (0)\n3\n= -  1\n2 U v0 , and E (0)\n4\n= -  3\n2 U v0 , using the same state labeling as \nin Example 10.1.\nperturbation\nEnergy\nE(2)\n2\nE(0)\n2\nE(0)\n1\nE(2)\n1\nFIGURE 10.7 Energy levels repel each other in second-order perturbation theory.\n",
    "10.4  Second-Order Nondegenerate Perturbation Theory \n333\nThe perturbation Hamiltonian H\u0004 is determined by the ﬁeld B\u0004 = B1xn and is characterized by \na different Larmor frequency v2 = gNaeB2>2mp :\n \nH\u0004 = -M~B\u0004 = v2Sx \u0003 •\n0\n13\n2  U v2\n0\n0\n13\n2  U v2\n0\n14\n2  U v2\n0\n0\n14\n2  U v2\n0\n13\n2  U v2\n0\n0\n13\n2  U v2\n0\nμ. \n(10.83)\nThis perturbation Hamiltonian has no diagonal elements, so there are no ﬁrst-order energy cor-\nrections, in contrast to Example 10.1. The off-diagonal elements give ﬁrst-order state vector \ncorrections [Eq. (10.62)] and second-order energy corrections [Eq. (10.81)]. We’ll calculate the \nsecond-order energy corrections ﬁrst.\nThe second-order correction to the energy is proportional to the squares of matrix elements \nconnecting states and inversely proportional to the energy differences between those states:\n \nE (2)\nn\n= a\nm\u0002n\n@8n(0)@H\u0004@ m(0)9@\n2\n1E (0)\nn\n- E (0)\nm 2\n. \n(10.84)\nFor the @ 1(0)9 state, the energy shift is given by a sum, but there is only one term in the sum because \nonly H =\n12 \u0002 0 :\n \n E (2)\n1\n= a\nm\u0002n\n@81(0)@H\u0004@ m(0)9@\n2\n1E (0)\n1\n- E (0)\nm 2\n=\n@81(0)@H\u0004@ 2(0)9@\n2\n1E (0)\n1\n- E (0)\n2 2\n \n \n =\n@13\n2  U v2@\n2\n13\n2 U v0 - 1\n2 U v02\n \n(10.85)\n \n =\n3U v2\n2\n4v0\n.\n \nFor the @ 2(0)9 state, the energy shift is\n \n E (2)\n2\n= a\nm\u0002n\n@82(0)@H\u0004@ m(0)9@\n2\n1E (0)\n2\n- E (0)\nm 2\n=\n@82(0)@H\u0004@ 1(0)9@\n2\n1E (0)\n2\n- E (0)\n1 2\n+\n@82(0)@H\u0004@ 3(0)9@\n2\n1E (0)\n2\n- E (0)\n3 2\n \n =\n@13\n2  U v2@\n2\n11\n2 U v0 - 3\n2 U v02\n+\n@14\n2  U v2@\n2\n11\n2 U v0 - -1\n2  U v02\n \n(10.86)\n \n =\nU v2\n2\n4v0\n.\n",
    "334 \nPerturbation Theory\nSimilarly, the shifts for the @ 3(0)9 and @ 4(0)9 state are:\n \n E (2)\n3\n= -  \nU v2\n2\n4v0\n \n \n E (2)\n4\n= -  \n3U v2\n2\n4v0\n.\n \n(10.87)\nAdding these energy shifts to the zeroth-order energies gives the perturbed energies shown in \nFig. 10.8 as a function of the perturbing ﬁeld magnitude B2. The new energies exhibit the quadratic \ndependence we expect for second-order shifts and also illustrate the repulsion of levels that we \nexpect. The NMR transitions are indicated by the arrows in Fig. 10.8. The resonance shifts in this \ncase are different from the shifts in Fig. 10.4, because the perturbing ﬁeld is perpendicular to, rather \nthan parallel to, the constant B0 ﬁeld. Hence chemical shifts also provide information about the \nspatial alignment of the system with respect to the constant B0 ﬁeld.\nThe ﬁrst-order state vector correction is\n \n@ n(1)9 = a\nm\u0002n\n 8m(0)@H\u0004@ n(0)9\n1E (0)\nn\n- E (0)\nm 2\n0 m(0)9. \n(10.88)\nFor the @ 1(0)9 state, the ﬁrst-order correction again has only one term in the sum because only \nH =\n12 \u0002 0 :\n \n @ 1(1)9 = a\nm\u0002n\n 8m(0)@H\u0004@ 1(0)9\n1E (0)\n1\n- E (0)\nm 2\n @ m(0)9 = 82(0)@H\u0004@ 1(0)9\n1E (0)\n1\n- E (0)\n2 2\n @ 2(0)9\n \n =\n13\n2  U v 2\n13\n2 U v0 - 1\n2 U v02\n @ 2(0)9\n \n = 23v2\n2v0\n @ 2(0)9.\n \n(10.89)\n0.5\n1.0\n1.5\n2.0\nB2(Tesla)\n\u000640\n\u000620\n20\n40\nE/h(MHz)\nm \u0007 3\n2\nm \u0007 1\n2\nm \u0007 –3\n2\nm \u0007 –1\n2\nFIGURE 10.8 Energy shifts of a sodium nucleus in a perturbing magnetic ﬁeld \nthat is perpendicular to the constant zeroth-order ﬁeld.\n",
    "10.4  Second-Order Nondegenerate Perturbation Theory \n335\nThe state vector correct to ﬁrst order includes the zeroth-order state:\n \n0 19 = @ 1(0)9 + 23v 2\n2v0\n @ 2(0)9. \n(10.90)\nSimilarly, the other corrected states to ﬁrst order are (Problem 10.6)\n \n 0 29 = @ 2(0)9 - 23v 2\n2v0\n @ 1(0)9 + v 2\nv0\n @ 3(0)9\n \n 0 39 = @ 3(0)9 - v 2\nv0\n @ 2(0)9 + 23v 2\n2v0\n @ 4(0)9 \n \n(10.91)\n \n 0 49 = @ 4(0)9 - 23v 2\n2v0\n @ 3(0)9.\nTo conclude, we repeat the matrix calculation of the second-order energy correction using Dirac \nbra-ket notation. As before, we project out the desired state by taking the inner product of the second-\norder equation (10.37) with the nth zero-order eigenstate and get\n \n8n(0)@1H0 - E (0)\nn 2@ n(2)9 = 8n(0)@1E (1)\nn\n- H\u00042@ n(1)9 + 8n(0)@E (2)\nn @ n(0)9. \n(10.92)\nThe Hamiltonian H0 is Hermitian, so it can act backwards on the bra 8n(0)@ to give the energy \n18n(0)@H0 = 8n(0)@E (0)\nn 2, giving zero on the left-hand side:\n \n 8n(0)@1E (0)\nn\n- E (0)\nn 2@ n(2)9 = 8n(0)@1E (1)\nn\n- H\u00042@ n(1)9 + E (2)\nn  \n \n 0 = E (1)\nn 8n(0)@ n(1)9 - 8n(0)@H\u0004@ n(1)9 + E (2)\nn .\n \n(10.93)\nThe ﬁrst-order state vector correction @ n(1)9 is orthogonal to the original state @ n(0)9, making the ﬁrst \nterm on the right-hand side zero. Now solve for the second-order energy correction\n \n 0 = 0 - 8n(0)@H\u0004@ n(1)9 + E (2)\nn  \n \n E (2)\nn\n= 8n(0)@H\u0004@ n(1)9.\n \n(10.94)\nUse the previous result for the ﬁrst-order state vector correction to get the second-order energy \ncorrection\n \n E (2)\nn\n= a\nm\u0002n\ncmn8n(0)@H\u0004@ m(0)9\n \n \n E (2)\nn\n= a\nm\u0002n\n 8m(0)@H\u0004@ n(0)9\n1E (0)\nn\n- E (0)\nm 2\n 8n(0)@H\u0004@ m(0)9 \n(10.95)\n \n E (2)\nn\n= a\nm\u0002n\n \n@8n(0)@H\u0004@ m(0)9@\n2\n1E (0)\nn\n- E (0)\nm 2\n,\n \nwhich is the general result we found in Eq. (10.81). We haven’t bothered to ﬁnd the second-order \ncorrections to the eigenstates, because we are primarily concerned with energy corrections. If you are \nso inclined, you can derive them from the m\u00023 rows of Eq. (10.78).\n",
    "336 \nPerturbation Theory\n10.5 \u0002 DEGENERATE PERTURBATION THEORY\nAs we have pointed out several times, states that are degenerate in energy with respect to the zeroth-\norder Hamiltonian create a problem in perturbation theory. If two states have the same zeroth-order \nenergy, then the term E (0)\nn\n- E (0)\nm  in the denominator of the ﬁrst-order state vector correction in \nEq. (10.67) becomes zero and the correction is no longer small, but rather diverges. This same \nproblematic denominator appears in the second-order energy correction in Eq. (10.81). Though \ndegeneracy creates no problem with the ﬁrst-order energy correction, these divergences still call \ninto question our overall approach to the problem. So we must explicitly address systems with \nenergy degeneracy.\nIf we look back at the matrix solution to the ﬁrst-order perturbation equation in Section 10.3, we \ncan identify the source of the divergence problem and decide how to proceed. The terms that cause \nthe divergence are the diagonal terms E (0)\nn\n- E (0)\n3  in the matrix representing H0 - E (0)\n3  in Eq. (10.43). \nWe knew that the term E (0)\n3\n- E (0)\n3  was zero and we used that fact to ﬁnd the ﬁrst-order energy cor-\nrection E (1)\n3 , as we indicated with the boxed parts of the matrix equations. However, we didn’t expect \nany of the other diagonal terms to be zero and so we ended up dividing by them in Eq. (10.53). \nLet’s identify which diagonal terms are zero before we start the solution and then we can avoid the \ndivision- by-zero problem.\nAgain, it helps to choose a speciﬁc example to illustrate the basic idea and then generalize at the \nend. Let’s assume that the n = 2 and n = 3 states of a system are degenerate with the same energy \nE (0)\n2 . The zeroth-order Hamiltonian in that case is\n \nH0 \u0003 ß\nE (0)\n1\n0\n0\n0\ng\n0\nE (0)\n2\n0\n0\ng\n0\n0\nE (0)\n2\n0\ng\n0\n0\n0\nE (0)\n4\ng\nf\nf\nf\nf\nf\n∑. \n(10.96)\nThe ﬁrst-order perturbation equation we want to solve is [see Eq. (10.36)]\n \n1H0 - E (0)\nn 2@n(1)9 = 1E (1)\nn\n- H\u00042@n(0)9, \n(10.97)\nwhere we are trying to ﬁnd the correction E (1)\n2 . The matrix 1H0 - E (0)\n2 2 on the left-hand side of \nEq. (10.97) is then\n \n1H0 - E (0)\n2 2 \u0003 ß\nE (0)\n1\n- E (0)\n2\n0\n0\n0\ng\n0\n0\n0\n0\ng\n0\n0\n0\n0\ng\n0\n0\n0\nE (0)\n4\n- E (0)\n2\ng\nf\nf\nf\nf\nf\n∑. \n(10.98)\n",
    "10.5 Degenerate Perturbation Theory \n337\nNow we have two zeros along the diagonal instead of one. More important, there is a whole submatrix, \nindicated by the central boxed item, that is equal to zero, instead of a single number [Eq. (10.43)], and \nthat submatrix is isolated from the rest of the matrix by virtue of the zeros in all the corresponding \nrows and columns.\nTurning now to the right-hand side of Eq. (10.97), the matrix 1E (1)\n2\n- H\u00042 is\n \n1E (1)\n2\n- H\u00042 \u0003 ß\nE (1)\n2\n- H =\n11\n-H =\n12\n-H =\n13\n-H =\n14\ng\n-H =\n21\nE (1)\n2\n- H =\n22\n-H =\n23\n-H =\n24\ng\n-H =\n31\n-H =\n32\nE (1)\n2\n- H =\n33\n-H =\n34\ng\n-H =\n41\n-H =\n42\n-H =\n43\nE (1)\n2\n- H =\n44\ng\nf\nf\nf\nf\nf\n∑, \n(10.99)\nwhere we have identiﬁed the submatrix corresponding to the zero submatrix in Eq. (10.98). In the \nnondegenerate case, we equated the two sides of Eq. (10.97) and found the solution for the ﬁrst-order \nenergy correction in the row corresponding to the energy level of interest [Eq. (10.45)]. But now the \nenergy level of interest is degenerate and corresponds to two rows and two columns of the matrices, \nas indicated in Eqs. (10.98) and (10.99). So instead of a single equation for the energy correction, we \nhave a matrix equation. To ﬁnd this matrix equation, we must include the column vectors representing \nthe states in Eq. (10.97). On the left-hand side is the unknown state correction @ 2(1)9 or @ 3(1)9:\n \n@ 2(1)9 \u0003 ß\nc(1)\n21\nc(1)\n22\nc(1)\n23\nc(1)\n24\nf\n∑, @ 3(1)9 \u0003 ß\nc(1)\n31\nc(1)\n32\nc(1)\n33\nc(1)\n34\nf\n∑.  \n(10.100)\nOn the right-hand side is the known eigenstate @ 2(0)9, or its degenerate partner @ 3(0)9:\n \n@ 2(0)9 \u0003 ¶\n0\n1\n0\n0\nf\n∂, @ 3(0)9 \u0003 ¶\n0\n0\n1\n0\nf\n∂. \n(10.101)\nBut the energy degeneracy of these two states creates an ambiguity. Both @ 2(0)9 and @ 3(0)9 satisfy \nthe zeroth-order energy eigenvalue equation for the energy E (0)\n2 , but so does any linear combination of \nthe two states. If we are trying to ﬁnd the energy correction to the state with zeroth-order energy E (0)\n2 ,\nhow do we know whether to use the state @ 2(0)9 or the state @ 3(0)9 in the perturbation equation? We don’t \n",
    "338 \nPerturbation Theory\nknow which one to use. We have no information that would help us decide which linear combination is \n“correct,” so we let the solution to the problem tell us! We leave the state unspeciﬁed and write it as\n \n@ 2(0)\nnew9 \u0003 ¶\n0\na\nb\n0\nf\n∂. \n(10.102)\nThis ambiguity turns out to be the answer to our degeneracy problem in perturbation theory.\nNow the left-hand side of Eq. (10.97) is\n \n 1H0 - E (0)\nn 2@ n(1)9 \u0003 ß\nE (0)\n1\n- E 2\n(0)\n0\n0\n0\ng\n0\n0\n0\n0\ng\n0\n0\n0\n0\ng\n0\n0\n0\nE (0)\n4\n- E (0)\n2\ng\nf\nf\nf\nf\nf\n∑ ß\nc(1)\n21\nc(1)\n22\nc(1)\n23\nc(1)\n24\nf\n∑ \n \n \u0003 ß\n1E (0)\n1\n- E (0)\n3 2c(1)\n21\n0\n0\n1E (0)\n4\n- E (0)\n3 2c(1\n24\n)\nf\n∑\n \n(10.103)\nand the right-hand side is\n \n 1E (1)\nn\n- H\u00042@ n(0)9 \u0003 ß\nE (1)\n2\n- H =\n11\n-H =\n12\n-H =\n13\n-H =\n14\ng\n-H =\n21\nE (1)\n2\n- H =\n22\n-H =\n23\n-H =\n24\ng\n-H =\n31\n-H =\n32\nE (1)\n2\n- H =\n33\n-H =\n34\ng\n-H =\n41\n-H =\n42\n-H =\n43\nE (1)\n2\n- H =\n44\ng\nf\nf\nf\nf\nf\n∑ ß\n0\na\nb\n0\nf\n∑\n \n \u0003 ß\n-H =\n12a - H =\n13b\n1E 2\n(1) - H =\n222a - H =\n23b\n-H =\n32a + 1E 2\n(1) - H =\n332b\n-H =\n42a - H =\n43b\nf\n∑.  \n(10.104)\n",
    "10.5 Degenerate Perturbation Theory \n339\nWe equate the two rows of interest to get the set of equations\n \n 1E (1)\n2\n- H =\n222a - H =\n23b = 0  \n \n -H =\n32a + 1E (1)\n2\n- H =\n332b = 0. \n(10.105)\nWe write these equations in matrix form\n \naE 2\n(1) - H =\n22\n-H =\n23\n-H =\n32\nE 2\n(1) - H =\n33\nb aa\nbb = 0, \n(10.106)\nwhich shows that we have isolated the submatrix of the full perturbation equation corresponding to the \ntwo degenerate levels. Now rewrite Eq. (10.106) in a simpler form\n \naH =\n22\nH =\n23\nH =\n32\nH =\n33\nb aa\nbb = E 2\n(1) aa\nbb . \n(10.107)\nThis equation looks surprisingly like a standard energy eigenvalue equation. However, this equation \nis limited to only the two states comprising the degenerate energies, with the perturbation Hamilto-\nnian playing the role of the Hamiltonian, while the energy eigenvalue is the ﬁrst-order correction that \nwe are seeking. We commonly refer to this isolated subspace of the whole system as the  degenerate \nsubspace.\nWe solve Eq. (10.107) by the standard procedure of diagonalization for eigenvalue equations, \nwhich yields two eigenvalues and two eigenstates. The eigenvalues are the two corrections E (1)\n2  to the \nzeroth-order energy E (0)\n2 . If we label the two energy solutions E (1)\n2a and E (1)\n2b , then the energies correct \nto ﬁrst order are\n \n E 2a = E (0)\n2\n+ E (1)\n2a  \n \n E 2b = E (0)\n2\n+ E (1)\n2b . \n(10.108)\nThe two sets of a and b coefﬁcients from solving Eq. (10.107) give the two eigenstate solutions @ 2a9 \nand  @ 2b9 that form a new basis in the degenerate subspace that was originally deﬁned by @ 2(0)9 and \n@ 3(0)9. In this new basis, the perturbation Hamiltonian is diagonal.\nNow let’s generalize our speciﬁc solution. The result of the twofold degenerate example was the \neigenvalue equation (10.107) for the perturbation Hamiltonian within the degenerate subspace. The \nperturbation corrections are found by solving that eigenvalue equation through the diagonalization \nprocedure we use throughout quantum mechanics. So there is no silver bullet formula for degenerate \nperturbation theory. There is just the mantra:\nDiagonalize the perturbation Hamiltonian in the degenerate subspace.\nThat’s it! Let’s see how it works.\nExample 10.4 An electron is bound to move on the surface of a sphere. Find the energy correc-\ntions caused by a perturbing magnetic ﬁeld B\u0004 = B1xn , limiting your consideration to the ﬁrst two \nenergy levels.\n",
    "340 \nPerturbation Theory\nThe zeroth-order Hamiltonian of a particle on a sphere is the kinetic energy, as we found in \nSection 7.6:\n \nHsphere = L2\n2I . \n(10.109)\nIn that problem, we found the zeroth-order eigenstates to be the angular momentum eigenstates\n \n0 /m9 \u0003 Y m\n/ 1u, f2 \n(10.110)\nand the zeroth-order energies to be\n \nE/ = U2\n2I\n /1/ + 12, \n(10.111)\nas illustrated in the energy spectrum of Fig. 7.16. The energy is independent of the magnetic quan-\ntum number m, so each energy level is degenerate except the / = 0 ground state, with (2/ + 1) \npossible m states for a given /. Hence the need for degenerate perturbation theory.\nThe perturbation Hamiltonian is the potential energy of interaction H\u0004 = -ML~B\u0004 between the \napplied magnetic ﬁeld and the magnetic moment of the electron due to its orbital angular momen-\ntum (we ignore spin angular momentum here). The electron magnetic moment associated with the \norbital motion is\n \nML = -  e\n2me\n L \n(10.112)\nand the resultant perturbation Hamiltonian is\n \n H\u0004 =\ne\n2me\n L~B\u0004 =\ne\n2me\n B1Lx \n \n = v1Lx.\n \n(10.113)\nThe Larmor frequency in this case is v1 = eB1>2me.  \nWe limit ourselves to the ﬁrst two energy levels: E (0)\n0\n= 0 and E (0)\n1\n= U2>I. The ground state \nis nondegenerate and the ﬁrst excited state is threefold degenerate, with the zeroth-order Hamilto-\nnian for these states\n \nH0 \u0003 •\n0\n0\n0\n0\n0\nU2>I\n0\n0\n0\n0\nU2>I\n0\n0\n0\n0\nU2>I\nμ \n/\nm\n0\n0\n1\n1\n1\n0\n1\n-1\n. \n(10.114)\nUsing the matrix representation of Lx in Eq. 7.62 for / = 1, we ﬁnd the perturbation Hamiltonian:\n \nH\u0004 \u0003 •\n0\n0\n0\n0\n0\n0\nU v1> 12\n0\n0\nU v1> 12\n0\nU v1> 12\n0\n0\nU v1> 12\n0\nμ \n/\nm\n0\n0\n1\n1\n1\n0\n1\n-1\n. \n(10.115)\n",
    "10.5 Degenerate Perturbation Theory \n341\nThe / = 0 level is nondegenerate, but the diagonal and off-diagonal matrix elements for \n/ = 0 are zero, so there is neither a ﬁrst-order nor a second-order correction to that energy. The \n/ = 1 level is threefold degenerate, so we must diagonalize the perturbation Hamiltonian in the \ndegenerate subspace:\n \nH =\n/=1 \u0003 §\n0\nU v1> 12\n0\nU v1> 12\n0\nU v1> 12\n0\nU v1> 12\n0\n¥ . \n(10.116)\nThe characteristic equation is\n \n 0 = 4  \nl\n-U v1> 12\n0\n-U v1> 12\nl\n-U v1> 12\n0\n-U v1> 12\nl\n 4\n \n \n = l1l2 - U2v2\n1>22 - 1-U v1> 1221-U v1> 122l \n \n = l1l2 - U2v2\n12\n \n(10.117)\nwith solutions\n \nl = U v1, 0, -U v1. \n(10.118)\nThis result is to be expected because the eigenvalues of Lx for / = 1 are U, 0, -U and the perturba-\ntion is H\u0004 = v1Lx . The resultant energy shifts are\n \nE (1)\n1a = U v1\n \n \nE (1)\n1b = 0\n \n(10.119)\n \nE (1)\n1c = -U v1. \nThe perturbed energy spectrum of the ﬁrst two levels is shown in Fig. 10.9.\nThe eigenstates of the perturbation Hamiltonian are the Lx eigenstates @ 1mx9 because the per-\nturbation is H\u0004 = v1Lx . If we had written the perturbation Hamiltonian in the Lx basis instead of \nthe usual Lz basis, then the matrix representing the perturbation Hamiltonian would have already \nbeen diagonal and the problem would be solved by inspection. The Lx eigenstates are thus the “cor-\nrect” basis for the perturbation problem. The solution for higher-order states now becomes clear. \nEach energy state is split into the (2/ + 1) possible mx states for that / state.\n",
    "342 \nPerturbation Theory\nAs we pointed out in the example above, if the perturbation Hamiltonian is already diagonal \nin the degenerate subspace, then the solution is obtained by inspection of the perturbation matrix: \nthe energy corrections are the diagonal elements H =\nnn = 8n(0)@H\u0004@ n(0)9. But this is exactly what \nﬁrst-order nondegenerate perturbation theory tells us to do! So if the perturbation is diagonal in the \ndegenerate subspace, then degenerate perturbation theory reduces to nondegenerate theory. In this \ncase, the problem with the divergent denominators fades because the off-diagonal elements that \nappear in those same numerators [Eqs. (10.67) and (10.81)] are now zero. If the perturbation is not \ndiagonal, then we make it diagonal by applying the diagonalization procedure to solve Eq. (10.107). \nThe resultant eigenvalues (i.e., the diagonal elements in the new basis) are the desired energy cor-\nrections and the resultant eigenstates form the “correct” basis that avoids the divergence problems.\nGiven the ambiguity of the zeroth-order eigenstates in the degenerate subspace, we can try to be \nclever enough to start the problem by choosing the “correct” or “good” basis that makes the pertur-\nbation diagonal. We can do that by using eigenstates of some operator that commutes with both the \nzeroth-order Hamiltonian and the perturbation Hamiltonian. In Example 10.4, the Lx basis is the “cor-\nrect” basis because the perturbation Hamiltonian is diagonal in that basis.\nLet’s conclude with some remarks about what is really going on with degenerate perturbation the-\nory. We know that the general method for ﬁnding the energy eigenvalues of a system is to diagonalize \nthe Hamiltonian. When we add a perturbation to a system, we must rediagonalize the full Hamiltonian. \nIf that is possible, by all means do it and you will have the exact answer to the problem. But perturba-\ntion theory is designed to tackle problems where we cannot, for whatever reason, diagonalize the full \nHamiltonian. So we diagonalize the zeroth-order Hamiltonian and then try to ﬁnd a way to account for \nthe perturbation Hamiltonian as best we can.\nIn the case of degenerate energy levels, we found that the nondegenerate perturbation theory \ndeveloped in Section 10.3 led to divergences when the degenerate energy states were involved. The \nsolution we presented above is to diagonalize the perturbation Hamiltonian within the degenerate sub-\nspace. However, one might ask whether we are justiﬁed in ignoring the original Hamiltonian and only \ndiagonalizing the perturbing Hamiltonian because the diagonalization procedure amounts to a trans-\nformation of bases, which one would expect to disturb the original basis and hence “ undiagonalize” the \nB1\n0\n1\n2\nE(\u00022/2I)\n0\n0\n1 \n1\n1\n0\n1\n1\nmx\nl\nFIGURE 10.9 Perturbed energies of an electron bound to a sphere with \nan applied magnetic ﬁeld.\n",
    "10.6  More Examples \n343\noriginal Hamiltonian. But the original Hamiltonian is degenerate within the subspace  corresponding \nto the degenerate energy, so it is proportional to the identity matrix within that subspace [Eq. (10.96)], \nand a transformation of the identity matrix leaves it unchanged. This is the mathematical consequence \nof the arbitrariness of choice of basis in the original problem. Hence, we are able to diagonalize the \nperturbation Hamiltonian within the degenerate subspace without undiagonalizing the zeroth-order \nHamiltonian.\nIn some cases, the perturbing Hamiltonian is already diagonal, in which case there is not much \nwork to do. The ﬁrst-order energy corrections are obtained by inspection of the matrix as the diagonal \nelements H =\nnn , which are the expectation values of the perturbation in the degenerate subspace. This \nresult is the same as we obtained with nondegenerate perturbation theory, so if we choose the original \nbasis correctly, then degenerate and nondegenerate perturbation theory are the same.\nIf the perturbing Hamiltonian is not diagonal, then we must go through the diagonalization proce-\ndure to ﬁnd the new eigenvalues and eigenstates. The energy results are the same that we would have \nobtained if we had done nondegenerate perturbation theory using the “correct” basis, (i.e., the one we \nhave found to diagonalize the perturbation). So in some sense we have merely found the “right” basis \nin which nondegenerate perturbation theory is valid.\nYou might ask whether we have found the exact solution and not just an approximation. The \nanswer is no. By diagonalizing the perturbation Hamiltonian only within the degenerate subspace, we \nneglect other terms in the perturbation Hamiltonian that connect the degenerate states with other states \nin the system [Eq. (10.104)]. That we can safely neglect these other states in degenerate perturbation \ntheory is suggested by the second-order energy correction in Eq. (10.81), which says that states farther \naway contribute less to perturbation. Degenerate states are the closest to each other and hence contrib-\nute the most to the corrections.\n10.6 \u0002 MORE EXAMPLES\nLet’s apply perturbation theory to some of our favorite systems.\n10.6.1 \u0002 Harmonic Oscillator\nThe harmonic oscillator has zeroth-order Hamiltonian\n \nH0 = pn 2\n2m + 1\n2 mv2xn  2 = U v1a-a + 1\n22,  \n(10.120)\nwhere we have written it using ladder operators because that makes the calculations easier. Now con-\nsider adding a perturbation to the Hamiltonian of the form\n \nH\u0004 = e 1\n2 mv2 xn  2, \n(10.121)\nwhere \u0015 is a small dimensionless term parameterizing the strength of the perturbation. This perturba-\ntion has a parabolic spatial dependence, which is the same as the zeroth-order Hamiltonian, so the \nsolution is known exactly, but we proceed with the example to see how the method is applied. For \na positive value of \u0015, the strength of the harmonic well is increased, which conﬁnes the particle to a \nsmaller region of space. The uncertainty principle then leads us to expect an increase in the energy.\n",
    "344 \nPerturbation Theory\nThe energy levels of the harmonic oscillator are nondegenerate, so we use nondegenerate pertur-\nbation theory. The ﬁrst-order correction to the energy is the expectation value\n \nE (1)\nn\n= 8n(0)@H\u0004@ n(0)9. \n(10.122)\nTo calculate this, it is most convenient to express the perturbation Hamiltonian using ladder  operators \n[Eq. (9.95)]:\n \n H\u0004 = e 1\n2 mv2 a\nU\n2mvb1a- + a2\n2\n \n \n H\u0004 = e 1\n4 U v 1a-a- + a-a + aa- + aa2. \n(10.123)\nThe expectation value of the perturbation is\n \nE (1)\nn\n= e 1\n4 U v8n(0)@1a-a- + a-a + aa- + aa2@ n(0)9. \n(10.124)\nThe operators a-a- and aa contribute zero because they raise or lower the state @ n(0)9 twice and produce \na new state that is orthogonal to @ n(0)9. The remaining terms are calculated using a@ n9 = 2n0 n - 19\n and a-0 n9 = 2n + 10 n + 19 :\n \n E (1)\nn\n= e 1\n4 U v8n(0)@1a-a + aa-2@ n(0)9\n \n \n = e 1\n4 U v8n(0)@A 2n2n + 2n + 12n + 1B @ n(0)9 \n \n = e 1\n4 U v1n + n + 12\n \n \n = e 1\n2 U v1n + 1\n22.\n \n(10.125)\nThe resultant energy of level n to ﬁrst order in the perturbation is\n \n En = E (0)\nn\n+ E (1)\nn\n \n \n = U v1n + 1\n22 + e 1\n2 U v1n + 1\n22 \n \n = U v1n + 1\n2211 + 1\n2 e2.\n \n(10.126)\nEach state is shifted upwards, with the shift larger for larger states. The original and ﬁrst-order per-\nturbed energy levels are shown in Fig. 10.10(a).\nNow consider the second-order energy correction\n \nE (2)\nn\n= a\nm\u0002n\n@8n(0)@H\u0004@ m(0)9@\n2\n1E (0)\nn\n- E (0)\nm 2\n. \n(10.127)\nThis looks like an inﬁnite sum, which would be problematic, but we plow ahead and ﬁnd that the sum \nis reduced for the harmonic oscillator case. The matrix elements are\n \n 8n(0)@H\u0004@ m(0)9 = e 1\n4 U v8n(0)@1a-a- + a-a + aa- + aa2@ m(0)9\n \n \n = e 1\n4 U vC\n2m + 12m + 2dn,m+2 + 2m2mdn,m\n+ 2m + 12m + 1dn,m + 2m2m - 1dn,m-2\nS. \n(10.128)\n",
    "10.6  More Examples \n345\nFor a given energy level n, only two terms in the sum (m \u0002 n) contribute, yielding \n \n E (2)\nn\n=\nC1\n4 e U v2n - 12n D  \n2\nE (0)\nn\n- E (0)\nn - 2\n+\nC1\n4 e U v2n + 22n + 1 D  \n2\nE (0)\nn\n- E (0)\nn  + 2\n \n \n = 11\n4 eU v2\n2c n(n - 1)\n2U v\n+ (n + 1)(n + 2)\n-2U v\nd\n \n =\n1\n32 e2U v3n2 - n - 1n2 + 3n + 224\n \n = -1\n8 e2U v1n + 1\n22.\n \n(10.129)\nNote that the second-order contribution is negative. Only the two levels m = n + 2 and m = n - 2 \ncontribute to the energy correction in Eq. (10.129). They each have the same magnitude energy \ndenominators, but the matrix element is larger for the m = n + 2 state above the state of interest, so \nthe level is pushed down. The resultant energy of level n to second order in the perturbation is\n \n En = E (0)\nn\n+ E (1)\nn\n+ E (2)\nn\n \n \n = U v1n + 1\n2211 + 1\n2 e - 1\n8 e22. \n(10.130)\nThe perturbed energies to ﬁrst and second order and the exact result are plotted in Fig. 10.10(b) as a \nfunction of the perturbation strength.\nx\nE\n0.0\n0.5\n1.0\nΕ\n0.0\n0.2\n0.4\n(a)\n(b)\nE(1)\n2\nE(0)\n2\nE(1)\n1\nE(0)\n0\nE(1)\n0\nE(0)\n1\n\u0012E/E0\nFIGURE 10.10 Perturbation of the harmonic oscillator. (a) Shifts of the ﬁrst \nthree energy levels. (b) Dependence of the shift of the ground state energy on  \nthe perturbation strength to ﬁrst order (dotted), second order (dashed), and  \nexact (solid).\n",
    "346 \nPerturbation Theory\nIn this example, we can ﬁnd the exact answer, so we can check the perturbation result and conﬁrm \nthat perturbation theory works. The exact Hamiltonian is\n \n H = H0 + H\u0004\n \n \n = pn 2\n2m + 1\n2 mv2xn  2 + e 1\n2 mv2xn  2 \n \n = pn 2\n2m + 1\n2 mv2xn  211 + e2\n \n \n = pn 2\n2m + 1\n2 mv2\np\n xn  2,\n \n(10.131)\nwhere we have deﬁned a new perturbed harmonic frequency\n \nvp = v21 + e. \n(10.132)\nThis new Hamiltonian has the same form as the original harmonic oscillator problem we have already \nsolved, but with a new characteristic frequency. Hence, we know the energy eigenvalues exactly. \nThey are\n \nEn = 1n + 1\n22U vp = 1n + 1\n22U v21 + e. \n(10.133)\nThe perturbation theory result in Eq. (10.130) was obtained to second order in the perturbation param-\neter e, so we must compare it to the exact result at this same order. Expanding the exact result in powers\nof e gives\n \n En = 1n + 1\n22U v11 + e2\n1>2\n \n \n \n = 1n + 1\n22U v11 + 1\n2 e - 1\n8 e2 + ...2. \n(10.134)\nThus we see that the two results agree, at least to second order.\nNote that the operator approach made our life very easy here—there was no need to do any spatial \nintegrals. However, this is not always the case. For the harmonic oscillator problem, one can always \nuse the operator approach because the spatial dependence of the perturbing potential energy, no matter \nhow complicated, can be expressed as a polynomial in x and written in terms of the ladder operators. \nBut for other potential wells, we do not know how to write the spatial function in terms of any simple \noperators and a spatial integral is often required.\n10.6.2 \u0002 Stark Effect in Hydrogen\nNow consider a perturbation of the hydrogen atom. The Stark effect is the perturbation of energies \ncaused by an external electric ﬁeld. For hydrogen, this example gives us a chance to practice degener-\nate perturbation theory. The unperturbed Hamiltonian is the sum of kinetic and Coulomb potential \nenergies that we studied in Chapter 8:\n \n H0 = p2\n2m + V1r2\n \n \n = p2\n2m -\nZe2\n4pe0r . \n(10.135)\n",
    "10.6  More Examples \n347\nThe eigenstate solutions to this zeroth-order problem are labeled with quantum numbers n/m\n \n@ c(0)9 = @ n/m(0)9 \u0003 cn/m\n(0) 1r, u, f2 = Rn/1r2Y /\nm1u, f2 \n(10.136)\nand the eigenenergies are\n \nE (0)\nn\n= -  Z 2\nn2 Ryd . \n(10.137)\nThese energy levels are degenerate because the energy is not a function of / and m. Each energy level \nhas n2 states, so only the ground state with n \u0003 1 is nondegenerate. Thus, we expect to have to use both \nnondegenerate and degenerate perturbation theory.\nTo perturb the system, we apply a uniform electric ﬁeld of magnitude E. When we solved the \nhydrogen atom problem in Chapter 8, we used spherical coordinates because of the spherical symme-\ntry of the problem. The applied electric ﬁeld has a speciﬁc direction in space and breaks the spherical \nsymmetry of the problem, but we continue to use spherical coordinates because we use the original \nbasis states in the perturbation solutions. Because the z-axis in spherical coordinates is special (the \npolar angle is measured from it and the azimuthal angle is measured about it), it is simplest to assume \nthat the applied ﬁeld is aligned along the z-axis: E = E zn.  The perturbation Hamiltonian is the poten-\ntial energy of the hydrogen atom in the applied ﬁeld. The electron and nucleus form an electric dipole, \nwhich the ﬁeld tries to orient along its direction. An electric dipole d has a magnitude given by the \nproduct of the charge (the positive and negative charges are assumed equal) and the displacement \nbetween the charges, and a direction pointing from negative to positive charge. We have placed the \norigin of our coordinate system at the nucleus, so r represents the location of the electron with respect \nto the nucleus. Thus the dipole moment of the atom is\n \nd = -er. \n(10.138)\nThe classical potential energy of an electric dipole in an electric ﬁeld is\n \nU = -d~E. \n(10.139)\nThe quantum mechanical potential energy is obtained by using this same expression as long as we \nclarify what the proper operators are. In this case only the position r is an operator. The charge and the \napplied electric ﬁeld are parameters. Thus the Hamiltonian representing the perturbation is\n \n H\u0004 = -d~E\n \n \n = -1-er2~E zn \n \n = eE z\n \n \n = eE r cos u.\n \n(10.140)\nLet’s ﬁrst consider the ground state of hydrogen. This state is nondegenerate, so we use nonde-\ngenerate perturbation theory. The ﬁrst-order perturbed energy is the expectation value of the perturba-\ntion in the state:\n \n E (1)\n1\n= 8100(0)@H\u0004@ 100(0)9\n \n \n = 8100(0)@eE z@ 100(0)9\n \n \n = eE 8100(0)@ z@ 100(0)9\n \n \n = eE\nL\nz@ c(0)\n100 1r, u, f2 @\n2dV. \n(10.141)\n",
    "348 \nPerturbation Theory\nThe expectation value of z is zero in the ground state because the function z has odd parity and the \nsquare of the wave function has even parity. The resultant integrand has odd parity and so yields zero \nwhen integrated over all space. To formally do the integral, one would use the substitution z = r cos u \nbecause the wave functions are in r, u, f, coordinates. The theta integral is the one that is zero, because \ncos u is odd with respect to u = p>2 (Problem 10.12).\nThe result of this calculation is that there is no ﬁrst-order (i.e., linear) Stark effect in the ground \nstate of hydrogen. As we saw in Chapter 8, all hydrogen atom eigenstates have deﬁnite parity, odd \nor even, yielding even wave function squares and zero expectation values of the electric dipole per-\nturbation. However, the degeneracy in the excited states of hydrogen means that a given energy state \nincludes states of differing parity, which permits a linear Stark effect, as we will see shortly. The \nabsence of a linear Stark effect in the ground state implies that the atom does not have a permanent \nelectric dipole moment in its ground state, because the expectation value of the perturbation that we \ncalculated in Eq. (10.141) is just the expectation value of the dipole moment d = -er times the value \nof the applied ﬁeld. Given the calculation in Eq. (10.141), we attribute that lack of dipole moment \nto the deﬁnite parity of the atomic wave function, which arises from the symmetry of the atomic \n system. We can now turn this whole argument on its end and say that if we measure the atom to have \na permanent electric dipole moment (by observing a linear Stark effect or by other means), then we \ncan conclude that parity is not an obeyed property of the atom. There is a whole cottage industry of \nexperiments designed to search for such effects because they indicate “parity violation.” Parity violat-\ning effects are attributed to the weak nuclear interaction and are usually studied in high-energy particle \ncollision experiments. Atomic parity violation experiments provide a unique opportunity for “low-\nenergy” physicists to do “high-energy”  measurements.\nThere is a second-order (i.e., quadratic) Stark effect in the ground state of hydrogen, but the \ncalculation is tedious because it involves an inﬁnite sum. We’ll skip that calculation and move on to \nexcited hydrogen states that require degenerate perturbation theory.\nThe n \u0003 2 state of the hydrogen atom is fourfold degenerate, with one 2s state and three 2p states. \nDegenerate perturbation theory tells us to diagonalize the perturbation Hamiltonian in the degenerate \nsubspace. To do this, we ﬁrst need to ﬁnd the matrix representing the Stark effect perturbation within \nthe subspace of the four degenerate states. The four n \u0003 2 states of hydrogen are \n \n0 2009 \u0003 c(0)\n2001r, u, f2 = R201r2Y 0\n01u, f2 =\n2\n12a023>2 a1 - r\na0\nbe -r>2a0 \n1\n24p\n \n \n0 2109 \u0003 c (0)\n2101r, u, f2 = R211r2Y 0\n11u, f2 =\n1\n4312a023>2 r\na0\n e -r>2a0\nB\n3\n4p cosu \n \n(10.142)\n \n0 21,{19 \u0003 c 21{1\n(0) 1r, u, f2 = R211r2Y 1\n{11u, f2 =\n{1\n4312a023>2 r\na0\n e -r>2a0\nB\n3\n8p e {ifsinu, \nand we need to calculate the 16 matrix elements\n \n82/m(0)@H\u0004@ 2/\u0004m\u0004(0)9 \n(10.143)\nto construct the matrix of the perturbation in the degenerate subspace.\nAs we saw in the ﬁrst-order ground state calculation above, the consideration of parity is impor-\ntant in evaluating the required matrix elements. The parity of the hydrogen atom wave functions is \ndetermined by the parity of the spherical harmonics Y m\n/ , which is (-1)/, independent of m. The Ham-\n",
    "10.6  More Examples \n349\niltonian for the perturbation has odd parity, so the matrix elements between states of the same parity \ngive an integrand that is odd and hence a zero integral. The only nonzero matrix elements are those \nbetween states of different parity, which are the s and p states. Thus we expect to do three integrals, \nbetween the s state and each of the three p states. However, we can reduce our task even further by \nconsidering the f part of the integrals for these cases. There is no f dependence in the perturbation \nHamiltonian, so the matrix elements in Eq. (10.143) have azimuthal integrals\n \nL\n2p\n0\ne -imfeim\u0004fdf =\nL\n2p\n0\nei(m\u0004-m)fdf. \n(10.144)\nThis integral is zero unless the magnetic quantum numbers m and m\u0004 are the same, which only  happens \nfor the matrix element between the 2s state and the 2p0 state. Thus the only nonzero matrix element in \nthe degenerate subspace is\n \n8210(0)@H\u0004@ 200(0)9 \u0002 0, \n(10.145)\nwhere the O’s are different but the m’s are the same. Thus the matrix representing the perturbation \nHamiltonian in the degenerate subspace has the form\n \nn\n2\n2\n2\n2\n/\n0\n1\n1\n1\nm\n 0\n1\n0\n-1\n \n \nH\u0004 \u0003 §\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n¥, \n(10.146)\nwhere the columns are labeled with n/m (of course one must use the same labeling order for both rows \nand columns), and the nonzero elements are boxed.\nNow we do the integral to ﬁnd the nonzero matrix elements\n \n8200(0)@ H\u0004@210(0)9 =\n \nL\nc (0)*\n2001r, u, f2eE r cosu c (0)\n2101r, u, f2r 2 sinu dr du df\n \n= eE \n2\n12a023>2 \n1\n24p\n \n1\n2312a023>2 B\n3\n4p L\n\u0005\n0\n a1 - r\na0\nb e-r>a0 r 4dr \n \nL\np\n0\ncos2 u sinu du\nL\n2p\n0\n df.  \n(10.147)\nThe u and f angular integrals are straightforward and give 2/3 and 2\u000b , respectively (Problem 10.13). \nDoing the radial integral yields the ﬁnal result\n \n 8200@ H\u0004@ 21094 = eE \n2\n12a02\n3 1\n4p 2\n3 2pc\nL\n\u0005\n0\nr4e-r>a0 dr -\n1\n2a0 L\n\u0005\n0\nr 5e-r>a0 drd\n \n = eE 1\n6a 2\n0\n c4!a 5\n0 -\n1\n2a0\n5!a6\n0d\n \n(10.148)\n \n = -3eEa0.\n",
    "350 \nPerturbation Theory\nNow we have the matrix representing the perturbation Hamiltonian in the original basis within \nthe n \u0003 2 subspace, and the recipe we have for degenerate perturbation theory tells us to diagonalize \nthis matrix. It is convenient to reorder the rows and columns of the matrix in a way that makes the \nmathematics of diagonalization easier and the physics of the perturbation more obvious:\n \nH\u0004 \u0003 §\n0\n-3eEa0\n0\n0\n-3eEa0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n¥ \n200\n210\n211\n21,-1\n. \n(10.149)\nBe careful to note the new labeling of rows and columns. Only the 2s and 2p0 states are connected by \nthe perturbation; the 2p{1 states are not affected by the perturbation and their energies are unchanged. \nWe diagonalize the perturbation Hamiltonian to get the energies and states:\n \n4\n-l\n-3eEa0\n0\n0\n-3eEa0\n-l\n0\n0\n0\n0\n-l\n0\n0\n0\n0\n-l\n4 = 0 \n \nCl2 - A3eEa0B\n2Dl2 = 0\n \n \nl = {3eEa0, 0, 0.\n \n(10.150)\nAs we said, the 2p{1 states are not shifted, and those eigenstates remain the same. The 2s and 2p0 \nstates are mixed by the perturbation, with the normalized states from the diagonalization being \n(Problem 10.14):\n \n@ c+I =\n1\n12 C @200I  - @210ID  \n \n@ c-I =\n1\n12 C @200I  + @210ID , \n(10.151)\nwhere the { subscript on the states corresponds to the energies E { = {3eEa0 . The perturbed \nenergy states are shown in Fig. 10.11. Note that the perturbation has lifted some of the degeneracy, but \nnot all of it; two states remain degenerate.\nE\n\bE\n\u00073eEa0\n\u00070\n\u0007\t3eEa0\nE(1)\n+\nE(1)\n0\nE(1)\n\nFIGURE 10.11 Stark effect in the hydrogen n = 2 state.\n",
    "Summary \n351\nThe shifts of the two superposition states in Eq. (10.151) are linear in the applied ﬁeld because the \ncombined s and p state has an electric dipole moment. This is allowed because it is not a state of deﬁ-\nnite parity. The 0 c-9 state of lower energy is the state we studied in Fig. 8.9(b), which has an electric \ndipole moment pointing in the ﬁeld direction (Problem 10.15). This is what we expect from the clas-\nsical model of an electric dipole moment that aligns with the electric ﬁeld in its lowest energy state.\nSUMMARY\nPerturbation theory allows us to calculate the effects of adding new terms to the Hamiltonian of a \nsystem we have already solved exactly. The zeroth-order Hamiltonian with exact solutions obeys the \neigenvalue equation\n \nH0@ n(0)9 = E (0)\nn @ n(0)9. \n(10.152)\nThe system is perturbed by the addition of a new term H\u0004, and the new Hamiltonian has the energy \neigenvalue equation\n \n1H0 + H\u000420 n9 = En0 n9. \n(10.153)\nThe approximate solutions to this equation are expressed as a series in increasing orders of the strength \nof the perturbation:\n \n En = E (0)\nn\n+ E (1)\nn\n+ E (2)\nn\n+ ...\n \n \n 0 n9 = @ n(0)9 + @ n(1)9 + @ n(2)9 + ... . \n(10.154)\nThe ﬁrst-order energy correction in nondegenerate perturbation theory is the expectation value of \nthe perturbation in the state\n \nE (1)\nn\n= H =\nnn = 8n(0)@H\u00040 n(0)9. \n(10.155)\nIn wave function notation, the expectation value is expressed as an integral\n \nE (1)\nn\n= H =\nnn =\nL\nw(0)*\nn 1r2 H\u0004w(0)\nn 1r2dV , \n(10.156)\nwhere w(0)\nn 1r2 are the energy eigenstates of the zeroth-order Hamiltonian. The ﬁrst-order eigenstate \ncorrection is\n \n@ n(1)9 = a\nm\u0002n\n 8m(0)@H\u0004@ n(0)9\nAE (0)\nn\n- E (0)\nm B\n @ m(0)9.  \n(10.157)\nThe second-order energy correction is\n \nE (2)\nn\n= a\nm\u0002n\n \n@8n(0)@H\u0004@ m(0)9@\n2\nAE (0)\nn\n- E (0)\nm B\n. \n(10.158)\nFor degenerate states, we must use degenerate perturbation theory, which tells us to diagonalize \nthe perturbation Hamiltonian in the degenerate subspace.\n",
    "352 \nPerturbation Theory\nPROBLEMS\n 10.1 Diagonalize the Hamiltonian in Eq. (10.3) and conﬁrm the energy eigenvalues in Eq. (10.9).\n 10.2 Show that the assumed power series expansions in Eq. (10.34) lead to the set of equations \n(10.35) through (10.38).\n 10.3 Assume a 3-state quantum mechanical system and use the matrix approach of Section 10.3 \nto explicitly show that the ﬁrst-order energy shift of the n = 2 state is given by \nE\n (1)\n2\n= H =\n22 = 82(0)@H\u0004@ 2(0)9.\n 10.4 Do the explicit integral in Eq. (10.71) to conﬁrm the result in Eq. (10.72).\n 10.5 Find the ﬁrst-order eigenstate corrections to the ground state of the asymmetric square well of \nExample 10.2.\n 10.6 Show that the eigenstates correct to ﬁrst order in Example 10.3 are given by Eq. (10.91).\n 10.7 The nitrogen nucleus has spin 1 and a gyromagnetic ratio gN = 0.404. A nitrogen nucleus is \nplaced in a constant magnetic ﬁeld in the z-direction B0 = B0zn . An additional, perturbative \nmagnetic ﬁeld B\u0004 = B1zn is applied to the system. Find the ﬁrst-order energy shifts due to the \nperturbation. Plot your results as a function of the perturbing ﬁeld strength, assuming that the \nconstant ﬁeld is B0 = 2.35 Tesla.\n 10.8 The nitrogen nucleus has spin 1 and a gyromagnetic ratio gN = 0.404. A nitrogen nucleus is \nplaced in a constant magnetic ﬁeld in the z-direction B0 = B0zn . An additional, perturbative \nmagnetic ﬁeld B\u0004 = B2xn is applied to the system. Find the second-order energy shifts due to \nthe perturbation. Plot your results as a function of the perturbing ﬁeld strength, assuming that \nthe constant ﬁeld is B0 = 2.35 Tesla.\n 10.9 An electron is bound to move on the surface of a sphere. Find the energy corrections caused \nby a perturbing magnetic ﬁeld B\u0004 = B1yn . Identify the “correct” zeroth-order basis.\n 10.10 Consider a particle bound in the harmonic oscillator potential V1x2 = 1\n2 mv2x 2. A perturbation \nH\u0004 = gx 3 is applied to the system.\na) Calculate the ﬁrst-order corrections to the energies.\nb) Calculate the second-order corrections to the ﬁrst three energy levels.\nc) Find the ﬁrst-order corrections to the eigenstates for these three states.\n10.11 Consider a particle bound in the harmonic oscillator potential V1x2 = 1\n2 mv2x 2. A perturba-\ntion H\u0004 = h x4 is applied to the system. Calculate the ﬁrst-order corrections to the energies.\n10.12 Conﬁrm by explicit integration of Eq. (10.141) that the linear Stark shift of the hydrogen \nground state is zero.\n10.13 Do the angular integrals in Eq. (10.147) and conﬁrm the results quoted in the text.\n10.14 Find the eigenstates of the perturbation Hamiltonian in Eq. (10.149) and verify the results in \nEq. (10.151).\n10.15 Calculate the expectation value of the electric dipole moment for the lower energy state 0 c-9 \nin Eq. (10.151) and verify that the moment is aligned with the ﬁeld.\n10.16 Consider the inﬁnite square well with the shelf perturbation shown in Fig. 10.6(b). Calculate \nthe second-order energy shift of the ground state.\n10.17 Consider the inﬁnite square well shown in Fig. 10.6(a). Add a linear “ramp”  perturbation \nH\u0004 = V1x2 = bx for 0 6 x 6 L to the system and ﬁnd the ﬁrst-order energy shift of the \nground state.\n",
    "Problems \n353\n10.18 Consider an inﬁnite square well potential with walls at x = 0 and x = L; that is, \nV1x2 = 0 for 0 6 x 6 L; V1x2 = \u0005 otherwise. Now impose a perturbation on this poten-\ntial of the form H\u0004 = L V0 d1x - L>2), where d(x) is the Dirac delta function.\na) Calculate the ﬁrst-order correction to the energy of the nth state of the inﬁnite well.\nb) Give some physical insight into why your answer is different for even and odd values of n.\nc) The ground state wave function is modiﬁed under the inﬂuence of the perturbation. \n Calculate the largest contribution to the ﬁrst-order correction. (In other words, which state \nis mixed in the most?)\n \n Now consider the case where we impose a perturbation on the inﬁnite square well potential as \nshown in Fig. 10.12, with \u0015 a small number.\nd) Calculate the ﬁrst-order correction to the energy of the ground state of the inﬁnite well.\ne) In the limit where \u0015 goes to zero, compare your answer to (d) with the answer in (a). Discuss.\n10.19 Calculate the ﬁrst-order energy corrections for all levels of an inﬁnite square well potential \nwith a perturbation H\u0004 = V0 sin1px>L2.\n10.20 Calculate the ﬁrst-order energy corrections for all levels of an inﬁnite square well potential \nwith a perturbation H\u0004 = gx1L - x 2. \n10.21 Consider a charged particle bound in the harmonic oscillator potential V1x2 = 1\n2 mv2x2. A \nweak electric ﬁeld E is applied to the system such that the potential energy is shifted by an \namount H\u0004 = -qE x.\na) Calculate the energy levels of the perturbed system to second order in the small  perturbation.\nb) Show that the perturbed system can be solved exactly by completing the square in the \nHamiltonian. Compare the exact energies with the perturbation results found in (a).\n10.22 Extend the Stark effect calculation in Section 10.6.2 to the n = 3 state of hydrogen. The \n symmetry and azimuthal integral arguments allow you to reduce the 81 = 9 * 9 required \nmatrix elements to only 8 non-zero matrix elements and 4 necessary integrals. Find the \n perturbed energies and the new preferred basis. Discuss the electric dipole moments of the \nnew states.\n0\nL\n2\nL\nL\n2 \u0006 2\nL\n2 \u000b \fL\n\fL\n2\nx\nV(x)\nV0\n\u0005\n\f\nFIGURE 10.12 Perturbed square well.\n",
    "354 \nPerturbation Theory\n10.23 Consider a quantum system with three states and a Hamiltonian given by\n \nH \u0003 V0\n £\n1\n2e\n0\n2e\n1\n3e\n0\n3e\n4\n≥, \n \n where V0 is a constant and e is a small number (\u0015 V 1) that characterizes the perturbation of \nthe system.\na) Write down the eigenvectors and eigenvalues of the unperturbed Hamiltonian (\u0015 \u0003 0).\nb) Find the leading correction to the energy of the state that is nondegenerate in the \n zeroth-order Hamiltonian.\nc) Use degenerate perturbation theory to ﬁnd the ﬁrst-order corrections to the two initially \ndegenerate energies.\nd) Plot the results of (b) and (c) as a function of the parameter e and discuss your results.\n10.24 Consider a quantum system with four states and a Hamiltonian given by\n \nH \u0003 V0\n §\n3\ne\n0\n0\ne\n3\n2e\n0\n0\n2e\n5\ne\n0\n0\ne\n7\n¥ , \n \n where V0 is a constant and \u0015 is a small number (\u0015 V 1) that characterizes the perturbation of \nthe system.\na) Write down the eigenvectors and eigenvalues of the unperturbed Hamiltonian (\u0015 \u0003 0).\nb) Use perturbation theory to ﬁnd corrections to the energy of each energy eigenstate. Find \nthe ﬁrst nonvanishing order for each state. \n",
    " \n355\nC H A P T E R \n11\nHyperﬁne Structure and the \nAddition of Angular Momenta\nSpectroscopy of the hydrogen atom reveals structure in the energy levels beyond the 1/n2 pattern \ndetermined by the Coulomb interaction between the electron and proton. These additional energy lev-\nels arise from a variety of perturbations to the zeroth-order Coulomb interaction Hamiltonian. In this \nchapter, we focus on the hyperﬁne structure, so named because its effects are smaller than another \neffect called the ﬁne structure. Studying hyperﬁne structure in the hydrogen atom gives us a chance to \nuse perturbation theory to calculate an important energy and also gives us a chance to learn some new \nangular momentum tools. In Example 10.4, we found that we could have solved the problem more \neasily if we had chosen the “correct” basis at the start of the problem. The “correct” basis is not so \nmuch correct as it is convenient because the perturbation Hamiltonian is already diagonal in that basis \nand we avoid the tedious diagonalization required by degenerate perturbation theory. The hyperﬁne \nstructure calculation presents us with a similar scenario, but not yet knowing how to choose the most \nconvenient basis, we will solve the hyperﬁne problem by brute force in the “inconvenient” basis and \nthen analyze the results to learn how to choose bases. The two bases for the hyperﬁne problem are the \nuncoupled and coupled bases, which refer to the coupling or addition of angular momenta. The \ntheory of the addition of angular momenta complements perturbation theory to allow us to more easily \nsolve for the many rich details in the hydrogen atom, which we will continue in the next chapter.\n11.1 \u0002 HYPERFINE INTERACTION\nThe hyperﬁne interaction between the electron and the nucleus arises from higher electromagnetic \nmultipole moments of the nucleus, beyond the electric monopole moment (i.e., charge) that is already \nincluded in the Coulomb interaction. The dominant hyperﬁne effect is due to the magnetic moment of \nthe nucleus and its interaction with the internal magnetic ﬁelds in the atom caused by the electron’s \norbital motion and by the electron’s spin magnetic moment. The intrinsic magnetic moment of the \nelectron associated with its spin is\n \nMe = -ge e\n2me\n S = -ge\n mB S\nU , \n(11.1)\nwhere the gyromagnetic ratio ge is approximately 2 and the Bohr magneton is mB = e U>2me. The \nproton is also a spin-1/2 particle and has an associated magnetic moment. We label the proton spin, \nand nuclear spin in general, as I, so the intrinsic magnetic moment of the proton is\n \nMp = gp e\n2mp\n I = gp\n mN I\nU, \n(11.2)\n",
    "356 \nHyperﬁne Structure and the Addition of Angular Momenta\nwhere the nuclear magneton is mN = e U>2mp. The gyromagnetic ratio gp of the proton is 5.59, which \narises from the composite quark structure of the proton. The proton mass is 1836 times larger than the \nelectron mass, so the proton magnetic moment is approximately three orders of magnitude smaller \nthan the electron magnetic moment. This factor is responsible for the small scale of the hyperﬁne \nstructure.\nThe hyperﬁne interaction Hamiltonian is\n \nH =\nhf = Mp~ m0\n4p eL\nmr 3 + m0\n4p 1\nr 3 cMe~Mp - 3 \n1Me~r21Mp~r2\nr 2\nd - m0\n4p 8p\n3 Me~Mpd1r2. \n(11.3)\nThe ﬁrst term represents the interaction between the proton magnetic moment and the magnetic ﬁeld \narising from the electron’s orbital angular momentum. The second term is the interaction between the \ntwo magnetic dipoles for the case r \u0002 0. The third term is the same dipole-dipole interaction for the \ncase r = 0 and is often called the Fermi contact interaction. Substituting the electron and proton spin \noperators into Eq. (11.3), we obtain the hyperﬁne Hamiltonian for the hydrogen atom:\n \nH =\nhf = m0\n4p \nge\n mB\n gp\n mN\nU2\n c 1\nr 3 I~L - 1\nr 3 S~I + 3\nr 5 1S~r21I~r2 + 8p\n3\n S~Id1r2d . \n(11.4)\nWe limit our discussion to the 1s ground state of hydrogen. For s states, the ﬁrst three terms of \nEq. (11.4) are zero, and only the Fermi contact term of the hyperﬁne Hamiltonian need be considered. \nHence, the hyperﬁne Hamiltonian for the ground state of hydrogen is\n \nH =\nhf = m0\n4p \nge\n mB\n gp\n mN\nU2\n 8p\n3\n S~Id1r2. \n(11.5)\nPerturbation theory requires us to take matrix elements of the perturbation Hamiltonian. The hyperﬁne \nHamiltonian has space and spin dependence, so the matrix elements have the form\n \n8space08spin0  m0\n4p \nge\n mB\n gp\n mN\nU2\n 8p\n3\n S~Id1r2 0\n spin9 0\n space9, \n(11.6)\n which can be factored into spin and space parts\n \nm0\n4p \nge\n mB\n gp\n mN\nU2\n 8p\n3\n 8space 0 d 1r2 0\n space98spin 0 S~I0\n spin9. \n(11.7)\nThe spatial matrix element in the ground state of hydrogen\n \n 8space 0 d 1r2 0\n space9 =\nLspace\n c*\n1s\n 1r, u, f2d 1r2c1s\n 1r, u, f2d 3r \n(11.8)\n \n = 0 c1s\n 102 0\n2\n \n",
    "11.2 Angular Momentum Review \n357\nresults in the probability density at the origin, which we found in Chapter 8 [Eq. (8.77)]. But we don’t \nknow how to calculate the spin matrix elements because we neglected to include the spins of the \nelectron and proton in the solution of the energy eigenstates in Chapter 8. Developing the tools to ﬁnd \nthese spin matrix elements is one of the goals of this chapter.\nUsing the result in Eq. (11.8), we simplify the hyperﬁne Hamiltonian for the ground state of \nhydrogen to just the spin aspect\n \nH =\nhf = A\nU2 S~I  ,  \n(11.9)\nwhere the constant A has dimensions of energy and includes the spatial integral:\n \nA = 2m0\n3\n ge\n mB\n gp\n mN0 c1s\n 1020\n2. \n(11.10)\nFor hydrogen, A is less than one-millionth of the Rydberg energy. The hyperfine interaction \nHamiltonian in Eq. (11.9) has the form you might expect classically. Two classical magnetic dipoles \ntend to align themselves in the same direction when you put them on top of each other, as depicted in \nFig. 11.1. The electron magnetic moment is opposite to its spin, so the aligned magnetic dipoles cor-\nrespond to antialigned electron and proton spins, which according to the Hamiltonian in Eq. (11.9) \nlowers the energy.\n11.2 \u0002 ANGULAR MOMENTUM REVIEW\nBefore we embark on ﬁnding the matrix elements of the hyperﬁne Hamiltonian, a quick review of \nangular momentum is in order. You have studied spin angular momentum S and orbital angular \nmomentum L. They have different physical origins—spin angular momentum is an intrinsic prop-\nerty of fundamental particles while orbital angular momentum depends on the state of motion of a \nparticle—but they share many similarities. There are many instances where the physical origin of \nthe angular momentum is not important, and we refer to a generalized angular momentum, which we \nsymbolize by J.\nelectron\nproton\n\u0004e\n\u0004p\nI\nS\nFIGURE 11.1 Hyperﬁne interaction between electron and \nproton magnetic moments.\n",
    "358 \nHyperﬁne Structure and the Addition of Angular Momenta\nThe eigenvalue equations for a generalized angular momentum have the same form as spin and \norbital angular momentum eigenvalue equations\n \n J2@\n jmj9 = j1j + 12U2@\n jmj9  \n \n Jz@\n jmj9 = mj U@\n jmj9   ,  \n(11.11)\nwhere @\n jmj9 are simultaneous eigenstates of the commuting operators J2 and Jz and are labeled with the \nrespective eigenvalues. The angular momentum quantum number j can be any integer or half integer. \nThe magnetic quantum number mj is restricted to the values\n \nmj = -j, -j + 1,....., j -1, j \n(11.12)\nfor a given value of j. This yields 2j +1 possible mj states for each j value. Because all angular momenta \nobey the eigenvalue equations (11.11), they exhibit the same spectra of angular momentum quantum \nnumbers. For example, s = 1 and / = 1 both have three possible component states ms = 1,0,-1 and \nm/ = 1,0,-1. In the general case, a given value of j yields a spectrum or manifold of mj states, as \nshown in Fig. 11.2.\nThe mathematical rules for generalized angular momentum are the same as those we have learned \nfor spin and orbital angular momentum. Speciﬁcally, the rectangular components of angular momen-\ntum do not commute with each other:\n \n3Jx , Jy4 = iUJz  \n \n3Jy , Jz4 = iUJx  \n \n3Jz , Jx4 = iUJy. \n(11.13)\nNote that these commutation relations are cyclic in xyz. As we discussed in Section 2.5, the angular \nmomentum commutation relations imply that we cannot measure two different components simulta-\nneously, so we cannot know the direction of the angular momentum vector. We can, however, know \nthe square of its magnitude J2. The operator J2 commutes with each of the angular momentum com-\nponents (see Section 2.6)\n \n3J2, Jx4 = 3J2, Jy4 = 3J2, Jz4 = 0 \n(11.14)\nso we can simultaneously measure the magnitude of the vector and its projection along one axis.\nmj \u0004\u0007j\nmj \u0004\u0007j \n\u00071\nmj \u0004\u0007j \n\u00072\nmj \u0004\u0007\n\u0007j \n\u00071\nmj \u0004\u0007\n\u0007j\nFIGURE 11.2 Manifold of angular momentum component eigenstates for a given j.\n",
    "11.3 Angular Momentum Ladder Operators \n359\nWe can represent angular momentum operators as matrices in the angular momentum basis (vs. \nthe position or momentum basis). It is the standard convention to write separate matrices for each \nparticular value of j (i.e., each j subspace), and use the eigenstates of the angular momentum compo-\nnent operator Jz as the basis. Each j matrix has 2j +1 rows and columns, corresponding to the number \nof possible mj component states. The matrices do not distinguish between spin and orbital angular \nmomentum. Thus we get, for example:\n \n j = 1\n2 1  J2 \u0003 3\n4\n U2 a1\n0\n0\n1b,         Jz \u0003 U\n2 a1\n0\n0\n-1b \n \n j = 1 1  J2 \u0003 2U2 °\n1\n0\n0\n0\n1\n0\n0\n0\n1\n¢,  Jz \u0003 U °\n1\n0\n0\n0\n0\n0\n0\n0\n-1\n¢.\n \n(11.15)\n11.3 \u0002 ANGULAR MOMENTUM LADDER OPERATORS\nThe manifold of angular momentum component states in Fig. 11.2 is very similar to the harmonic \noscillator problem (see Fig. 9.4), where the energy levels are labeled with n, which is the eigenvalue of \nthe number operator N. Similarly, the angular momentum states in Fig. 11.2 differ by one unit of the \nmagnetic quantum number mj (we don’t know anything about the energy in the angular momentum \ncase yet). In the harmonic oscillator problem, we found a pair of ladder operators that connected the \ndifferent energy states and that proved very useful. For angular momentum, there are similar ladder \noperators that connect the states within a given j manifold (see Problem 7.26). The angular momentum \nladder operators are deﬁned as\n \nJ+ = Jx + iJy\n \nJ- = Jx - iJy  . \n(11.16)\nThese new operators are analogous to the raising and lowering operators a- and a of the harmonic \noscillator. Like a- and a, J+ and J- are not Hermitian, so they do not represent physical observables. \nThey are Hermitian conjugates of each other\n \nJ+ = J -\n-, \n(11.17)\nand they do not commute with each other:\n \n3J+, J-4 = 2UJz, \n(11.18)\nbut they do commute with J2:\n \n3J2, J{4 = 0. \n(11.19)\nThe important commutation relations for these new angular momentum ladder operators are \n(Problem 11.1)\n \n3Jz, J+4 = +UJ+\n \n3Jz, J-4 = -UJ- , \n(11.20)\n",
    "360 \nHyperﬁne Structure and the Addition of Angular Momenta\nwhich are analogous to the relations\n \n3H, a-4 = +U va- \n \n3H, a4 = -U va  \n(11.21)\nfrom the harmonic oscillator problem [Eqs. (9.24) and (9.25)].\nIn the harmonic oscillator problem, we used the commutation relations in Eq. (11.21) to show \nthat a- and a raise and lower, respectively, the energy by one energy quantum U v and hence change \nthe label n. The physical requirement that the energy of the harmonic oscillator cannot be negative \nyielded the termination condition for the ladder, a0 09 = 0, which resulted in the energy spectrum \nEn = 1n + 1\n22U v. In the angular momentum case, the commutation relations in Eq. (11.20) simi-\nlarly imply that J+ and J- raise and lower, respectively, the angular momentum component by one \nquantum U and hence change the label mj (Problem 11.2). The physical condition that limits the \nextent of the angular momentum ladder of states is that the angular momentum component Jz can-\nnot be greater than the magnitude of J, which is the square root of J2. This physical condition leads \nto the conclusion that the top and bottom states of the angular momentum manifold for a given j are \nmj = {j, with the termination equations (Problem 11.3)\n \nJ+ 0\n j j9 = 0\n \nJ- 0\n j, -j9 = 0. \n(11.22)\nIn the angular momentum case, the operator J2 provides an additional label j, in contrast to the \nsingle label n for the harmonic oscillator states. But the ladder operators J+ and J- commute with J2 \n[Eq. (11.19)], so their action does not change the j label, only the mj label. Hence, the ladder operators \ndo not connect manifolds with different j values. That is why we commonly restrict our attention to a \nmanifold of mj states for a given j, such as in Fig. 11.2. The actions of the ladder operators are sum-\nmarized by the equation (Problem 11.2):\n \nJ{ 0  jmj9 =  U3j1 j +12 - mj1mj{124\n1>20  j, mj{19  . \n(11.23)\nSimilar to the harmonic oscillator ladder operators, the angular momentum ladder operators do not \npreserve the normalization of states. In addition, the angular momentum ladder operators have dimen-\nsions of U, as evidenced in Eq. (11.23). An example of the manifold or ladder of angular momentum \nstates for j = 2 is shown in Fig. 11.3. This view of the ladder of angular momentum states will be \nuseful when we apply perturbation theory to these states.\nJ\nJ\u000f\nJ\u000f\nJ\u000f\nJ\u000f\nJ\nJ\nJ\n\u00022,2\u0003\u0007\n\u00022,1\u0003\u0007\n\u00022,0\u0003\u0007\n\u00022,\n1\u0003\u0007\n\u00022,\n2\u0003\u0007\nFIGURE 11.3 Manifold or ladder of angular momentum states for j = 2.\n",
    "11.4 Diagonalization of the Hyperﬁne Perturbation \n361\n11.4 \u0002 DIAGONALIZATION OF THE HYPERFINE PERTURBATION\nWe are now ready to ﬁnd the matrix elements of the hyperﬁne Hamiltonian that we need in order to \napply perturbation theory to the ground state of hydrogen. The full quantum state vector includes the \nspatial wave function and the spin vectors for the electron and proton. For example, if the electron spin \nis up and the proton spin is down, then the atomic state vector is\n \n0 c1s9 \u0003  c1s 1r, u, f2 0  +9e 0  -9p, \n(11.24)\nwhere subscripts distinguish the electron and proton spin vectors. We have already used the spatial \nwave function to reduce the hyperﬁne Hamiltonian to a simple form in Eq. (11.9), so we limit our dis-\ncussion to the spin vectors. The electron (s = 1/2) and proton (I = 1/2) are both spin-1/2 particles, so \nthe individual spin states satisfy the eigenvalue equations:\n \nSz0{9e = { U\n2\n 0{9e , S20{9e = 3U2\n4\n 0{9e\n \nIz0{9p  = { U\n2\n 0{9p , I20{9p = 3U2\n4\n 0{9p. \n(11.25)\nDue to the ﬂexibility of Dirac notation, we can write the state 0  +9e 0  -9p many equivalent ways—what \nyou put inside the ket symbol is just a mnemonic label. We can specify all the quantum numbers in \neach ket:\n \n0  +9e 0  -9p = @ s = 1\n2 , ms = 1\n29@ I = 1\n2 , mI = -1\n29 \n(11.26)\nor we can write a single ket for the whole system, with all quantum numbers speciﬁed:\n \n 0  +9e 0  -9p = @ s = 1\n2 , ms = 1\n2 , I = 1\n2 , mI = -  1\n29\n \n = @ s = 1\n2 , I = 1\n2 , ms = 1\n2 , mI = -  1\n29 \n(11.27)\nor we can suppress the spin quantum numbers s and I because we know that they do not change:\n \n 0  +9e 0  -9p = @ ms = 1\n2 , mI = -  1\n29\n \n = 0  + -9.\n \n(11.28)\nIn the last case, we use the ﬁrst symbol for the electron and the second symbol for the proton, so that \n0  + -9 and 0  - +9 are distinct states.\nThe electron-proton spin states exist in a new vector space obtained by combining the spaces for \nthe spins of the two individual particles. There are four possible ways to combine the electron and \nproton spin states, so this new vector space is spanned by four basis states:\n \n0  + +9 , 0  + -9 , 0  - +9 , 0  - -9. \n(11.29)\nThis basis, in which the states are eigenstates of both Sz and Iz, is called the “uncoupled” basis. The \nreason for its name will become clearer in the next section when we learn about the alternative, “cou-\npled” basis. The four possible spin combinations in Eq. (11.29) imply that the zeroth-order hydrogen \nground state is not nondegenerate as we learned in Chapter 8, but rather is fourfold degenerate. Hence, \nin order to use perturbation theory to ﬁnd the effect of the hyperﬁne interaction, we must use degener-\nate perturbation theory, which requires us to diagonalize a 4*4 matrix.\n",
    "362 \nHyperﬁne Structure and the Addition of Angular Momenta\nLet’s ﬁrst clarify how the spin operators act in this new four-dimensional vector space. Because \neach spin operator is associated with a speciﬁc particle, each operator acts only on those aspects of the \nsystem kets that are associated with that particle. For example, Sz is associated with electron spin only:\n \nSz0  + -9 = Sz0  +9e 0  -9p = 5Sz0  +9e60  -9p = e+ U\n2\n 0  +9ef 0  -9p = U\n2\n 0  + -9. \n(11.30)\nIn general, the spin component eigenvalue equations in the four-dimensional vector space are\n \n Sz0 ms\n mI9 = ms\n U0 ms\n mI9\n \n Iz0 ms\n mI9 = mI\n U0 ms\n mI9, \n(11.31)\nwhere the allowed magnetic quantum numbers are ms = {1/2 and mI = {1/2. Because these spin \noperators act only on their own parts of the states, they commute with each other (Problem 11.5).\nIn Chapter 2, we represented spin operators as 2*2 matrices. In this example, we have four basis \nstates, so we need 4*4 matrices. Using the eigenvalue equations (11.31), the new matrices represent-\ning the spin component operators are straightforward to derive (Problem 11.6 and activity on system \nof two spin-1/2 particles):\n \n Sz \u0003 U\n2 §\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n-1\n¥  \n+ +\n+ -\n- +\n- -\n \n(11.32)\n \n Iz \u0003 U\n2 §\n1\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n-1\n¥  \n+ +\n+ -\n- +\n- -\n  ,\nwhere we have been explicit about labeling the rows, and by inference the columns, with the four basis \nstates of the two-particle system using the labeling convention in Eq. (11.29).\nThe hyperﬁne Hamiltonian is proportional to the operator \n \nS~I = Sx Ix + Sy Iy + Sz Iz. \n(11.33)\nThe matrix representing this operator is off-diagonal because the Sx , Sy , Ix , and Ix operators are off-\ndiagonal. To make the matrix elements easier to calculate, it helps to rewrite S~I using the angular \nmomentum ladder operators (Problem 11.7):\n \nS~I = 1\n2 1S+\n I- + S-\n I+2 + Sz Iz. \n(11.34)\nRecall that the ladder operators yield zero when acting on the extreme states; for example\n \n S+ 0 + +9 = 0\n \n I- 0 + -9 = 0. \n(11.35)\n",
    "11.4 Diagonalization of the Hyperﬁne Perturbation \n363\nThus, the action of the ladder operators on the basis states yields only a few nonzero results. For \nexample, using Eq. (11.23), we ﬁnd (Problem 11.8)\n \n S+ 0  - +9 = U3s1s + 12 - ms1ms + 124\n1>20  + +9\n \n = U31\n2 3\n2 - 1-  1\n221-  1\n2 + 124\n1>20  + +9\n \n = U33\n4 + 1\n44\n1>20  + +9\n \n = U0  + +9.\n \n(11.36)\nThe action of S~I on the basis states @ ms\n mI9 is:\n \n S~I0  + +9 = 51\n2\n 1S+\n I- + S- I+2 + Sz Iz6 0  + +9\n \n = 51\n2\n 10 + 02 + 1\n2\n U 1\n2\n U6 0  + +9\n \n = 1\n4\n U20  + +9\n \n S~I0  - -9 = 51\n2\n 10 + 02 + 1-1\n2 2U1-1\n2 2U6 0  - -9\n \n = 1\n4\n U20  - -9\n \n(11.37)\n \n S~I0  + -9 = 0 + 1\n2\n UU0  - +9 + 1\n2\n U1-1\n2 2U0  + -9\n \n = 1\n4\n U2320  - +9 - 0  + -94\n \n S~I0  - +9 = 0 + 1\n2\n UU0  + -9 + 1\n2\n U1-1\n2 2U0  - +9\n \n = 1\n4\n U2320  + -9 - 0  - +94.\n \nProjecting these results onto the basis states yields the matrix representation\n \nS~I \u0003 U2\n4  •\n1\n0\n0\n0\n0\n-1\n2\n0\n0\n2\n-1\n0\n0\n0\n0\n1\nμ  \n+ +\n+ -\n- +\n- -\n  . \n(11.38)\nThe operator S~I is not diagonal in the 0 ms\n mI9 (uncoupled) basis, so the 0 ms\n mI9 states are not eigen-\nstates of S~I. The two extreme states 0  + +9 and 0  - -9 are eigenstates of S~I, but the states 0  + -9and \n0  - +9 are not.\nUsing the result in Eq. (11.38), the hyperﬁne perturbation Hamiltonian becomes\n \nH =\nhf \u0003 A\n4 •\n1\n0\n0\n0\n0\n-1\n2\n0\n0\n2\n-1\n0\n0\n0\n0\n1\nμ  \n+ +\n+ -\n- +\n- -\n  . \n(11.39)\n",
    "364 \nHyperﬁne Structure and the Addition of Angular Momenta\nDegenerate perturbation theory tells us to diagonalize the perturbation Hamiltonian in the degenerate \nsubspace. The hyperﬁne Hamiltonian is block diagonal, so we know two eigenvalues and eigenstates \nby inspection:\n \nE1 = A>4,  0 E19 = 0  + +9  \n \nE2 = A>4,  0 E29 = 0  - -9.\n \n(11.40)\nThe other two eigenvalues are found by diagonalizing the submatrix indicated by the box in \nEq. (11.39):\n \n2 -A>4-l\nA>2\nA>2\n-A>4-l\n2 = 0  \n \n1-A>4 - l2\n2 - 1A>22\n2 = 0\n \n \n1-A>4 - l2 = {1A>22\n \n(11.41)\n \nl = -A>4 { A>2\n \n \nl = b\nA>4\n-3A>4.\n \nThe resultant eigenstates are superpositions of the two states 0  + -9 and 0  - +9:\n \nE3\n = A>4,     0 E39 =\n1\n12 30  + -9 + 0  - +94 \n \nE4\n = -3A>4,  0 E49 =\n1\n12 30  + -9 - 0  - +94. \n(11.42)\nThe energy level diagram of this perturbation is shown in Fig. 11.4. The degeneracy has been \npartially lifted. The two spin-aligned states  0  + +9 and 0  - -9 have the same positive energy shift, \nas you might expect because the magnetic moments are anti-aligned. Perhaps surprisingly, the spin-\nanti-aligned states  0  + -9 and 0  - +9  combine into two different superposition states with different \nenergy shifts. There must be something different about those two superposition states, 0 E39 and 0 E49, \nthat is not yet obvious. We will address this in the next section. The energy difference between the two \nhyperﬁne levels is A, so that [using Eq. (11.10)]\n \n\u0006Ehf = A = 2m0\n3\n ge\n mB\n gp\n mN 0 c1s 1020\n2. \n(11.43)\nThe square of the hydrogen 1s wave function at the origin is [Eq. (8.77)]\n \n0 c1s 1020\n2 =\n1\npa 3\n0\n , \n(11.44)\nwhere a0 is the Bohr radius. Hence, the hyperﬁne splitting of the hydrogen ground state is\n \n \u0006Ehf = 2m0\n3  \nge\n mB\n gp\n mN\npa3\n0\n= a4mec2 4\n3\n ge\n gp a\nme\nmpb\n \n \n = 5.88 * 10-6 eV = h * 1420.4057517667(9) MHz. \n(11.45)\n",
    "11.5 The Coupled Basis \n365\nThe energy scale in hydrogen is set by the Rydberg energy 13.6 eV = h * 3.285 * 1015 Hz, so the \nhyperﬁne splitting is about a million times smaller. The transition between the two hyperﬁne states is \na magnetic dipole transition in the microwave region of the spectrum. The electric dipole transition is \nforbidden because the matrix element is zero due to parity (both states have the same 1s spatial wave \nfunction). The transition between the two hyperﬁne states has a wavelength of 21 cm, and has played a \npivotal role in radio astronomy. Because most of the elemental matter in the universe is in the form of \nhydrogen, observation of the 21-cm line is used to map the matter distribution in our galaxy and in the \nuniverse. The hydrogen hyperﬁne transition is also observed in the laboratory, where it is used as the \nactive transition in the hydrogen maser, which permits the extremely precise frequency measurements \nindicated by the precision of the energy separation quoted above. An analogous transition in cesium is \nthe basis of the atomic clock used in national standards laboratories.\n11.5 \u0002 THE COUPLED BASIS\nLet’s return to the question posed in the last section: What distinguishes the two states \n0 E39 = 30  + -9 + 0  - +94> 12 and 0 E49 = 30  + -9 - 0  - +94> 12 from each other? How can that \nlittle minus sign play such a large role in the energy of the two states? The answer to these questions \ncomes from considering what we do in applying degenerate perturbation theory. In the new basis of \nhyperﬁne energy eigenstates, the hyperﬁne Hamiltonian is diagonal. As we discussed when we derived \ndegenerate perturbation theory, if we are clever enough to choose the right basis for  representing the \nperturbation Hamiltonian at the start of a problem, then the matrix is already diagonal and the prob-\nlem is solved. So the question we really want to answer is: What is special about the new hyperﬁne \nbasis states, and how should we have known to choose that basis to start the problem rather than the \n(uncoupled) basis we did choose?\nWhen two classical magnetic dipoles interact, they exert torques on each other that try to align the \nmagnetic moments. Because of the torque, the angular momentum of each particle is not conserved. \nThe magnitude of each particle’s angular momentum stays the same, but the direction changes. The \nquantum mechanical manifestation of this is that the electron and proton spin observables S2 and I2 \n1s\n\u0002\u000f\u000f\u0003\u0007\n\u0002\n\n\u0003\u0007\n(\u0002\u000f\n\u0003\u0005\u000f\u0007\u0002\n\u000f\u0003)\u0007\n1\n√2\n(\u0002\u000f\n\u0003\u0005\n\u0007\u0002\n\u000f\u0003)\u0007\n1\n√2\n1420MHz\n3A/4\nA/4\nFIGURE 11.4 Hyperﬁne structure of the ground state of hydrogen.\n",
    "366 \nHyperﬁne Structure and the Addition of Angular Momenta\ncommute with the hyperﬁne Hamiltonian and the component observables Sz and Iz do not (Problem \n11.9). The quantum numbers s and I are thus “good” quantum numbers, but ms and mI are not good \nquantum numbers, which is evident in Eq. (11.42) because the 0 E39 and 0 E49 hyperﬁne eigenstates \ninvolve superpositions of eigenstates of the original 0 ms\n mI9 basis—the uncoupled basis. To ﬁnd the \ngood quantum numbers for this problem and hence the “correct” or convenient basis, we must look for \na conserved quantity.\nThe hyperﬁne interaction between the electron and proton arises from the torques the particles \nexert on each other. This is an internal torque, so the total angular momentum of the system is con-\nserved. Hence, the basis of eigenstates of the total angular momentum of the system of the two par-\nticles is the basis in which the hyperﬁne Hamiltonian is diagonal. If we had chosen that basis to start \nthe problem, then the hyperﬁne Hamiltonian would already be diagonal and the perturbation results \nwould come by inspection. Let’s now show this.\nIn the hydrogen ground state, the total angular momentum of the system is the sum of the spin \nangular momentum of the electron and the spin angular momentum of the proton because there is no \norbital angular momentum. For historical reasons, we label the total angular momentum F:\n \nF = S + I. \n(11.46)\nThis new total spin operator behaves like an angular momentum because it is a sum (or coupling) of \ntwo angular momenta, and obeys the commutation relations of an angular momentum. For example,\n \n 3Fx , Fy4 = 3Sx + Ix , Sy + Iy4\n \n \n = 3Sx , Sy4 + 3Sx , Iy4 + 3Ix , Sy4 + 3Ix , Iy4 \n(11.47)\n \n = iUSz + 0 + 0 + iUIz\n \n \n = iUFz,\n \nwhere we have used the fact that the operators for the electron and the proton commute with each \nother. Because the total angular momentum behaves like all other angular momenta, it must have a \nset of basis kets 0 FMF9 that are eigenstates of the total angular momentum operators F2 and Fz. The \nquantum number F is the total angular momentum quantum number of the two-particle system. The \nquantum number MF is the total magnetic quantum number of the two-particle system. We refer to \nthese new states 0 FMF9 as the coupled basis because they arise from coupling or adding together two \nangular momenta. We refer to the original 0 ms\n mI9 basis as the uncoupled basis because it uses quan-\ntum numbers from the individual angular momenta before we consider their coupling or addition.\nWe are familiar with the uncoupled basis 0 ms\n mI9, which means that we know the single-particle \nquantum numbers s, I, ms, and mI. But we do not yet know the quantum numbers F and MF of the \ncoupled basis, which characterize the two-particle system. How do we ﬁnd them? Well, how do we \nﬁnd any eigenstates and eigenvalues? We solve the eigenvalue equation. This will also tell us how the \ncoupled basis eigenstates 0 FMF9 relate to the uncoupled basis eigenstates 0 ms\n mI9.\nSo our task is to ﬁnd the eigenstates and eigenvalues of the total angular momentum operators \nF2 and Fz. We do this by diagonalizing the matrices representing these physical observables, so we \nneed to ﬁnd the matrices ﬁrst. We have already written down the matrices for Sz and Iz above, in the \nuncoupled basis we started with—the eigenstates 0 ms\n mI9 of the two individual particles. We’ll start \nwith that approach and ﬁnd the matrix representations of the total angular momentum operators in this \nbasis. Let’s ﬁrst consider the total angular momentum component operator Fz, which is\n \nFz = Sz + Iz. \n(11.48)\n",
    "11.5 The Coupled Basis \n367\nTo ﬁnd the matrix for Fz , we add the matrices in Eq. (11.32) to get\n \nFz \u0003 U •\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\nμ  \n+ +\n+ -\n- +\n- -\n  . \n(11.49)\nThe Fz operator is already diagonal in the uncoupled 0 ms\n mI9 basis—that was easy! This immediately \ntells us that the states 0 ms\n mI9 are eigenstates of Fz . This is also clear if we operate with Fz on the \n0 ms\n mI9 states:\n \n Fz0 ms\n mI9 = 1Sz + Iz20 ms\n mI9\n \n \n = 1ms + mI2 U0 ms\n mI9. \n(11.50)\nThe eigenvalue equation for Fz in the coupled 0 FMF9 basis is\n \nFz0 FMF9 = MF U0 FMF9. \n(11.51)\nComparing Eqs. (11.50) and (11.51), we ﬁnd that the allowed magnetic quantum numbers MF are\n \nMF = ms + mI. \n(11.52)\nThe allowed single-particle magnetic quantum numbers are ms = {1>2 and mI = {1>2, so the \nfour allowed values of the total angular momentum magnetic quantum number MF are 1, 0, 0, and \n-1. These eigenvalues are also evident by inspection of the diagonal elements of the Fz matrix in \nEq. (11.49). A key point about these eigenvalues is that two of them are the same. Both the 0  + -9 and \n0  - +9 states have the magnetic quantum number MF = 0. This degeneracy of states is emphasized \nwith the box in the Fz matrix in Eq. (11.49), showing that the matrix is block diagonal. The degeneracy \nof the 0  + -9 and 0  - +9 states implies an ambiguity as to the eigenstates corresponding to MF = 0. \nThis degeneracy is an important aspect that we exploit in a moment.\nNow consider the matrix representation of the F2 operator, which is not so easy. The total angular \nmomentum operator is\n \n F2 = 1S + I2\n2 = S2 + I2 + S~I + I~S\n \n(11.53)\n \n = S2 + I2 + 2S~I,\nwhere again we have used the fact that the spin operators for the two particles commute with each \nother. Because the uncoupled states 0 ms\n mI9 are eigenstates of S2 and I2, those operators are  diagonal—\nin fact they are proportional to the identity in this subspace (Problem 11.6). We already know the \nmatrix representing S~I from the hyperﬁne Hamiltonian, so using Eq. (11.38), we ﬁnd the F2 operator \nin the 0 ms\n mI9 basis:\n \nF2 \u0003 U2 •\n2\n0\n0\n0\n0\n1\n1\n0\n0\n1\n1\n0\n0\n0\n0\n2\nμ  \n+ +\n+ -\n- +\n- -\n  . \n(11.54)\n",
    "368 \nHyperﬁne Structure and the Addition of Angular Momenta\nThe F2 matrix is block diagonal, as was Fz, but it is not diagonal within the 0  - +9, 0  + -9 subspace of \ndegenerate MF = 0 states.\nTo ﬁnd the eigenvalues and eigenvectors of the F2 operator, we diagonalize the matrix in \nEq. (11.54). However, because the matrix is block-diagonal, we ﬁnd the two eigenvalues 2U2 for the \n0  + +9 and 0  - -9 states by inspection. The eigenvalue equation for F2 in the coupled basis is\n \nF2@ FMF9 = F1F + 12U2@ FMF9, \n(11.55)\nso the eigenvalues 2U2 imply a quantum number F = 1 for the states 0  + +9 and 0  - -9. The Fz \neigenvalues MF of these two states are obtained by inspection of the Fz matrix in Eq. (11.49) or from\nEq. (11.52). These two eigenstates are thus\n \ncoupled basis | uncoupled basis \n \n 0 F = 1 , MF = 19 = 0  + +9  \n \n0 F = 1 , MF = -19 = 0  - -9. \n(11.56)\nTo ﬁnd the other two eigenvalues and eigenvectors of the F2 operator, we diagonalize the subma-\ntrix within the 0  + -9 and 0  - +9 states, as indicated by the box in Eq. (11.54). As you can show \nin Problem 11.10, the eigenvalues of the submatrix are 2U2 and 0, which correspond to the values \nF = 1 and F = 0. The magnetic quantum numbers are both MF = 0 for these two states, and the \neigenstates are\n \ncoupled basis | uncoupled basis \n \n0 F = 1 , MF = 09 =  1\n12 30  + -9 + 0  - +94 \n \n0 F = 0 , MF = 09 =  1\n12 30  + -9 - 0  - +94. \n(11.57)\nThe diagonalization procedure is equivalent to a rotation in Hilbert space, so it will, in general, \nundiagonalize other matrices that are diagonal in the original basis. So you might expect that the \ndiagonalization of F2 would undiagonalize the Fz matrix that we found to be diagonal in the 0 ms\n mI9 \nbasis. However, because the 0  + -9 and 0  - +9 states are degenerate with respect to Fz (i.e., they have \nthe same MF = 0 values), the Fz matrix is proportional to the identity matrix in the degenerate sub-\nspace of 0  + -9 and 0  - +9 states. The identity matrix is not altered by a rotation, so the ambiguity of \neigenstates of Fz in the degenerate subspace has the beneﬁt that the diagonalization of F2 does not \nundiagonalize Fz.\nIn summary, the four eigenstates 0 FMF9 of the coupled basis expressed in terms of the eigenstates \n0 ms\n mI9 of the uncoupled basis are \n \ncoupled basis | uncoupled basis \n \n 0 119 = 0  + +9\n \n 0 109 =\n1\n12 30  + -9 + 0  - +94  t Triplet state\n \n 0 1, -19 = 0  - -9\n \n(11.58)\n \n 0  009 =\n1\n12 3 0  + -9 - 0  - +94   r Singlet state  . \n",
    "11.5 The Coupled Basis \n369\nThese states are typically referred to as the triplet 1F = 12 and singlet 1F = 02 states. These are \nexactly the eigenstates we found in Eqs. (11.40) and (11.42) when we diagonalized the hyperﬁne per-\nturbation Hamiltonian in the 1s ground state.\nLet’s take a moment to reﬂect on what we have done. We started with two spin-1/2 particles and \nfound that the total angular momentum of the combined system could be 0 or 1, (i.e., F = s + I = 1\nand F = s - I = 0 were both allowed). For each allowed value of F, the allowed magnetic quantum \nnumbers run from -MF to +MF in unit steps as is the case for all angular momenta, (i.e., MF = 1, 0, -1\nfor the F = 1 case and MF = 0 for the F = 0 case). We learned how to express the new coupled \nbasis states 0 FMF9 in terms of the old uncoupled basis states 0 ms\n mI9, as shown in Eq. (11.58). The \nexpansion coefﬁcients in Eq. (11.58) that connect the two bases are called Clebsch-Gordan coef-\nﬁcients. They are commonly tabulated as in Table 11.1.\nNow we have two complete orthonormal bases to choose from—the coupled basis 0 FMF9 and \nthe uncoupled basis 0 ms\n mI9. The choice of which basis to use depends on which basis is best suited \nto the problem at hand, which typically depends on the Hamiltonian. For the hyperﬁne Hamiltonian, \nthe coupled basis is the “good” basis because the coupled basis eigenstates are the energy eigenstates, \nwhich reﬂects the fact that the total angular momentum is conserved.\nNow that we know that we could have chosen the coupled basis to solve the hyperﬁne problem \nmuch more easily, let’s do that and make sure we get the same answer that we obtained from the \nuncoupled basis analysis. Choosing the coupled basis means writing the matrix representing the per-\nturbation Hamiltonian using the 0 FMF9 states, rather than using the uncoupled 0 ms\n mI9 states as we \ndid in Section 11.4. To do this, we need to know how the S~I operator in the hyperﬁne Hamiltonian \nacts on the 0 FMF9 states. Using Eq. (11.53), we ﬁnd that the operator S~I can be expressed in terms of \nother operators whose action on the 0 FMF9 basis is known:\n \nS~I = 1\n21F2 - S2 - I22. \n(11.59)\nThis leads to the hyperﬁne Hamiltonian\n \n H =\nhf = A\nU2 S~I\n \n \n =\nA\n2U2 1F2 - S2 - I22.\n \n(11.60)\nTable 11.1 Clebsch-Gordan Coefﬁcients for System of Two Spin-1/2 Particles\ns = 1\n2\nF\n1\n1\n1\n0\nl = 1\n2\nM)\n1\n0\n-1\n0\n mV \nmO\n 1\n2 \n1\n2\n 1\n2 \n-  1\n2\n-  1\n2 \n1\n2\n -  1\n2 \n-  1\n2\n1\n0\n0\n0\n0\n1\n12\n1\n12\n0\n0\n0\n0\n1\n0\n1\n12\n-  1\n12\n0\n",
    "370 \nHyperﬁne Structure and the Addition of Angular Momenta\nThe matrix representing F2 is diagonal because 0 FMF9 are eigenstates of F2. The matrices represent-\ning S2 and I2 are diagonal because the 0 FMF9 states all have the same quantum numbers s = 1>2 and \nI = 1>2. The result is that the matrix representing the hyperﬁne Hamiltonian in the coupled basis is \n(Problem 11.11)\n \nH =\nhf \u0003 A\n4 §\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n-3\n¥ \n11\n10\n1, -1\n00\n  , \n(11.61)\nwhere the rows (and columns) are labeled with the F, MF quantum numbers.\nAs advertised, the hyperﬁne perturbation Hamiltonian is diagonal in the coupled basis. Degener-\nate perturbation theory calls for us to diagonalize this matrix in the degenerate 1s ground state, so we \nﬁnd the energy shifts by inspection of Eq. (11.61). The perturbation corrections are\n \nE (1)\nhf = b +A>4; F = 1\n-3A>4; F = 0 , \n(11.62)\njust as we found in Eqs. (11.40) and (11.42) by working in the uncoupled basis. We have solved the \nhyperﬁne perturbation problem in the coupled basis in a few lines instead of the several pages required \nfor the uncoupled basis approach. That is the sense in which the coupled basis is the convenient basis \nfor this problem. More important, the hyperﬁne eigenstates are the basis states of the coupled basis, so \nwe call the coupled basis the “correct” basis.\nWe can also solve for the hyperﬁne energy corrections using an operator approach because the \nperturbation Hamiltonian is diagonal in the coupled basis. Degenerate perturbation theory is equiva-\nlent to nondegenerate perturbation theory when the matrix is already diagonal, so the ﬁrst-order energy \nshifts are the expectations values of the perturbation:\n \n E (1)\nhf = 8FMF0 H =\nhf 0 FMF9\n \n =\nA\n2U2 8FMF@F2 - S2 - I2 @ FMF9.\n \n(11.63)\nThe 0 FMF9 states are eigenstates of the three operators in Eq. (11.63), so the result is\n \n E (1)\nhf =\nA\n2U2 3F1F + 12 - s1s + 12 - I1I + 124U2\n \n(11.64)\n \n = A\n2\n c F1F + 12 - 3\n2 d\n \n \n = b +A>4; F = 1\n-3A>4; F = 0 ,\n \nwhich is the same result again.\n11.6 \u0002 ADDITION OF GENERALIZED ANGULAR MOMENTA\nTo generalize from the example of adding the angular momenta of two spin-1/2 particles to the prob-\nlem of adding any two generalized angular momenta, it is instructive to consider an alternative deri-\n",
    "11.6 Addition of Generalized Angular Momenta \n371\nvation of the coupled states 0 FMF9 that employs the angular momentum ladder operators. The state \n0 119 = 0  + +9 is an eigenstate in both the uncoupled and coupled bases, with the largest possible val-\nues of the magnetic quantum numbers ms, mI, and MF. Such a state is called a stretched state. In this \nalternative method, we use the lowering operator F-  of the total angular momentum to generate other \ncoupled basis states, and by comparing the action of F-  in the two bases, we learn how the coupled \nand uncoupled basis states are related to each other.\nThe action of the lowering operator F-  of the total angular momentum on the stretched state \n0 119 = 0  + +9 generates the state 0 109, with a multiplicative factor 22U according to Eq. (11.23). In \nthe uncoupled basis, the lowering operator is F- = S - + I- , so we can also calculate the result of the \nlowering operation in the uncoupled basis:\n \ncoupled basis | uncoupled basis \n \n F- 0 119 = F- 0  + +9\n \n \n F- 0 119 = 1S - + I-2 0  + +9\n \n \n F- 0 119 = S - 0  + +9 + I- 0  + +9 \n \n 22U0 109 = U3 0  + -9 + 0  - +94.\n \n(11.65)\nThis lowering operation produces the same state no matter which basis we work in, so we  conclude that \n \n0 109 =\n1\n12 3 0  + -9 + 0  - +94 \n(11.66)\njust as we learned from the previous diagonalization procedures. Successive application of the lower-\ning operator then allows us to construct the ladder of states from the initial stretched state 0 119 down to \n0 109 and ﬁnally 0 1, -19 = 0  - -9, as shown in Fig. 11.5. However, the ladder operator changes only \nthe MF quantum number, not the F quantum number, so we do not generate the 0  009 state with this \nprocedure. Rather, we generate the 0  009 state by using the orthogonality condition to ﬁnd a state that \nis orthogonal to the 0 109 state and comprises the same uncoupled states 10  + -9 and 0  - +92 used in the \n0 109 state. The general linear combination of 0  + -9 and 0  - +9 state is\n \n0  009 = a0  + -9 + b0  - +9 \n(11.67)\nF\nF\northogonality\n\u000211\u0003\u0007\n\u000200\u0003\u0007\n\u000210\u0003\u0007\n\u00021,\n1\u0003\u0007\nFIGURE 11.5 Generation of the coupled state manifold using the lowering operator \nand the  orthogonality condition for the case of two spin-1/2 particles.\n",
    "372 \nHyperﬁne Structure and the Addition of Angular Momenta\nand the orthogonality condition is\n \n 0 = 8100 009\n \n \n =\n1\n12 18+ - 0 + 8- + 021a0  + -9 + b0  - +92 \n(11.68)\n \n =\n1\n12 1a + b2.\n \nHence, we conclude that a = -b = 1> 12 and \n \n0 009 =\n1\n12 1 0  + -9 - 0  - +92 \n(11.69)\nas we found earlier.\nThis general procedure of using ladder operators and orthogonality generates all the states in \nany coupled basis, and also yields the proper Clebsch-Gordan coefﬁcients. For the spin-1/2 case, the \nquantum numbers MF = {1\n2 never occur because the ladder starts with integer values of MF and the \nlowering operator changes MF by 1 each time. The orthogonality step preserves MF but changes F.\nNow let’s generalize to the problem of adding any two angular momenta. Consider two angular \nmomenta J1 and J2 coupled together to form a total angular momentum J:\n \nJ = J1 + J2. \n(11.70)\nThe angular momenta J1 and J2 are characterized by the quantum numbers j1, m1 and j2, m2, respec-\ntively, and the total angular momentum J is characterized by the quantum numbers J, M. Specifying \nall the eigenvalues, we write the uncoupled and coupled bases as:\n \n0  j1 j2\n m1\n m29  uncoupled basis\n \n(11.71)\n \n0  j1 j2\n JM9 \n coupled basis.  \nThe uncoupled basis vectors are eigenstates of J2\n1, J2\n2, J1z, and J2z. The coupled basis vectors are \neigenstates of J2\n1, J2\n2, J2, and Jz. In any given problem, the values of j1 and j2 are ﬁxed, so we could \nsuppress these labels. The convention we use is to suppress the j1 and j2 labels in the coupled states \n0 JM9, but not in the uncoupled states 0  j1 j2\n m1\n m29. This way, when we put in actual numbers (vs. \nalgebraic symbols) we can immediately tell whether a state is in the coupled basis (two labels) or the \nuncoupled basis (four labels). Using this notation, the relation between the coupled and uncoupled \nbases for the system of two spin-1/2 particles is\n \ncoupled basis | uncoupled basis \n \n 0119 = @1\n2 1\n2 1\n2 1\n29\n \n \n 0109 =\n1\n12 1@ 1\n2 1\n2 1\n2 -1\n2 9 + @ 1\n2 1\n2 -1\n2  1\n292  \n(11.72)\n \n 01, -19 = @ 1\n2 1\n2 -1\n2  -1\n2 9\n \n \n 0 009 =\n1\n12 1@ 1\n2 1\n2 1\n2 -1\n2 9 - @ 1\n2 1\n2 -1\n2  1\n292. \nIn terms of these generalized angular momentum labels, the spin-1/2 problem has j1 = 1/2 and \nj2 = 1>2 and the coupled J is equal to 0 or 1, which corresponds to the extreme values j1 + j2 and \n",
    "11.6 Addition of Generalized Angular Momenta \n373\nj1 - j2 . In the general case, the state generation procedure in Fig. 11.5 yields allowed values of J at all \nthe integer steps in between these extreme values:\n \nJ = j1 + j2,  j1 + j2 - 1,  j1 + j2 - 2, ... 0  j1 - j20   . \n(11.73)\nThe absolute value is needed because we require J Ú 0. For example, if j1 = 3 and j2 = 1, then the \nallowed values of J are 4, 3, 2. For each allowed value of J, the allowed values of M are -J to J in \ninteger steps:\n \nM = -J, -J + 1, ... , J - 1, J  , \n(11.74)\nwhich is what we expect for a generalized angular momentum. As a check, note that the total num-\nber of states is 12 j1 + 1212 j2 + 12, whether one counts in the coupled or uncoupled bases, which \nmust be the case (Problem 11.14). The state generation procedure in the general case is depicted in \nFig. 11.6, and an example of the resultant spectrum of states is shown in Fig. 11.7 for the case j1 = 1 \nand j2 = 1. The stretched state 0  J = j1 + j2, M = j1 + j29 = 0  j1 j2 , m1 = j1 , m2 = j29 is always an \neigenstate of both bases, so the coupled state generation procedure can start there in all cases. In Fig. \n11.7, the state 0 229=  0 11119 is the starting point for the generation procedure.\nThe general state generation procedure depicted in Fig. 11.6 works for any pair of j1, j2 values, \nand it tells us the Clebsch-Gordan coefﬁcients we need to express the coupled basis states in terms of \nthe coupled basis states, or vice versa. This procedure can be quite tedious for large angular momenta, \nas can the other method of diagonalizing the J2 and Jz matrices that we used in Section 11.5 for two \nspin-1/2 particles coupled together. Fortunately, the angular momentum addition problem has been \nsolved by others and the resultant Clebsch-Gordan coefﬁcients are conveniently tabulated. A general \nformula relating the coupled and the coupled states can be found by using the completeness relation. \nBecause we don’t mix j1, j2 manifolds, the completeness relation for the states in the uncoupled basis \nwithin a given j1, j2 manifold is\n \na\nj1\nm1= -j1\na\nj2\nm2= -j2\n 0  j1 j2\n m1\n m298 j1 j2\n m1\n m20 = 1. \n(11.75)\northogonality\northogonality\n\u0002j1\u000fj2,j1\u000fj2\u0003 \n\u0002j1\u000fj2,j1\u000fj2\n1\u0003 \n\u0002j1\u000fj2\n1,j1\u000fj2\n1\u0003 \n\u0002j1\u000fj2\n1,j1\u000fj2\n2\u0003 \n\u0002j1\u000fj2\n2,j1\u000fj2\n2\u0003 \n\u0002j1\u000fj2,j1\u000fj2\n2\u0003 \nJ\nJ\nJ\nJ\nJ\nJ\nFIGURE 11.6 The generation of the coupled state manifold using the lowering operator\nand the  orthogonality condition for the case of two generalized angular momenta j1 and j2.\n",
    "374 \nHyperﬁne Structure and the Addition of Angular Momenta\nOperate with this projection operator on a coupled state to get\n \n 0 JM9 = e\na\nj1\nm1= -j1\n  a\nj2\nm2= -j2\n 0  j1 j2\n m1\n m298 j1 j2\n m1\n m20 f 0 JM9 \n \n =\na\nj1\nm1= -j1\n  a\nj2\nm2= -j2\n58 j1 j2\n m1\n m20 JM960  j1 j2\n m1\n m29\n \n(11.76)\nwith the result\n \n0 JM9 =\na\nj1\nm1= -j1\n  a\nj2\nm2= -j2\nC j1 j2 J\nm1m2M0  j1 j2\n m1\n m29    , \n(11.77)\nwhere the scalar products 8 j1 j2\n m1\n m20 JM9 connecting the coupled and uncoupled bases are  written as\n \nC j1 j2 J\nm1m2M = 8 j1 j2\n m1m20 JM9. \n(11.78)\nThe coefﬁcients C j1 j2 J\nm1m2M are the Clebsch-Gordan coefﬁcients we introduced in Section 11.5.\nClebsch-Gordan coefﬁcients are tabulated in many books, although there are several different \nconventions on how to write the tables. A few examples of Clebsch-Gordan coefﬁcients are shown \nin Tables 11.2–11.5. Columns represent coupled states expressed in terms of uncoupled states. For \nexample, in the case of j1\n =1, j2 =1>2, the coupled state @ 3\n2 1\n29 can be read from the second column of \nTable 11.3:\n \n@ 3\n2 1\n29 = 4\n1\n3@  1 1\n2 1, -  1\n29 + 4\n2\n3@  1 1\n2 0 1\n29. \n(11.79)\nAll the Clebsch-Gordan coefﬁcients are real, so the inverse expansion uses the same coefﬁcients:\n \n 0  j1 j2\n m1\n m29 =\na\nj1+  j2\nJ= 0  j1-j20\n0 JM98JM0  j1 j2\n m1\n m29 \n \n =\na\nj1+  j2\nJ= 0  j1-j20\nC j1 j2 J\nm1m2M0 JM9.\n \n(11.80)\n\u000222\u0003\u0004\u00021111\u0003 \n\u00022\n2\u0003\u0004\u000211\n1\n1\u0003 \n\u000221\u0003\u0004\n\u00021110\u0003\u0005\u000f\u0007\u0007\u0007\u0007\u0007\u0007\u00021101\u0003\n1\n√2\n1\n√2\n\u000211\u0003\u0004\n\u00021110\u0003\u0005\n\u0007\u0007\u0007\u0007\u0007\u0007\u00021101\u0003\n1\n√2\n1\n√2\n\u00022 \n1\u0003\u0004\n\u0002110\n1\u0003\u0005\u000f\u0007\u0007\u0007\u0007\u0007\u0007\u000211\n10\u0003\n1\n√2\n1\n√2\n\u00021\n1\u0003\u0004\n\u0002110\n1\u0003\u0005\n\u0007\u0007\u0007\u0007\u0007\u0007\u000211\n10\u0003\n1\n√2\n1\n√2\n\u000210\u0003\u0004\n\u0002111\n1\u0003\u0005\n\u0007\u0007\u0007\u0007\u0007\u0007\u000211\n11\u0003\n1\n√2\n1\n√2\n\u000200\u0003\u0004\n\u0002111\n1\u0003\u0005\n\u0007\u0007\u0007\u0007\u0007\u0007\u00021100\u0003\u000f\u0007\u0007\u0007\u0007\u0007\u0007\u000211\n11\u0003\n1\n√3\n1\n√3\n1\n√3\n\u000220\u0003\u0004\n\u0002111\n1\u0003\u0005\u000f\u0007\u0007\u0007\u0007\u0007\u0007\u0007\u0007\u0007\u0007\u0007\u0007\u0007\u00021100\u0003\u000f\u0007\u0007\u0007\u0007\u0007\u0007\u0007\u000211\n11\u0003\n1\n√6\n1\n√6\n√\n2\n3\nFIGURE 11.7 The manifold of coupled angular momentum states for j1 = 1, j2 = 1, \nshowing the coupled states 0 JM9 expressed in terms of the uncoupled states 0  j1 j2\n m1\n m29.\n",
    "11.6 Addition of Generalized Angular Momenta \n375\nTable 11.2 Clebsch-Gordan Coefﬁcients for j1 \u0003 1\n2 and j2 \u0003 1\n2\nj1 = 1\n2\nj\n1\n1\n1\n0\nj2 = 1\n2\nm\n1\n0\n-1\n0\nm1\n1\n2\n1\n2\n-  1\n2\n-  1\n2\nm2\n1\n2\n-  1\n2\n1\n2\n-  1\n2\n1\n0\n0\n0\n0\n1\n12\n1\n12\n0\n0\n0\n0\n1\n0\n1\n12\n-  1\n12\n0\nNote that there is no sum over M in Eq. (11.80) because M = m1 + m2 is the only allowed M state. \nReading a row in the Clebsch-Gordan tables gives the inverse expansion of uncoupled states expressed \nin terms of coupled states. For example, in the case of j1=1, j2 =1/2, the uncoupled state @ 11\n2 0 1\n29 can \nbe read from the third row of Table 11.3: \n \n@ 11\n2 0 1\n29 = 4\n2\n3@ 3\n2 1\n29 - 4\n1\n3@ 1\n2 1\n29. \n(11.81)\nNote the correspondence between the Clebsch-Gordan Table 11.5 for the case of j1=1, j2 =1, and the \nmanifold of states depicted in Fig. 11.7.   \nNote the large number of zeroes in the Clebsch-Gordan tables. These zeroes are important \nbecause angular overlap integrals of wave functions that are used to ﬁnd transition probabilities can \nbe expressed in terms of Clebsch-Gordan coefﬁcients. The zeroes thus imply selection rules for transi-\ntions based upon the geometry of the states and the type of transition in question.\nTable 11.3 Clebsch-Gordan Coefﬁcients for j1 \u0003 1 and j2 \u0003 1\n2\nj1 = 1\nj\n3\n2\n3\n2\n3\n2\n3\n2\n1\n2\n1\n2\nj2 = 1\n2\nm\n3\n2\n1\n2\n-  1\n2\n-  3\n2\n1\n2\n-  1\n2\nm1\nm2\n1\n0\n0\n0\n0\n0\n0\n1\n13\n4\n2\n3\n0\n0\n0\n0\n0\n0\n4\n2\n3\n1\n13\n0\n0\n0\n0\n0\n0\n1\n0\n4\n2\n3\n-  1\n13\n0\n0\n0\n0\n0\n0\n1\n13\n-4\n2\n3\n0\n1\n1\n0\n0\n-1\n-1\n1\n2\n-  1\n2\n1\n2\n-  1\n2\n1\n2\n-  1\n2\n",
    "376 \nHyperﬁne Structure and the Addition of Angular Momenta\nTable 11.4 Clebsch-Gordan Coefﬁcients for j1 \u0003 3\n2 and j2 \u0003 1\n2\nj1 = 3\n2\nj\n2\n2\n2\n2\n2\n1\n1\n1\nj2 = 1\n2\nm\n2\n1\n0\n-1\n-2\n1\n0\n-1\nm1\nm2\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n2\n13\n2\n0\n0\n0\n0\n0\n0\n0\n0\n1\n12\n1\n12\n0\n0\n0\n0\n0\n0\n0\n0\n13\n2\n1\n2\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n13\n2\n-  1\n2\n0\n0\n0\n0\n0\n0\n0\n0\n1\n12\n-  1\n12\n0\n0\n0\n0\n0\n0\n0\n0\n1\n2\n-  13\n2\n0\n3\n2\n3\n2\n1\n2\n1\n2\n-  1\n2\n-  1\n2\n-  3\n2\n-  3\n2\n1\n2\n-  1\n2\n1\n2\n-  1\n2\n1\n2\n-  1\n2\n1\n2\n-  1\n2\nTable 11.5 Clebsch-Gordan Coefﬁcients for j1 \u0003 1 and j2 \u0003 1\nj1 = 1\nj\n2\n2\n2\n2\n2\n1\n1\n1\n0\nj2 = 1\nm\n2\n1\n0\n-1\n-2\n1\n0\n-1\n0\nm1\nm2\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n12\n0\n1\n12\n0\n0\n0\n0\n0\n0\n0\n1\n16\n0\n4\n2\n3\n0\n1\n16\n0\n0\n0\n0\n0\n0\n0\n1\n12\n0\n1\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n12\n0\n-  1\n12\n0\n0\n0\n0\n0\n0\n0\n1\n12\n0\n0\n0\n-  1\n12\n0\n0\n0\n0\n0\n0\n0\n1\n12\n0\n-  1\n12\n0\n0\n0\n1\n13\n0\n-  1\n13\n0\n1\n13\n0\n0\n1\n1\n1\n0\n0\n0\n-1\n-1\n-1\n1\n0\n-1\n1\n0\n-1\n1\n0\n-1\n",
    "Summary \n377\n11.7 \u0002 ANGULAR MOMENTUM IN ATOMS AND SPECTROSCOPIC NOTATION\nAn important application of angular momentum addition occurs in atoms where we must combine \nthe orbital angular momentum and the spin angular momentum of the electrons (we neglect the \nspin angular momentum of the nucleus here). The total angular momentum of the electrons is \ntypically denoted by J :\n \nJ = L + S . \n(11.82)\nFor a single electron atom such as hydrogen, the spin is s = 1/2, so the allowed values of the total \nangular momentum j are\n \n j = / + s, ...,0 / - s0\n \n \n = b/ + 1\n2 , / - 1\n2 ,   \n/ Ú 1\n   1\n2    ,   \n/ = 0 . \n(11.83)\nThe standard convention is to use lower-case letters (j, l, s) for the angular momenta of a single elec-\ntron, and upper-case letters (J, L, S) for the angular momenta of the complete atom.\nFor atoms with more than one electron, we must add all the electron angular momenta together. \nUsually we add all the orbital angular momenta together to get the total orbital angular momentum L, \nadd all the spin angular momenta to get the total spin S, and then couple L and S to get J. The results of \nthis angular momentum coupling are denoted with spectroscopic notation to specify the atomic state \n(also called term notation or Russell-Saunders notation)\n \n2S+1LJ, \n(11.84)\nwhere S and J are numbers and L is a letter specifying the orbital angular momentum. The letters used \nfor the orbital angular momentum states are\n \nL =    0\n1\n2\n3\n4\n5\n6\n7\n...\nletter = S\nP\nD\nF\nG\nH\nI\nK\n.... \n(11.85)\nFor example, the ground state of hydrogen has L = 0, S = 1/2, J = 1/2, and is denoted as 2S1>2. \nThis designation is the same for all the alkali atoms because they each have one electron outside a \nclosed shell. The ground state of carbon has L = 1 and S = 1, which couple to form the 3P0 state with \nJ = 0. Other values of J are possible according to the rules we have developed, but they turn out to \nhave higher energy because of internal perturbations.\nSUMMARY\nIn this chapter, we have introduced the concept of adding or coupling angular momenta. We noted that \nall angular momenta, whether spin or orbital, obey the general eigenvalue equations\n \n J2@  jmj9 = j1 j + 12U2@  jmj9 \n \n Jz@  jmj9 = mj U@  jmj9.\n \n(11.86)\n",
    "378 \nHyperﬁne Structure and the Addition of Angular Momenta\nWe introduced the angular momentum ladder operators\n \nJ+ = Jx + iJy \n \nJ- = Jx - iJy \n(11.87)\nthat raise and lower the magnetic quantum number according to the relation\n \nJ{ @  jmj9 = U3j1j + 12 - mj1mj { 124\n1>2@  j,mj { 19. \n(11.88)\nWe considered the general problem of coupling two angular momenta J1 and J2 together to form \nthe total angular momentum\n \nJ = J1 + J2. \n(11.89)\nWe described a system of two angular momenta using either the uncoupled basis or the coupled basis\n \n 0  j1 j2\n m1\n m29  uncoupled basis \n \n 0 JM9 \n coupled basis.  \n(11.90)\nThe allowed values of the coupled angular momentum quantum number are\n \nJ = j1 + j2 ,  j1 + j2 - 1 ,  j1 + j2 - 2 , ... 0  j1 - j20  \n(11.91)\nand the allowed coupled magnetic quantum numbers are\n \nM = -J , -J + 1, ... , J - 1, J. \n(11.92)\nWe expressed the coupled basis vectors in terms of the uncoupled basis vectors using the expansion\n \n0 JM9 =\na\nj1\nm1= -j1\na\nj2\nm2=-j2\nC j1 j2 J\nm1m2\n M0  j1 j2\n m1\n m29,  \n(11.93)\nwhere the scalar products connecting the coupled and uncoupled bases are the Clebsch-Gordan \ncoefﬁcients\n \nC j1 j2 J\nm1m2M = 8 j1 j2\n m1m2@ JM9. \n(11.94)\nWe studied the concept of angular momentum addition in the hyperﬁne structure of the ground \nstate of hydrogen, which is governed by the Hamiltonian\n \nH =\nhf = A\nU2 S~I \n(11.95)\nthat couples together the electron spin S and the proton spin I. The utility of the coupled basis was \nevidenced by the fact that the hyperﬁne Hamiltonian is not diagonal in the uncoupled basis, but it is \ndiagonal in the coupled basis, where the coupled angular momentum is\n \nF = S + I . \n(11.96)\n",
    "Problems   \n379\nIn this system of two spin-1/2 particles, the coupled basis in terms of the uncoupled basis is\n \ncoupled basis | uncoupled basis \n \n 0 119 = 0 1\n2 1\n2 1\n2 1\n29 = 0  + +9\n \n 0 109 =\n1\n12 10 1\n2 1\n2 1\n2 -1\n2 9 + 0 1\n2 1\n2 -1\n2  1\n292 =\n1\n12 10  + -9 + 0  - +92t Triplet\n \n 0 1, -19 = 0 1\n2 1\n2 -1\n2  -1\n2 9 = 0  - -9\n \n  @  009 =\n1\n12 1@ 1\n2 1\n2 1\n2 -1\n2 9 - @ 1\n2 1\n2 -1\n2  1\n292 =\n1\n12 1@  + -9 - @  - +92r Singlet . \n(11.97)\nIn the hydrogen ground state, the hyperﬁne interaction causes the triplet levels to be displaced from \nthe singlet level.\nPROBLEMS \n 11.1 Verify the commutation relations in Eqs. (11.18), (11.19), and (11.20).\n 11.2 Use the commutation relations in Eq. (11.20) to demonstrate that the angular momentum lad-\nder operators act as advertised. Derive Eq. (11.23) that characterizes the action of the ladder \noperators. (Hint: review the harmonic oscillator ladder operators.)\n 11.3 Show that the restriction that the angular momentum component Jz cannot be greater than the \nmagnitude of J Ai.e., the square root of J2B implies that the largest possible value of the mag-\nnetic quantum number is mj = j.\n 11.4 Consider a generic spin-3/2 system.\na) Write down the eigenstates of this system and the eigenvalue equations for S2 and Sz.\nb) Write down the matrices representing S2 and Sz by inspection.\nc) Use Eq. (11.23) that characterizes the action of the ladder operators to generate the matri-\nces representing Sx and Sy.\nd) Find the eigenvalues of Sx. \n 11.5 Show that Eq. (11.31) implies that the electron spin and proton spin operators commute with \neach other.\n 11.6 Use the eigenvalue equations (11.31) to derive the matrix representations in Eq. (11.32) for \nthe electron spin and proton spin component operators in the uncoupled basis. By similar \nmeans, ﬁnd the matrix representations of S2 and I2 in the uncoupled basis and conﬁrm that \neach is proportional to the identity matrix (see activity on system of two spin-1/2 particles).\n 11.7 Show that S~I = Sx Ix + Sy Iy + Sz Iz can be rewritten using the angular momentum ladder \noperators as S~I = 1\n21S +I- + S -I+2 + Sz Iz.\n 11.8 Calculate the action of the ladder operators S +, S -, I+, I- on each of the four uncoupled angu-\nlar momentum states 0{{9 of the ground state of hydrogen. Use your results to calculate the \nmatrix representing the hyperﬁne Hamiltonian H =\nhf = AS~I>U2 in the uncoupled basis.\n 11.9 Show that the electron and proton spin observables S2 and I2 commute with the hyperﬁne \nHamiltonian H =\nhf = AS~I>U2 and that the component observables Sz and Iz do not.\n 11.10 Diagonalize the matrix representing F2 in Eq. (11.54) and conﬁrm the eigenvalues and eigen-\nstates quoted in the text.\n",
    "380 \nHyperﬁne Structure and the Addition of Angular Momenta\n 11.11 Consider the ground state hyperﬁne system of the hydrogen atom. Calculate the matrices for \nS2, I2, and F2 in the coupled basis and show that the hyperﬁne Hamiltonian is diagonal in \nthis basis.\n 11.12 Consider a system of two particles. Particle #1 has spin 1 1s1 = 12 and particle #2 has spin \n1/2 (s2 = 1>2). The total spin of the system is S = S1 + S2. \na) List all the possible uncoupled basis states 0 s1s2\n m1m29.\nb) Identify the stretched state 0 s1s2s1s29.\nc) Starting with the stretched state, generate all the coupled basis states 0 SM9 using the low-\nering operator and the orthogonality condition as outlined in Section 11.6 of the text.\nd) From the results in (c), construct the Clebsch-Gordan table for this system.\n11.13 Use the scheme developed in Section 11.6 for generating coupled basis states to create the \nClebsch-Gordan coefﬁcients in Table 11.3.\n11.14 Consider a system of two angular momenta j1 and j2. Demonstrate that the total number of \nstates is 12j1 + 1212j2 + 12 whether you count states in the coupled or the uncoupled basis.\n11.15 Consider a system of two angular momenta with j1 = 1 and j2 = 1\n2.\na) Write down all the possible states of this system in the uncoupled basis 0  j1 j2\n m1m29.\nb) What are the allowed values of the coupled angular momentum quantum numbers J and M \nfor this system?\nc) Write down all the possible states of this system in the coupled basis 0 JM9.\nd) Use the Clebsch-Gordan coefﬁcients in Table 11.3 to express the coupled basis states \n0 JM9 in terms of the uncoupled basis states 0  j1 j2\n m1m29.\n11.16 Deuterium is an isotope of hydrogen with one electron bound to a nucleus (the deuteron) \ncomprising a proton and a neutron. The deuteron has spin I = 1 and has a gyromagnetic ratio \ngD = 0.857, which is the only change needed to use Eq. (11.10) for the hyperﬁne interaction \nin deuterium. Determine the hyperﬁne structure of the ground state of deuterium (i.e., ﬁnd the \neigenvalues and eigenstates). Calculate the splitting of the ground state and produce a ﬁgure \nlike Fig. 11.4 for deuterium.\n11.17 A positronium atom is a hydrogen-like atom with a positron 1m = me, q = +e, spin 1/22 as \na nucleus and a bound electron. The hyperﬁne structure in the ground state of positronium is \ndescribed by a perturbation Hamiltonian H\u0004 = AS1~S2>U2 where Si are the spins of the elec-\ntron and positron.\na) What is the Bohr energy of the ground state of positronium (ignore hyperﬁne structure \nfor now)?\nb) The electron and positron spins can be coupled to form the total spin S of the atom. Write \ndown the spin states of the coupled and uncoupled bases and how they relate to each other.\nc) Express the hyperﬁne Hamiltonian in the ground state as a matrix in both the coupled and \nuncoupled spin bases.\nd) Determine the effect of the hyperﬁne perturbation interaction on the ground state of posi-\ntronium. Draw an energy level diagram to illustrate your results.\n11.18 Consider two electrons, each with spin angular momentum si = 1>2 and orbital angular \nmomentum /i = 1. \na) What are the possible values of the quantum number L for the total orbital angular momen-\ntum L = L1 + L2? \n",
    "Resources \n381\nb) What are the possible values of the quantum number S for the total spin angular momen-\ntum S = S1 + S2? \nc) Using the results from (a) and (b), ﬁnd the possible quantum numbers J for the total angu-\nlar momentum J = L + S.\nd) What are the possible values of the quantum number j1 of the total angular momentum \nJ1 = L1 + S1 of electron #1? Same question for electron #2.\ne) Using the results from (d), ﬁnd the possible quantum numbers J for the total angular \nmomentum J = J1 + J2 and compare to the results in (c).\n11.19 Express the angular momentum ladder operators in the position representation. Apply the \nraising operator to the spherical harmonic Y 0\n21u, f2 and verify that your result agrees with \nEq. (11.23).\n11.20 Consider a system of two particles. Particle #1 has spin 1 (s1 = 1) and particle #2 has spin 1/2 \n(s2 = 1>2). The system is in a state with total spin 1/2 and z–component -U>2. If you mea-\nsure the z–component of the spin of particle #1, what are the possible results, and what are the \nprobabilities of the measurements? Same question for particle #2.\n11.21 Consider a system comprising three electrons, each with spin angular momentum si = 1>2. \nIgnore the orbital angular momentum.\na) How many possible spin states are there in this system? Identify the states in the uncou-\npled basis.\nb) Identify the stretched state of the system and use the lowering operator and the orthogonal-\nity condition as outlined in Section 11.6 of the text to generate all the coupled basis states. \n(Hint: use Gram-Schmidt orthogonalization.)\nc) From the results in (b), construct a “Clebsch-Gordan table” for this system. Your answer is \nnot unique.\nRESOURCES\nActivities\nThis activity is available at\nwww.physics.oregonstate.edu/qmactivities\nSystem of two spin-1/2 particles: Students review the spin eigenvalue equations, determine the \npossible states of a two-spin system, and ﬁnd the matrix representations of operators in the two-\nspin system.\nFurther Reading\nPedagogical articles on the hyperﬁne interaction:\nD. J. Grifﬁths, “Hyperﬁne splitting in the ground state of hydrogen,” Am. J. Phys. 50, 698–703 \n(1982).\nG. W. Parker, “Spin current density and the hyperﬁne interaction in hydrogen,” Am. J. Phys. 52, \n36–39 (1984).\n",
    "C H A P T E R \n12\nPerturbation of Hydrogen\nSpectroscopy of the hydrogen atom has played a central role in the development of quantum mechan-\nics itself and of perturbation theory in particular. Nobel Laureates Arthur Schawlow and Theodor \nHänsch made important advances in hydrogen atom spectroscopy and noted that, “The spectrum of \nthe hydrogen atom has proved to be the Rosetta stone of modern physics: once this pattern of lines \nhad been deciphered much else could also be understood.” The scientiﬁc process whereby advances in \nexperimental precision of spectroscopic measurements have led to advances in theoretical understand-\ning of the hydrogen atom has been repeated throughout the last century. State of the art techniques \nnow permit the energy levels of the hydrogen atom to be measured with 15 digits of precision, provid-\ning one of the best testing grounds for quantum theory.\nIn Chapter 10, we studied the Stark effect—the perturbation of hydrogen energy levels by an \nexternal electric ﬁeld. In Chapter 11, we studied the hyperﬁne interaction—the perturbation of hydro-\ngen energy levels by the magnetic interaction between the electron spin and the nuclear spin. In this \nchapter we study further magnetic perturbations—due to both external and internal magnetic ﬁelds. \nThe internal ﬁelds give rise to the ﬁne structure of the hydrogen energy levels and to the hyperﬁne \nstructure that we have already studied. The external ﬁelds give rise to the Zeeman effect—the mag-\nnetic analog to the Stark effect. We also study internal perturbations due to relativistic effects, which \nare part of the ﬁne structure. We treat all of these effects as small perturbations to the hydrogen energy \nlevels we found in Chapter 8. It is possible to treat some of the internal perturbations exactly using the \nrelativistic Dirac equation, but that is beyond the scope of this text.\n12.1 \u0002 HYDROGEN ENERGY LEVELS\nLet’s review what we know about the energy levels of hydrogen. The zeroth-order hydrogen atom \nHamiltonian is a combination of kinetic and potential energy terms:\n \nH0 = p 2\n2m -\ne2\n4pe0r . \n(12.1)\nThe zeroth-order energy eigenvalue equation is\n \nH00 n/m9 = E (0)\nn 0 n/m9, \n(12.2)\nwhere the zeroth-order eigenstate wave functions are\n \n0 n/m9 \u0003 Rn/1r2Y m\n/ 1u,f2 \n(12.3)\n",
    "12.1 Hydrogen Energy Levels \n383\nand the zeroth-order energy eigenvalues are\n \nE (0)\nn\n= -  1\nn2 m\n2U2 a e2\n4pe0\nb\n2\n. \n(12.4)\nWe refer to these energy levels as the Bohr energies, even though Bohr found them using an incomplete \nquantum mechanical analysis. The Bohr energies are independent of the quantum numbers / and m, \nwhich implies a degeneracy of n2 for each n level. A diagram of the zeroth-order energy levels is shown \nin Fig. 12.1, where we have identiﬁed the separate / states of each n level. One important result of this \nchapter is that some of the degeneracy of the n levels is lifted. In Eq. (12.2), we have suppressed the \nsuperscript on the zeroth-order eigenstates 0 n/m9 but not the energies E (0)\nn  because we are focused on \nﬁnding energy corrections and will not be concerned with eigenstate corrections, so all references to \neigenstates in this chapter are to zeroth-order eigenstates.\nThe zeroth-order hydrogen energies are often expressed in terms of the Rydberg energy\n \nRyd = m\n2U2 a e 2\n4pe0\nb\n2\n\u0002 13.6 eV. \n(12.5)\n1s\n2s\n2p\n3s\n3p\n3d\n4s\n4p\n4d\n4f\n\r\nEnergy (eV)\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\nFIGURE 12.1 Energy level diagram of hydrogen showing the n \u0003 1 through n \u0003 4 \nstates. Arrows indicate the allowed optical transitions between the levels shown.\n",
    "384 \nPerturbation of Hydrogen\nIt is convenient to express the Rydberg energy in terms of another characteristic energy multiplied by \na dimensionless constant:\n \nRyd = 1\n2\n mc2 a\ne2\n4pe0\n Ucb\n2\n. \n(12.6)\nThe characteristic energy is the electron rest mass energy Erest = mc2, and the dimensionless constant \nis the ﬁne-structure constant [Eq. (8.41)]\n \na =\ne2\n4pe0\n Uc . \n(12.7)\nWith these choices, the hydrogen energy levels take on the simple form:\n \nE (0)\nn\n= -  1\nn2 1\n2\n a2mc2  . \n(12.8)\nGiven all these different formulae, it may not be clear which numbers are most important. Most \nwould agree that there are three numbers from these formulae that you should have ingrained into your \nmemory as well as your own name. With these in your memory banks, you will be able to do quantum \nmechanical calculations when stranded on a desert island. They are: (1) the hydrogen ground state energy:\n \nE (0)\n1s = -13.6 eV  , \n(12.9)\n(2) the electron rest mass energy:\n \nmc2 = 511 keV  , \n(12.10)\nand (3) the ﬁne-structure constant:\n \na =\n1\n137  . \n(12.11)\nBecause these three constants are related by Eq. (12.8),\n \n E (0)\n1s = -  1\n2\n a2mc2\n \n \n -13.6 eV \u0002 -  1\n2 \n1\n(137)2 511 keV \n \n(12.12)\nyou need remember only two of them if you know the hydrogen energy level equation (12.8).\nOur plan in this chapter is to discuss the real hydrogen atom by looking at various perturbations \nthat shift the energy levels from the Bohr energies shown in Fig. 12.1. In order to provide a road \nmap for our journey, we show these perturbation corrections in Fig. 12.2 for the ﬁrst two states of \nhydrogen, ordered from left to right by decreasing magnitude of the correction. The ﬁrst correction \nis the ﬁne structure, which includes several terms arising from the electron spin and from relativity. \nThe Lamb shift is a quantum electrodynamic effect that we discuss only qualitatively. The hyperﬁne \nstructure is caused by the interaction of the electron and nuclear magnetic moments, as we saw in the \n",
    "12.1 Hydrogen Energy Levels \n385\nprevious chapter. The ﬁne-structure constant a sets the scale for these perturbations, as outlined in \nTable 12.1. The reason is that the ﬁne-structure constant is a dimensionless measure of the strength \nof the electromagnetic interaction; it is also called the electromagnetic coupling constant. Because \na V 1, the perturbation approach to quantum electrodynamics is valid.\nThe ﬁne-structure constant in hydrogen perturbations also sets the scale of the electron velocity. \nThe total energy of the hydrogen atom is roughly equal parts kinetic and potential energies:\n \nE \u0007 T \u0007 V. \n(12.13)\nRelating the Bohr energy to the kinetic term\n \n1\n2\n a2mc 2 \u0007 1\n2\n mv 2   1    a2 \u0007 v 2\nc 2 \n(12.14)\ntells us that\n \na \u0007 v\nc . \n(12.15)\nn \u0004\u00072\nn \u0004\u00071\n,\nF \u0004\u00070\nF \u0004\u00071\nF \u0004\u00070\nF \u0004\u00071\nF \u0004\u00070\nF \u0004\u00071\nF \u0004\u00071\nF \u0004\u00072\nBohr Energies\nFine Structure\nLamb Shift\nHyperfine Structure\n2p 3\n2\n2p 3\n2\n2p 1\n2\n2p 1\n2\n2s 1\n2\n2s 1\n2\n1s 1\n2\n1s 1\n2\nFIGURE 12.2 Corrections to the n \u0003 1 and n \u0003 2 Bohr energy levels ordered by magnitude (large to \nsmall, left to right). The shifts are not drawn to scale and are increasingly magniﬁed from left to right.\nTable 12.1 Hydrogen Energy Scales\nTerm\nScale\nBohr energy\na2mc 2\nFine structure\na4mc 2\nLamb shift\na5mc 2\nHyperﬁne structure\n1me>mp2a4mc 2\n",
    "386 \nPerturbation of Hydrogen\nHence, the electron has a speed roughly 1% of the speed of light 1a = 1>137 \u0005 1%2. This velocity is \nlarge enough to make relativity important to the bound state energies, but it is small enough that the \nperturbative approach is valid.\n12.2 \u0002 FINE STRUCTURE OF HYDROGEN\nThe ﬁne structure of hydrogen has two primary contributions: (1) the relativistic correction caused by the \nelectron velocity, and (2) the spin-orbit correction caused by the magnetic interaction between the elec-\ntron spin magnetic moment and the magnetic ﬁeld generated by the electron orbital angular momentum.\n12.2.1 \u0002 Relativistic Correction\nThe relativistic energy of a particle includes kinetic energy and rest mass energy, but not potential energy:\n \nE = 1m2c4 + p2c22\n1>2.\n \n(12.16)\nWe would like to expand E in terms of a small parameter. From our discussion above, we expect that \nsmall parameter to be the ratio v>c. In quantum mechanics, we use momentum instead of velocity, so \nthe relevant small parameter is the ratio p>mc. We create a perturbative expansion of the relativistic \nenergy by factoring out mc2 from Eq. (12.16) to isolate the ratio p>mc\n \nE = mc2\nC1 + a p\nmcb\n2\n, \n(12.17)\nusing the binomial expansion\n \nE = mc2£ 1 + 1\n2\n a p\nmcb\n2\n- 1\n8\n a p\nmcb\n4\n+ ...§ , \n(12.18)\nand keeping the three leading terms:\n \nE \u0002 mc2 + p2\n2m -\np4\n8m3c2 .  \n(12.19)\nThe leading term in Eq. (12.19) is the rest energy of the electron, which we did not include in the \nzeroth-order Hamiltonian in Eq. (12.1). There is no need to include it now because it only shifts the \nzero level of energies. The second term in Eq. (12.19) is the classical expression for the kinetic energy \nthat we used in the zeroth-order Hamiltonian. The third term is the new relativistic kinetic energy cor-\nrection, and it becomes the Hamiltonian for the relativistic perturbation:\n \nH =\nrel = -  \np4\n8m3c2 . \n(12.20)\nNote that this perturbation is negative, which means it increases the binding energy. This perturba-\ntion Hamiltonian is two orders of p>mc smaller than the zeroth-order Hamiltonian, so we expect the \nresultant energy correction to be smaller than the zeroth-order energy differences by two orders of the \nﬁne-structure constant a, as indicated in Table 12.1.\n",
    "12.2 Fine Structure of Hydrogen \n387\nNow that we have the Hamiltonian, we apply perturbation theory to ﬁnd the energy level correc-\ntions due to this relativistic term. We ﬁrst ask whether we should use nondegenerate or degenerate \nperturbation theory. We know that the hydrogen energy levels are degenerate with respect to the quan-\ntum numbers / and m, so we expect to require degenerate perturbation theory. However, the operator \np4 commutes with the operators L2 and Lz, so the perturbation Hamiltonian is diagonal within each \ndegenerate subspace (Problem 12.1). Degenerate perturbation theory requires us to diagonalize the \nperturbation matrix within each degenerate subspace, so we simply identify the energy corrections \nfrom the diagonal elements. This perturbation has nothing to do with the spin, so it commutes with the \nspin operator. This means we can ignore the spin for now and use the states 0 n/m9 for the unperturbed \nbasis. (Note that we use m for mass and for the magnetic quantum number, but it is clear from the \ncontext which is which.)\nThe ﬁrst-order relativistic energy correction is\n \n E (1)\nrel = 8H =\nrel9 = 8n/m0 H =\nrel0 n/m9 \n \n = -  \n1\n8m3c2 8n/m0 p40 n/m9.\n \n(12.21)\nThe matrix element of the operator p4 requires integrals of fourth-order derivatives of the wave func-\ntions. There is an easier way. Use the zeroth-order Hamiltonian to express the operator p2 in terms of \nother operators:\n \n H0 = p2\n2m -\ne2\n4pe0r\n \n \n p2 = 2maH0 +\ne2\n4pe0rb. \n(12.22)\nThe matrix elements we need are\n \n 8n/m@p4@ n/m9 = 8n/m@p2p2@ n/m9\n \n \n = 8n/m0 2maH0 +\ne2\n4pe0rb 2maH0 +\ne2\n4pe0rb 0 n/m9. \n(12.23)\nThe zeroth-order eigenvalue equation H00 n/m9 = E (0)\nn 0 n/m9 tells us the action of the  Hamiltonian H0 \non the eigenstates, so we get\n \n 8n/m@p4@ n/m9 = 4m2 8n/m@ aE (0)\nn\n+\ne2\n4pe0rbaE (0)\nn\n+\ne2\n4pe0rb@ n/m9\n \n \n = 4m2 aAE (0)\nn B\n2\n+ 2E (0)\nn  e2\n4pe0\n h 1\nr i\nn/\n+ a e2\n4pe0\nb\n2\nh 1\nr 2 i\nn/\nb, \n(12.24)\nwhere we have used shorthand for the matrix elements (and have dropped the m label because the \nmatrix elements do not depend on m):\n \n8 f 1r29n/ = 8n/m0  f 1r2 0 n/m9. \n(12.25)\n",
    "388 \nPerturbation of Hydrogen\nThe resultant relativistic energy correction is\n \n8H =\nrel9 = -  \n1\n2mc2 cAE (0)\nn B\n2\n+ 2 e2\n4pe0\n E (0)\nn h 1\nr i\nn/\n+\ne4\n14pe022 h 1\nr 2 i\nn/\nd . \n(12.26)\nNow we are left with two spatial integrals that are much simpler than the ones you would have \nobtained from the matrix elements of p4 directly. These integrals are quite common because they \ntell us the expectation values of different powers of the radial position. The radial operator does not \ninvolve the angular variables, so the angular parts of the integrals are unity Athe Rn/1r2 and Y m\n/ 1u, f2 \nwave functions are separately normalizedB and the matrix elements reduce to radial integrals that we \npresented in Chapter 8 [Eq. (8.89)]:\n \n h 1\nr i\nn\u0002\n=\nL\n\u0005\n0\n1\nr\n R 2\nn\u00021r2r 2 dr =\n1\nn2a0\n \n(12.27)\n \n h 1\nr 2 i\nn\u0002\n=\nL\n\u0005\n0\n1\nr 2 R 2\nn\u00021r2r 2 dr =\n1\n1/ + 1\n22n3a 2\n0\n. \nTo make the three terms in Eq. (12.26) have the same form and to collect constants, use the hydrogen \nenergy formula\n \nE (0)\nn\n= -  1\nn2 \ne2\n214pe02a0\n= -  1\n2n2 a2mc2  \n(12.28)\nto rewrite e2>4pe0\n \ne2\n4pe0\n= a0a2mc2,  \n(12.29)\nwith the ﬁnal result that\n \nE (1)\nrel = -  1\n2\n a4mc2 c\n1\nn31/ + 1\n22\n-\n3\n4n4d . \n(12.30)\nAs we expected, the relativistic correction is a2 times smaller than the Bohr energy and is negative.\n12.2.2 \u0002 Spin-Orbit Coupling\nSpin-orbit coupling is the second part of the hydrogen ﬁne structure. The orbital motion of the elec-\ntron causes the electron to experience a magnetic ﬁeld, which interacts with the spin magnetic moment \nof the electron, hence the name. This internal magnetic ﬁeld effect is distinct from effects due to exter-\nnal magnetic ﬁelds—the Zeeman effect—that we study later in this chapter. It is also distinct from the \nmagnetic ﬁeld generated by the nuclear spin, which causes the hyperﬁne interaction. The origin of \nthe internal orbital magnetic ﬁeld can be understood in either of two ways. In one view, the electron \nmoves in the electric ﬁeld of the proton, which gives rise to a motional magnetic ﬁeld. Or we can view \nthe problem from the electron’s point of view: the electron sees a proton orbiting it, which creates a \nmagnetic ﬁeld at the electron, with the same resultant interaction. Let’s use this second viewpoint to \ncalculate the effect classically and then extend it to quantum mechanics.\n",
    "12.2 Fine Structure of Hydrogen \n389\nWe treat the proton orbiting the electron as a current loop, as shown in Fig. 12.3, and use the \nBiot-Savart law to calculate the magnetic ﬁeld from that loop of current. The magnetic ﬁeld at the \ncenter of the loop is\n \nB = m0\n I\n2r , \n(12.31)\nwhere the current I is that of the proton with charge + e orbiting in a period T = 2pr>v, with v being \nthe speed. The speed of the proton in the electron frame is the same as the speed of the electron in the \nproton frame, so we relate the ﬁeld experienced by the electron to the electron angular momentum \nthrough its relation with the velocity:\n \nL = mvr \n \nv = L\nmr . \n(12.32)\nThe resulting magnetic ﬁeld is\n \nB = m0\n2r \neL\n2pmr 2 =\nm0eL\n4pmr 3 =\neL\n4pe0\n mc2r 3 .  \n(12.33)\nWe make this into a vector equation because B and L point in the same direction:\n \nB =\ne\n4pe0\n mc2r 3 L. \n(12.34)\nThe energy of interaction between a magnetic dipole and a magnetic ﬁeld is\n \nE = -M~B. \n(12.35)\nThe intrinsic (spin) magnetic moment of the electron is\n \nM = -  e\nm\n S , \n(12.36)\ne\np\u000f\nr\nB\nv\nFIGURE 12.3 A proton rotating about an electron generates a magnetic \nﬁeld at the electron position.\n",
    "390 \nPerturbation of Hydrogen\nusing the electron gyromagnetic ratio ge = 2. The resultant spin-orbit interaction energy is\n \n ESO = - a-  e\nm\n Sb~\ne\n4pe0mc2r 3 L \n(12.37)\n \n =\ne2\n4pe0\n m2c2r 3 L~S.\n \nBy substituting the quantum mechanical operators for spin and orbital angular momentum, we arrive \nat the Hamiltonian for the spin-orbit perturbation. However, the classical result in Eq. (12.37) is incor-\nrect by a factor of 1>2 due to Thomas precession—a relativistic effect due to the acceleration of the \nelectron. We include this correction in our quantum mechanical Hamiltonian:\n \nH =\nSO =\ne2\n8pe0\n m2c2r 3 L~S. \n(12.38)\nWe are now in a position to use the addition of angular momentum tools that we learned in \nChapter 11. We are including spin in the hydrogen atom now, so we must incorporate spin into the \neigenstates. Previously, we speciﬁed the hydrogen energy eigenstates as 0 n/m9. We now specify the \nzeroth-order eigenstates as 0 n/sm/ms9, where the subscript on the orbital magnetic quantum num-\nber m/ distinguishes it from the spin magnetic quantum number ms. The states 0 n/sm/ms9 are still \nzeroth-order energy eigenstates because spin does not play a role in the zeroth-order Hamiltonian. The \nstates 0 n/sm/ms9 are the “uncoupled states” we introduced in Chapter 11, but we learned there that we \ncould also use the “coupled basis” 0 n/sjmj9, which is characterized by the total angular momentum \nJ = L + S. In Chapter 11, we found that the coupled basis was preferred for the hyperﬁne interac-\ntion problem because the total angular momentum F = S + I is conserved, but the individual angular \nmomenta S and I are not due to the interaction S~I. Similarly, the L~S interaction in the spin-orbit \nperturbation Hamiltonian in Eq. (12.38) causes the individual angular momenta L and S to not be \nconserved and the total angular momentum J = L + S to be conserved. Hence, the spin-orbit Hamil-\ntonian is not diagonal in the uncoupled 0 n/sm/ms9basis, but it is diagonal in the coupled 0 n/sjmj9basis. \nOnce again, the “smart” choice is the coupled basis because it makes the diagonalization procedure \nrequired by degenerate perturbation theory much easier—trivial, in fact.\nAs we did with the hyperﬁne interaction in Chapter 11, we use the deﬁnition of the total angular \nmomentum to ﬁnd a convenient expression for the scalar product term L~S in the spin-orbit interac-\ntion in terms of coupled basis operators:\n \n J = L + S\n \n \n J2 = L2 + S2 + 2L~S\n \n(12.39)\n \n L~S = 1\n2\n 1J2 - L2 - S22. \nThe operators J 2, L2, and S2 are diagonal in the coupled basis, so the spin-orbit Hamiltonian is diago-\nnal. If we had chosen the uncoupled basis, then we would have expressed the scalar product L~S using \nuncoupled basis operators:\n \n L~S = Lx\n Sx + Ly\n Sy + Lz\n Sz\n \n(12.40)\n \n = 1\n2\n 1L +\n S - + L -\n S +2 + Lz\n Sz. \n",
    "12.2 Fine Structure of Hydrogen \n391\nL~S is not diagonal in the uncoupled basis because the angular momentum ladder operators connect \nadjacent states, just as we found for the S~I matrix in Eq. (11.38) for the hyperﬁne structure calcula-\ntion (Problem 12.2).\nAs we have noted several times, when the perturbing Hamiltonian is already diagonal within the \ndegenerate subspace, then ﬁrst-order nondegenerate and degenerate perturbation theory are equivalent. \nHence, the energy correction due to spin-orbit coupling is obtained by ﬁnding the expectation values\n \nE (1)\nSO = 8H =\nSO9 = 8n/sjmj@H =\nSO@ n/sjmj9, \n(12.41)\nwhich are the diagonal matrix elements of the perturbation Hamiltonian in the degenerate subspace in \nthe coupled basis. Substituting the spin-orbit Hamiltonian from Eq. (12.38) yields\n \nE (1)\nSO =\ne2\n8pe0m2c2 8n/sjmj@ 1\nr 3 L~S@ n/sjmj9, \n(12.42)\nand using Eq. (12.39), we ﬁnd\n \nE (1)\nSO =\ne2\n16pe0m2c2 h 1\nr 3 i\nn/\n8/sjmj0 J2 - L2 - S20 /sjmj9.  \n(12.43)\nThe angular momentum matrix element is\n \n8/sjmj 0 J2 - L2 - S20 /sjmj9 = 3 j1 j + 12 - /1/ + 12 - s1s + 124 U2, \n(12.44)\nand the radial matrix element is [from Chapter 8, Eq. (8.89)]\n \nh 1\nr 3 i\nn/\n=\n1\na 3\n0 n3/1/ + 1\n221/ + 12\n.  \n(12.45)\nCollecting constants, and writing the Bohr radius as a0 = U>amc, we ﬁnd the spin-orbit energy \ncorrection\n \nE (1)\nSO = 1\n4\n a4mc2  \nj1 j + 12 - /1/ + 12 - 3\n4\nn3/1/ + 1\n221/ + 12\n. \n(12.46)\nThe spin-orbit shift is a2 times smaller than the Bohr energy, as is the relativistic correction. For an s \nstate, with / = 0, the expression in Eq. (12.46) is problematic because the denominator is zero, but \nthe numerator is also zero because j = 1>2 is the only possibility for the total angular momentum \nquantum number when / = 0. This problem with the / = 0 case is not really a problem, because we \ndo not expect any spin-orbit coupling for s states with no orbital angular momentum to create a mag-\nnetic ﬁeld. Hence, the spin-orbit correction in Eq. (12.46) applies only to the cases / \u0002 0.\nThe total ﬁne-structure correction is the sum of the relativistic and spin-orbit corrections. If we \nignore the problem with the / = 0 spin-orbit term for the moment and add together the results in \nEqs. (12.30) and (12.46), we obtain (Problem 12.3):\n \nE (1)\nfs\n= E (1)\nrel + E (1)\nSO = -  1\n2\n a4mc2 1\nn3 c\n1\nj + 1\n2\n- 3\n4n d . \n(12.47)\n",
    "392 \nPerturbation of Hydrogen\nThis result depends on j, but not on /, so the problem of the / = 0 singularity is gone (see below) and \nmiraculously we can use Eq. (12.47) for all / levels. The j dependence in the ﬁne-structure correction \nlifts some of the degeneracy of the nonrelativistic E (0)\nn  levels in hydrogen, which are 2n2 degener-\nate when spin is included. The lifting of the degeneracy is depicted in Fig. 12.4 for the energy levels \nn = 1, 2, 3. States with the same quantum numbers n and j have the same ﬁne-structure energy, even \nthough they may have different values of the orbital angular momentum quantum number /. This \ndegeneracy is exact to all orders in the relativistic Dirac theory (but not in the real atom—see below).\nThe miraculous resolution of the / = 0 problem where the relativistic and spin-orbit terms add \nto give the total ﬁne-structure correction, masks a subtle physical effect. By ignoring the restriction of \nthe spin-orbit correction to / \u0002 0 as we did, the sum of the spin-orbit and relativistic corrections just \nhappens to include a new term for the / = 0 case. This new term is known as the Darwin term. The \nphysical explanation of the Darwin term requires relativistic quantum mechanics in the form of the \nDirac equation, which predicts that the electron wave function includes some components at relativistic \nenergies that lead to high frequency oscillation of the electron motion. This trembling or jittering motion \nof the electron—zitterbewegung in German—smears out the electron, making it appear bigger than an \nideal point particle. A larger electron is bound less strongly to a point nucleus if the electron-nucleus \nseparation is less than the effective size of the trembling electron. The zitterbewegung effect is still \nmuch smaller than the Bohr radius a0, so the Darwin term is limited to s-states because they are the only \nstates with a ﬁnite probability of being near the origin [Eq. (8.77)].\nThe separate contributions of the spin-orbit, relativistic, and Darwin terms to the ﬁne structure are \nshown in Fig. 12.5 for the n = 2 states. The /-dependent spin-orbit interaction splits the degeneracy \nof the n = 2 levels, but the relativistic effect plus the Darwin term brings the two j = 1>2 levels, \n2s1>2 and 2p1>2, back together. The sign difference between the spin-orbit corrections of the 2p1>2 and \n2p3>2 states arises from the differing relative orientations of the spin and orbital angular momenta in \nthese states, which is also evident later when we study the Zeeman effect.\nn \u0004\u00071\nn \u0004\u00072\nn \u0004\u00073\n,\n43.8 GHz\n2.74 GHz\n4.87 GHz\n1.62 GHz\n0.54 GHz\nBohr Energies\nFine Structure\n3d 5\n2\n3p 3\n2 3d 3\n2\n,\n3s 1\n2 3p 1\n2\n,\n2s 1\n2\n1s 1\n2\n2p 1\n2\n2p 3\n2\n13.7 GHz\nFIGURE 12.4 Fine structure of the n = 1, 2, 3 states of hydrogen. The vertical scale \nis different for each n level and the separation between n levels is not to scale.\n",
    "12.3 Zeeman Effect \n393\nThe degeneracy of j states in the Dirac model is ﬁnally lifted when we consider quantum electro-\ndynamics (QED). In QED, the electromagnetic ﬁeld is quantized in a manner similar to the harmonic \noscillator problem in Chapter 9. This approach works because the electromagnetic ﬁeld energy is the \nsum of squares 1E2 + B22 and the same concept of ladder operators is applicable. The ladder opera-\ntors of the electromagnetic ﬁeld correspond to the creation and annihilation of photons, with the state \n0  n9 representing n photons in one mode of the light ﬁeld. The ground state 0  09 represents the vacuum \nstate, where no photons are present. However, the ground state energy Uv>2 implies that there is some \nresidual electromagnetic ﬁeld in the vacuum, even when no photons are present. This residual ﬁeld acts \non the electron and causes it to move about and become smeared out—similar to the zitterbewegung of \nthe Darwin term (the same effect but a different cause). The result of this perturbation is that s-states \nare bound less tightly and are shifted up slightly in energy, as shown in Fig. 12.2. This shift is known as \nthe Lamb shift, named after Willis Lamb who discovered this effect in 1947. Lamb and his graduate \nstudent Robert Retherford measured the energy difference between the 2s1>2 and 2p1>2 states by induc-\ning transitions between these two states using microwave equipment that had been developed during \nWorld War II. Soon after the experiment, Hans Bethe made a theoretical estimate of the effect, which \nlaid the groundwork for the development of QED. Willis Lamb won the Nobel Prize in physics in 1955 \nfor this groundbreaking work.\n12.3 \u0002 ZEEMAN EFFECT\nThe Zeeman effect is the shift of atomic energy levels caused by an external applied magnetic ﬁeld. \nThe applied ﬁeld couples to the magnetic moments in the atom associated with the orbital and spin \nangular momenta of the electron and the proton spin angular momentum. The Zeeman effect occurs in \nall atoms, but we limit this presentation to the hydrogen atom.\nThe energy of interaction between the applied magnetic ﬁeld and the magnetic moments in the \natom is\n \nE = -M~B. \n(12.48)\nThe magnetic moments of the electron are proportional to the Bohr magneton\n \nmB = e U\n2me\n\u0002 h * 1.4 MHz\nGauss . \n(12.49)\nn \u0007 2\nBohr\nSpin-Orbit\nRelativistic\nDarwin\n2p 3\n2\n2p 1\n2\n2p 1\n2\n2p 1\n2\n2s 1\n2\n2s 1\n2\n2s 1\n2\n2p 3\n2\n2p 3\n2\nFIGURE 12.5 Hydrogen ﬁne structure in the n \u0003 2 level.\n",
    "394 \nPerturbation of Hydrogen\nThe magnetic moment of the proton is approximately 1000 times smaller because of the large proton \nmass, so we neglect the proton contribution to the Zeeman effect.\n12.3.1 \u0002 Zeeman Effect without Spin\nLet’s begin by considering a spin-independent Zeeman effect, where we ignore the spin of the elec-\ntron. This effect is called the “normal Zeeman effect” in the literature, but that name is misleading \nbecause the spin and orbital magnetic moments contribute equally. Nevertheless, we use this unrealis-\ntic model because it is an easy calculation and it introduces us to the essential features of the Zeeman \neffect. Because we are ignoring the electron spin, we also ignore the ﬁne structure and consider the \nmodel of the hydrogen atom we used in Chapter 8 that includes only the kinetic energy and the Coulomb \ninteraction energy, with eigenstates 0 n/m/9 and Bohr energies E (0)\nn\n= -  Ryd>n2. We perturb the system \nby applying a magnetic ﬁeld B aligned along the z-axis. The electron magnetic moment associated with \nthe orbital motion\n \nML = -  e\n2me\n L = -  g/\n mB\nL\nU  \n(12.50)\ninteracts with the applied ﬁeld B, giving an energy\n \n E = -ML~B\n \n(12.51)\n \n = g/\n mB 1\nU\n L~B, \nwhere g/ = 1 is the orbital gyromagnetic ratio (we keep the g/ so we can distinguish orbital and spin \ncontributions later). The resultant Zeeman perturbation Hamiltonian is\n \n H =\nZ = g/\n mB 1\nU\n L~B \n(12.52)\n \n = g/\n mB B\nU\n Lz.  \nThe zeroth-order energy eigenstates 0 n/m/9 are degenerate, so we must use degenerate perturba-\ntion theory to ﬁnd the energy corrections caused by the Zeeman perturbation in Eq. (12.52). However, \nthe states 0 n/m/9 are eigenstates of Lz, so once again we have made the “smart” choice of basis. The \nmatrix representing H =\nZ is diagonal, so the ﬁrst-order energy corrections are the diagonal elements of \nthe perturbation Hamiltonian\n \n E (1)\nZ\n= 8H =\nZ9 = 8n/m/@H =\nZ@ n/m/9 \n \n = g/mB\nB\nU8n/m/0 Lz0 n/m/9.\n \n(12.53)\nEvaluating the matrix elements yields\n \nE (1)\nZ\n= g/\n mBBm/  . \n(12.54)\nThe normal Zeeman effect is proportional to the orbital gyromagnetic ratio g/, the Bohr magneton mB, \nthe magnetic ﬁeld strength B, and the magnetic quantum number m/. This general form is repeated \n",
    "12.3 Zeeman Effect \n395\nwhen we study the Zeeman effect in more realistic models. The gyromagnetic ratio and the particular \nmagnetic quantum number change as we include other magnetic moments in the model. A typical \nZeeman energy level diagram without spin is shown in Fig. 12.6 for the 2p state of hydrogen, with \nthe energy shifts proportional to m/ and the applied ﬁeld strength B. The m/ = 1 state has the orbital \nangular momentum aligned with the ﬁeld, which means that the magnetic moment is anti-aligned, and \nthe magnetic interaction energy is therefore positive. The degeneracy present in the zeroth-order state \nis lifted by the perturbation.\nThe Zeeman energy structure displayed in Fig. 12.6 provides a better understanding of the Stern-\nGerlach experiment we studied in Chapter 1. The experiments in Chapter 1 measured spin magnetic \nmoments, but the Stern-Gerlach effect applies equally well to the measurement of magnetic moments \narising from orbital angular momentum as in the normal Zeeman effect. In a Stern-Gerlach device, the \nexternal magnetic ﬁeld varies spatially, which implies a spatially varying Zeeman energy perturba-\ntion. A spatial dependence of the energy (strictly speaking a potential energy, which is the case here) \ngives rise to a force\n \n Fz = -  0\n0z E (1)\nz\n \n \n = -  \n0E (1)\nz\n0B  0B\n0z\n \n(12.55)\n \n \n = -g/\n mB\n m/ 0B\n0z . \nEach value of the magnetic quantum number m/ leads to a different value of the force and hence to a \ndifferent deﬂection of the beam in a Stern-Gerlach device. For example, a p state has three m/ values \n11, 0, -12 corresponding to the three energy levels in the Zeeman structure of Fig. 12.6, and results in \nthree beams exiting a Stern-Gerlach analyzer, as depicted in Fig. 12.7. The effective magnetic moment \nof the atom is given by the slope -0E (1)\nz >0B of the Zeeman energy plot, which is -g/\n mB\n m/ for the \nZeeman effect without spin.\nBext\nE2p\nE\nml \u0004\u00071\nml \u0004\u00070\nml \u0004\u0007\n1\nFIGURE 12.6 Zeeman level structure of the hydrogen 2p state, ignoring spin.\n",
    "396 \nPerturbation of Hydrogen\n12.3.2 \u0002 Zeeman Effect with Spin\nA more realistic model of the atom includes the electron spin. In this case, the Zeeman effect is referred \nto as the “anomalous Zeeman effect” in the literature because it was discovered experimentally before \nspin was known and it did not agree with the expected “normal” Zeeman effect predictions. In the \nZeeman effect with spin, we include the interaction of the spin magnetic moment with the external \napplied ﬁeld, in addition to the interaction of the orbital magnetic moment with the applied ﬁeld as we \ndid in the previous section. Thus, the total atomic magnetic moment is\n \nM = MS + ML = -ge e\n2me\n S - g/ e\n2me\n L = -gemB S\nU - g/mB\nL\nU ,  \n(12.56)\nwhere the spin gyromagnetic ratio ge is approximately 2 Awith its own correction of order a2B. The \ninteraction Hamiltonian is\n \n H =\nz = -M~B = mB\nU\n 1g/L + geS2~B \n \n = mBB\nU\n 1g/\n Lz + ge\n Sz2.\n \n(12.57)\nIn this more realistic model of the atom, we must include the ﬁne structure that we calculated \nin Section 12.2. The relative roles of the ﬁne structure and the Zeeman effect are determined by \nthe magnitudes of the energy corrections caused by each effect. The magnitude of the ﬁne-structure \ncorrections are displayed in Fig. 12.4 for a few states. The magnitude of the Zeeman corrections scale \nwith the magnitude of the applied ﬁeld, as illustrated in Fig. 12.6. The Zeeman effect applied to the \nﬁne-structure states of Fig. 12.4 lifts some of the degeneracy and splits levels into Zeeman structures \nlike Fig. 12.6. For small enough magnetic ﬁelds, the Zeeman corrections are much smaller than \nthe ﬁne-structure splittings, in which case we treat the ﬁne structure as part of the zeroth-order \nHamiltonian and treat the Zeeman effect as a small perturbation. For large magnetic ﬁelds, we include \nthe Zeeman effect in the zeroth-order Hamiltonian, and treat the ﬁne structure as a small perturbation. \nFor magnetic ﬁelds where the ﬁne structure and Zeeman corrections are comparable, we must treat \nboth effects as one perturbation.\n12.3.2.1 \u0002 Weak magnetic ﬁeld\nLet’s start with the weak magnetic ﬁeld case, and treat the ﬁne structure as part of the zeroth-order \nHamiltonian. In this case, the zeroth-order states are the coupled basis states @ n/sjmj9 in which the ﬁne \nstructure is diagonal, and the zeroth-order energies are the Bohr energies E (0)\nn  plus the ﬁne-structure \n?\n?\nLz\n0\n?\n1\n\n1\n\u00021\u0003\u0007\n\u00020\u0003\u0007\n\u0002\n1\u0003\nFIGURE 12.7 Stern-Gerlach measurement of the orbital \nangular momentum component Lz.\n",
    "12.3 Zeeman Effect \n397\nshifts in Eq. (12.47). Our task is to ﬁnd the effect of the Zeeman perturbation in Eq. (12.57) on these \nzeroth-order energies.\nInspection of the Zeeman interaction Hamiltonian in Eq. (12.57) shows that it is diagonal in \nthe uncoupled basis @ n/sm/ms9, but it is not diagonal in the coupled basis @ n/sjmj9. Unfortunately, \nwe must use the coupled basis because that is the basis we used to diagonalize the zeroth-order \nHamiltonian, which now includes the ﬁne structure. We no longer have the freedom to search for a \n“good” basis where the perturbation is diagonal. There doesn’t appear to be a straightforward way \nto ﬁnd a general expression for the Zeeman energy shifts in this case. Of course, we can solve the \nproblem by brute force by ﬁnding the matrix representations in the coupled basis of the non-diagonal \nLz and Sz matrices that comprise the Zeeman Hamiltonian, and then diagonalizing the perturbation \nHamiltonian in a degenerate subspace, which is given by a speciﬁc n and j in this case. It turns out \nthat there is a way to solve this problem in general, but it requires some advanced concepts from \nangular momentum theory that we do not have yet. So, to motivate why these advanced concepts \nwork, let’s do one problem by brute force and then quote without proof the angular momentum con-\ncepts we need to derive a general result.\nThe angular momentum operators Lz and Sz are both diagonal in the uncoupled basis 0 n/sm/ms9, \nand we know that the uncoupled and coupled bases are connected through the Clebsch-Gordan coef-\nﬁcients. Hence, it is straightforward (but tedious) using the Clebsch-Gordan expansion\n \n@  jmj9 = a\nm/\n ms\n@ /sm/\n ms98/sm/\n ms@ jmj9 \n(12.58)\nto ﬁnd the matrix representations of Lz and Sz in the coupled basis. We’ll calculate one matrix element \nto demonstrate the method and leave the others for you to do (Problem 12.6 and activity on Zeeman \nperturbation matrices in the coupled basis).\nConsider the hydrogen 2p states with n = 2, / = 1, and s = 1>2. The allowed values of the \ncoupled basis quantum number j 1J = L + S2 are j = 3>2 and j = 1>2. Table 11.3 of Clebsch-\nGordan coefﬁcients tells us that two particular coupled states are\n \n @ 3\n2 1\n29 =\n1\n13 @1 1\n2 1 -1\n2 9 + 4\n2\n3 @ 1 1\n2 0 1\n29  \n \n @ 1\n2 1\n29 = 4\n2\n3 @1 1\n2 1 -1\n2 9 -\n1\n13 @1 1\n2 0 1\n29. \n(12.59)\nUsing these expansions, we calculate the matrix element of Lz in the coupled basis in terms of uncoupled-\nbasis matrix elements:\n \n81\n2 1\n2 @Lz@ 3\n2 1\n29 = 14\n2\n3 81 1\n2 1 -1\n2 @ -\n1\n1381 1\n2 0 1\n2 @2Lz1 1\n13 @1 1\n2 1 -1\n2 9 + 4\n2\n3 @1 1\n2 0 1\n292.  \n(12.60)\nThe uncoupled-basis matrix elements are diagonal, leaving\n \n81\n2 1\n2 @Lz@ 3\n2 1\n29 = 12\n3 81 1\n2 1 -1\n2 0 Lz@ 1 1\n2 1 -1\n2 9 - 12\n3 81 1\n2 0 1\n2 @Lz@ 1 1\n2 0 1\n29.  \n(12.61)\nThe diagonal uncoupled-basis matrix elements are m/\n U, yielding the result\n \n 81\n2 1\n2 @Lz@ 3\n2 1\n29 = 12\n3\n 11U2 - 12\n3\n 10U2 \n \n = 12\n3  U.\n \n \n(12.62)\n",
    "398 \nPerturbation of Hydrogen\nRepeating this calculation for all possible values of j, mj in the hydrogen 2p level, we ﬁnd the com-\nplete Lz and Sz matrices in the coupled basis (Problem 12.6):\n \n Lz \u0003 U ß  \n1\n0\n0\n0\n0\n0\n0\n1\n3\n0\n0\n12\n3\n0\n0\n0\n1\n3\n0\n0\n12\n3\n0\n0\n0\n-1\n0\n0\n0\n12\n3\n0\n0\n2\n3\n0\n0\n0\n12\n3\n0\n0\n2\n3\n ∑ \n3\n2 , 3\n2 \n3\n2 , 1\n2 \n3\n2 , -1\n2  \n3\n2 , -3\n2  \n1\n2 , 1\n2 \n1\n2 , -1\n2  \n  \n(12.63)\n \n Sz \u0003 U ß  \n1\n2\n0\n0\n0\n0\n0\n0\n1\n6\n0\n0\n-12\n3\n0\n0\n0\n-1\n6\n0\n0\n-12\n3\n0\n0\n0\n-1\n2\n0\n0\n0\n-12\n3\n0\n0\n-1\n6\n0\n0\n0\n-12\n3\n0\n0\n1\n6\n∑ \n3\n2 , 3\n2 \n3\n2 , 1\n2 \n3\n2 , -1\n2  \n3\n2 , -3\n2  \n1\n2 , 1\n2 \n1\n2 , -1\n2  \n, \n(12.64)\nwhere we have labeled the rows and columns with the j, mj quantum numbers and boxed subspaces as \ndiscussed below. Degenerate perturbation theory requires us to construct the perturbation Hamiltonian \nEq. (12.57) using these matrices and then diagonalize the resultant. However, we need do that only \nwithin each degenerate subspace. The ﬁne structure lifts the degeneracy of the j = 3>2 and j = 1>2 \nstates, so we treat these subspaces separately. Inspection of the Lz and Sz matrices in Eqs. (12.63) and \n(12.64) shows that the only off-diagonal matrix elements in Lz and Sz are between states with different \nvalues of j, which have different ﬁne-structure shifts. Lz and Sz are each diagonal within the separate \nj = 3>2 and j = 1>2 subspaces boxed in Eqs. (12.63) and (12.64) and we can neglect the off-diagonal \nelements in applying degenerate perturbation theory. We got lucky! We did not have the freedom to \nchoose a basis to make the perturbation diagonal, but in the basis of ﬁne-structure eigenstates, the \nZeeman perturbation is already diagonal within each degenerate subspace. For the j = 3>2 subspace, \nthe Zeeman Hamiltonian is represented by the matrix\n \nH =\nZ \u0003 mBB •\n2\n0\n0\n0\n0\n2\n3\n0\n0\n0\n0\n-2\n3\n0\n0\n0\n0\n-2\nμ \n3\n2 , 3\n2 \n3\n2 , 1\n2\n3\n2 , -1\n2  \n3\n2 , -3\n2  \n, \n(12.65)\nwhere we have substituted the gyromagnetic ratios: g/ = 1, ge = 2. From the matrix in Eq. (12.65), \nwe ﬁnd the Zeeman energy shifts by inspection to be 2mBB, 2mBB>3, -2mBB>3, -2mBB. For the \nj = 1>2 subspace, the Zeeman Hamiltonian must also include the 2s1>2 states in addition to the 2p1>2 \nstates (Problem 12.7).\n",
    "12.3 Zeeman Effect \n399\nTo ﬁnd a general expression for the Zeeman energy shift rather than solving each case by matrix \nconstruction, we invoke a result from advanced angular momentum theory. To motivate this new idea, \nconsider the total angular momentum component operator Jz = Lz + Sz. For the hydrogen 2p state, \nthe matrix representing Jz is\n \n Jz \u0003 U ß\n   \n3\n2\n0\n0\n0\n0\n0\n0\n1\n2\n0\n0\n0\n0\n0\n0\n-1\n2\n0\n0\n0\n0\n0\n0\n-3\n2\n0\n0\n0\n0\n0\n0\n1\n2\n0\n0\n0\n0\n0\n0\n-1\n2\n    ∑  \n3\n2 , 3\n2 \n3\n2 , 1\n2 \n3\n2 , -1\n2  \n3\n2 , -3\n2  \n1\n2 , 1\n2 \n1\n2 , -1\n2  \n. \n(12.66)\nCompare this Jz matrix to the Lz and Sz matrices in Eqs. (12.63) and (12.64), concentrating on the \nseparate j = 3>2 and j = 1>2 boxed subspaces and ignoring the off-diagonal elements in Lz and Sz. \nThe submatrices for Lz and Sz within a given j subspace are proportional to the Jz submatrix in that \nsame subspace, with proportionality factors that are j-dependent. For Lz, the proportionality factors \nare 2>3 for the j = 3>2 case and 4>3 for the j = 1>2 case. For Sz, the proportionality factors are \n1>3 for the j = 3>2 case and -1>3 for the j = 1>2 case. These relations between the matrices in \nthe coupled basis that represent Lz and Sz and the matrix that represents the total angular momen-\ntum Jz are speciﬁc examples of the Wigner-Eckhart theorem, which is a fundamental part of the \ntheory of angular momentum.\nThe speciﬁc formula we need from the Wigner-Eckhart theorem relates the matrix element of \nany general vector component Vz to the matrix element of the total angular momentum component Jz:\n \n8 jmj@ Vz@ jm=\nj9 =\n8 jmj@V~J@  jmj9\nU2j1 j +12\n 8 jmj@  Jz@ jm=\nj9. \n(12.67)\nFor the Zeeman calculation, L or S play the role of the vector V. Equation (12.67) is called the projec-\ntion theorem because of the role of the projection V~J in determining the constant of proportional-\nity between the matrix elements of Vz and Jz. Note that the matrix element of the projection V~J is a \ndiagonal element, but the Vz and Jz matrix elements are general matrix elements between different mj \nstates within a given j subspace.\nTo use the projection theorem, we need to know the diagonal matrix elements 8 jmj0 V~J0  jmj9, \nwhich depend on the vector V we are using. For the orbital angular momentum L, the required projec-\ntion is obtained from the relation\n \nJ = L + S \n(12.68)\nby rearranging and squaring:\n \n S = J - L\n \n \n S2 = J2 + L2 - 2L~J\n \n(12.69)\n \n L~J = 1\n21J2 + L2 - S22. \n",
    "400 \nPerturbation of Hydrogen\nSimilarly, for the spin angular momentum:\n \nS~J = 1\n21J2 + S2 - L22. \n(12.70)\nHence, the diagonal matrix elements required in the projection theorem are\n \n 8 jmj@S~J@  jmj9 = U2\n2\n 3 j1 j +12 + s1s +12 - /1/ +124  \n \n 8 jmj@L~J@  jmj9 = U2\n2\n 3 j1 j +12 + /1/ +12 - s1s +124. \n(12.71)\nUsing these coefﬁcients in the projection theorem yields the Lz and Sz matrix elements in the coupled \nbasis within a given j subspace\n \n 8 jmj@Sz@  jm =\nj9 =\nj1 j +12 + s1s +12 - /1/ +12\n2 j1 j +12\n 8 jmj@ Jz@ jm =\nj9  \n \n 8 jmj@Lz@  jm =\nj9 =\nj1 j +12 + /1/ +12 - s1s +12\n2 j1 j +12\n 8 jmj@ Jz@ jm =\nj9. \n(12.72)\nThus, the projection theorem has allowed us to ﬁnd the matrix representations, within a speciﬁc sub-\nspace, of operators that are not diagonal in the coupled basis. Because Jz is diagonal in the coupled \nbasis, Eq. (12.72) tells us that Lz and Sz are also diagonal within a given j subspace, as we saw in \nEqs. (12.63) and (12.64). For the hydrogen 2p example, the proportionality constants in Eq. (12.72) \nare exactly those we found by inspection above.\nWe put all these results together to ﬁnd the ﬁrst-order Zeeman energy correction:\n \n E (1)\nZ\n= 8H =\nZ9 = 8n/sjmj@ H =\nZ@n/sjmj9\n \n = 8n/sjmj@ mBB\nU\n 1g/\n Lz + ge\n Sz2@ n/sjmj9\n \n = mBB\nU\n 1g/8n/sjmj@Lz@ n/sjmj9 + ge8n/sjmj@Sz@ n/sjmj92\n  (12.73)\n \n = mBB\nU\n ag/ \nj1 j +12+/1/ +12-s1s +12\n2 j1 j +12\n+ ge \nj1 j +12+s1s +12-/1/ +12\n2 j1 j +12\nb8n/sjmj@ Jz@n/sjmj9\n \n = mBB\nU\n ag/ \nj1 j +12+/1/ +12-s1s +12\n2 j1 j +12\n+ ge \nj1 j +12+s1s +12-/1/ +12\n2 j1 j +12\nb mj\n U.\nThis result can be written in the standard Zeeman form\n \nE (1)\nZ\n= gjmBBmj \n(12.74)\n",
    "12.3 Zeeman Effect \n401\nif we deﬁne a new gyromagnetic ratio gj :\n \ngj = g/ \nj1 j +12 + /1/ +12 - s1s +12\n2 j1 j  +12\n+ ge \nj1 j +12 + s1s +12 - /1/ +12\n2 j1 j +12\n, \n(12.75)\nwhich we refer to as the Landé g factor. This gyromagnetic ratio accounts for the relative contributions \nof the magnetic moments due to the spin and orbital angular momenta caused by their differing magni-\ntudes (gyromagnetic ratios: g/ = 1, ge = 2) and differing alignments (projection theorem). Substituting \nthe gyromagnetic ratios into Eq. (12.75), we obtain the Landé g factor\n \ngj = 1 +\nj1 j +12 + s1s +12 - /1/ +12\n2 j1 j +12\n. \n(12.76)\nFor the hydrogen 2p example, the Landé factors are g3>2 = 4>3 and g1>2 = 2>3. For the 2s1>2 state, \nthe Landé g factor is 2, i.e. gj = ge because the only magnetic moment comes from the electron spin in \nthat state. For hydrogen, with only one electron, s = 1>2, so j = /{1>2, and we can write the Landé \ng factor in general as\n \ngj = 1{ \n1\n2/ + 1. \n(12.77)\nWe thus get a Zeeman energy correction that is dependent on j and /:\n \nE (1)\nZ\n= mBBmj a\n 1{ \n1\n2/ + 1b, \n(12.78)\nwith the { sign for j = /{1>2. Figure 12.8 shows the weak-ﬁeld Zeeman level splittings in the 2p \nlevels of hydrogen.\nBext\nE\n3/2 3/2\n3/2 1/2\n1/2 1/2\n3/2 \n1/2\n1/2 \n1/2\n3/2 \n3/2\nj\nmj\n2p 1\n2\n2p 3\n2\n FIGURE 12.8 Weak-ﬁeld Zeeman structure of the hydrogen 2p ﬁne-structure levels \nlabeled with the quantum numbers of the coupled basis states.\n",
    "402 \nPerturbation of Hydrogen\n12.3.2.2 \u0002 Strong magnetic ﬁeld\nNow consider the case where the magnetic ﬁeld is strong enough that the Zeeman shifts are much \nlarger than the ﬁne-structure shifts. The perturbation assumption regarding the Zeeman effect is no \nlonger valid and it is more appropriate to include the Zeeman Hamiltonian\n \nH =\nZ = mBB\nU\n 1g/\n Lz + ge\n Sz2 \n(12.79)\nin zeroth-order and treat the ﬁne structure as a perturbation. In that case, the uncoupled basis 0 n/sm/ms9 \nis the preferred basis because the Zeeman Hamiltonian is diagonal in that basis. With this new choice, \nthe zeroth-order energies are the Bohr energies plus the the Zeeman corrections:\n \nE (0)\nn\n= -  Ryd\nn2\n+ 8n/sm/\n ms@ H =\nZ@n/sm/\n ms9. \n(12.80)\nThe additional Zeeman energies are the expectation values\n \n \u0006EZeeman = 8n/sm/\n ms@H =\nZ@ n/sm/\n ms9\n \n \n = 8n/sm/\n ms@  mBB\nU\n 1g/\n Lz + ge\n Sz2@ n/sm/\n ms9 \n \n(12.81)\n \n = mBB\nU\n 1g/\n m/\n U + ge\n ms\n U2.\n \nSubstituting the values g/ = 1 and ge = 2 yields\n \n\u0006EZeeman = mBB1m/ + 2ms2 \n(12.82)\nfor the strong-ﬁeld Zeeman effect. These zeroth-order energies for the 2p state as a function of mag-\nnetic ﬁeld are shown as dashed lines in Fig. 12.9, keeping in mind that these are valid only at high \nﬁelds. The Zeeman effect lifts most of the degeneracy of the Bohr energies.\nNow we treat the ﬁne structure as a perturbation to the zeroth-order states that include the \nZeeman interaction. Because we are using the uncoupled basis, we have to revisit our calculations \nof the ﬁne-structure corrections. The relativistic Hamiltonian is diagonal in both the uncoupled and \ncoupled bases, but the spin-orbit Hamiltonian is diagonal only in the coupled basis. However, the \noff-diagonal matrix elements of the spin-orbit Hamiltonian do not couple any of the states that are \ndegenerate with respect to the Zeeman Hamiltonian (Problem 12.8). Hence we can use nondegenerate \nperturbation theory to ﬁnd the ﬁrst-order ﬁne-structure corrections. In the uncoupled basis, the spin-\norbit energy corrections are the expectation values\n \n E (1)\nSO = 8H =\nSO9 = 8n/sm/\n ms@ H =\nSO@n/sm/\n ms9\n \n \n =\ne2\n8pe0\n m2c2 8n/sm/\n ms@ 1\nr 3 L~S@ n/sm/\n ms9\n \n \n =\ne2\n8pe0\n m2c2 h 1\nr 3 i\nn/8/sm/\n ms@  1\n2 1L +\n S - + L -\n S +2 + Lz\n Sz@/sm/\n ms9. \n(12.83)\n",
    "12.3 Zeeman Effect \n403\nBext\n2p\nE\nml ms\n1\n1\n2\n1\n\n1\n2\n\n1\n1\n2\n\n1 \n1\n2\n0\n\n1\n2\n0\n1\n2\n FIGURE 12.9 Strong-ﬁeld Zeeman structure of the 2p states of hydrogen. Solid lines \nshow the ﬁne-structure corrections to the Zeeman levels (dashed lines). The quantum  \nnumbers indicate the uncoupled basis states.\nBecause the ladder operators connect adjacent states, they produce no diagonal matrix elements and \nEq. (12.83) reduces to\n \n E (1)\nSO =\ne2\n8pe0\n m2c2 h 1\nr 3 i\nn/8/sm/\n ms\n @Lz\n Sz@ /sm/\n ms9 \n \n =\ne2\n8pe0\n m2c2 h 1\nr 3 i\nn/\nm/\n ms\n U2.\n \n(12.84)\nUsing Eq. (12.45) for the radial matrix element, we obtain\n \nE (1)\nSO = 1\n2\n a4mc2 \nm/\n ms\nn3/1/ + 1\n221/ + 12\n. \n(12.85)\nAdding together the relativistic [Eq. (12.30)] and spin-orbit corrections yields the ﬁne-structure shifts\n \n E (1)\nfs\n= 1\n2\n a4mc2 J 3\n4n3 -\n/1/ + 12 - m/\n ms\nn3/1/ + 1\n221/ + 12 R,  \n(12.86)\nwhich are shown in Fig. 12.9 for the 2p states.\n12.3.2.3 \u0002 Intermediate magnetic ﬁeld\nIn the intermediate magnetic ﬁeld regime where the Zeeman corrections and the ﬁne-structure correc-\ntions are comparable, we have to treat both effects as perturbations to the zeroth-order Bohr energy \nlevels. We then have to diagonalize the perturbation Hamiltonian H\u0004 = H =\nfs + H =\nZ in each degener-\nate E (0)\nn  energy subspace. In the uncoupled basis @ n/sm/ms9, H =\nZ is diagonal but H =\nfs is not diagonal, \nwhile in the coupled basis @ n/sjmj9, H =\nfs is diagonal but H =\nZ is not diagonal. Hence, there is no obvious \n",
    "404 \nPerturbation of Hydrogen\npreferred basis; we have to do some work to diagonalize the perturbation Hamiltonian matrix in either \ncase. For example, in the n \u0003 2 subspace the matrix representing the perturbation Hamiltonian in the \ncoupled basis is [we have left out the 2s states here—they do not couple to the 2p states so they can be \ndiagonalized separately (Problem 12.9)]\n \nH\u0004 \u0003 ß\n-a + 2b\n0\n0\n0\n0\n0\n0\n-a + 2\n3b\n0\n0\n- 12\n3 b\n0\n0\n0\n-a - 2\n3b\n0\n0\n- 12\n3 b\n0\n0\n0\n-a - 2b\n0\n0\n0\n- 12\n3 b\n0\n0\n-5a + 1\n3b\n0\n0\n0\n- 12\n3 b\n0\n0\n-5a - 1\n3b\n∑ \n3\n2 , 3\n2 \n3\n2 , 1\n2 \n3\n2 , -1\n2  \n3\n2 , -3\n2  \n1\n2 , 1\n2 \n1\n2 , -1\n2  \n, \n(12.87)\nwhere we have deﬁned\n \n a =\n1\n128 a4mc2 \n \n b = mBB.\n \n(12.88)\nThe off-diagonal terms in Eq. (12.87) come from the off-diagonal matrix elements of the Lz and Sz \nmatrices in Eqs. (12.63) and (12.64). We were able to ignore those terms in the weak-ﬁeld Zeeman \ncalculation because they represented coupling between different zeroth-order degenerate subspaces. \nIn the intermediate ﬁeld case, these terms represent coupling between states within the same degen-\nerate subspace, so they must be included in the degenerate perturbation theory calculation. Diago-\nnalization of the matrix in Eq. (12.87) yields the energy shifts shown in Fig. 12.10 (Problem 12.9). \nBext\nE\nWeak Field\nIntermediate Field\nStrong Field\n1\nml ms\nmJ\n1\n2\n\n1\n1\n2\n1 \n1\n2\n\n1 \n1\n2\n0 \n1\n2\n0\n1\n2\n3\n2\n1\n2\n1\n2\n\n1\n2\n\n1\n2\n\n3\n2\n2p 1\n2\n2p 3\n2\n FIGURE 12.10 Perturbation of the hydrogen 2p states caused by the Zeeman effect and the \nﬁne structure as a function of the applied magnetic ﬁeld.\n",
    "12.3 Zeeman Effect \n405\nNote the linear-to-quadratic-to-linear behavior of some of the energy shifts as the applied ﬁeld goes \nfrom weak to intermediate to strong. For the weak ﬁeld case, the coupled basis is the “good” basis with \nstates deﬁned by the quantum numbers j and mj as in Fig. 12.8, while for the strong ﬁeld, the uncou-\npled basis is the “good” basis, with states deﬁned by the quantum numbers m/ and ms as in Fig. 12.9. \nThe results of the intermediate-ﬁeld calculation are valid at all ﬁelds and give the previous results in \nthe appropriate limits (Problem 12.9).\n12.3.3 \u0002 Zeeman Perturbation of the 1s Hyperﬁne Structure\nLet’s study the Zeeman effect in the ground state of hydrogen. The only energy level structure in the \nhydrogen ground state is the hyperﬁne structure we studied in Chapter 11. The ﬁne-structure effects \nand the QED Lamb shift do not lift any of the degeneracy in the ground state—they only shift the \nlevel, as shown in Fig. 12.2. Hence we include the ﬁne structure and Lamb shifts as part of the zeroth-\norder energy and treat the Zeeman effect and the hyperﬁne interaction as the two relevant perturba-\ntions for this problem.\nIn Chapter 11, we found the energy eigenvalues and eigenstates of the hyperﬁne perturbation in \nthe 1s1>2 ground state of hydrogen. We found that the hyperﬁne Hamiltonian\n \nH =\nhf = A\nU2 S~I \n(12.89)\nwas diagonal in the coupled basis of 0 FMF9 states. For the 1s1>2 state of hydrogen, the orbital angular \nmomentum is zero and the Zeeman Hamiltonian of Eq. (12.57) reduces to\n \nH =\nZ = 2mBB\nU\n Sz, \n(12.90)\nwhere we have set ge = 2.\nThe solution of this problem is analogous to the anomalous Zeeman effect where we had two per-\nturbations: the ﬁne structure and the Zeeman effect. In this problem, the hyperﬁne interaction takes the \nplace of the ﬁne-structure interactions, and we also consider three regimes of magnetic ﬁeld strength \ncorresponding to the relative magnitudes of the Zeeman and hyperﬁne shifts. For weak ﬁelds, we (1) \ninclude the hyperﬁne structure in the zeroth-order Hamiltonian, (2) use the coupled states 0 FMF9 as \nthe “good” basis because the hyperﬁne Hamiltonian is diagonal in that basis, and (3) treat the Zeeman \neffect as a perturbation. For strong ﬁelds, we (1) include the Zeeman effect in the zeroth-order Hamil-\ntonian, (2) use the uncoupled states 0 sIms\n mI9 as the “good” basis because the Zeeman Hamiltonian is \ndiagonal in that basis, and (3) treat the hyperﬁne interaction as a perturbation. In intermediate ﬁelds, \nwe treat both the hyperﬁne interaction and the Zeeman effect as perturbations and diagonalize the full \nperturbation Hamiltonian in whichever basis we like because neither is preferred. We’ll do the inter-\nmediate case and leave the weak and strong cases for homework (Problem 12.12).\nIn the coupled basis, the hyperﬁne interaction is diagonal, as given in Eq. (11.61):\n \nH =\nhfs \u0003 A\n4 §\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n-3\n¥ \n11\n10\n1, -1\n00\n , \n(12.91)\n",
    "406 \nPerturbation of Hydrogen\nwhere A is the hyperﬁne splitting between the degenerate F = 1 triplet states and the F = 0 singlet \nstate and the matrix rows are labeled with the coupled basis quantum numbers F and MF. In this same \nbasis, the Zeeman Hamiltonian is (Problem 12.12)\n \nH =\nZ \u0003 mBB §\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n-1\n0\n0\n1\n0\n0\n¥ \n11\n10\n1, -1\n00\n . \n(12.92)\nDiagonalization of the sum of these two matrices yields the energies plotted in Fig. 12.11 (Problem \n12.13). In weak ﬁelds, the Zeeman shift is linear in the ﬁeld and proportional to the coupled basis \nmagnetic quantum number MF , analogous to the weak ﬁeld result in Eq. (12.78) and in Fig. 12.8. In \nstrong ﬁelds, the Zeeman shift is linear in the ﬁeld and proportional to the uncoupled basis magnetic \nquantum number ms, analogous to the strong ﬁeld result in Eq. (12.82) and in Fig. 12.9. In intermedi-\nate ﬁelds, neither basis is a “good” basis and some of the energies exhibit quadratic dependence on the \nﬁeld strength, analogous to Fig. 12.10.\nThe deﬂection of an atom in a Stern-Gerlach device is proportional to the slope -0E (1)\nz >0B of the \nZeeman energy plot [Eq. (12.55)]. In the linear case of Fig. 12.6, this results in three beams output \nfrom the Stern-Gerlach analyzer, as shown in Fig. 12.7. The Zeeman structure of the ground state \nof hydrogen displayed in Fig. 12.11 tells us that the slope and therefore the number of output beams \ndepends on the magnitude of the magnetic ﬁeld. Hence, the effective magnetic moment of the atom \nas measured by a Stern-Gerlach analyzer depends on the strength of the magnetic ﬁeld. The hydrogen \nground state atom produces three beams from a Stern-Gerlach analyzer in weak ﬁelds, two beams in \nstrong ﬁelds, and four beams in intermediate ﬁelds (Problem 12.16). I. I. Rabi used this concept to \nstudy nuclear magnetic moments, for which he was awarded the Nobel Prize in physics in 1944.\nBext\nF\u00041\nF\u00040\nE\n1\n\n /2 \n1/2\n\n1/2\n1/2\n1/2 \n1/2\n1/2\n1/2\nms\nmI\nWeak Field\nIntermediate Field\nStrong Field\nMF\n1\n0\n0\n\n1\nE(0)1s\nFIGURE 12.11  The Zeeman structure of the hydrogen ground state hyperﬁne levels.\n",
    "Problems \n407\nSUMMARY\nThe Bohr energy levels of hydrogen are perturbed by the ﬁne-structure effects, which include the \nrelativistic correction and the spin-orbit interaction. These effects partially lift the degeneracy of the \nn levels, splitting each into its possible j states. The ﬁne-structure energy corrections are a2 smaller \nthan the Bohr energies, where a \u0002 1>137 is the ﬁne-structure constant.\nAn applied magnetic ﬁeld causes Zeeman splitting of each degenerate ﬁne-structure or hyperﬁne-\nstructure level into the states labeled with the magnetic quantum number m. For weak or strong ﬁelds, \nthe Zeeman corrections are linear in the applied magnetic ﬁeld, with the splitting given by the general \nexpression\n \nE (1)\nZ\n= gmBBm,  \n(12.93)\nwhere the appropriate g-factor and the magnetic quantum number m depend on which level is being \nsplit. For intermediate ﬁelds, the Zeeman perturbation and the ﬁne structure and/or hyperﬁne structure \nmust be simultaneously diagonalized, leading to energy corrections that are quadratic in the applied \nmagnetic ﬁeld.\nPROBLEMS\n 12.1 Show that the operator p4 commutes with the operators L2 and Lz, so that the relativistic per-\nturbation Hamiltonian is diagonal within each degenerate subspace.\n 12.2 Find the matrix representation of L~S in the coupled basis and in the uncoupled basis for the \nn = 2 state of hydrogen.\n 12.3 Derive the ﬁne-structure result in Eq. (12.47) to demonstrate the miraculous resolution of the \n/ = 0 problem.\n 12.4 Explain in words why the Zeeman interaction Hamiltonian in Eq. (12.57) is diagonal in the \nuncoupled basis 0 n/sm/ms9 but not diagonal in the coupled basis @ n/sjmj9.\n 12.5 Find the matrix representations of Lz and Sz in the uncoupled basis 0 n/sm/ms9 for the 2p states \nof hydrogen.\n 12.6 Use the Clebsch-Gordan expansion method demonstrated in Section 12.3.2 to calculate the \nmatrix representations of Lz and Sz in the coupled basis @ n/sjmj9 for the 2p states of hydrogen.\n 12.7 Find the matrix representation of the Zeeman Hamiltonian in the coupled basis @ n/sjmj9 for the \nn = 2, j = 1>2 subspace. Make sure to include the 2s1>2 states in addition to the 2p1>2. Find \nthe weak-ﬁeld Zeeman energy shifts and plot them as a function of the magnetic ﬁeld.\n 12.8 Consider the strong-ﬁeld Zeeman effect perturbation calculation, where we found it best to \nwork in the uncoupled basis. Show that the off-diagonal matrix elements of the spin-orbit \nHamiltonian do not couple any of the states that are degenerate with respect to the Zeeman \nHamiltonian.\n 12.9 Diagonalize the intermediate-ﬁeld perturbation matrix for the 2p states in Eq. (12.87) and \nproduce the plot in Fig. 12.10. Show that the energy shifts approach the weak- and strong-ﬁeld \nresults in the appropriate limits. Calculate the energy shifts of the 2s levels and add them to \nthe plot.\n",
    "408 \nPerturbation of Hydrogen\n 12.10 Find the matrix representation of the perturbation Hamiltonian H\u0004 = H =\nfs + H =\nZ in the \nuncoupled basis for the 2p states of hydrogen. Diagonalize the perturbation Hamiltonian and \nproduce the plot in Fig. 12.10.\n 12.11 Rearrange the rows and columns of the intermediate-ﬁeld perturbation matrix for the 2p states \nin Eq. (12.87) in order to make it appear block diagonal. Explain how the block diagonal \nnature of the matrix is manifested in Fig. 12.10.\n 12.12 Calculate the perturbed energies of the hydrogen 1s ground state caused by the Zeeman effect \nand the hyperﬁne interaction in (a) the weak ﬁeld limit, and (b) the strong ﬁeld limit. Estimate \nthe magnitude of the applied magnetic ﬁeld that separates these two limits.\n 12.13 Diagonalize the intermediate-ﬁeld perturbation Hamiltonian representing the Zeeman effect \nand the hyperﬁne interaction in the hydrogen 1s ground state and produce the energy diagram \nin Fig. 12.11, with energy and magnetic ﬁeld scales added.\n 12.14 Calculate the size of the following energy terms and spin orbit and relativistic corrections for \nthe hydrogen atom Afor (a)-(d), tabulate your results and give answers in three forms: theoreti-\ncal in terms of anmc2, numerical in eV or meV, and in GHzB.\na) The energy difference between the n = 1 and n = 2 states BEFORE any perturbations \nwere considered.\nb) The correction to the n = 1 and n = 2 states due to spin-orbit coupling. Note that the for-\nmula we derived in class is problematic for / = 0. Show that if you set j = / + 1\n2 and then \nuse j = 1\n2, the problem goes away. (This is the Darwin term we talked about, but go ahead \nand call it spin-orbit here.)\nc) The correction to the n = 1 and n = 2 states due to the relativistic term.\nd) The total correction to these states, (i.e., the ﬁne-structure correction).\ne) What wavelength resolution must your detector have to be able to resolve the two lines \nin the n = 2 to n = 1 transition? Be careful here. When you include the correction, you \nwill ﬁnd that it is very small compared to the unperturbed value. Be sensible about how to \ninclude the effects.\nf) Is it important to use the reduced mass of the electron in your calculations or is it OK to \nuse the free mass?\n 12.15 Deuterium is an isotope of hydrogen with one electron bound to a nucleus (the deuteron) \ncomprising a proton and a neutron. The deuteron has spin I = 1 and has a gyromagnetic ratio \ngD = 0.857, which is the only change needed to use Eq. (11.10) for the hyperﬁne interaction \nin deuterium.\na) Find the hyperﬁne structure of the ground state of deuterium. Calculate the splitting of the \nground state (in MHz) and produce a ﬁgure like Fig. 11.4. (This part is Problem 11.16.)\nb) Solve for the Zeeman splitting of the ground state of deuterium in intermediate ﬁelds and \nproduce a ﬁgure like Fig. 12.11.\n 12.16 Calculate the effective magnetic moment of the hydrogen atom in its ground state and conﬁrm \nthat the hydrogen ground state atom produces three beams from a Stern-Gerlach analyzer in \nweak ﬁelds, two beams in strong ﬁelds, and four beams in intermediate ﬁelds.\n",
    "Resources \n409\nRESOURCES\nActivities\nThis activity is available at\nwww.physics.oregonstate.edu/qmactivities\nZeeman perturbation matrices in the coupled basis: Students write down angular momentum \nmatrices in the uncoupled basis by inspection, and use Clebsch-Gordan coefﬁcients to calculate angu-\nlar momentum matrices in the coupled basis.\nFurther Reading\nThe history of hydrogen atom spectroscopy and advances afforded by laser techniques are detailed in \nT. W. Hänsch, A. L. Schawlow, and G. W. Series, “The spectrum of atomic hydrogen,” \nScientiﬁc American 94–110 (March 1979).\nArthur L. Schawlow, Nobel lecture. \nhttp://nobelprize.org/nobel_prizes/physics/laureates/1981/schawlow-lecture.html\nTheodor W. Hänsch, Nobel Lecture. \nhttp://nobelprize.org/nobel_prizes/physics/laureates/2005/hansch-lecture.html\nT. W. Hänsch, “Nobel Lecture: Passion for Precision,” Rev. Mod. Phys. 78, 1297 (2006).\nRabi’s technique for studying nuclear magnetic moments:\nG. Breit and I. I. Rabi, “Measurement of Nuclear Spin,” Phys. Rev. 38, 2082 (1931).\nG. H. Fuller, “Nuclear Spins and Moments,” J. Phys. Chem. Ref. Data 5, 835 (1976).\n",
    "C H A P T E R  \n13\nIdentical Particles\nTo study systems like multielectron atoms, we need to properly account for the fact that all  fundamental \nparticles like electrons and protons are identical. In classical physics, particles are not identical—we \ncan always ﬁnd a way to uniquely identify a particular particle. Even if we make two classical particles \n“the same” to the utmost level of precision, we can still ﬁnd a way to identify the two particles without \naffecting their classical motion. For example, billiard balls behave identically, but can be identiﬁed \nby their numbers. In quantum mechanics, there is no way to identify two different electrons—they \nare indistinguishable. Two hydrogen atoms are identical no matter where they are in the universe. \nResearchers rely on this fact when they compare their experimental results on the spectra of hydrogen \natoms in different laboratories. To account for the indistinguishability of fundamental particles, we \nintroduce a new postulate in quantum mechanics, which leads to the Pauli exclusion principle that \nis responsible for the periodic table and all of chemistry. We apply this new postulate to the helium \natom to learn how the indistinguishability of the two electrons in the atom affects the energies and the \nallowed states.\n 13.1 \u0002  TWO SPIN-1/2 PARTICLES\nTo start our discussion of identical particles, let’s return to the system of two spin-1/2 particles that we \nstudied in Chapter 11. We found that we could describe the system using either of two bases:\n \n 0  + +9,  0  + -9,  0  - +9,  0  - -9    uncoupled basis   0  s1s2m1m29 \n \n 0\n 119,  0\n 109,  0\n 1, -19,  0\n 009     coupled basis    0  SMS9. \n(13.1)\nThe coupled basis is preferred when the two particles or systems interact, such as in the hyperﬁne \ninteraction or the spin-orbit interaction, because the Hamiltonian is diagonal in that basis. In general, \nour choice of basis depends on the problem at hand, but that choice is one we make based solely on \nconvenience in ﬁnding the energy eigenvalues. If we choose the other basis, we still ﬁnd the correct \neigenvalues—it just takes a little more work. However, if the two particles are identical, then that free-\ndom of choice of basis is no longer available. Let’s see why.\nConsider the ket 0  + -9 in the uncoupled basis that represents a quantum state in which particle 1 \nhas spin up and particle 2 has spin down. If the two particles in question are a proton and an electron, \nthen this representation is clear and unambiguous. However, if the two particles are electrons, then \nthis representation is more problematic. How can we possibly know that the particle that we measure \nto have spin up is electron 1 and not electron 2? We cannot. In quantum mechanics there is no way to \ndistinguish the two particles, and we cannot tell which particle has spin up and which has spin down. \n",
    "13.1 Two Spin-1/2 Particles \n411\nOr as Dr. Seuss said, we cannot know “Whether this one was that one . . . or that one was this one Or \nwhich one was what one . . . or what one was who. ” There is no experiment we can perform on the \nsystem of two electrons that would distinguish the state 0  + -9 from the state 0  - +9. This leads us \nto conclude that the uncoupled basis is inappropriate for representing this system of two identical \nspin-1/2 particles. So how do we mathematically represent the state with one particle having spin up \nand one particle having spin down?\nThe best way would be to start over and abandon the attempt at labeling the quantum numbers of \nindividual particles and instead specify how many particles have particular sets of quantum numbers. \nThis approach is the basis of more advanced treatments, but is too much of a change for us to make \nat this stage. So we adapt our labeling scheme to this new problem. A reasonable guess for represent-\ning the state with one particle having spin up and one particle having spin down would be to use a \nsuperposition of the two states 0  + -9 and 0  - +9 in a way that does not favor one over the other. From \nChapter 11 on the addition of angular momenta, we know that such superpositions already exist in the \ncoupled basis representation:\n \n 0 109 =\n1\n12 30  + -9 + 0  - +94 \n \n 0\n 009 =\n1\n12 30  + -9 - 0  - +94.\n \n(13.2)\nThese two states differ only by the minus sign coefﬁcient. Let’s see what the importance of that minus \nsign is, and in doing so, learn why the coupled basis is appropriate for describing systems of identical \nparticles.\nImagine that, unbeknownst to us, someone exchanged the two identical particles in the system, \nso that what we originally thought was particle 1 is now particle 2 and vice versa. The two states in \nEq. (13.2) would then become\n \n 1\n12 30  + -9 + 0  - +94    exchange   \n>  1\n12 30  - +9 + 0  + -94 =\n1\n12 30  + -9 + 0  - +94 \n \n 1\n12 30  + -9 - 0  - +94    exchange   \n>  1\n12 30  - +9 - 0  + -94 = - 1\n12 30  + -9 - 0  - +94.\n \n(13.3)\nThe ﬁrst state is unchanged and the second state acquires a minus sign, which is an overall phase shift \nof 180°. An overall phase shift causes no measurable change (Problem 1.3), so the physical states are \nunchanged by this exchange operation. We denote the exchange operation by the exchange operator \nP12, whose action on uncoupled basis states is\n \nP120 s1s2m1m29 = 0 s2s1m2m19. \n(13.4)\nIn terms of the coupled basis representation, the action of the exchange operator P12 on the two \nstates in Eq. (13.2) is\n \n P120 109 = + 0 109  \n \n P120 009 = - 0 009.\n \n(13.5)\nThese results tell us that the coupled basis states 0 109 and 0 009 are eigenstates of the exchange  operator \nwith eigenvalues +1 and -1, respectively. We call these states symmetric 1 0 1092 and  antisymmetric \n1 0 0092 states, by which we mean that they are symmetric and antisymmetric with respect to the \nexchange of the two particles, as opposed to symmetric and antisymmetric with respect to space or \n",
    "412 \nIdentical Particles\nsomething else. Note that the other two coupled basis states 0 119 and 0 1,-19 are both symmetric with \nrespect to exchange, (i.e., have eigenvalues of  +1):\n \n P120 119 = P12\n 0  + +9 = 0  + +9 = + 0 119 \n \n P12\n 0 1, -19 = P12\n 0  - -9 = 0  - -9 = + 0 1, -19.\n \n(13.6)\nNow consider a measurement upon an eigenstate of the exchange operator, which we denote by \n0 c{9, where the {  indicates the eigenvalue. The probability of recording some ﬁnal result is\n \n Pc{S cf = @8cf @ c{9@\n2. \n(13.7)\nIf we exchange the two particles before the measurement, then the probability is\n \n PP\n 12\n c{S cf = @8cf @5P12\n @ c{96@\n2 = @8cf @  P12\n @ c{9@\n2 \n \n = @8 cf @1{12@ c{9@\n2 = @8cf  @  c{9@\n2\n \n(13.8)\n \n  = Pc{S cf . \nHence, the calculated probability for a measurement made upon a state  0 c{9 is not changed if the \nparticles are exchanged. This agrees with our statement above that experiments cannot distinguish \nbetween systems with the identical particles exchanged. Thus the coupled basis superpositions are \npromising representations of the physical system of identical particles.\nAt this point, both coupled states 0 109 and 0 009 plausibly represent the physical state of a sys-\ntem of two identical spin-1/2 particles with one particle having spin up and one having spin down. \nHowever, we know from Chapter 11 on angular momentum addition that these two states have an \nimportant difference. The state 0 109 has total spin angular momentum S = 1, while the state 0 009 has \ntotal spin angular momentum S = 0. Thus, they are clearly not describing the same system. So how \ndo we know which of these two states to use to describe our system of two electrons with one having \nspin up and the other having spin down?\nNature chooses for us. You probably already know about the Pauli exclusion principle that for-\nbids having two electrons in the same quantum state. The Pauli exclusion principle is a speciﬁc exam-\nple of a broader quantum mechanical principle that we call the symmetrization postulate.\nThe symmetrization postulate stipulates that a system of identical particles is required to have a \nquantum state vector that is either symmetric or antisymmetric with respect to exchange of any pair \nof particles. Nature has sorted particles into two classes depending on whether they obey the sym-\nmetric or antisymmetric version of this principle. Particles that are required to have symmetric states \nare called bosons and particles that are required to have antisymmetric states are called fermions. \nFurthermore, this symmetry property is correlated with the spin angular momentum of the particles \ncomprising the system. Bosons are particles with integer spin (0, 1, 2, …) and are required to have \nsymmetric quantum states. Fermions are particles with half-integer spin (1/2, 3/2, 5/2, …) and are \nrequired to have antisymmetric quantum states. This connection between the spins of particles and \ntheir exchange symmetry can be proved using relativistic quantum mechanics, so we take it as a pos-\ntulate. It has been conﬁrmed in many experiments. Because the exchange symmetry determines the \nstatistical behavior of these particles, this concept is often called the spin-statistics theorem.\nElectrons have spin 1/2, so they are fermions, as are protons and neutrons. Photons and mesons are \nexamples of bosons. Composite particles, like atoms, are fermions or bosons, depending on the total \n",
    "13.1 Two Spin-1/2 Particles \n413\nFor the system of two identical spin-1/2 particles we discussed above, you would then conclude that \nthe two particles must be in the antisymmetric state 0 009, and the symmetric states 0 119, 0 109, and \n0 1, -19 are not allowed. However, we cannot yet reach that conclusion because the symmetrization \npostulate applies to the complete state vector, and we have not yet included the spatial part of the state \nvector.\nWe assume that we can separate the spin and spatial aspects of the state vectors. This is not \nalways possible, but it works for all the systems we study. The complete quantum state vector then has \nthe form\n \n@ c9 = @ cspatial9@ cspin9. \n(13.9)\nThe symmetrization postulate must be applied to this complete quantum state. For bosons, the com-\nplete state vector must be symmetric under exchange of particles. Thus, the spatial part must be \nsymmetric if the spin part is symmetric, or the spatial part must be antisymmetric if the spin part is \nantisymmetric:\n \n @  c SS\nboson I = @  c S\nspatial I  @  c S\nspin I  \n \n @  c AA\nboson I = @  c A\nspatial I  @  c A\nspin I. \n(13.10)\nThe two parts must have the same exchange symmetry, or else the full eigenstate would not be sym-\nmetric. For fermions, the complete state vector must be antisymmetric under exchange. Thus, the spa-\ntial part must be symmetric if the spin part is antisymmetric, or the spatial part must be antisymmetric \nif the spin part is symmetric:\n \n 0\n c SA\nfermion I = 0\n c S\nspatial I 0\n c A\nspin I \n \n 0\n c AS\nfermion I = 0\n c A\nspatial I 0 c  S\nspin I. \n(13.11)\nThe two parts cannot have the same exchange symmetry, or else the full eigenstate would not be anti-\nsymmetric. In Eqs. (13.10) and (13.11) we use superscripts to denote the exchange symmetry of the \nstate vectors.\nBosons (integer spin: 0, 1, 2, …) must have symmetric states. \nFermions (half-integer spin: 1/2, 3/2, 5/2, … ) must have antisymmetric states.\nspin of the system. For example, hydrogen has one electron and one proton—two spin-1/2 fermions— \nso it has integer total spin (1 or 0) and is a boson. Deuterium has one electron and a nucleus (the deu-\nteron) comprising one proton and one neutron, so it has half-integer spin and is a fermion. Thus, dif-\nferent isotopes of the same atom can behave differently when one considers the collective behavior of \natoms. For example, samples of liquid 3He (fermion) and liquid 4He (boson) behave quite differently \nat very low temperatures.\nTo summarize, the symmetrization postulate tells us that the quantum state vector of a system of \ntwo (or more) identical particles must be either symmetric or antisymmetric with respect to exchange \nof the two (or any two) particles. All particles are divided into either fermions or bosons:\n",
    "414 \nIdentical Particles\n13.2 \u0002 TWO IDENTICAL PARTICLES IN ONE DIMENSION\nWe take the example of a system of two identical particles bound within a one-dimensional potential \nenergy well to study the application of the symmetrization postulate to the complete state vector. Lim-\niting the discussion to one dimension is sufﬁcient to illustrate the most important ramiﬁcations of the \nsymmetrization principle. Let’s ﬁrst look at the spatial part of the state vector. The Hamiltonian for a \nsingle particle in one dimension is\n \nHsingle = p2\n2 m + V 1x2. \n(13.12)\nAssume we know the wave function solutions to the energy eigenvalue equation:\n \nHsingle w n 1x2 = En w n 1x2. \n(13.13)\nThe Hamiltonian for two particles in this potential energy well is\n \nH =\np2\n1\n2 m + V 1x12 +\np2\n2\n2 m + V 1x22, \n(13.14)\nwhere x1 labels the position of particle 1 along the x-axis, and p1 is the momentum of particle 1; x2 and \np2 are the equivalent for particle 2. We assume for the moment that the two particles do not interact \nwith each other and that the Hamiltonian has no spin dependence. The two-particle energy eigenvalue \nequation is\n \nHc 1x1, x22 = Ec 1x1, x22. \n(13.15)\nThe wave function c1x1, x22 of the system is a function of the two coordinates x1 and x2 locating the \ntwo particles. This two-particle wave function is a new concept, so a few comments about it are in \norder.\nThe complex square of the two-particle wave function yields the probability density\n \nP1x1, x22 = @\n c 1x1, x22@\n2, \n(13.16)\nbut because we have two particles, we must be clear what this density means. We interpret the prob-\nability density as a two-particle probability density, and so\n \n@ c 1x1, x22@\n2\n dx1 dx\n 2 \n(13.17)\nis the probability of ﬁnding particle 1 at position x1 within a volume dx1 and ﬁnding particle 2 at posi-\ntion x2 within a volume dx2. (“Volume” in this case is a length, but in a three-dimensional problem, it \nwould be a true volume.) We normalize the system wave function by integrating the two-particle prob-\nability density over both coordinates:\n \n \nO\n@ c 1x1, x22@\n2 dx1 dx2 = 1, \n(13.18)\nwhich means that the probability of ﬁnding both particles within the whole volume available to the \nsystem is unity.\n",
    "13.2 Two Identical Particles in One Dimension \n415\nIn this noninteracting example, the two-particle Hamiltonian is the sum of two single-particle \nHamiltonians, so the product function wna\n 1x12wnb\n 1x22 describing the system with particle 1 in energy \nstate na and particle 2 in energy state nb satisﬁes the energy eigenvalue equation (13.15) with energy \nEna + Enb. That would be the end of the story if the two particles were distinguishable (like an elec-\ntron and a proton, for example), but for indistinguishable or identical particles we must ﬁnd spatial \neigenstates that are either symmetric or antisymmetric with respect to exchange of the two particles. \nAs we did with the spin state vectors for the spin-1/2 states in the last section, we form the symmetric \nor antisymmetric superpositions\n \n @ c S\nspace9 \u0003 c S\nna\n nb1x1, x22 = NS 3wna1x12wnb1x22 + wna1x22wnb1x124 \n \n  @ c A\nspace9 \u0003 c A\nna\n nb1x1, x22 = NA 3wna1x12wnb1x22 - wna1x22wnb1x124, \n(13.19)\nwhere NS, A are the normalization constants. The wave function c S\nna\n nb1x1, x22 is symmetric with respect \nto exchange of the two particles, and the wave function c A\nna\n nb1x1, x22 is antisymmetric. Each of these \nsolutions has the two-particle energy\n \nEna\n nb = Ena + Enb. \n(13.20)\n13.2.1 \u0002 Two-Particle Ground State\nThe ground state of this system has both particles in the single-particle ground state, so na = 1 and \nnb = 1 and the energy of the state is E11 = 2E1. The symmetric two-particle ground-state wave\nfunction is\n \nc\n S\n111x1, x22 = w11x12w11x22. \n(13.21)\nHowever, the antisymmetric two-particle wave function is identically equal to zero:\n \nc\n A\n111x1, x22 = NA  3w11x12w11x22 - w11x22w11x124 = 0, \n(13.22)\nso there is no possibility of having an antisymmetric spatial wave function in the ground state, regard-\nless of whether the system comprises bosons or fermions.\nTo properly apply the symmetrization postulate to the complete state vector of this system, we \nmust include the spin in the state vector. Let’s assume that the system is either composed of two spin-0 \nbosons, or two spin-1/2 fermions. For two spin-0 bosons, the total spin must be zero and the only possible \nsystem spin state is 0 SM9 = 0 009, which is equal to the uncoupled basis state 0 s1s2\n m1\n m29 = 0 00009. \nThis spin state is symmetric under exchange of the two particles ( Problem 13.2). The complete state \nvector for bosons must be symmetric, so the spatial wave function must always be symmetric in this \nspin-0 example. Hence, the ground state of the two spin-0 bosons in a one-dimensional system is\n \n @ c\n SS\n11\n 9 \u0003 c\n S\n111x1, x22@  009 = w11x12w11x22@  009. \n(13.23)\nThe notation in Eq. (13.23) is a mixture of wave function language and abstract ket notation, but it \nmakes the space-spin distinction clear.\n",
    "416 \nIdentical Particles\nFor two spin-1/2 fermions, the total spin is 0 or 1, with the coupled basis states\n \n 0 119 = 0  + +9 \n \n 0 109 =\n1\n12 30  + -9 + 0  - +94 t Symmetric Triplet states \n \n 0 1,-19 = 0  - -9 \n \n 0 009 =\n1\n12 30  + -9 - 0  - +94 6  Antisymmetric Singlet state \n(13.24)\nbeing the eigenstates of the exchange operator we need to construct the complete state vectors. For the \nground state, the symmetric spin triplet states are excluded because the required antisymmetric spatial \nstate is identically zero [Eq. (13.22)]. The ground state of two spin-1/2 fermions is therefore\n \n@\n c\n SA\n11 9 \u0003 c\n S\n11 1x1, x22@\n 009 = w11x12w11x22@\n 009. \n(13.25)\nThis result exposes a problem with our notation. The spin state 0 009 in Eq. (13.23) is not \nthe same as the spin state 0 009 in Eq. (13.25). For two spin-0 bosons, the state 0 009 is really \n@ s1 = 0,  s2 = 0,  S = 0,  M = 09 and is symmetric under particle exchange, whereas for two spin-1/2\nfermions, the state 0 009 is @ s1 = 1\n2,  s2 = 1\n2,  S = 0,  M = 09 and is antisymmetric under particle \nexchange. We will continue with the notation 0 SM9 for coupled basis states, but note this limitation.\n13.2.2 \u0002 Two-Particle Excited State\nThe ﬁrst excited state of the two-particle system has one particle in the single-particle ground state \nand one particle in the ﬁrst single-particle excited state, so na = 1, nb = 2 and the energy of the \nstate is E12 = E1 + E2 . In this case, both the symmetric and antisymmetric spatial wave functions in\nEq. (13.19) are nonzero.\nFor the spin-0 boson case, the spatial wave function must be symmetric because there is only a \nsymmetric spin state, so the state vector is\n \n@\n c\n SS\n129 \u0003 c\n S\n12 1x1, x22@\n 009 =\n1\n12 3w1 1x12w2 1x22 + w21x12w1 1x224@\n 009. \n(13.26)\nFor the spin-1/2 fermion case, the total state vector must be antisymmetric, so the symmetric spa-\ntial wave function must combine with the antisymmetric singlet spin state\n \n@\n c\n SA\n12 9 \u0003 c\n S\n12 1x1, x22@\n 009 =\n1\n12 3w1 1x12w2 1x22 + w11x22w2 1x124@\n 009, \n(13.27)\nand the antisymmetric spatial wave function must combine with the symmetric triplet spin states\n \n@\n c\n AS\n12  9 \u0003 c\n A\n12 1x1, x22@ 1M 9 =\n1\n12 3w1 1x12w2 1x22 - w11x22w2 1x124@ 1M 9, \n(13.28)\nwith M = 1, 0, -1. The ﬁrst excited state of the fermion system is four-fold degenerate, while the \nboson state is nondegenerate.\nIf the two particles were distinguishable, then the ﬁrst excited state would be two-fold degener-\nate (assuming no spin), with states c12 1\n x1, x2 2 = w1 1\n x1 2w2 1\n x2 2 and c21 1\n x1, x2 2 = w2 1\n  x12w1 1\n x2 2. A \nschematic of the ground and ﬁrst excited states for all three cases is shown in Fig. 13.1. For the spin-1/2 \nfermions in Fig. 13.1(c), we use arrows to indicate the spin combinations. The states 0 119 and 0 1, -19 \nhave the two spins aligned, in which case the notation is clear. However, the states 0 109 and 0 009 are \ndifferent superpositions of spin up and down states, so the notation is somewhat unclear. You must \nremember that 0 109 =\n1\n12 10  + -9 + 0  - +92 and 0 009 =\n1\n12 10  + -9 - 0  - +92.\n",
    "13.2 Two Identical Particles in One Dimension \n417\n13.2.3 \u0002 Visualization of States\nTo visualize the spatial aspect of these states, assume the two particles are bound in an inﬁnite square \npotential energy well. The one-particle energy eigenstate wave functions for the inﬁnite square well are\n \n0 n9 \u0003 wn1x2 = A\n2\nL sin anpx\nL b. \n(13.29)\nThe spatial wave function for the ground state is the same [Eq. (13.21)] for all three cases of distin-\nguishable particles, identical bosons, and identical fermions. The two-particle probability density for \nthe ground state is thus\n \n P1x1, x22 = 0  c1x1, x22 0\n2\n  \n \n = 0 w11x12w11x22 0\n2\n \n \n = 4\nL2 sin2 apx1\nL b sin2 apx2\nL b , \n(13.30)\nas shown in Fig. 13.2. The system probability density is two-dimensional because there are two \nparticles, each with a one-dimensional probability density.\ndistinguishable particles\nbosons\nfermions\nE11\u00042E1\n1\n2\n1\n2\n2\n1\n(a)\n(b)\n(c)\nE12\u0004E1\u000fE2\n\u00101(x1)\u00101(x2)\nΨs11(x1,x2)\u000200\u0003\u0007\nΨs11(x1,x2)\u000200\u0003\nΨA12(x1,x2)\u000211\u0003\nΨA12(x1,x2)\u00021,\n1\u0003\nΨA12(x1,x2)\u000210\u0003\nΨS12(x1,x2)\u000200\u0003\nΨs12(x1,x2)\u000200\u0003\n\u00101(x1)\u00102(x2)\n\u00102(x1)\u00101(x2)\n FIGURE 13.1 Schematic diagrams of ground and ﬁrst excited states of two \nparticles in a one-dimensional well for (a) distinguishable particles, (b) identical \nspin-0 bosons,and (c) identical spin-1/2 fermions.\n",
    "418 \nIdentical Particles\nFor the ﬁrst excited state of the system with one particle in the single-particle ground state and \none particle in the ﬁrst single-particle excited state, the wave function does depend on the type of par-\nticle, as shown in Fig. 13.1. For the distinguishable particle case, there are two possible states, shown \nin Fig. 13.3(a) and (b). For the spin-0 boson case, there is only one possible state, which is the symmetric \nwave function c\n S\n12 1x1, x22 given in Eq. (13.26) with the probability density shown in Fig. 13.3(c). \nFor the case of two spin-1/2 fermions, the excited state can either be in the symmetric [Fig. 13.3(c)] or \nantisymmetric [Fig. 13.3(d)] spatial wave function, depending on the spin state as given in Eqs. (13.27) \nand (13.28), respectively. For bosons and fermions, the probability density is symmetric with respect \nto particle exchange, which is evident in the symmetry about the diagonal line x1 = x2 in Figs. 13.3(c) \nand (d). The wave function c\n A\n12 1x1, x22 that underlies the probability density in Fig. 13.3(d), is antisy-\nmetric about the line x1 = x2 , but its square—the probability density—is symmetric. The probability \ndensity of the asymmetric spatial state c\n A\n12 1x1, x22 is identically zero along the line x1 = x2:\n \nc\n A\n12 1x1, x12 =\n1\n12 3w1 1x12w2 1x12 - w1 1x12w2 1x124 = 0, \n(13.31)\nillustrating that two fermions in a symmetric spin state cannot be in the same location. This is the \nPauli exclusion principle that two electrons with the same spin orientation 1 0 119 = 0  + +9 or \n0 1, -19 = 0  - -9 statesB cannot be in the same spatial state. The symmetrization postulate tells us that \nthis also applies to two electrons with opposite spin but combined in a symmetric manner 1 0 109 stateB. \nThe spatial probability density shown in Fig. 13.3(d) illustrates the idea that fermions in a symmetric \nspin state appear to “repel” each other. In contrast, two fermions with opposite spins combined in \nan antisymmetric manner 1 0 009 stateB to make a spin-singlet state have a symmetric spatial wave \nfunction and the probability density shown in Fig. 13.3(c), the same as two bosons. In this case, the \nprobability density is peaked along the line x1 = x2 , illustrating that two bosons or two spin-singlet \nfermions have a tendency to “attract” each other.\nFIGURE 13.2 Two-particle spatial probability density for the ground state of a system of two particles \nin an inﬁnite square well, displayed using height (left) or grayscale with contours (right). This probability \ndensity is the same for distinguishable particles, identical bosons, and identical fermions.\n",
    "13.2 Two Identical Particles in One Dimension \n419\nFIGURE 13.3 Two particle probability \ndensities for the ﬁrst excited state of a sys-\ntem of two particles in an inﬁnite square \nwell. (a, b) Distinguishable particles, \n(c) Symmetric spatial state for spin-0 \nbosons or spin-singlet fermions, (d) Anti-\nsymmetric spatial state for spin-triplet \nfermions.\n",
    "420 \nIdentical Particles\n13.2.4 \u0002 Exchange Interaction\nThis apparent spatial attraction between bosons and spin-singlet fermions and repulsion between spin-\ntriplet fermions does not reﬂect any potential energy of interaction between the two particles—we \nhave assumed that the particles do not interact in this simple model. Rather, this apparent interaction \nis a consequence of the symmetrization requirement imposed on the wave functions. This effect is \ncalled the exchange force or the exchange interaction. One way to quantify the exchange interaction \nand demonstrate the difference between particles in symmetric and antisymmetric spatial states is to \ncalculate the expectation value of the square of the separation between the two particles\n \n H1x1 - x22\n2 I = H  x2\n1 - 2x1x2 + x2\n2\n I\n \n \n = H  x2\n1\n I + H  x2\n2\n I - 2 H  x1x2\n I. \n(13.32)\nWe’ll leave the bulk of this calculation to you, but let’s demonstrate how to calculate one of these two-\nparticle expectation values.\nConsider the expectation value 8x2\n19 in the fermionic state @ c\n AS\n12 9 = @ c\n A\n129@ 1M9, where \n@ c\n A\n129 \u0003 c\n A\n121x1, x22 is the spatial part. Using Dirac bra-ket notation to begin, we have\n \n H x2\n1I = H c\n AS\n12\n @ x2\n1@ c\n AS\n12  I\n \n \n = H c\n A\n12 @ H1M @ x2\n1@ c\n A\n12 I @1M I. \n(13.33)\nWe separate the space and spin parts of the matrix element and recall that the position x1 does not act \non the spin states, so\n \nHx 2\n1I = H c\n A\n12 @ x2\n1@ c\n A\n12 I H1M @1MI. \n(13.34)\nThe spin state projection is unity: 81M0 1M 9 = 1. Let’s keep the spatial matrix element in Dirac nota-\ntion by using the notation 0 n91 \u0003 wn1x12, such that 0 191 \u0003 w11x12 and 0 192 \u0003 w11x22. The Dirac ket \nrepresentation of the two-particle spatial state [Eq. (13.28)] in terms of the single-particle spatial states \n3@  c\n A\n129 =\n1\n12 1@ 191@ 292 - @ 291@ 19224 yields\n \n Hx2\n1I =\n1\n12  A1H1@2H2@  -2 H1@1H2@ B Ax2\n1B 1\n12  A @1I1@2I2 - @1I2@2I1B\n \n \n = 1\n2  EA1H1@ x2\n1@1I1B\n \nA2H2@2I2B - A1H1@ x2\n1@2I1B\n \nA2H2@1I2B\n \n \n        - A1H2@ x2\n1@1I1B\n \nA2H1@2I2B + A1H2@ x2\n1@2I1B\n \nA2H1@1I2BF, \n(13.35)\nwhere we have isolated the separate matrix elements and projections for particles 1 and particle 2. \nInvoking the orthonormality of the single-particle eigenstates yields\n \n Hx2\n1I = 1\n2 A1H1@x2\n1@1I1 + 1H2@x2\n1@2I1B. \n(13.36)\nThus we are left with calculating two single-particle expectation values. The subscript label indicating \nthe particle number is irrelevant for that calculation, leaving\n \n Hx2\n1I = 1\n2  AH1@ x2@1I + H2@ x2@\n 2IB. \n(13.37)\n",
    "13.2 Two Identical Particles in One Dimension \n421\nTo calculate the single-particle expectation values for the particle in the inﬁnite square well, we must \nuse an integral in the position representation:\n \n 8n@ x2@\n n9 =\nL\nL\n0\nw*\nn1x2 x2 wn1x2dx . \n(13.38)\nFollowing this example, you can calculate the expectation value of the interparticle spacing in \nEq. (13.32). The difference between the results for different spatial states resides in the cross-term \n8x1x29. The ﬁnal result for the state with one particle in the n = 1 state and one particle in the n = 2 \nstate of the inﬁnite square well is (Problem 13.6)\n \n 481x1 - x2229S = 0.20L \n \n 481x1 - x2229D = 0.32L \n \n 481x1 - x2229A = 0.41L \n(13.39)\nfor the three cases of distinguishable particles (D), identical particles in symmetric spatial states (S) \n(bosons or spin-singlet fermions), and identical particles in antisymmetric spatial states (A) (spin-\ntriplet fermions). These results indicate that particles in symmetric spatial states (typically bosons) are \ncloser to each other, and particles in antisymmetric spatial states (typically fermions) are farther apart \nfrom each other, compared to the distinguishable particle case.\nThe relation between the symmetry/antisymmetry of the spatial wave function and the interpar-\nticle spacing is also evident if we measure the particle separation probability density P1x1 - x22. This \none-dimensional probability density is measured by recording the positions of each particle and ﬁnd-\ning the interparticle separation 1x1 - x22. To calculate this one-dimensional probability density, we \nintegrate the two-particle probability density P1x1, x22 parallel to the x1 = x2 line (i.e., project the \ntwo-particle probability densities of Fig. 13.3 onto the diagonal line x2 = L - x1). The result of this \ncalculation for the state with one particle in the n = 1 state and one particle in the n = 2 state of the \ninﬁnite square well is shown in Fig. 13.4 (Problem 13.8). The distribution for symmetric spatial states \nis peaked at the origin, indicating that these identical particles are more likely than distinguishable \nparticles to be found close to each other. The distribution for antisymmetric spatial states is zero at the \norigin 1i.e., x1 = x22, indicating that the two identical particles cannot be found at the same location.\n13.2.5 \u0002 Consequences of the Symmetrization Postulate\nWe have seen the effect of the symmetrization postulate on a two-particle system (an effective interac-\ntion that leads to changes in the interparticle spacing). The consequences of the symmetrization pos-\ntulate for a many-particle system are much more radical, and are much different for systems of bosons \nand fermions. For example, in a three-particle system, the ground states are different because only two \nspin-1/2 fermions can be in the single-particle ground state, as illustrated in Figs. 13.5(a) and (b). For \nsystems of N particles, the boson ground state has all N particles in the single-particle ground state and \nsystem energy NE1 [Fig. 13.5(c)], while the spin-1/2 fermion ground state has energy levels occupied \nup to the N/2 single-particle state and system energy W NE1 [Fig. 13.5(d)]. The proper study of these \ntypes of systems requires statistical mechanics and thermodynamics. The states depicted in Fig. 13.5 \nrequire temperatures near absolute zero so that the thermal energy is much less than the energy spacings.\nFor a system of bosons, if the requisite low temperature is reached and the density of the particles \nis high enough, then the ground state of the system exhibits a wealth of interesting quantum mechanical \n",
    "422 \nIdentical Particles\neffects. When the inter-particle spacing is comparable to the de Broglie wavelength of the particles, \nthen the system of bosons begins to behave as a single macroscopic quantum object. As the critical \nvalue of low temperature and high density is reached, the quantum mechanical attraction of the bosons \narising from the symmetrization postulate takes over and the system “collapses” into the ground \nstate. This dramatic event is a phase transition in the state of the matter and is called Bose-Einstein\ncondensation. Liquid helium exhibits this phase transition at 2.18 K. The speciﬁc heat and the ther-\nmal conductivity increase discontinuously to signal the onset of the Bose-Einstein condensation. \nThe viscosity of liquid helium drops dramatically and the system behaves as a superﬂuid, easily ﬂow-\ning through small capillaries and even up and out of its container. Liquid helium is a strongly interact-\n(a)\n(b)\n(c)\n(d)\nbosons\nfermions\nbosons\nfermions\nFIGURE 13.4 Probability density of interparticle separation for the ﬁrst excited state \n(na = 1, nb = 2) of a system of two particles in the inﬁnite square well for the cases of \ndistinguishable particles (dashed line), identical particles in symmetric spatial states  \n(peaked at zero) and identical particles in antisymmetric spatial states (minimum at zero).\nFIGURE 13.5 Ground states of multiple particle systems in a one-dimensional potential \nfor a three-particle system of (a) bosons or (b) spin-1/2 fermions, and a six-particle system \nof (c) bosons or (d) spin-1/2 fermions.\n\nL\n0\nL\nP(x1\nx2)\nx1\nx2\nsymmetric\nantisymmetric\ndistinguishable\n",
    "13.3 Interacting Particles \n423\ning system, so the theory of its low temperature quantum behavior is quite complicated. Dilute atomic \ngases provide a better testing ground for the study of the basic quantum mechanics of Bose-Einstein \ncondensation. Recent experiments have cooled atoms to temperatures below 1 μK and achieved Bose-\nEinstein condensation. The atoms are close enough to have overlapping de Broglie wavelengths, but \nfar enough apart that the atomic interactions are small. Moreover, the strength of the interactions can \nbe adjusted through magnetic ﬁeld changes, and the quantum effects can be studied as a function of the \nstrength of the interaction. This new ﬁeld has spawned a wealth of interesting effects and garnered the \nNobel Prize in Physics in 2001.\nFor fermions, the behavior of a multiparticle system is dominated by the particles near the high-\nest occupied state. The fermions at the low energy levels are “buried” in the sea of fermions and have \nnowhere to go, because the Pauli exclusion principle forbids them from making transitions to states \nthat are already occupied. Only particles near the top of the distribution see nearby unoccupied lev-\nels to which they might make transitions. For example, in atoms, the electrons near the top are the \nvalence electrons that determine the spectroscopy and chemistry of the atom. For electrons in solids, \nthe energy at the top of the distribution of fermions is called the Fermi energy and plays a vital role in \nthe behavior of the solid.\n13.3 \u0002 INTERACTING PARTICLES\nThe apparent spatial “attraction” or “repulsion” of identical particles evident in Eq. (13.39) and \nFig. 13.4 has a profound effect when we consider a real interaction between the two particles. The \ndifferent spatial correlations of the particles lead to different energy shifts for the different spatial \nsymmetry states. Consider two particles in a one-dimensional potential energy well, with an interac-\ntion potential energy between the two particles. We assume that this new term is small enough that we \ncan treat it with perturbation theory. Assume that this interaction potential energy depends only on the \nparticle separation:\n \n H\u0004 = Vint 1x1 - x22. \n(13.40)\nWe use perturbation theory to ﬁnd the ﬁrst-order energy corrections\n \nE(1) = 8c(0)@  H\u0004@ c(0)9 \n(13.41)\nusing the states from the last section as the zeroth-order states. We assume that the interaction is spin \nindependent, so the spin does not affect this perturbation calculation. That is, in the matrix element\n \n 8  c(0)@  H\u0004@\n c(0)9 = 8cspatial\n @8  cspin\n @  H\u0004@\n cspatial\n 9@  cspin\n 9 \n \n = 8  cspatial\n @  H\u0004@ cspatial\n 9 8  cspin\n @\n cspin\n 9 \n \n = 8  cspatial\n @  H\u0004@\n cspatial\n 9,\n \n(13.42)\nthe spatial and spin states separate and only the spatial states enter into the perturbation calculation. The \nonly role of the spin is to determine the allowed spatial states through the symmetrization postulate.\nFor a system of two identical spin-0 bosons, the zeroth-order ground state is\n \n@ c\n SS\n11 9 \u0003 c\n S\n111x1, x22@  009 = w11x12w11x22@  009, \n(13.43)\n",
    "424 \nIdentical Particles\nso the ﬁrst-order perturbation is\n \n E (1)\n11 = H  c\n S\n11@  H\u0004@  c\n S\n11I H00 @  00I\n \n = H c\n S\n11@Vint 1x1 - x22@ c\n S\n11\n I\n \n =\nL\n\u0005\n- \u0005 L\n\u0005\n- \u0005\nw*\n11x12w*\n11x22Vint 1x1 - x22w11x12w11x22dx1 dx2\n \n =\nL\n\u0005\n- \u0005 L\n\u0005\n- \u0005\n0\n w11x12 0\n2 Vint 1x1 - x22 0 w11x22 0\n2dx1 dx2.\n \n(13.44)\nIt is convenient to deﬁne the general form of this matrix element as the direct integral\n \n Jnm =\nL\n\u0005\n- \u0005 L\n\u0005\n- \u0005\n0\n wn1x12 0\n2 Vint 1x1 - x22 0 wm1x22 0\n2dx1 dx2. \n(13.45)\nThe direct integral is the interaction energy between the two probability densities Pn1x12 = 0\n wn1x12 0\n2 \nand Pm1x22 = 0\n wm1x22 0\n2 that represent the two particles. With this deﬁnition, the perturbed ground-\nstate energy for a system of two spin-0 bosons is\n \n E 11 = 2E (0)\n1\n+ J11. \n(13.46)\nFor a system of two identical spin-1/2 fermions, the zeroth-order ground state is\n \n@ c\n SA\n11 I \u0003 c\n S\n111x1, x22@  00I = w11x12w11x22@  00I \n(13.47)\nand the ﬁrst-order perturbation is\n \n E (1)\n11 = H c\n S\n11@ H\u0004@ c\n S\n11\n I H00 @00I\n \n \n = H c\n S\n11@Vint 1x1 - x22@ c\n S\n11\n I \n \n = J11 ,\n \n(13.48)\nwhich is the same as the boson case. The interaction is spin independent, and the ground-state spatial \nwave function is the same for bosons and fermions.\nFor the ﬁrst excited state of the two-particle system, the identical spin-0 bosons must have a sym-\nmetric wave function [Eq. (13.26)], so the state vector is\n \n@\n c\n SS\n129 \u0003 c\n S\n12 1x1, x2@\n 002 =\n1\n12 3w1 1x12w2 1x22 + w11x22w2 1x124@\n 009 \n(13.49)\nand the ﬁrst-order perturbation is\n \n E(1)\n12 = Hc\n S\n12@  H\u0004@  c\n S\n12 I H  00@00I\n \n \n = Hc\n S\n12@Vint 1x1 - x22@c\n S\n12\n I \n \n = 1\n2 L\n\u0005\n- \u0005 L\n\u0005\n- \u0005\n3w*\n11x12w*\n21x22 + w*\n11x22w*\n21x124Vint 1x1 -  x22 \n(13.50)\n \n3w11x12w21x22 + w11x22w21x124dx1\n  dx2. \n",
    "13.3 Interacting Particles \n425\nThis gives four terms, but they are equal in pairs if we swap the integration dummy variables x1 and x2,\nyielding (Problem 13.10)\n \n E (1)\n12 =\nL\n\u0005\n- \u0005L\n\u0005\n- \u0005\n0 w11x12 0\n2Vint 1x1 - x22 0 w21x22 0\n2dx1 dx2\n \n +\nL\n\u0005\n- \u0005L\n\u0005\n- \u0005\nw*\n11x12w*\n21x22Vint 1x1 - x22w11x22w21x12dx1 dx2. \n(13.51)\nThe ﬁrst term in Eq. (13.51) is the direct integral J12 deﬁned in Eq. (13.45). The second term is a new \nterm, which we call the exchange integral and deﬁne, in general, as\n \nKnm =\nL\n\u0005\n- \u0005L\n\u0005\n- \u0005\nw*\nn1x12w*\nm1x22Vint 1x1 - x22wn1x22wm1x12dx1 dx2. \n(13.52)\nWith this deﬁnition, the energy of the ﬁrst excited state of the system of two identical spin-0 bosons is\n \nE12 = E (0)\n1\n+ E (0)\n2\n+ J12 + K12. \n(13.53)\nThe exchange integral has no classical explanation. It is a manifestation of the symmetrization require-\nment. It is not caused by spin, but it is intimately related to spin because of the role of spin in the sym-\nmetrization postulate.\nFor two identical spin-1/2 fermions, the excited state spatial wave function can be either symmet-\nric or antisymmetric depending on the spin state. For the antisymmetric singlet spin state, the spatial \nwave function must be symmetric\n \n@\n c\n SA\n12 9 \u0003 c\n S\n12 1x1, x22@\n 009 =\n1\n12 3w1 1x12w2 1x22 + w11x22w2 1x124@\n 009. \n(13.54)\nThe ﬁrst-order energy shift is\n \n E (1)\n12 = H c\n S\n12\n @  H\u0004@  c\n S\n12\n I  H00@00I\n \n \n = H c\n S\n12\n @Vint Ax1 - x2B @ c\n S\n12 I \n \n = J12 + K12.\n \n(13.55)\nThis is the same shift as the boson excited state because the spatial wave function is the same and the \nspin does not affect the expectation value.\nFor the symmetric triplet spin state, the spatial wave function is antisymmetric\n \n@\n c\n AS\n12 9 \u0003 c\n A\n12 1x1, x22@1M 9 =\n1\n12 3w1 1x12w2 1x22 - w21x12w1 1x224@1M 9. \n(13.56)\nThe resultant ﬁrst-order energy correction\n \n E(1)\n12 = Hc\n A\n12\n @  H\u0004@c\n A\n12\n I H1M @1M I  \n \n = Hc\n A\n12\n @Vint 1x1 - x22@\n c\n A\n12 I \n \n = J12 - K12\n \n(13.57)\n",
    "426 \nIdentical Particles\nhas a negative exchange integral contribution because of the minus sign in the asymmetric spatial \nwave function (Problem 13.12). We combine the results in Eqs. (13.55) and (13.57) to express the \nenergy of the ﬁrst excited state of the two spin-1/2 fermion system as\n \n E12 = E (0)\n1\n+ E (0)\n2\n+ J12 { K12, \n(13.58)\nwhere the +1-2 sign refers to the symmetric (antisymmetric) spatial state and the respective singlet \n(triplet) state.\nThe energies of the ground and excited states are shown in Fig. 13.6, where we assume that \nthe direct and exchange integrals J and K are positive, which is typical for the Coulomb interaction \nbetween identical charged particles. The direct integral raises all energy states because of the posi-\ntive repulsive interaction expected classically for charged particles of the same sign. The exchange \nintegral reﬂects the additional repulsive interaction caused by the spatial correlation or anticorrelation \nof the particles arising from the symmetrization postulate. For bosons and spin-singlet fermions, the \nspatial “attraction” that arises from the symmetrization postulate [Fig. 13.3(c)] increases the positive \nrepulsive interaction energy because they are closer together in space. For spin-triplet fermions, the \nspatial “repulsion” that arises from the symmetrization postulate [Fig. 13.3(d)] decreases the interac-\ntion energy. The degeneracy of the excited state in the fermion case is partially lifted by the exchange \n(a)  bosons\n(b)  fermions\nJ12\nE1 E2\nE1 E2\n2E1\n2E1\nJ12\nJ11\nJ11\nK12\n2K12\nΨS12 00\nΨS11 00\nΨS11 00\nΨS12 00\nΨA12 1M\nFIGURE 13.6 Energies and state vectors for the ground and ﬁrst excited states of two identical \n(a) spin-0 bosons or (b) spin-1/2 fermions in a one-dimensional potential energy well.\n",
    "13.4 Example: The Helium Atom \n427\nintegral term. The resultant energies depend on the spin of the system even though spin is not part of \nthe interaction Hamiltonian. The spin plays its role by determining which spatial states are allowed.\n13.4 \u0002 EXAMPLE: THE HELIUM ATOM\nThe symmetrization postulate and the resultant Pauli exclusion principle are key elements in under-\nstanding atomic structure and the periodic table. The ramiﬁcations of the symmetrization postulate are \nﬁrst evident in the case of the helium atom with two electrons. The helium Hamiltonian is similar to \nhydrogen but with added potential energy terms due to the second electron interacting with the doubly \ncharged nucleus and the two electrons interacting with each other. The helium Hamiltonian is\n \n  H = ¢\np2\n1\n2m\n -\n 2e2\n4pe0\n r1\n≤+ ¢\np2\n2\n2m\n -\n 2e2\n4pe0\n r2\n≤+\ne2\n4pe0\n r12\n , \n(13.59)\nwhere r12 is the separation of the two electrons, as shown in Fig. 13.7. The Coulomb repulsion term \nbetween the two electrons is clearly of the same order of magnitude as the Coulomb terms represent-\ning the interaction of each electron with the nucleus, but we treat it as a perturbation so that we can \nwrite the Hamiltonian as a zeroth-order term whose solutions we know, plus a perturbation:\n \n  H = H0 + H\u0004  \n \n  H\u0004 =\ne2\n4pe0\n r12\n. \n(13.60)\nThe zeroth-order Hamiltonian is the sum of two hydrogen atom Hamiltonians, each with a nuclear \ncharge Z = 2. The eigenstates and eigenenergies of a hydrogenic atom with nuclear charge Z are \nobtained from the hydrogen atom solutions from Chapter 8 by the substitution e2 S Ze2, which scales \nthe energies by a factor Z\n 2 and the size of the radial wave function by 1>Z. For example, the ground-\nstate wave function of a hydrogenic atom with nucleus +Ze is\n \n  c100 1r, u, f2 =\nC\nZ  3\npa3\n0\n  e -Zr>a0 , \n(13.61)\nr2\nr1\nr12\nHe\u0002\u0002\n\u0003e\n\u0003e\nFIGURE 13.7 Helium atom coordinates.\n",
    "428 \nIdentical Particles\nwhere a0 = 4pe0\n  U2>me2 is the Bohr radius. The zeroth-order energy of the helium atom with one \nelectron in state na and one electron in state n b is the sum of the hydrogenic energies:\n \n E (0)\nnanb = -Z\n 2 Ryd ¢  1\nn2\na\n+ 1\nn2\nb\n ≤ \n \n = -4 Ryd ¢  1\nn2\na\n+ 1\nn2\nb\n ≤ . \n(13.62)\nThough we don’t expect this perturbation approach to yield very precise results, it is a useful ﬁrst \nattempt and illustrates many of the important new aspects that arise from the symmetrization postulate.\nAs we did in the previous section, we separate the spin and spatial aspects of the state vectors \nbecause spin and position are not coupled. The complete eigenstates have the form\n \n@ c9 = @ cspatial9@ cspin9 \n(13.63)\nand must be antisymmetric under exchange of the two fermions. Thus the spatial part must be symmetric \nif the spin part is antisymmetric, or the spatial part must be antisymmetric if the spin part is symmetric.\nThe spatial part of the state vector represents the state with one particle in the hydrogenic state \nna /a ma and one particle in the state nb /b  mb . The properly symmetrized spatial wave functions are\n \nc S\nna\n /a\n  ma\n , nb\n /b\n  mb 1r1 , r22 =\n1\n12 3cna\n /a\n  ma 1r12 cnb\n /b\n  mb 1r22 + cna\n /a\n  ma 1r22 cnb\n /b\n  mb 1r124 \n \nc A\nna\n /a\n  ma\n , nb\n /b\n  mb 1r1 , r22 =\n1\n12 3cna\n /a\n  ma 1r12 cnb\n /b\n  mb 1r22 - cna\n /a\n  ma 1r22 cnb\n /b\n  mb 1r124. \n(13.64)\nThe spin part of the state vector is obtained by properly symmetrizing the spin states of two spin-1/2 \n fermions, which yields the eigenstates 0 SM9 of the total spin with S = 0 or 1. The four states are\n \n 0 119 = 0  + +9 \n \n 0 109 =\n1\n12 30  + -9 + 0  - +94   t Triplet state \n \n 0 1, -19 = 0  - -9 \n \n 0\n 009 =\n1\n12 30  + -9 - 0  - +94   6 Singlet state. \n(13.65)\nThe complete antisymmetric quantum state vector of the helium atom is obtained by combining \nthe antisymmetric singlet state with symmetric spatial wave function or by combining the symmetric \ntriplet state with antisymmetric spatial wave function. Thus, the only possible states are:\n \n @  c SA\nna\n /a\n ma, nb\n /b\n mb\n I = @  c S\nna\n /a\n ma, nb\n /b\n mb\n I  @ 00I  \n \n @  c AS\nna\n /a\n ma, nb\n /b\n mb\n I = @  c A\nna\n /a\n ma, nb\n /b\n mb\n I  @1M I . \n(13.66)\nThe other combinations are not possible states for this system.\n13.4.1 \u0002  Helium Ground State\nThe ground state of helium has both electrons in hydrogenic ground states, so the antisymmetric spa-\ntial state is identically zero [Eq. (13.22)] and only the symmetric spatial state is allowed. To ensure \n",
    "13.4 Example: The Helium Atom \n429\n0eV\n\u000310eV\n\u000320eV\n\u000330eV\n\u000340eV\n\u000350eV\n\u000360eV\n\u000370eV\n\u000380eV\n\u000390eV\n\u0003100eV\n\u0003110eV\n0eV\n\u000310eV\n\u000320eV\n\u000330eV\n\u000340eV\n\u000350eV\n(na, nb)\n(4, 4)\n(1, 4)\n(1, 3)\n(1, 2)\n(1, 1)\n(1, \u0004)\n(3, 4)\n(3, 3)\n(2, 4)\n(2, 3)\n(2, 2)\n(\u0004, \u0004)\nthat the total state vector is antisymmetric, Eq. (13.66) tells us that the spin part of the ground state \nmust be the antisymmetric singlet state 0 009. The triplet state 0 1M9 is not permitted in the helium \nground state. Thus the ground state of helium is\n \n @  cground\n I = @  c\n SA\n1s,1s I = @ c\n S\n1s,1s I@  00I, \n(13.67)\nwith a zeroth-order energy determined by the sum of two hydrogenic ground-state energies:\n \n E (0)\n1s,1s = -4 Ryd a 1\n12 + 1\n12b = -8 Ryd = -108.8 eV. \n(13.68)\nThe zeroth-order helium energy states are shown in Fig. 13.8, obtained using Eq. (13.62).\nAn aside about energy levels is in order here. In hydrogen, the ground-state energy is -13.6 eV,\nwhere zero energy corresponds to the electron and proton inﬁnitely far apart and at rest. We refer to \nthis zero energy level as the ionization level. For the calculation we have just done for the helium \nground state, the zero of energy corresponds to both electrons removed to inﬁnity, and so is referred to \nas the double ionization level. However, it is more common in the literature to quote atomic energy \nFIGURE 13.8 Helium atom energies in zeroth order, where hydrogenic Bohr energies are \nassumed. The energy scale on the left is referenced to the double ionization level and the \nenergy scale on the right is referenced to the single ionization level, which is at -54.4 eV \nwith respect to the double ionization level.\n",
    "430 \nIdentical Particles\nlevels with respect to the single ionization level corresponding to one electron removed from the \natom. If we remove one electron from helium, we are left with a hydrogenic ion with an energy\n \n E (0)\n1s,\u0005 = -4 Ryd a 1\n12 -\n1\n\u00052b = -4 Ryd = -54.4 eV, \n(13.69)\nwhich is half of the energy in Eq. (13.68). To quote energies referenced to the single ionization level, \nwe must subtract this energy (as shown in Fig. 13.8), in which case we get a helium ground-state \nenergy of -54.4 eV.\nThe experimental value for the helium ground-state energy is -25 eV (referenced to the single \nionization level), which is quite different from our zeroth-order estimate. This is not unexpected, as we \nsaid above that the electron-electron interaction, which is neglected in zeroth-order, is the same order \nof magnitude as the electron-nucleus interactions that are responsible for the binding. Even though they \nare the same order, our approach is to treat the electron-electron repulsion as a perturbation and ﬁnd the \nperturbed energies of this system. The helium ground state is nondegenerate, so we ﬁnd the shift caused \nby the perturbation by ﬁnding the expectation value of the perturbation in the zeroth-order state:\n \n E (1)\n1s,1s = H  c\n SA\n1s,1s\n @  H\u0004@  c\n SA\n1s,1s\n I\n \n \n = H  c\n S\n1s,1s @\n \nH00 @ \ne2\n4pe0\n r12\n @  cS\n1s,1s I  @  00 I\n \n \n = H  cS\n1s,1s @ \ne2\n4pe0\n r12\n @  cS\n1s,1s  I  H00 @  00 I\n \n \n =\nO\nc*\n100 1r12  c*\n100 1r22 \ne2\n4pe00 r1 - r20\n c100 1r22c100 1r12d 3r1  d 3r2. \n(13.70)\nThis integral is the direct integral we deﬁned in the one-dimensional example in Eq. (13.45). In this \nthree-dimensional Coulomb interaction problem, we deﬁne the direct integral as\n \nJn/,n\u0004/\u0004 =\nO\n0 cn/m1r12 0 2 \ne2\n4pe00 r1 - r20 0 cn\u0004/\u0004m\u00041r22 0 2  d 3r1  d 3r2. \n(13.71)\nThese integrals are independent of m, but not /, which is why we drop the m subscript on the \nenergies. To calculate the direct integrals, it is useful to use the spherical harmonic addition theorem\n \n1\n0 r1 - r20 = a\n\u0005\n/=0\n a\n/\nm=-/\n \n4p\n2/ + 1 r/\n6\nr/+1\n7\n Y*\n/m1u1, f12Y/m 1u2, f22, \n(13.72)\nwhere r7 stands for the larger of the two distances r1 and r2, and r6 the smaller.\nThe ground-state direct integral in Eq. (13.70) can be done and the result is (Problem 13.14):\n \n E (1)\n1s,1s = 5\n8 \nZe2\n4pe0\n a0\n= 5\n4 \ne2\n4pe0\n a0\n= 10\n4  Ryd = 34 eV. \n(13.73)\nThe shift is positive because the electrons repel each other, yielding a positive Coulomb potential \nenergy. The new estimate of the ground-state energy (relative to the single ionization level) is\n \nE1s,1s \u0002 E (0)\n1s,1s + E (1)\n1s,1s = -54.4 eV + 34 eV = -20.4 eV, \n(13.74)\nwhich is now much closer to the experimental value of -25 eV. To make a better estimate, we would \nhave to account for the shielding of the nuclear charge by the presence of the second electron.\n",
    "13.4 Example: The Helium Atom \n431\n 13.4.2 \u0002 Helium Excited States\nNow let’s turn our attention to the excited states of helium. The zeroth-order energy level diagram in \nFig. 13.8 makes it clear that all states with both electrons excited have a zeroth-order energy above \nthe single ionization level E(0)\n1s,\u0005. For example, the doubly excited state na = 2, nb = 2 has an energy\n \nE (0)\n2,2 = -4 Ryd a 1\n22 + 1\n22b = -2 Ryd = -27.2 eV, \n(13.75)\nwhich is 27.2 eV above the single ionization level. Such doubly excited states are not stable. They decay \nto a lower energy state with one electron in the hydrogenic ground state and the second electron travel-\ning to inﬁnity with the excess energy. This decay is very likely and so the lifetime of the doubly excited \nstates is very short. The likelihood of this process leads to its name: auto-ionization. For this reason, it is \ncommon to limit the discussion of excited atomic states (in this helium example as well as other atomic \nsystems) to those where only one electron is excited and the others remain in the atomic ground state.\nBecause the excited electron is in a different spatial state than the remaining ground-state elec-\ntron, both the symmetric and antisymmetric spatial states are allowed. We also expect additional \ndegeneracy because the hydrogen excited states are degenerate with respect to the angular momentum \nquantum numbers / and m.\nThe ﬁrst excited state of helium has na = 1, nb = 2. The two possible states are:\n \n@  c\n SA\n1s, 2/I = @  c\n S\n1s, 2/I 0 009 \u0003\n1\n12 3 c100 1r12  c2/m 1r22 + c100 1r22  c2/m 1r124 0 009    \n \n@  c\n AS\n1s, 2/9 = @  c\n A\n1s, 2/I 0 1M9 \u0003\n1\n12 3 c100 1r12  c2/m 1r22 - c100 1r22  c2/m 1r124 0 1M 9. \n(13.76)\nIn both the symmetric and antisymmetric spatial cases, there are four possible states corresponding to \nthe single 2s 1/ = 0, m = 02 and the three 2p 1/ = 1, m = 0, {12 states. When we combine these \nstates with the single spin singlet state and the three spin triplet states, we ﬁnd that there are 16 possible \nstates overall. All these states are degenerate in the unperturbed system with Hamiltonian H0.\nThe unperturbed energy of these states is the hydrogenic energy shown in Fig. 13.8. We apply \ndegenerate perturbation theory to ﬁnd the effect of the electron-electron repulsion term H\u0004 on these\n16 degenerate states. The perturbation Hamiltonian is diagonal, so the energy corrections are the \ndiagonal elements\n \nE (1)\n1s, 2/ = Hc\n SA\n1s, 2/\n @  H\u0004@  c\n SA\n1s, 2/\n I \n(13.77)\nfor the symmetric spatial state, and\n \nE\n (1)\n1s, 2/ = Hc\n AS\n1s, 2/\n @  H\u0004@  c\n AS\n1s, 2/\n I \n(13.78)\nfor the antisymmetric spatial state. In both cases, the spin states are unaffected by the perturbation \n[see Eq. (13.70)] so we are left with a spatial integral:\n \nE 112\n1s,2/ =\nO\n1\n12\n 3c*\n100 1r12c*\n2/m 1r22 { c100 1r22c2/m 1r124\ne2\n4pe00 r1 - r20  \n \n 1\n12 3  c*\n100 1r12  c*\n2/m 1r22 { c100 1r22  c2/m 1r124d 3r1 d 3r2, \n(13.79)\n",
    "432 \nIdentical Particles\nwhere the { distinguishes the two states in Eq. (13.76). This gives four terms, but they are equal in pairs \nif we swap the integration dummies r1 and r2. Hence, we cancel the factor of 1/2 and get two terms:\n \n E (1)\n1s, 2/ =\nO\n0\n c100 1r12  0\n2\n \ne2\n4pe0 0 r1 - r20\n 0\n c 2/m1r22  0\n2 d 3r1 \n d 3r2 \n(13.80)\n \n {\nO\nc*\n100 1r12  c*\n2/m 1r22 \ne2\n4pe0\n 0 r1 - r20\n  c100 1r22  c2/m 1r12d 3r1 d 3r2. \nThe ﬁrst term is the direct integral and the second term is the exchange integral, which in general is\n \nKn/, n\u0004/\u0004 =\nO\nc *\nn/m1r12c *\nn\u0004/\u0004m\u0004 1r22 \ne2\n4pe0 0 r1 - r20\n c n/m 1r22  c n\u0004/\u0004m\u0004 1r12  d 3r1  d 3r2. \n(13.81)\nSo we write the energy perturbation as\n \n E (1)\n1s, 2/ = J1s, 2/ { K1s, 2/, \n(13.82)\nwhere the +1-2 sign refers to the symmetric (antisymmetric) spatial state and the respective singlet (trip-\nlet) state. The direct integral is also called the Coulomb interaction energy because it is the electrostatic \ninteraction potential energy of the two electrons: one in the 1s state and the other in the 2s or 2p state.\n1s2s\n1s2p\n2 1P\n2 3P\n2 1S\n2 3S\n1 1S\n2K1s,2p\n1s2s\n2K1s,2s\nJ1s,2s\nJ1s,1s\nJ1s,2p\n1s2p\n1s2\nFIGURE 13.9 Shifts and splittings of the helium ground state and ﬁrst excited state \ncaused by the direct and exchange interactions.\n",
    "13.4 Example: The Helium Atom \n433\n41S\n41P\n41D\n31S\n21S\n11S\n21P\n23S\n23P\n31P\n31D\n43S\n43P\n43D\n33S\n33P\n33D\nOrthohelium\nParahelium\nHe\nFIGURE 13.10 Helium energy spectrum.\nBoth the direct and exchange integrals in helium are positive, so the singlet states are higher in \nenergy than the corresponding triplet states, as shown in Fig. 13.9. The energy levels are labeled with \na modiﬁed spectroscopic notation n 2S+1L [see Eq. (11.84)], with n being the state of the excited elec-\ntron. The J label is suppressed because it has no bearing on the energy at this order of approximation. \nWe understand the singlet-triplet ordering of the energy levels by noting that in the singlet state, the \nspin state is antisymmetric and the spatial state is symmetric, implying that the two electrons get closer \nto each other and therefore increase the repulsive Coulomb potential energy. In the triplet state, the \nspin state is symmetric, the spatial state is antisymmetric, and the two electrons are farther apart, thus \nlowering the Coulomb potential energy.\nThis ordering of the energy levels, with the singlet state above the triplet state, is evident through-\nout the excited states of helium. Another important feature of the singlet and triplet states is that opti-\ncal transitions between these states are forbidden. The electromagnetic light ﬁeld does not couple to \nthe spin, so the selection rules for optical transitions require there to be no change in the spin quantum \nnumber between two states. Hence, transitions between the singlet and triplet states of helium are \nforbidden, and it was originally believed that there were two types of helium: parahelium 1S = 02 \nand orthohelium 1S = 12. Thus, energy diagrams of helium often show the singlet and triplet levels \nseparately, as in Fig. 13.10. We now know that transitions between parahelium and orthohelium do \noccur, with small probability, due to higher-order effects. Note that the 23S state of orthohelium is \nthe lowest state on the triplet side. Due to the spin selection rule, it is metastable against decay to the \nground state 11S. The lifetime of this metastable state is 8000 seconds, which is generally much longer \nthan the time it takes a helium atom to travel through an experimental system, so the state effectively \nhas an inﬁnite lifetime in laboratory experiments.\n",
    "434 \nIdentical Particles\n13.5  \u0002   THE PERIODIC TABLE\nThe helium atom illustrates the importance of the symmetrization postulate in determining the spec-\ntrum of energy levels of a multielectron atom. Let’s now qualitatively explain how the symmetrization \npostulate, in the guise of the Pauli exclusion principle, determines the structure of the periodic table \nas the atomic number Z increases. To a zeroth approximation, the states of multielectron atoms are \nthe hydrogenic states labeled with n, /, and m. However, we must also include spin, so there are four \nquantum numbers n, /, m/ and ms labeling each electron (the ﬁfth number s = 1/2 is the same for all \nelectrons so we suppress it). The zeroth-order hydrogen energy states En depend only on the quantum \nnumber n and are n2 degenerate with respect to / and m/. The additional ms degree of freedom doubles \nthe degeneracy, so that each hydrogenic energy level is 2n2 degenerate. We refer to each n energy \nlevel as a shell and to each n/ orbital as a subshell. Each subshell has 212/ + 12 possible states.\nIf electron were bosons, any number could occupy the hydrogenic ground state n = 1, similar \nto Fig. 13.5(c), and chemistry would be boring. But because electrons are fermions, only one electron \ncan occupy each state speciﬁed by the four quantum numbers n, /, m/, and ms. Hence, as the atomic \nnumber Z increases through the periodic table, we expect that each additional electron occupies the \nlowest available hydrogenic energy state, ﬁlling each subshell with 212/ + 12 electrons and each \nshell with 2n2 electrons, analogous to Fig. 13.5(d). The resulting electronic conﬁgurations are denoted \nby listing the subshells with the number of electrons in each subshell as a superscript, Ae.g., 1s2B. We \nthus expect the periodic table to reﬂect the pyramidal structure shown in Table 13.1. But that would \nmean that the seven rows of the periodic table would have 280 atoms, whereas we know there are just \nover 100 atoms. The periodic table does have a pyramidal structure, but not one that reﬂects the num-\nbers in Table 13.1. Why not?\nThe primary reason is that the nuclear charge is shielded by inner shell electrons in a way that lifts \nthe / degeneracy we expect from the simple hydrogen case, giving rise to energy levels speciﬁed by \nthe n and / quantum numbers. For a given n, higher values of / correspond to orbits farther from the \nnucleus because the increased angular momentum leads to a larger centrifugal barrier. Hence, the elec-\ntrons in high angular momentum orbitals are shielded from the nuclear attraction by the electrons in \nlower orbits and are bound less tightly. This screening effect explains why electrons ﬁll the hydrogenic \norbitals in the sequence 1s, 2s, 2p, 3s, etc. However, the screening effect is so large that it exceeds the \nhydrogenic n S n + 1 level separation in some cases, which disturbs the expected shell ﬁlling struc-\nture of Table 13.1. A schematic of the ordering of the energy levels of multielectron atoms is shown \nTable 13.1  Electronic Conﬁgurations in a Periodic Table Based Upon \nPurely Hydrogenic Energy Levels\nShell (n)\nSubshell Conﬁguration\nDegeneracy (2n2)\n1\n1s 2\n2\n2\n2s 2 2p6\n8\n3\n3s 2 3p6 3d 10\n18\n4\n4s 2 4p6 4d 10 4f  14\n32\n5\n5s 2 5p6 5d 10 5f\n  \n 14 5g 18\n50\n6\n6s 2 6p6 6d 10 6f  14 6g18 6h 22\n72\n7\n7s 2 7p6 7d\n 10 7f  14 7g18 7h 22 7i  26\n98\n",
    "13.5 The Periodic Table \n435\nin Fig. 13.11. The screening effect results in four major differences from the unshielded hydrogenic \nmodel: (1) the / degeneracy is lifted, (2) the nd levels are shifted up to lie above the 1n + 12s levels, \n(3) the nf levels are shifted up to lie above the 1n + 22s levels, and (4) the np levels are the high-\nest levels within their “group” of levels. Hence the energy ﬁlling proceeds in the manner shown in \nTable 13.2, with the number of atoms per row shown at right.\n1s\n2s\n2p\n3s\n3p\n4s\n3d\n4p\n5s\u0005\n4d\n5p\n6s\n4f\u0005\n5d\n6p\u0005\nEnergy\nFIGURE 13.11 Approximate ordering of the energies of subshells after \naccounting for the shielding of the nuclear charge. The energies are not to scale.\nTable 13.2 Electronic Conﬁgurations in the Periodic Table\nRow\nSubshell Conﬁguration\nNumber of Atoms\n1\n 1s 2\n2\n2\n 2s 2 \n \n \n2p6\n8\n3\n 3s 2 \n \n \n3p6\n8\n4\n 4s 2 \n \n3d  10 \n4p6\n18\n5\n 5s 2 \n \n4d  10 \n5p6\n18\n6\n 6s  2 \n4 f   14 \n5d  10 \n6p6\n32\n7\n 7s  2 \n5 f   14 \n6d  10 \n7p6\n32\n",
    "436 \nIdentical Particles\nH\n1\nLi\n3\nNa\n11\nK\n19\nRb\n37\nCs\n55\nFr\n87\nBe\n4\nMg\n12\nCa\n20\nSr\n38\nBa\n56\nRa\n88\nB\n2p\n3p\n4p\n5p\n6p\n7p\n3d\n4d\n5d\n6d\n7s\n6s\n5s\n4s\n3s\n2s\n1s\n5\nAl\n13\nGa\n31\nIn\n49\nTl\n81\nC\n6\nSi\n14\nGe\n32\nSn\n50\nPb\n82\nN\n7\nP\n15\nAs\n33\nSb\n51\nBi\n83\nO\n8\nS\n16\nSe\n34\nTe\n52\nPo\n84\nF\n9\nCl\n17\nBr\n35\nI\n53\nAt\n85\nNe\n10\nHe\n2\nAr\n18\nKr\n36\nXe\n54\nRn\n86\nUut\n113\nUuq\n114\nUup\n115\nUuh\n116\nUus\n117\nUuo\n118\nSc\n21\nY\n39\nLu\n71\nLr\n103\nTi\n22\nZr\n40\nHf\n72\nRf\n104\nV\n23\nNb\n41\nTa\n73\nDb\n105\nCr\n24\nMo\n42\nW\n74\nSg\n106\nMn\n25\nTc\n43\nRe\n75\nBh\n107\nFe\n26\nRu\n44\nOs\n76\nHs\n108\nMt\n109\nDs\n110\nRg\n111\nCn\n112\nCo\n27\nRh\n45\nIr\n77\nNi\n28\nPd\n46\nPt\n78\nCu\n29\nAg\n47\nAu\n79\nZn\n30\nCd\n48\nHg\n80\n4f\n5f\nLa\n57\nAc\n89\nCe\n58\nTh\n90\nPr\n59\nPa\n91\nNd\n60\nU\n92\nPm\n61\nNp\n93\nSm\n62\nPu\n94\nEu\n63\nAm\n95\nGd\n64\nCm\n96\nTb\n65\nBk\n97\nDy\n66\nCf\n98\nHo\n67\nEs\n99\nEr\n68\nFm\n100\nTm\n69\nMd\n101\nYb\n70\nNo\n102\nAlkalis\nNoble gases\nHalogens\nAlkaline earths\nTransition metals\nLanthanides (rare earths)\nActinides\nFIGURE 13.12 Periodic table of the elements.\nTable 13.3 Electronic Conﬁgurations of Some Elements\n1\nH\n1s 2\n25\nMn\n[Ar] 4s 2 3d 5\n2\nHe\n1s 2\n28\nNi\n[Ar] 4s 2 3d 8\n3\nLi\n[He] 2s1\n29\nCu\n[Ar] 4s1 3d 10\n4\nBe\n[He] 2s 2\n30\nZn\n[Ar] 4s 2 3d 10\n5\nB\n[He] 2s 2 2p1\n36\nKr\n[Ar] 4s 2 3d 10 4p6\n6\nC\n[He] 2s 2 2p 2\n37\nR b\n[Kr] 5s1\n7\nN\n[He] 2s 2 2p 3\n46\nPd\n[Kr] 4d 10\n8\nO\n[He] 2s 2 2p 4\n54\nXe\n[Kr] 5s 2 4d 10 5p6\n9\nF\n[He] 2s 2 2p 5\n55\nCs\n[Xe] 6s1\n10\nNe\n[He] 2s 2 2p6\n57\nLa\n[Xe] 6s 2 5d 1\n11\nNa\n[Ne] 3s1\n58\nCe\n[Xe] 6s 2 4f 1 5d 1\n18\nAr\n[Ne] 3s 2 3p6\n59\nPr\n[Xe] 6s 2 4f  3\n19\nK\n[Ar] 4s1\n86\nRn\n[Xe] 6s 2 4f 14 5d 10 6p6\n21\nSc\n[Ar] 4s 2 3d 1\n87\nFr\n[Rn] 7s1\n23\nV\n[Ar] 4s 2 3d 3\n92\nU\n[Rn] 7s 2 5f 3 6d 1\n24\nCr\n[Ar] 4s1 3d 5\n94\nPt\n[Rn] 7s 2 5f  6\nThe full periodic table is shown in Fig. 13.12 and reﬂects the pyramidal structure of Table 13.2 \nrather than Table 13.1. Electrons ﬁll up the 1s subshell in the ﬁrst row and the 2s and 2p subshells \nin the second row, as shown in Table 13.3. So far, this follows the purely hydrogenic case. But the \n",
    "13.6 Example: The Hydrogen Molecule \n437\nscreening effect pushes the 3d energy level up near the 4s energy level, so the third row has only \nthe 3s and 3p subshells. The 3d states are not ﬁlled until the fourth row of the periodic table. The 4s \nstates are ﬁlled ﬁrst for potassium and calcium, then the 3d states are ﬁlled for scandium through \nzinc, and ﬁnally the 4p states are ﬁlled for gallium through krypton. The 4s and 3d levels are so \nclose that there are some anomalies in the transition metals in the fourth row, as indicated in Table \n13.3. Chromium and copper each have only one 4s electron and one more 3d electron than you \nmight expect. The ﬁfth row ﬁlls in the order 5s, 4d, and 5p, analogous to the fourth row because \nthe f subshells are pushed up two groups. The ﬁfth row transition metals also exhibit anomalies in \nthe 5s and 4d ordering, with palladium being the most extreme in having no 5s electrons. The sixth \nand seventh rows both include f subshells and also have anomalous ﬁlling among the s, f, and d \nsubshells.\n13.6 \u0002  EXAMPLE: THE HYDROGEN MOLECULE\nNow let’s take a look at another two-electron system that will introduce us to some molecular phys-\nics and prepare us for the periodic systems in Chapter 15. Consider the hydrogen molecule with two \nnuclei (protons), each with a bound electron, with the two atoms bound to each other to make a four-\nparticle system, as shown in Fig. 13.13. We label the electrons 1 and 2 and the protons A and B. The \nHamiltonian for the molecule includes hydrogen Hamiltonians for each electron-proton pair; addi-\ntional Coulomb potential energy terms for the electron-electron, proton-proton, and electron-other-\nproton pairs; and kinetic energy for the nuclei:\n \nH = Hatom,1A + Hatom,2B + Vee + Vpp + Vep + TN, \n(13.83)\nwhere\n \n Hatom,1A = ¢\np 2\n1\n2m -\ne2\n4pe0\n r1A\n≤\n \n Hatom,2B = ¢\np 2\n2\n2m -\ne2\n4pe0\n r2B\n≤\n \n Vee =\ne2\n4pe0\n r12\n \n(13.84)\n \n Vpp =\ne2\n4pe0\n RAB\n \n Vep = - \ne2\n4pe0\n r1B\n-\ne2\n4pe0\n r2A\n \n Tnuc =\np 2\nA\n2MA\n+\np 2\nB\n2MB\n.\n \nWe treat this two-electron system as we did the helium atom in the sense that we put the electrons into \nthe lowest energy states of the one-electron system and then account for the required symmetrization \nof the two identical electrons. So we must ﬁrst discuss the one-electron molecule.\n",
    "438 \nIdentical Particles\n13.6.1 \u0002  The Hydrogen Molecular Ion H2\n+\nThe hydrogen molecular ion H +\n2  has one electron and has a Hamiltonian\n \n Hion =\np2\n1\n2m -\ne2\n4pe0\n r1A\n-\ne2\n4pe0\n r1B\n+\ne2\n4pe0\n RAB\n+\np2\nA\n2MA\n+\np2\nB\n2MB\n. \n(13.85)\nWe do not need the subscript labeling the electron as #1, but we keep it to connect with the H2 case. \nTo construct approximate energy eigenstates, we use the method of linear combination of atomic \norbitals (LCAO), which assumes that we can use the atomic energy eigenstates as basis functions. If \nthe two protons are far apart, then we expect that in the ground state of the ion, the electron is attached \nto one proton and is in the hydrogen atomic ground state. The electronic wave function in this case is\n \n0 cseparated9 \u0003 c1s 1r1A2, \n(13.86)\nassuming the electron is on proton A. However, the ion Hamiltonian in Eq. (13.85) is spatially sym-\nmetric about the center of the molecule located at the midpoint of the internuclear separation RAB and \nthe eigenstates should reﬂect this spatial symmetry. Hence we construct two possible ground states of \nthe ion that are symmetric and antisymmetric spatially:\n \n 0 c g\n1s91 \u0003\n1\n12 3c1s1r1A2 + c1s1r1B24 \n \n 0 c u\n1s91 \u0003\n1\n12 3c1s1r1A2 - c1s1r1B24. \n(13.87)\nThese states are even (g) and odd (u), respectively, under reﬂection about the midpoint of RAB , and \nare labeled as gerade and ungerade states (German for even and odd). We use this labeling notation to \ndistinguish the spatial symmetry (g, u) from the exchange symmetry (S, A) that we’ll need for the H2 \ntwo-electron molecule.\nTo estimate the ground-state energy of the ion, we calculate the expectation value of the energy \n8E9 = 8Hion9 using the wave functions in Eq. (13.87). We ignore the motion of the nuclei by assum-\ning that the internuclear separation RAB is ﬁxed. In calculating the energy expectation value, we inte-\ngrate over the electron position, but the result is still dependent on the choice for the ﬁxed value of \nRAB. This dependence is evident in the results shown in Fig. 13.14. For both the gerade and ungerade \nstates, the energy at large internuclear separation is simply the hydrogen energy -13.6 eV expected \ne1\n\u0003\ne2\n\u0003\npB\n\u0002\npA\n\u0002\nr1A\nr2A\nr1B\nr2B\nRAB\nr12\nFIGURE 13.13 Hydrogen molecule.\n",
    "13.6 Example: The Hydrogen Molecule \n439\nfor the system of one ground state atom and one distant proton. At very small internuclear separation \n(RAB V a0), the energy of both states becomes positive and very large due to the strong proton-\nproton Coulomb repulsion. For intermediate internuclear separation, the gerade and ungerade states \nhave different energies. The minimum in the gerade state energy indicates an attraction that leads \nto a stable molecule with an internuclear separation given by the bond length R0. The energy of the \nungerade state has no minimum and is repulsive at all distances, implying that a system in this state \nwill dissociate into a bound hydrogen atom and an isolated proton. Hence, we refer to the gerade state \nas a bonding orbital and the ungerade state as an antibonding orbital. Note that the energy of the \nbonding orbital shown in Fig. 13.14 is the potential energy function we used in Chapter 9 (Fig. 9.14) \nto ﬁnd the motion of the nuclei in a diatomic molecule. This approximate method of treating the elec-\ntron motion ﬁrst and then the nuclear motion is the Born-Oppenheimer approximation. It relies on \nthe assumption that the nuclear motion is much slower than the electron motion because of the large \nmass difference.\nTo gain a qualitative understanding of the differences between the gerade and ungerade states, \nconsider a one-dimensional view of the wave functions and probability densities of the two states. \nAlong the line of the internuclear separation, the gerade and ungerade states are\n \n @ c g\n1s9 \u0003\n1\n12 cc1s ar + RAB\n2 b + c1s ar - RAB\n2 b d  \n \n @ c u\n1s9 \u0003\n1\n12 cc1s ar + RAB\n2 b - c1s ar - RAB\n2 b d . \n(13.88)\nSubstituting the hydrogen atomic ground-state wave function c1s1r2 = e-r>a0\u00062pa3\n0 into \nEq. (13.88) yields the plots shown in Fig. 13.15. For the gerade state, the wave functions add\n[Fig. 13.15(a)] and the resulting electron probability density [Fig. 13.15(b)] is large between the two \nprotons. This excess negative charge increases the attractive Coulomb interaction of the electron \nand protons enough to overcome the proton-proton Coulomb repulsion and permit a stable bound \nmolecule. In contrast, the wave functions of the ungerade state subtract [Fig. 13.15(c)] and produce a \n1\n2\n3\nR0\nR(Å)\n\u000314\n\u000312\n\u000310\nE(eV)\nΨ1s\ng\nΨ1s\nu\nFIGURE 13.14 Energies of the bonding and antibonding orbitals of the \nhydrogen molecular ion, as a function of the internuclear separation.\n",
    "440 \nIdentical Particles\nzero point in the electron density [Fig. 13.15(d)] between the two protons. This deﬁciency of negative \ncharge between the protons causes the proton-proton Coulomb repulsion to dominate and leads to the \nantibonding behavior of the ungerade state.\n13.6.2  \u0002 The Hydrogen Molecule H2\nWe now return to the hydrogen molecule with two electrons. In the atomic case, our ﬁrst guess for the \nground state of the two-electron helium atom was to put both electrons in the 1s ground hydrogenic \natomic state (with Z = 2) in a symmetric spatial state and to form an antisymmetric spin-singlet state \nto satisfy the symmetrization postulate. By analogy, our ﬁrst guess for the ground state of the two-\nelectron hydrogen molecule puts each electron in the @\n c g\n1s9 ground hydrogen molecular ion state to \nmake a spatial state that is symmetric with respect to exchange and puts the two electrons in an anti-\nsymmetric spin-singlet state to satisfy the symmetrization postulate:\n \n@ c\n SA\n1s,1s9 = @ c\n g\n1s91 @ c\n g\n1s92 @\n 009 \u0003 1\n2 3c1s 1r1A2 + c1s 1r1B24 3c1s 1r2A2 + c1s1r2B24@\n 009. (13.89)\nJust as we found for helium, there is no possible way to make a spatial state that is antisymmetric \nwith respect to electron exchange when both electrons are in the @  c g\n1s9 one-electron ground state, so \n@  c\n SA\n1s,1s9 is the only possible state in the ground state of the molecule. We conclude that the ground state \nof the hydrogen molecule is a spin singlet state.\nWe can gain more insight into the molecular ground state by looking at the state @  c\n SA\n1s,1s9 more \nclosely. If we expand Eq. (13.89), we obtain\n \n @  c\n SA\n1s,1s9 \u0003 1\n2 3c1s 1r1A2 c1s 1r2 A2 + c1s 1r1B2 c1s1r2B2\n \n \n + c1s 1r1A2 c1s 1r2B2 + c1s 1r2 A2 c1s 1r1B24@ 009. \n(13.90)\n(a)\n(c)\n(b)\n(d)\nA\nB\nA\nB\nA\nB\nA\nB\nΨ1s\ng\n\u0002Ψ1s\u00022\ng\n\u0002Ψ1s\u00022\nu\nΨ1s\nu\nFIGURE 13.15 Hydrogen molecular ion wave functions (a, c) and probability densi-\nties (b, d) for gerade (a, b) and ungerade (c, d) states.\n",
    "13.6 Example: The Hydrogen Molecule \n441\nWe can divide this into two terms, labeled “covalent” and “ionic”\n \n @\n c\n S\ncov\n 9 \u0003 1\n2 3c1s 1r1A2 c1s 1r2B2 + c1s 1r2 A2 c1s 1r1B24  \n \n @\n c S\nion\n 9 \u0003 1\n2 3c1s 1r1A2 c1s 1r2 A2 + c1s 1r1B2 c1s 1r2B24, \n(13.91)\nso that\n \n@\n c\n SA\n1s,1s\n 9 = 1@\n c S\ncov\n 9 + @\n c S\nion\n 92@ 009. \n(13.92)\nThe state @\n c S\ncov9 corresponds to the situation with one electron associated with each nucleus, whereas the \nstate @ c S\nion9 corresponds to the situation with both electrons associated with one nucleus. When the nuclei \nare well separated, @\n c S\ncov9 corresponds to two isolated hydrogen atoms and @\n c S\nion9 corresponds to a proton \nand a negative hydrogen ion, which has an energy larger than the two isolated hydrogen atoms. Hence, we \nexpect that @\n c S\ncov9 would be a better guess for the ground state of the molecule. The state @\n c S\ncov9 represents \ncovalent bonding and the state @\n c S\nion9 represents ionic bonding.\nFor the covalent bond, we can also form an antisymmetric state\n \n@\n c A\ncov9 \u0003 1\n2 3c1s 1r1A2 c1s 1r2B2 - c1s 1r2 A2 c1s 1r1B24, \n(13.93)\nwhich must be associated with the symmetric spin-triplet state:\n \n@\n c AS\ncov9 = @\n c A\ncov9@ 1M9. \n(13.94)\nIf we use the two states @\n c SA\ncov9 = @\n c S\ncov9@ 009 and @\n c AS\ncov9 = @\n c A\ncov9@ 1M9 to ﬁnd the energy expectation \nvalues, then we are using the valence bond method. The results of this calculation for the case with \nboth electrons in the 1s atomic states are shown in Fig. 13.16. The results are qualitatively similar to\n0.5\n1.0\n1.5\n2.0\nR0\nR(Å)\n\u00033\n\u00032\n\u00031\n1\n2\nE(eV)\nΨA11 \u00021M\u0003\nΨS11 \u000200\u0003\nFIGURE 13.16 Energies of the bonding and antibonding orbitals of the \nhydrogen molecule obtained with the valence bond method, with the zero of \nenergy referenced to the dissociation limit.\n",
    "442 \nIdentical Particles\nFig. 13.14 in that there is a bonding orbital and antibonding orbital. This is to be expected because the state \n@\n c S\ncov9 is a gerade state and the state @\n c A\ncov9 is an ungerade state. But now for the H2 molecule, these states \nare also linked to the exchange symmetry and hence the spin. Following the argument for the hydrogen \nion, we conclude that the symmetric spatial state (singlet spin state) has a lower energy than the antisym-\nmetric spatial state (triplet spin state) because of the increased electron-proton Coulomb attraction in the \ngerade state. Note that this ordering of the singlet and triplet states is opposite the case for the excited \nstates of helium. In that case, the increased overlap of the electrons in the symmetric spatial state led to an \nincreased Coulomb repulsion of the two electrons and a higher energy for the spin singlet state.\nSUMMARY\nFor a proper quantum mechanical description of multiple-particle systems, we must account for the indis-\ntinguishability of fundamental particles. The symmetrization postulate requires that the quantum state \nvector of a system of identical particles be either symmetric or antisymmetric with respect to exchange of \nany pair of identical particles within the system. Nature dictates that integer spin particles—bosons—have \nsymmetric states, while half-integer spin particles—fermions—have antisymmetric states. The symme-\ntrization postulate applies to the complete quantum state vector, including both the spin and space \nparts of the system. As a consequence of the symmetrization postulate, some states are not allowed. \nThe best known manifestation of this is the Pauli exclusion principle, which limits the number of \nelectrons in given atomic levels and leads to the structure of the periodic table.\nPROBLEMS\n 13.1 Show that the eigenvalues of the exchange operator P12 are {1.\n 13.2 For a system of two identical spin-0 bosons, the total spin must be zero and the only possible \nsystem spin state is 0 SM9 = 0 009. Express this state in the uncoupled basis and show that it is \nsymmetric with respect to exchange of the two particles.\n 13.3 Consider a system of two identical spin-1 particles. Find the spin states for this system that are \nsymmetric or antisymmetric with respect to exchange of the two particles.\n 13.4 Specify the exchange symmetry of the following wave functions:\n \nca1x1, x22 =\n1\n1x1 + x22\n \ncb1x1, x22 =\na1x1 - x22\n1x1 - x222 + b\n \ncc1x1, x22 =\na1x1 - 3x22\n1x1 + x222 + b\n \ncd 1x1, x2, x32 =\nx1 x2 x3\nx 2\n1 + x 2\n2 + x 2\n3 + b\n.\n 13.5 Use your favorite software tool to plot the two-particle probability density for two non-\ninteracting particles in a one-dimensional harmonic oscillator potential for the case where one \nof the particles is in the single-particle ground state and the other is in the single-particle ﬁrst \nexcited state. Do this for (a) distinguishable particles (of the same mass), (b) identical spin-0 \n",
    "Problems \n443\nbosons, and (c) identical spin-1/ 2 fermions in a spin triplet state. In each case, write the system \nwave function and discuss the important features of your plots.\n 13.6 Consider two noninteracting particles of mass m in an inﬁnite square well. For the case with \none particle in the single-particle state 0 n9 and the other in the state 0 k91n \u0002 k2, calculate the \nexpectation value of the squared interparticle spacing 81x1 - x2229, assuming (a) the particles \nare distinguishable, (b) the particles are identical spin-0 bosons, and (c) the particles are identi-\ncal spin-1/2 fermions in a spin triplet state. Use bra-ket the notation as far as you can, but you \nwill have to do some integrals. Verify the results in Eq. (13.39).\n 13.7 Consider two noninteracting particles of mass m in the harmonic oscillator potential well. For \nthe case with one particle in the single-particle state 0 n9 and the other in state 0 k9 1n \u0002 k2,\ncalculate the expectation value of the squared interparticle spacing 81x1 - x2229, assuming (a) \nthe particles are distinguishable, (b) the particles are identical spin-0 bosons, and (c) the par-\nticles are identical spin-1/ 2 fermions in a spin triplet state. Use bra-ket notation as far as you \ncan, but you will have to do some integrals.\n 13.8 Calculate the one-dimensional particle separation probability density P1x1 - x22 for a \nsystem of two identical particles in an infinite square well with one particle in the single-\nparticle ground state 0 19 \u0003 w11x2 and the other in the state 0 29 \u0003 w21x2. Do this for \nthe three cases of (a) distinguishable particles (of the same mass), (b) identical particles \nin a symmetric spatial state, and (c) identical particles in an antisymmetric spatial state. \nReproduce Fig. 13.4.\n 13.9 Calculate the one-particle probability density P1x12 by integrating the two-particle probability \ndensity P1x1, x22 over the position x2 of particle 2 (i.e., projecting the two-particle probability \ndensity onto the x1 axis). Do this for the three cases of (a) distinguishable particles (of the same \nmass), (b) identical particles in a symmetric spatial state, and (c) identical particles in an anti-\nsymmetric spatial state. Demonstrate that measuring the position of one particle independent of \nthe location of the other particle is the same for all three cases.\n 13.10 Show that Eq. (13.51) follows from Eq. (13.50).\n 13.11 Consider two indistinguishable, uncharged spin-1/2 fermions in the one-dimensional harmonic \noscillator potential V1x2 = 1\n2 mv2x2. The two particles interact with each other through a perturb-\ning potential H\u0004 = 1\n2 a1x1 - x222, where the positive constant a is considered small (a V mv2).\na) For the unperturbed two-particle system, ﬁnd the energy eigenvalues and eigenstates \nof the ground state and the ﬁrst excited state (you need not determine the spatial  \nwave functions, bra-ket notation is sufﬁcient). Specify and discuss the degeneracy  \nof each level.\nb) Discuss qualitatively how the energies in (a) are perturbed by the interaction of the par-\nticles. Draw an energy level diagram showing the unperturbed and perturbed energy levels.\n 13.12 Show that the sign of the exchange contribution K12 is negative for the spin-triplet state in the \nﬁrst excited state of a system of two identical spin-1/ 2 particles [see Eq. (13.57)].\n 13.13 Consider the ﬁrst excited state of helium where one electron is in the n = 1 hydrogenic state \nand the other electron is in the n = 2 hydrogenic state.\na) Using term or spectroscopic notation, list all the allowed states of this system.\nb) How many total states are there?\nc) What is the energy of this level, ignoring the interactions of the electrons with each other?\nd) Describe qualitatively the shifts of this energy level that result from considering the interac-\ntions of the electrons with each other.\n",
    "444 \nIdentical Particles\n 13.14 Find the ﬁrst-order perturbed energy of the helium ground state by calculating the direct \nintegral J in Eq. (13.70). Find the numerical value of your result (in eV ) and conﬁrm \nEq. (13.73).\n 13.15 Find the ﬁrst-order perturbed energies of the helium excited states 1s 2 s and 1s 2 p by calculat-\ning the direct and exchange integrals J, K in Eqs. (13.71) and (13.81). Find the numerical val-\nues of your results (in eV ) and make a diagram similar to Fig. 13.9.\n 13.16 Show that the state of the hydrogen molecule that is antisymmetric with respect to electron \nexchange when both electrons are in the @ c g\n1S9 state is identically zero.\n 13.17 Consider two indistinguishable, noninteracting spin-1/2 fermions in a one-dimensional inﬁnite \nsquare well potential of length L.\na) What is the ground-state energy of the two-particle system?\nb) What is the ground-state wave function?\nc) What is the ﬁrst excited state energy of the two-particle system?\nd) What are the wave functions of the ﬁrst excited state?\ne) What is the degeneracy of the ﬁrst excited state?\nf) Discuss qualitatively how the excited-state energies change if we consider the particles to \nbe interacting through the Coulomb potential.\nRESOURCES\nFurther Reading\nThe work on Bose-Einstein Condensation that was awarded the 2001 Nobel Prize in Physics is \ndescribed at:\nnobelprize.org/nobel_prizes/physics/laureates/2001/\nFurther details on molecular energy calculations are presented in\nB. H. Bransden and C. J. Joachain, Physics of Atoms and Molecules, 2nd ed., Harlow, \nEngland:Prentice Hall, 2003.\nDr. Seuss’s take on indistinguishability can be found in\nDr. Seuss, The Sneetches and Other Stories, New York: Random House, 1961. \n",
    " \n445\nC H A P T E R \n14\nTime-Dependent  \nPerturbation Theory\nIn Chapters 10 and 12, we studied time-independent perturbation theory and found that changes in the \nHamiltonian lead to changes in the energy eigenstates of a system. In those examples, the Hamiltonian \nwas not time dependent, so the perturbed energy levels were still stationary states of the system. Now \nwe turn to the problem of understanding how a system responds to changes in the Hamiltonian that are \na function of time. We will ﬁnd that the new perturbed energy states are no longer stationary states and \nthat changes or transitions between states can occur. In Chapter 3, we solved the time-dependent case \nexactly for a sinusoidal perturbation of the two-level spin system. We found that spin ﬂips or transi-\ntions between spin up and down states occur when the frequency of the time dependence is close to the \nBohr frequency characterizing the energy splitting of the two states. This resonance condition is also \nan important idea in this chapter.\nThe transitions between energy states that arise from a time-dependent Hamiltonian play a major \nrole in experimental studies of quantum mechanical systems. We have referred many times to spec-\ntroscopic experiments that provide evidence of the energies of quantum systems. These spectroscopic \nexperiments rely on the interaction between the oscillating electromagnetic ﬁelds of laser beams and \natoms or molecules that respond to these time-dependent ﬁelds. The examples in this chapter will help \nus better understand these light-matter interactions.\n14.1 \u0002 TRANSITION PROBABILITY\nThe typical experiment that we wish to model with time-dependent perturbation theory is the fol-\nlowing: we start with a system in a particular initial quantum state 0 i9, we turn on a perturbing Ham-\niltonian H\u00041t2 at time t = 0, and then we measure the probability that the system is in a new ﬁnal \nquantum state 0  f 9 at a later time. For example, a hydrogen atom in its ground state 0 1s9 is perturbed \nby an incident laser beam, and we wish to know the probability of the atom making a transition to \nthe 0 3p9 excited state. The Hamiltonian is assumed to be H0 before the perturbation, and as in time-\nindependent perturbation theory, we assume that we know the solutions to the unperturbed energy \neigenvalue equation:\n \nH00 n9 = En0 n9. \n(14.1)\nIn the hydrogen example, H0 is the hydrogen atom Hamiltonian and En and 0 n9 Ashorthand for 0 n/m9B \nare the eigenenergies and eigenstates we solved for in Chapter 8.\n",
    "446 \nTime-Dependent Perturbation Theory\nThe Schrödinger equation that governs the time evolution of a quantum system is\n \nH0 c9 = iU d\ndt 0 c9. \n(14.2)\nThe full Hamiltonian\n \nH = H0 + H\u00041t2 \n(14.3)\nis now time dependent, so we cannot follow the standard recipe we developed in Chapter 3 for deter-\nmining the time evolution of the quantum state vector. In principle, we have to rediagonalize the \nHamiltonian and ﬁnd the new energy eigenstates, and then do that each time the Hamiltonian changes. \nBecause the Hamiltonian is continuously changing, that is nearly impossible to do.\nRather, we take an approach that is similar to that taken in time-independent perturbation \ntheory: we assume the perturbation is small enough that the zeroth-order energy eigenstates are \na good approximation for starting the solution. But now we are more interested in solving the \nSchrödinger equation than in solving the energy eigenvalue equation. We are not so interested in \nhow the perturbation changes the energies of the states; rather, we want to ﬁnd how the perturba-\ntion changes the time evolution of the system. We use the original energy basis for expanding gen-\neral states of the system, even though these states may not be energy eigenstates of the perturbed \nsystem.\nUsing the zeroth-order energy basis, the initial state of the system, before the perturbation is \nturned on, is\n \n0 c1t = 029 = a\nn\ncn0 n9. \n(14.4)\nWe know from the Schrödinger recipe of Chapter 3 that the time evolution of this initial state without \nany perturbation would be\n \n0 cH\u0004=01t29 = a\nn\ncne-iEnt\u0006U0 n9, \n (14.5)\nwhere each term acquires a time-dependent phase evolution factor dependent on the energy of that \nterm. The application of the perturbation H\u00041t2 gives rise to new energy eigenstates and hence new \ntime evolution phase factors. However, if the perturbation is small, then we expect that the new \nsolution will be close to the zeroth-order solution of Eq. (14.5). Hence, we assume that we can \nmodify the zeroth-order solution by including another factor that reﬂects the additional time depen-\ndence caused by the perturbation. We do this by allowing the expansion coefﬁcients to be time \ndependent:\n \n0 cH\u0004\u000201t29 = a\nn\ncn1t2e-iEnt\u0006U0 n9. \n(14.6)\nNow our task is to determine how the coefﬁcients cn1t2 depend on time, with the obvious restric-\ntion that they equal their original values cn102 at t \u0003 0. Substituting Eq. (14.6) for the time evolved \nstate into the Schrödinger equation (14.2), we ﬁnd\n \n 1H0 + H\u00041t220 c1t29 = iU d\ndt 0 c1t29\n \n(14.7)\n \n 1H0 + H\u00041t22a\nn\ncn1t2e-iEnt\u0006U0 n9 = iU d\ndt a\nn\ncn1t2e-iEnt\u0006U0 n9, \n",
    "14.1  Transition Probability \n447\nand using the zeroth-order energy eigenvalue equation (14.1) to cancel some terms yields\n a\nn\nc Encn1t2e-iEnt\u0006U0 n9 + H\u00041t2cn1t2e-iEnt\u0006U0 n9 d = iUa\nn\nc dcn1t2\ndt\n e-iEnt\u0006U0 n9 - iEn\nU\n cn1t2e-iEnt\u0006U0 n9 d\n \n a\nn\nH\u00041t2cn1t2e-iEnt\u0006U0 n9 = iUa\nn\ndcn1t2\ndt\n  e-iEnt\u0006U0 n9. \n(14.8)\nTo simplify this differential equation, we isolate one coefﬁcient in the sum on the right side by project-\ning the whole equation onto a particular energy state, say 0 k9, and use orthogonality to ﬁnd\n \n 8k0 a\nn\nH\u00041t2cn1t2e-iEnt\u0006U0 n9 = 8k0 iUa\nn\ndcn1t2\ndt\n e-iEnt\u0006U0 n9 \n \n a\nn\ncn1t2e-iEnt\u0006U8k0 H\u00041t2 0 n9 = iU dck1t2\ndt\n e-iEkt\u0006U.\n \n \n(14.9)\nRearranging terms yields a differential equation\n \niU \ndck1t2\ndt\n= a\nn\ncn1t2ei(Ek-En)t\u0006U8k 0 H\u00041t2 0 n9 \n(14.10)\nfor each coefﬁcient ck of the expansion. This result is still exact, but it gives us a set of coupled dif-\nferential equations that is difﬁcult to solve. We seek a perturbative solution by using an iterative \napproach. We expand the coefﬁcient cn in a perturbation series\n \ncn = c(0)\nn\n+ c(1)\nn\n+ c(2)\nn\n+ ..., \n(14.11)\nwhere the superscript denotes the order of the perturbation. The right side of Eq. (14.10) already has one \norder of the perturbation in H\u00041t2, so when we equate the two sides of the equation to the same order of \nthe perturbation, we end up with the order of cn on the right side being one less than the order on the left \nside (Problem 14.1). The zeroth-order term of Eq. (14.10) is\n \niU \ndc(0)\nk 1t2\ndt\n= 0. \n(14.12)\nThis says that the coefﬁcients cn have no time dependence when there is no perturbation, which is \nconsistent with Eq. (14.5) where all the Schrödinger evolution time dependence is already speciﬁed. \nThe ﬁrst-order term of Eq. (14.10) is\n \niU \ndc(1)\nk 1t2\ndt\n= a\nn\nc(0)\nn 1t2ei(Ek-En)t\u0006U8k0 H\u00041t2 0 n9. \n(14.13)\nWe can continue in this manner to all orders if we wish, but we will not go beyond the ﬁrst-order \nsolution. To collapse the sum on the right side of Eq. (14.13), we make the assumption mentioned \nabove that the system starts in one particular eigenstate 0 i9 of the zeroth-order Hamiltonian, that is \n0 c1029 = 0 i9. Thus the initial coefﬁcients obey:\n \ncn102 = dni. \n(14.14)\n",
    "448 \nTime-Dependent Perturbation Theory\nIn zeroth-order there is no time dependence, according to Eq. (14.12), so we obtain\n \nc(0)\nn 1t2 = dni. \n(14.15)\nSubstituting Eq. (14.15) into Eq. (14.13) collapses the sum to just one term and yields the ﬁrst-order \ndifferential equation for the coefﬁcient ck1t2:\n \niU \ndc(1)\nk 1t2\ndt\n= ei(Ek-En)t\u0006U8k0 H\u00041t2 0 i9. \n(14.16)\nWe solve Eq. (14.16) by integrating directly to give\n \nck1t2 = 1\niU L\nt\n0\n8k0 H\u00041t\u00042 0 i9ei(Ek-Ei)t\u0004\u0006Udt\u0004  . \n(14.17)\nWe have dropped the superscript on ck1t2 because c(0)\nk 1t2 = 0 for k \u0002 i and we will not solve for \nhigher-order terms, so Eq. (14.17) gives us the complete coefﬁcient to our desired order. Equation (14.17) \ntells us how the expansion coefﬁcient ck1t2 for the energy eigenstate 0 k9 evolves with time subject to the \nperturbation H\u00041t2, given that the system started in the state 0 i9. Equation (14.17) has the familiar form \nof a Fourier transform, so we interpret the result as the Fourier coefﬁcient (in frequency space) of the \nperturbation H\u00041t2 at the Bohr frequency\n \nvki = Ek - Ei\nU\n. \n(14.18)\nIf the perturbation H\u00041t2 has an appreciable component at the Bohr frequency vki associated with the \nenergy difference between the initial state 0 i9 and some other state 0 k9, then the probability of the sys-\ntem making a transition from the initial state 0 i9 to the state 0 k9 is large.\nTo ﬁnd the probability that the system is measured to be in a particular ﬁnal state 0  f 9 at a later \ntime, we project the time-evolved state Eq. (14.6) onto the ﬁnal state\n \n PiSf 1t2 = 08 f 0 c1t290\n2\n \n \n = ` 8 f 0 a\nn\ncn1t2e-iEnt\u0006U0 n9 `\n2\n \n(14.19)\n \n = 0 cf 1t20\n2\n \nand substitute Eq. (14.17) to obtain\n \nPiSf 1t2 = 1\nU2 `\nL\nt\n0\n8 f 0\n H\u00041t\u00042 0 i9ei(Ef-Ei)t\u0004\u0006Udt\u0004 `\n2\n    .  \n(14.20)\nOf course, to actually do the integral and ﬁnd the probability, we need to know the form of the \n perturbation H\u00041t2.\n",
    "14.1  Transition Probability \n449\nExample 14.1: Constant perturbation The simplest example of a time-dependent perturba-\ntion is one that is turned on at t = 0 and then turned off at a later time, but that is constant dur-\ning the time it is on. The integral in Eq. (14.17) to ﬁnd the coefﬁcient cf 1t2 of the ﬁnal state is \nstraightforward:\n \n cf 1t2 = 1\niU\n 8 f 0 H\u00040 i9\nL\nt\n0\neivfi\n t\u0004dt\u0004\n \n \n = 1\niU\n 8 f 0 H\u0004 0 i9 eivfi\n t - 1\nivfi\n \n \n = 1\niU\n 8 f 0 H\u0004 0 i9eivfit\u00062  eivfit\u00062 - e-ivfit\u00062\nivfi\n \n \n(14.21)\n \n = 2\niU\n 8 f 0 H\u0004 0 i9eivfit\u00062  \nsin1vfi\n t>22\nvfi\n. \nThe probability that the system is measured in the ﬁnal state 0  f 9 is\n \nPiSf 1t2 = @cf 1t2 @\n2 = 408 f 0 H\u00040 i9 0\n2\nU2v2\nfi\n sin21vfi\n t>22. \n(14.22)\nThis agrees with the Rabi formula found in Chapter 3 for the probability of a spin ﬂip caused by \na small perturbing constant magnetic ﬁeld [Eq. (3.63)]. Equation (14.22) tells us that to make the \ntransition from the state 0 i9 to the state 0  f 9, there are two essential requirements: (1) the matrix ele-\nment 8 f 0 H\u00040 i9 that determines whether the perturbation connects the two levels must be nonzero, \nand (2) to get appreciable probability, there must be frequency components in the time-dependent \nHamiltonian that include the Bohr frequency vfi for the transition. The ﬁrst requirement is related to \nthe selection rules that we have mentioned previously and that we discuss more fully in Section 14.4. \nThe second requirement is the resonance condition inherent in the Fourier integral of Eq. (14.17). \nEven though the perturbation H\u00041t2 in this example is constant during its application, there is a fre-\nquency component at the Bohr frequency vfi arising from the off-on-off time dependence.\nExample 14.2: Gaussian perturbation Now let’s add some more interesting time dependence \nby assuming that a perturbation is turned on and then turned off with a Gaussian time dependence as \nshown in Fig. 14.1. The form of the perturbation is\n \nH\u00041t2 = V0e-  t2\u0006t2, \n(14.23)\nwhere t is the characteristic time constant of the perturbation. This perturbation is peaked at t = 0 \nand becomes minimal a few time constants away from that. It differs mathematically from the \nsituation we had above where the perturbation started at t = 0. We accommodate this change by \nshifting the starting time of the integral in Eq. (14.17). The major contribution to the integral comes \nfrom times that are a few time constants before and after the peak at t = 0, but mathematically it \n",
    "450 \nTime-Dependent Perturbation Theory\nis simpler to integrate between {\u0005. The coefﬁcient cf 1\u00052 after the perturbation has been applied \nis therefore\n \n cf 1\u00052 = 1\niU L\n\u0005\n- \u0005\n8 f 0 H\u00041t\u00042 0 i9ei(Ef-Ei)t\u0004\u0006U dt\u0004 \n \n \n = 1\niU L\n\u0005\n- \u0005\n8 f 0 V00 i9e-t\u00042\u0006t2eivfit\u0004 dt\u0004. \n(14.24)\nThe sin1vfit\u00042 part of the complex exponential eivfit\u0004 is odd with respect to t\u0004 = 0, so that part of the \nintegral is zero, giving\n \n cf 1\u00052 = 1\niU\n 8 f 0 V00 i9\nL\n\u0005\n- \u0005\ne-t\u00042\u0006t2 cos1vfit\u00042dt\u0004 \n \n = 1\niU\n 8 f 0 V00 i91pte-v2\nfi t2\u00064.\n \n(14.25)\nThe probability that after the perturbation the system is measured in the ﬁnal state 0  f 9 is\n \nPiSf = 0 cf 1\u000520\n2 = pt2\nU2 08 f 0 V00 i90\n2e-v2\nfi t2\u00062. \n(14.26)\nThis result tells us that to have appreciable probability for the transition from the state 0 i9 to \nthe state 0  f 9, the time constant t must be of order 1>vfi , so that there are frequency components in \nthe time-dependent Hamiltonian that include the Bohr frequency vfi for the transition.\nAn important lesson from this example concerns a perturbation that is turned on and off very \nslowly (i.e., the time constant is very long compared with other times relevant to the system). As \nthe time constant t becomes large enough that the product vfi t approaches inﬁnity, the probability \nPiSf in Eq. (14.26) approaches zero, meaning that the system does not change states. This is an \nexample of the adiabatic theorem in quantum mechanics.\n14.2 \u0002 HARMONIC PERTURBATION\nThe previous examples have illustrated the importance of frequency components that match the Bohr \nfrequency of the transition. Frequency components in the time dependence of the Hamiltonian that are \n0\nt\nH'(t)\n2Τ\nFIGURE 14.1 Gaussian time dependence of perturbation.\n",
    "14.2 Harmonic Perturbation \n451\nfar from the Bohr frequency of a particular transition do not produce appreciable probability for that \ntransition. Hence, the most efﬁcient way to make a transition is to impose a sinusoidal perturbation \nat the transition frequency. The study of such resonant interactions is the most important example of \ntime-dependent perturbation theory.\nAt t = 0 we turn on a time-dependent perturbation Hamiltonian that has separate space and \ntime parts:\n \n H\u00041t2 = 2V1r\nu2cos vt\n \n \n = V1r\nu21eivt + e-ivt2. \n(14.27)\nThere are different conventions for including the factor of 2 in Eq. (14.27) or not; without it, one needs \na factor of 1/2 for each complex exponential. Substituting this harmonic perturbation into Eq. (14.17) \nyields the probability amplitude for making a transition from an initial state 0 i9 to a ﬁnal state 0  f 9:\n \n cf 1t2 = 1\niU L\nt\n0\n8 f 0 V1r\nu21eivt\u0004 + e-ivt\u00042 0 i9ei(Ef-Ei)t\u0004\u0006U dt\u0004\n \n= 1\niU\n 8 f 0 V 0 i9\nL\nt\n0\nc ei(vfi+v)t\u0004 + ei(vfi-v)t\u0004 d dt\u0004 \n(14.28)\n \n= 1\niU\n 8 f 0 V 0 i9£ ei1vfi +\n v2t - 1\ni1vfi + v2\n+ ei1vfi - v2t - 1\ni1vfi - v2 §  \n \n= 1\niU\n 8 f 0 V 0 i9£ ei(vfi+v)t\u00062 \nsin \nvfi +  v\n2\n t\nvfi +  v\n2\n+ ei(vfi-v)t\u00062 \nsin \nvfi -  v\n2\n t\nvfi -  v\n2\n§ . \nTo ﬁnd the probability, we square this amplitude, which leads to cross terms and a complicated \nexpression. This is what we have to do if the two terms inside the square brackets are of comparable \nsize, which happens if the frequency is far from a resonance. However, if the resonance condition is \nsatisﬁed or nearly satisﬁed, then one of the two terms inside the square brackets dominates because \nthe denominator approaches zero. Which term dominates depends on the sign of the energy difference \nEf - Ei = U vfi.\n1) If the initial state is lower in energy than the ﬁnal state, then the energy difference \nEf - Ei = U vfi is positive and the second term in Eq. (14.28) is large for an excitation frequency \nthat matches the Bohr frequency: v = vfi . In this case, the dominant probability amplitude is for the \ntransition from a lower state to an upper state, which we call absorption [see Fig. 14.2(a)]. The system \nabsorbs energy from the external perturbation.\nAbsorption\nEf \bEi\nEmission\nEf \tEi\n\u0002f \u0003\n\u0002f \u0003\n\u0002i\u0003\n\u0002i\u0003\nFIGURE 14.2 (a) Absorption and (b) emission processes.\n",
    "452 \nTime-Dependent Perturbation Theory\n2) If the initial state is higher than the ﬁnal state, then the energy difference Ef - Ei = U vfi is \nnegative and the ﬁrst term in Eq. (14.28) is large for an excitation frequency that matches the Bohr fre-\nquency: v = -vfi. In this case, the dominant probability amplitude is for the transition from an upper \nstate to a lower state, which we call emission [Fig. 14.2(b)]. The system emits energy to the external \nperturbation. This emission is caused by the applied ﬁeld, so it is referred to as stimulated emission.\nOnly one of these two terms plays a role in any particular experiment, so we needn’t worry about \nboth together. For now, we consider the absorption case only (we just need to change the sign preced-\ning v if we change to the emission case). The probability of measuring the system in the ﬁnal state is\n \nPiSf 1t2 = 0 Vfi0\n2\nU2  \n sin2 \nvfi -v\n2 t\n1\nvfi -v\n2 2\n2 , \n(14.29)\nwhere we have adopted a shorthand notation for the matrix element of the perturbation:\n \nVfi = 8 f 0 V0 i9. \n(14.30)\nIt is useful to look at this result both as a function of time and as a function of frequency. As a func-\ntion of time, there is an oscillatory dependence as shown in Fig. 14.3, with a period of 2p>1vfi - v2. \nWe saw similar results in Chapter 3 for the Rabi oscillations in the spin case (Fig. 3.12), with a slightly \ndifferent oscillation or ﬂopping frequency. The perturbation result in Eq. (14.29) is equal to the Rabi \nﬂopping probability in Eq. (3.104) for the case of small perturbations. In practice, this oscillating \nprobability is hard to observe, which is related to the ﬁnite lifetime of excited states that we address in \nSection 14.5.\nAs a function of frequency, the transition probability is shown in Fig. 14.4 and displays the \nexpected resonance behavior, with a peak in the probability at v = vfi. The similar result for resonant \nRabi spin ﬂopping was shown in Fig. 3.11. The peak of the probability in Fig. 14.4 grows as t 2, so it \ncould become greater than one, which would violate our perturbation approximation. The resonance \ncurve in Fig. 14.4 has a ﬁnite width, which implies that the resonance condition v = vfi is not an \nexact requirement. Rather, there is a spread of frequencies \u0006v that cause appreciable transition prob-\nability. The frequency width of the probability plot in Fig. 14.4 is approximately \u0006v = 4p>t, where \nthe time t is the duration of the interaction. If we call this duration \u0006t, then we have\n \n\u0006v\u0006t \u0003 4p\n\u0006t \u0006t = 4p, \n(14.31)\nt\n2Π(Ωfi\u0004Ω)\nP(t)\nFIGURE 14.3 Oscillations of the transition probability as a function of time.\n",
    "14.2 Harmonic Perturbation \n453\nwhich is the Fourier frequency-time uncertainty relation. We can convert this to a Fourier energy-time \nuncertainty relation by using \u0006E = U\u0006v to obtain\n \n \u0006E\u0006t \u0003 U\u0006v\u0006t = U4p \n \n \u0003 U.\n \n(14.32)\nThe neglect of the factor of 4p is consistent with the level of this approximation. This uncertainty rela-\ntion tells us that the longer we observe a system, the better we can measure the energy.\nThe resonance peak in Fig. 14.4 resembles a Dirac delta function in the limit that the frequency \nwidth \u0006v = 4p>t S 0, which implies t S \u0005 . Mathematically, the sinc function in Eq. (14.29) \nbecomes a Dirac delta function in this limit:\n \nlim\n tS \u0005\nsin2 \nvfi -v\n2 t\n1\nvfi -v\n2 2\n2\n= 2ptd1vfi - v2. \n(14.33)\nIf we assume this long time limit in Eq. (14.29), we obtain the probability\n \nPiSf 1t S \u00052 = 2pt\nU2  @ Vfi@\n2 d1vfi - v2. \n(14.34)\nThis form makes it evident that the probability increases linearly with time. This behavior is more com-\nmon than the oscillating probability of Fig. 14.3. The linear time dependence seems reasonable because \nwe expect that the more we perturb a system, the more likely it is to undergo a change. The linear time \ndependence in Eq. (14.34) allows us to deﬁne a transition rate as the probability per unit time, which \nwe obtain by differentiating the probability:\n \nRiSf = d\ndt PiSf 1t2, \n(14.35)\nwith the result:\n \nRiSf = 2p\nU2  @ Vfi@\n2 d1vfi - v2  . \n(14.36)\nThe delta function in Eq. (14.36) is called the energy conserving delta function—it requires that \nthe quantum of energy causing the transition (e.g., the photon of a laser beam) match the energy dif-\nference between the two states. In many practical applications, there is a spread of ﬁnal energy states \nΩ\n4Π/t\nΩfi\nP(Ω)\nFIGURE 14.4 Probability of excitation as a function of frequency.\n",
    "454 \nTime-Dependent Perturbation Theory\nrather than a discrete quantum state. For example, in a solid, the band structure of the electronic energy \nlevels represents a continuous range of allowed states (see Chapter 15). In those cases, the relevant \ntransition rate is a sum over the rates to all accessible states. We assume that these rates are incoherent \nso that we can add the rates (i.e., probabilities) rather than the amplitudes. We assume that the spread \nin energies is larger than the width of the sinc function that we turned into a delta function but small \nenough that the rates to all states are the same. Let g1E2 be the density of states per unit energy, such \nthat g1E2dE is the number of energy levels between E and E + dE. Then the total rate is given by the \nintegral over all the rates:\n \n RiSf =\nL\nEf\n +e\nEf\n -e\n2p\nU2  @ Vfi@\n2 d1vfi - v2g1E2dE  \n \n = 2p\nU2  @ Vfi@\n2\nL\nEf\n +e\nEf\n -e\nd1vfi - v2g1E2U  dv . \n \n(14.37)\nThe range over which we integrate in Eq. (14.37) is not important because of the Dirac delta function. \nThe resultant transition rate is\n \nRiSf = 2p\nU 0 Vfi0\n2g1Ef2  . \n(14.38)\nThis result is referred to as Fermi’s golden rule. It is much more practical than Eq. (14.36) because \nthe nonphysical delta function is gone. Fermi’s golden rule is general enough that it applies to many \ntypes of interactions. In the next section we’ll study one particular application.\n14.3 \u0002 ELECTRIC DIPOLE INTERACTION\nOne of the most important applications of time-dependent perturbation theory is to the interaction \nbetween an atom and an electromagnetic ﬁeld. Studying this problem tells us how lasers and atoms \ninteract, and it also leads to an understanding of the ﬁnite lifetime of excited quantum states. The \ninteraction Hamiltonian between an atom and an applied electromagnetic ﬁeld is the same as we used \nfor the Stark effect in Chapter 10, except that the electric ﬁeld is now time dependent. We neglect the \ninteraction of the atom with the magnetic component of the electromagnetic ﬁeld because it is smaller \nby a factor of the ﬁne structure constant a .\nThe electric dipole Hamiltonian is\n \nH\u0004 = -d~E . \n(14.39)\nThe electric ﬁeld is\n \n E1t2 = 2E 0en cosvt\n \n \n = en1E 0eivt + E 0e-ivt2, \n \n(14.40)\nwith the same form of the time dependence as in the generic harmonic perturbation example in the pre-\nvious section. The polarization of the electric ﬁeld is speciﬁed by the unit vector en. We ignore the spa-\ntial variation of the ﬁeld because the size of the atom (\u00070.1 nm) is much smaller than the wavelength \nof visible light (\u0007500  nm), which is the most common case. This means that at any given instant, the \n",
    "14.3 Electric Dipole Interaction \n455\nwhole atom sees the same electric ﬁeld. This assumption is the electric dipole approximation. We \nuse the previous harmonic perturbation calculation and identify the perturbation in Eq. (14.27) as:\n \nV = -d~enE 0. \n(14.41)\nThe atom’s electric dipole moment is\n \nd = -er, \n(14.42)\nresulting in the perturbation\n \nV = eE 0en~r. \n(14.43)\nApplication of Fermi’s golden rule in the form of Eq. (14.36) yields the transition rate:\n \nRiSf = 2p\nU2  08 f 0 eE 0en~r0 i90\n2 d1vfi - v2. \n(14.44)\nOnly the r term in the matrix element depends on the atomic states, so we simplify the rate to\n \nRiSf =\n2pe2E 2\n0\nU2\n 0 en~8 f 0 r0 i90\n2 d1vfi - v2. \n(14.45)\nThe delta function in Eq. (14.45) is not physical, so we must see how to apply this transition rate \nexpression to a real situation. The two most common situations are depicted in Fig. 14.5: (a) the per-\nturbing ﬁeld is not a single frequency and is not coherent, in which case we sum over transition rates \ncaused by the spread of frequencies; or (b) the quantum energy states are continuous, in which case \nwe sum over transition rates to a spread of energy states, as we did in the last section. The ﬁrst case \nis necessary when a broadband light source excites a discrete atomic transition. The second case is \nnecessary when using a monochromatic laser to excite a system to a spread of excited states, or even \nto a single excited state that is broadened by its ﬁnite lifetime. We start with the ﬁrst case because it \nallows us to study the interaction between blackbody radiation and atoms, which Einstein used to \nmodel the broadening of atomic states. Once we know how the atomic states are broadened, we’ll use \nthat knowledge to study the second case of single frequency excitation.\nBroadband Excitation\n(a)\nMonochromatic Excitation\n(b)\n\u0002f \u0003\n\u0002f \u0003\n\u0002i\u0003\n\u0002i\u0003\nFIGURE 14.5 (a) Broadband excitation to a discrete level, and (b) monochromatic \nexcitation to a broadened level.\n",
    "456 \nTime-Dependent Perturbation Theory\n14.3.1 \u0002 Einstein Model: Broadband Excitation\nThe Einstein model assumes a gas of two-level atoms in thermal equilibrium with blackbody radiation \nat temperature T. The atoms are considered to have discrete energy levels and the blackbody radiation \nis modeled as a broadband incoherent electromagnetic ﬁeld. The goal is to reduce Eq. (14.45) to a \nsimple form for the transition rate involving the atom properties and the ﬁeld properties.\nThe electromagnetic ﬁeld of the blackbody radiation has an energy density per unit volume given by\n \nu = e0\n2\n E2 +\n1\n2m0\n B2. \n(14.46)\nThe energy density in the electric and magnetic ﬁelds is the same. Substituting Eq. (14.40) into \nEq. (14.46) gives\n \nu = e0\n E 2 = 4e0\n E 2\n0 cos21vt2. \n(14.47)\nThe time-average over one cycle gives a factor of 1/2, resulting in\n \nurms = 2e0\n E 2\n0. \n(14.48)\nFor broadband radiation, the energy density in the electromagnetic ﬁeld is\n \nurms = r1v2dv, \n(14.49)\nwhere r1v2 is the ﬁeld energy per unit volume per unit angular frequency interval. Combining \nEqs. (14.48) and (14.49) gives\n \nE 2\n0 =\nr1v2\n2e0\n dv , \n(14.50)\nwhich we substitute into Eq. (14.45) to obtain the transition rate. We integrate over all the transition \nrates due to each frequency component because the blackbody light is incoherent, which gives\n \n RiSf = pe2\ne0\n U2 0 en~8 f 0 r0 i90\n2\nL\n\u0005\n0\nr(v)  d(vfi - v)  dv \n \n = pe2\ne0\n U2 r1vfi20 en~8 f 0 r0 i90\n2.\n \n \n(14.51)\nInside the black box containing the blackbody radiation and the atoms, the radiation is isotropic \nand the polarization vector is random, so we average Eq. (14.51) over all possible directions of the \npolarization vector en. To do this average, let u be the angle between en and r . The three-dimensional \nspatial average of 0 en~rn 0\n2 is\n \n H 0 en~rn 0\n2I =\n1\n4p L\n0 en~rn 0\n2d\t\n \n \n =\n1\n4p L\n2p\n0\nL\np\n0\ncos2 u sin u du df \n \n =\n1\n4p\n 2pc -  1\n3 cos3 ud\np\n0\n \n \n(14.52)\n \n = 1\n3.\n \n",
    "14.3 Electric Dipole Interaction \n457\nThus we get for the transition rate:\n \nRiSf =\npe2\n3e0\n U2 r1vfi208 f 0 r0 i90\n2. \n(14.53)\nEinstein grouped the atomic factors into the now-famous Einstein B coefﬁcient\n \nBif =\npe2\n3e0\n U2 08 f 0 r0 i90\n2 \n(14.54)\nand wrote the transition rate as\n \nRiSf = Bif r1vfi2. \n(14.55)\nThe Einstein B coefﬁcient is the same if we swap initial and ﬁnal states, as is the electromagnetic \nenergy density r1vfi2 evaluated at the transition frequency, so the rates of emission and absorption are \nthe same.\nIn the Einstein model, a collection of atoms is in thermal equilibrium with blackbody radiation \nat a temperature T. The atoms are treated as having two states 0 19 and 0 29 with energies E1 and E2, \nrespectively. The radiation induces transitions from 0 19 to 0 29—absorption, and from 0 29 to 0 19—\nstimulated emission, as depicted in Fig. 14.6. Because the absorption and stimulated emission rates are \nthe same, the populations of the two levels would be the same if there were no other processes. But the \nBoltzmann thermal distribution law tells us that the populations of levels decrease as the energy level \nincreases. Einstein argued that there must be a third process—spontaneous emission—connecting the \ntwo levels in order for thermal equilibrium to be maintained. This is called the principle of detailed \nbalance. Spontaneous emission occurs spontaneously, independent of the applied ﬁeld, and therefore \nit causes the excited state 0 29 to decay to the ground state 0 19 even when no ﬁeld is present. Because of \nthis spontaneous decay, the excited state 0 29 has a ﬁnite lifetime, in contradiction to our previous dec-\nlaration that all energy eigenstates are stationary states. The transition rate for spontaneous emission \nwas deﬁned by Einstein as A21 and is called the Einstein A coefﬁcient. The spontaneous emission rate \nis independent of the applied ﬁeld. The stimulated emission rate depends on the applied ﬁeld accord-\ning to Eq. (14.55).\nLet’s now calculate the Einstein A coefﬁcient of spontaneous emission. Assume that there are N1 \natoms in the lower state 0 19 and N2 atoms in the upper state 0 29. The transition rates we have discussed \nso far are the rates for single atoms, so the rates for a collection or ensemble of atoms are obtained by \nmultiplying the single atom rates by the population of the initial state. For example, the number of \natoms per second that absorb photons and change from state 0 19 to state 0 29 is the transition rate for \nAbsorption\nStimulated Emission\nSpontaneous Emission\n\u00022 \u0003\n\u00021 \u0003\nFIGURE 14.6 Einstein model of absorption and emission of photons.\n",
    "458 \nTime-Dependent Perturbation Theory\na single atom B12\n r1v212 times the number of atoms N1 in state 0 19: N1B12 r 1v212. This absorption \nprocess decreases the number of atoms in state 0 19, while the two emission processes increase the \nnumber. The sum of the three rates yields the rate equation for state 0 19:\n \ndN1\ndt\n= -N1B12\n r1v212 + N2B21\n r1v212 + N2\n A21. \n(14.56)\nThe rate equation for state 0 29 is similarly\n \ndN2\ndt\n= +N1B12\n r1v212 - N2B21\n r1v212 - N2\n A21. \n(14.57)\nNote that dN1>dt = -dN2>dt because there are only two levels in this model system and all atoms \nleaving one state end up in the other state.\nIn steady state, the number of atoms in either state is constant (but not equal to each other), with \nas many atoms making upward transitions as downward transitions. Hence, the change dN1>dt equals \nzero and we can solve Eq. (14.56) for the radiation energy density:\n \nr1v212 = A21\nB12\n \n1\n1N1>N221B12>B212 - 1. \n(14.58)\nThe blackbody energy density is determined by the Planck blackbody radiation formula:\n \nr1v2 =\nU\np2c3 \nv3\ne Uv\u0006kBT -1\n, \n(14.59)\nwhich comes from the Boltzmann probability of occupation of the modes of the radiation ﬁeld. In ther-\nmal equilibrium, elementary statistical mechanics tells us that the number of atoms in an energy level \nE is proportional to the Boltzmann factor exp1-E>kBT2, where kB is Boltzmann’s constant. Hence, \nthe ratio of level populations is:\n \nN1\nN2\n= e-E1\u0006kBT\ne-E2\u0006kBT = e(E2-E1)\u0006kBT = e Uv21\u0006kBT. \n(14.60)\nCombining Eqs. (14.58), (14.59), and (14.60) leads to two conditions:\n \nB21 = B12, \n(14.61)\nwhich we already knew from Eq. (14.54), and\n \nA21 =\nU v3\n21\np2c3 B21, \n(14.62)\nwhich relates the Einstein A and B coefﬁcients. Using Eq. (14.54) for the Einstein B coefﬁcient leads \nus to the spontaneous emission rate:\n \nA21 =\ne2v3\n21\n3pe0\n Uc3 0820 r0 190\n2  . \n(14.63)\nThe decay of a state caused by spontaneous emission implies that excited states are not stationary \nstates, as we have assumed all along about quantum energy eigenstates. Rather, excited states have \n",
    "14.3 Electric Dipole Interaction \n459\nan inherent ﬁnite lifetime due to spontaneous emission. If we have a system of atoms in the excited \nstate, with no electromagnetic ﬁelds present, then the rate equation for the upper level is\n \ndN2\ndt\n= -N2 A21. \n(14.64)\nSolving this differential equation yields the time dependence of the upper level population\n \nN21t2 = N2102e-A21t = N2102e-t\u0006t. \n(14.65)\nThe upper level population decays exponentially, as shown in Fig. 14.7, with a lifetime t given by the \ninverse of the Einstein A coefﬁcient\n \nt =\n1\nA21\n  . \n(14.66)\nThis inherent ﬁnite lifetime of the excited state means that there is a fundamental limit to the time we \nhave to observe the system in this state. Therefore, the energy-time uncertainty relation in Eq. (14.32) \nimplies that the ﬁnite lifetime of the excited state places a fundamental limit on how well we can mea-\nsure the energy. Hence, the energy of an excited state is uncertain or broadened. The uncertainty in our \nmeasurement of the energy difference between the ground and the excited state is\n \n\u0006E = U\n\u0006t = U\nt = U A21. \n(14.67)\nNo matter how precise our measurement apparatus is, we cannot overcome this limitation. The energy \nuncertainty in Eq. (14.67) is the spread in energy of a state that was depicted in Fig. 14.5(b), which we \naddress in the next section.\nThough we now have a way to calculate the spontaneous emission rate, we have not discovered \nthe mechanism that is responsible for the decay of excited states in the absence of a perturbing radia-\ntion ﬁeld. The approach that we have taken here to atom-light interactions is known as the semiclas-\nsical method because we have treated the atoms quantum mechanically, but we have treated the light \nas a classical ﬁeld. To properly explain spontaneous emission, we must use quantum electrodynamics \n(QED). Quantum electrodynamics treats the light quantum mechanically as a harmonic oscillator, \nwith the state 0 n9 representing a light ﬁeld with n photons. The ground state 0 09 has no photons (i.e., \nno ﬁeld excitations), but has an energy U v>2 , just as the ground state of the harmonic oscillator does. \nThis vacuum state energy represents residual energy in the electromagnetic ﬁeld, which “stimulates” \nΤ\nt\nN0\nN\ne\u0003t/Τ\nN0\ne\nFIGURE 14.7 Exponential time decay of the population of an excited atomic state.\n",
    "460 \nTime-Dependent Perturbation Theory\nthe emission of photons from excited atoms. Hence, spontaneous emission can be considered to be \nemission that is stimulated by the vacuum. Recent experiments have made this interpretation clear by \nshowing that the spontaneous emission rate can be changed by altering the vacuum, which is possible \nif you put an atom in a specially sized box, a cavity, that alters the allowed radiation modes at the fre-\nquency of interest. The quantum mechanical interaction of atoms and quantized light ﬁelds is known \nas cavity QED.\n14.3.2 \u0002 Laser Excitation\nWe now address the problem of a monochromatic laser exciting an atom with an upper level that is \nnot sharply deﬁned, as depicted in Fig. 14.5(b). We assume that the spread in energy of the upper \natomic level is caused by spontaneous emission, though collisions or other environmental factors are \nalso possible causes. Fermi’s golden rule in Eq. (14.38) tells us that the transition rate depends on the \ndensity of energy states. We found the spread of the ﬁnal energy state in Eq. (14.67) for the case of \nspontaneous emission. The functional form of the density of energy states g1E2, or equivalently the \nfrequency density, is determined by the Fourier transform of the emitted electromagnetic ﬁeld. For the \nexponential time dependence of a spontaneously decaying upper state, this Fourier transform yields \na Lorentzian function (Problem 14.5). For a transition between states 0 19 and 0 29 with a spontaneous \ndecay rate A21 from state 0 29, the Lorentzian density of states for the upper state is\n \ng1E2 =\nU A21\u00062p\n1E - U v212\n2 + aU A21\n2 b\n2 . \n(14.68)\nThe density of states is normalized to unity, 1\n\u0005\n0 g1E2dE = 1, because there is only one state at the \nupper level; it is just spread out by its ﬁnite lifetime. Substituting the density of states into Fermi’s \ngolden rule in Eq. (14.38) yields the transition rate\n \nR1S2 = 2p\nU\n 0 V210\n2 g1Ef2 \n \n=\n2pe2E 2\n0\nU\n 0 en~820 r0 190\n2\n \nU A21\u00062p\n1E - U v212\n2 + a\nU A21\n2 b\n2 . \n \n(14.69)\nUsing the frequency Lorentzian 1 f 1v2dv = g1E2dE2\n \nf 1v2 =\nA21\u00062p\n1v - v212\n2 + aA21\n2 b\n2 , \n(14.70)\nwe express the transition rate as\n \nR1S2 =\n2pe2E 2\n0\nU2\n 0 en~820 r0 19 0 2 f 1v2 \n(14.71)\n",
    "14.3 Electric Dipole Interaction \n461\nIn terms of the Einstein B coefﬁcient, the transition rate is\n \nR1S2 = 6e0 E 2\n0\n B21 f 1v2. \n(14.72)\nIf we excite the transition with a monochromatic laser with intensity I = 2ce0E 2\n0 , the transition rate is\n \nR1S2 = 3 I\nc\n B12 f 1v2. \n(14.73)\nThis excitation probability rate has the Lorentzian frequency dependence shown in Fig. 14.8, with a \nfull width at half maximum (FWHM) of A21 . Once again, we see the resonance behavior of the inter-\naction, such that the laser must be tuned within this frequency window in order to have appreciable \nprobability of inducing excitation of the atom.\nAnother useful way to quantify the excitation of an atom by a laser is with a quantity known as \nthe cross section. To understand why an area is useful in this regard, consider characterizing the efﬁ-\nciency of the interaction as the ratio of what you get out to what you put in:\n \nefficiency = output\ninput . \n(14.74)\nIn this case, you put in light and get out excited atoms.\nWe usually characterize the input laser light in terms of the intensity I, measured in Watts per \nsquare meter. However, to simplify matters, let’s quantify the light in terms of the number of photons \nper unit area per unit time. Each photon in the laser beam has an energy U v , so the number of photons \nper unit area per unit time is the intensity divided by the energy per photon:\n \n# photons\narea ~ time =\nI\nU v. \n(14.75)\nWe quantify the output of excited atoms by the transition rate R, which is a probability (i.e., num-\nber) per unit time. Thus the efﬁciency we have deﬁned becomes:\n \n efficiency = output\ninput\n \n \n = R1S2\nI\nU v\n=\n# per unit time\n# per unit time per unit area . \n \n(14.76)\nΩ21\nΩ\nR1\u00022(Ω)\nA21\nFIGURE 14.8 Lorentzian frequency dependence of the excitation probability.\n",
    "462 \nTime-Dependent Perturbation Theory\nBy dimensional analysis, the efﬁciency we have deﬁned is really an effective area for excitation. This \neffective area is what we call the cross section \u0016. To calculate the cross section, we assume that the \nlaser is on resonance (v = v21) in order to provide the maximum efﬁciency:\n \n s = R1S2\nI>U v =\n31I>c2B12 f 1v212\nI>U v21\n \n \n =\n31I>c2B1212>pA212\nI>U v21\n \n \n(14.77)\n \n = 6 U v21\npc  B12\nA21\n. \nFrom Eq. (14.62) relating the Einstein rate coefﬁcients, we know the ratio of B12 to A21. This allows us \nto calculate the cross section for on-resonance excitation:\n \n s = 6 U v21\npc  B12\nA21\n= 6 U v21\npc  p2c3\nU v3\n21\n \n = 6p c2\nv2\n21\n= 6p 1\nk2\n21\n= 6p \n1\n12p>l212\n2 , \n(14.78)\nresulting in\n \ns = 3 \nl2\n21\n2p  . \n(14.79)\nThis result is amazingly simple. It says that the atom is effectively the size of the wavelength of light \n(\u0007100 - 1000 nm) when considering its interaction with resonant light! The physical size of the \natom (the Bohr radius \u00070.1 nm) is irrelevant in this case, although it would be more appropriate if \nwe were considering collisions between two atoms. For atom-light interactions, the atom acts as an \nefﬁcient antenna, despite its small size.\n14.4 \u0002 SELECTION RULES\nThe Einstein A and B coefﬁcients depend upon the matrix element 8 f 0 en~r0 i9 from the electric dipole \ninteraction between the two states. If this matrix element is zero for some reason, then there is no \nprobability that the transition between the states will occur. There are some general guidelines as to \nwhen such matrix elements are expected to be zero, and we call these selection rules. Transitions \nfor which the matrix element is zero are therefore not allowed and are called forbidden transitions. \nHowever, recall that we are working within the electric dipole approximation, which means that we \nhave neglected magnetic dipole, electric quadrupole, and higher multipole interactions. It may happen \nthat a transition is forbidden within the electric dipole approximation but is allowed by a higher-order \nmultipole interaction. The higher-order interactions typically have transition rates that are reduced by \nan extra order of the ﬁne structure constant \u0017.\n",
    "14.4 Selection Rules \n463\nThe selection rules derive from general properties of the electric dipole matrix elements, not from \nthe details of a speciﬁc atom or molecule. To see this, ﬁrst separate the radial and angular parts of the \nmatrix element:\n \n 8 f 0 en~r0 i9 = en~8 f 0 r rn 0 i9\n \n \n =\nL\n\u0005\n0\nr 2 dr\nL\nd\tR*\nnf/f 1r2 Y mf*\n/f 1u,f2 en~rn r Rni/i 1r2 Y mi\n/i \n 1u, f2 \n(14.80)\n \n = a\nL\n\u0005\n0\nR*\nnf /f 1r2 Rni /i 1r2r 3 drba\nL\nY mf*\n/f 1u,f2 en~rn Y mi\n/i  1u, f2  d\tb. \nThe radial integral does depend critically on the details of a speciﬁc atom or molecule and is not typi-\ncally zero. The angular integral, however, depends on the spherical harmonics, which are independent \nof the details of the central potential (see Section 7.4). The dot product term in Eq. (14.80) is expressed \nin terms of the angles \u0007 and \u0018 between the electric ﬁeld polarization vector en and the electron position \nunit vector rn\n \nen~rn = ex sin u cos f + ey sin u sin f + ez cos u. \n(14.81)\nIt is useful to express the trigonometric functions in Eq. (14.81) in terms of the spherical harmonics:\n \nen~rn = A\n4p\n3\n aezY 0\n1 1u, f2 +\n-ex + iey\n12\n Y 1\n1 1u, f2 +\nex + iey\n12\n Y \n -1\n1  1u, f2b. \n(14.82)\nThus, the dot product en~rn is proportional to spherical harmonics of order 1. This key point derives \nfrom making the electric dipole approximation. Higher-order multipole matrix elements involve \nhigher-order spherical harmonics and hence yield different selection rules.\nUsing Eq. (14.82), we ﬁnd that the angular integral of the electric dipole matrix element in \nEq. (14.80) becomes three integrals, each of which is an integral of the product of three spherical \n harmonics:\n \nL\nY mf*\n/f 1u, f2Y m\n1  1u, f2Y mi\n/i 1u, f2d\t, \n(14.83)\nwhere one spherical harmonic is limited to order 1 by the electric dipole approximation and the index \nm varies over 1, 0, –1 according to the three terms in Eq. (14.82). You would expect that such an \nintegral over three spherical harmonic functions would be difﬁcult to do. However, we now make use \nof the Clebsch-Gordan coefﬁcients from Chapter 11 on the addition of angular momenta. We found \nthere that a coupled angular momentum state can be expressed in terms of uncoupled states using \nthe Clebsch-Gordan coefﬁcients. For orbital angular momentum this means that one spherical har-\nmonic can be decomposed into products of pairs of other spherical harmonics. Given this knowledge \nof Clebsch-Gordan coefﬁcients, we ﬁnd the angular integral:\n \nL\nY mf*\n/f 1u, f2Y m\n1  1u, f2Y mi\n/i  1u, f2d\t = c 312/i + 12\n4p12/f + 12 d\n1\n2\n8/i1mi m0 /f mf98/i1000 /f 09. (14.84)\nThe Clebsch-Gordan coefﬁcient 8/i1mi m0 /f mf9 is the key to understanding the selection rules. \nWe know from Chapter 11 that only certain values of the coupled angular momentum quantum \n",
    "464 \nTime-Dependent Perturbation Theory\nnumbers are allowed for a given set of uncoupled angular momentum quantum numbers, and that \nmany entries in the tables of the Clebsch-Gordan coefﬁcients are zero. The Clebsch-Gordan coefﬁcient \n8/i1mi m0 /f mf9 characterizes the addition of the uncoupled angular momenta j1 = /i and j2 = 1 to \nform the coupled angular momentum j = /f . The rules of adding angular momenta limit the values of \nthe coupled angular momentum /f to:\n \n/f = /i + 1, /i, /i - 1. \n(14.85)\nThe magnetic quantum numbers are also limited by the Clebsch-Gordan coefﬁcient 8/i1mi m0 /f mf9 to:\n \n mi + m = mf  \n \n m = mf - mi\n . \n(14.86)\nA further restriction on the matrix elements comes from a consideration of parity. The integrand \nin Eq. (14.84) must be even with respect to spatial symmetry inversion for the integral to be nonzero. \nThe spherical harmonics have parity given by 1-12/, that is\n \nY m\nl 1u, f2 = 1-12/\n Y m\nl 1p - u, f + p2. \n(14.87)\nFor the three cases allowed by Eq. (14.85), the integrand with /f = /i has odd parity (2 /i + 1), while \nthe integrand with /f = /i { 1 has even parity (2 /i or 2 /i + 2) . We conclude that the angular inte-\ngral with /f = /i is identically zero, and ﬁnd another rule:\n \n/f \u0002 /i\n . \n(14.88)\nWe say that /f = /i is not allowed by parity. Combining the rules in Eqs. (14.85), (14.86), and (14.88), \nwe ﬁnd the selection rules for electric dipole transitions:\n \n \u0006/ = {1\n \n \n \u0006m = 0,{1  . \n \n(14.89)\nThese selection rules arise solely from the angular integral in Eq. (14.80) and they reﬂect the conserva-\ntion of angular momentum of the system of atom and photon. The photon has an angular momentum \nor spin of 1. When the atom absorbs or emits a photon, the ﬁnal atomic state must reﬂect the change in \nangular momentum of the electromagnetic ﬁeld.\nApplication of the \u0006/ = {1 selection rule to the hydrogen atom limits the possible spontaneous \nemission transitions to those shown in Fig. 14.9. For example, within the n = 2 level, the 2p state can \ndecay to the 1s state, and does so with a 1.6 ns lifetime (Problem 14.7). However, the 2s state cannot \ndecay to the ground state because \u0006/ = 0 is not allowed. The Lamb shift does displace the 2s1>2 state \nslightly above the 2p1>2 state to which it can decay, but the transition rate is very small due to the cube \nof the Bohr frequency in Eq. (14.63). Hence, the 2s1>2 state has a very long decay lifetime of 1/7 sec, \nwhich is caused by a two-photon decay mechanism to the ground state.\nApplication of the \u0006m = 0,{1 selection rule to the hydrogen 2p S 1s transition yields the \nallowed transitions shown in Fig. 14.10, for the case of emission. In the \u0006m = +1 transition, the \natom gains one unit of angular momentum projection along the z-axis, so the emitted photon must \nhave one unit of angular momentum projection in the negative z-direction. Such a photon is called \n",
    "14.4 Selection Rules \n465\nmf \u0005\u00060\nmi \u0005\u0006\u00031\nmi \u0005\u00060\nmi \u0005\u00061\n1s\n2p\nΣ\u0003\n\bm \u0005\u0006\t1\n\bm \u0005\u0006\u00031\nΠ\nΣ\t\n\bm \u0005\u00060\nFIGURE 14.10 Hydrogen 2p S 1s transition.\n1s\n2s\n2p\n3s\n3p\n3d\n4s\n4p\n4d\n4f\n\u000b\nEnergy (eV)\n0\n\u00031\n\u00032\n\u00033\n\u00034\n\u00035\n\u00036\n\u00037\n\u00038\n\u00039\n\u000310\n\u000311\n\u000312\n\u000313\n\u000314\nFIGURE 14.9 Decay scheme of hydrogen for allowed electric dipole transitions.\na s- polarized photon or a photon with negative helicity. The \u0006m = -1 transition produces a \ns+ photon with positive helicity. The s+ and s- photons have a polarization vector that rotates \naround the z-axis and are also called circularly polarized states. The \u0006m = 0 transition produces a \nphoton that has linear polarization along the z-axis, which is referred to as a p polarized photon. For \nthe case of absorption, the \u0006m values in Fig. 14.10 change sign, but the s+ and s- labels remain \nunchanged (Problem 14.13).\n",
    "466 \nTime-Dependent Perturbation Theory\nSUMMARY \nIn time-dependent perturbation theory, we focus on ﬁnding the probability that an applied perturba-\ntion causes a transition between energy levels of the unperturbed Hamiltonian. In contrast, in time-\nindependent perturbation theory, we focus on ﬁnding the changes in energy levels caused by the \nperturbing Hamiltonian (assumed static).\nThe probability amplitude for a transition from the initial state 0 i9 to the ﬁnal state 0  f 9 subject to \nthe time-dependent perturbation H\u00041t2 is\n \ncf 1t2 = 1\niU L\nt\n0\n8 f  0 H\u00041t\u00042 0 i9ei(Ef-Ei)t\u0004\u0006U dt\u0004. \n(14.90)\nThe probability of the transition is\n \nPiSf 1t2 = 1\nU2 `\nL\nt\n0\n8 f  0 H\u00041t\u00042 0 i9ei(Ef-Ei)t\u0004\u0006U dt\u0004 `\n2\n. \n(14.91)\nFor harmonic perturbation at frequency v , the transition probability for long times grows linearly \nwith time and we deﬁne the transition rate\n \nRiSf = 2p\nU2  @ Vfi@\n2\n d1vfi - v2. \n(14.92)\nFor an electric dipole interaction, the transition rate is\n \nRiSf = Bif r1vfi2 \n(14.93)\nif the excitation source is broadband where r1vfi2 is the energy density and Bif is the Einstein B coefﬁcient\n \nBif =\npe2\n3e0\n U2 08 f 0 r0 i90\n2. \n(14.94)\nIf the transition is excited with a monochromatic source with intensity I = 2ce0E 2\n0, the transition rate is\n \nR1S2 = 3 I\nc\n B12 f 1v2, \n(14.95)\nwhere f 1v2 is the frequency response function of the atom.\nAn excited state in an atom has a ﬁnite lifetime due to spontaneous emission. The lifetime is the \ninverse of the Einstein A coefﬁcient\n \nA21 =\ne2v3\n21\n3pe0\n Uc3 0820 r0 190\n2. \n(14.96)\nElectric dipole transitions have the selection rules\n \u0006/ = {1\n \n \u0006m = 0,{1. \n \n(14.97)\n",
    "Problems \n467\nPROBLEMS \n 14.1 Use the perturbation series expansion of the coefﬁcient cn given by Eq. (14.11) in the dif-\nferential equation (14.10) and verify the zeroth-order and ﬁrst-order equations (14.12) and \n(14.13). You may wish to use the l notation of Chapter 10 to keep track of orders.\n 14.2 A particle of mass m is initially in the ground state (E1) of an inﬁnite square well of width L. \nStarting at time t \u0003 0, the system is subject to the perturbation\nH\u00041t2 = V0 x2e-t\u0006t,\n \n where V0 and t are constants. Find the probability that the energy after time T is measured to \nbe E2. Calculate the probability in the limit T S \u0005.\n 14.3 A particle of mass m is initially in the ground state (E1) of an inﬁnite square well of width L. \nStarting at time t = 0, the system is subject to the perturbation\nH\u00041t2 = V0 xe-at\n 2,\n \n where V0 and a are constants.\na) Find the probability that the energy is measured to be E2 in the limit t S \u0005.\nb) Find the probability that the energy is measured to be E3 in the limit t S \u0005.\n 14.4 A particle of mass m is initially in the ground state (E1) of an inﬁnite square well of width L. \nFrom t = 0 to t = T, the potential is perturbed so that it becomes\nV1x2 = •\nV0,\n0 ,\n\u0005,   \n0 6 x 6 L\u00062\nL\u00062 6 x 6 L\nelsewhere,\n \n where V0 << E1. Find the probability that the energy after time T is measured to be E2 .\n 14.5 Spontaneous emission causes the population of an atom to decay with the form e -t>t . The \nradiated electromagnetic power exhibits this same time dependence, but the ﬁeld has the form \ne -t>2t because the power is proportional to the ﬁeld squared. Calculate the Fourier transform \nof the emitted ﬁeld and take its complex square to ﬁnd the frequency spectrum of the radiated \npower in spontaneous emission. Convert this frequency spectrum to an energy spectrum and \nnormalize it to unity to verify the energy density of states in Eq. (14.68).\n 14.6 A hydrogen atom in its ground state is subject to an applied electric ﬁeld\nE = E 01xn + yn + zn2e-t\u0006t.\n \n Find the probabilities that after a long time the atom is found be in each of the four n = 2 \nstates.\n 14.7 Calculate the lifetime (in seconds) of each of the four n = 2  states of hydrogen 10 n/m92. The \nlifetime is the inverse of the spontaneous emission rate (Einstein A coefﬁcient).\n 14.8 A particle in a square well potential (with walls at  x = 0 and x = L;  that is, V1x2 = 0 for \n0 6 x 6 L; V 1x2 = \u0005 otherwise) starts out in the ground state\n0 c1t = 029 = 0 19,\n",
    "468 \nTime-Dependent Perturbation Theory\n \n where 0 n9 are the normalized eigenstates of the unperturbed Hamiltonian. Starting at t = 0, a \ntime-dependent perturbation is applied given by\nH =1x, t2 = V0 sin px\nL\n e-gt.\na) Calculate the probability for the particle to make a transition to an excited state 0 n91n \u0002 12 \nafter a long time. Deﬁne “long time.”\nb) Are there any selection rules for this transition? If so, what are they?\n 14.9 A particle in the harmonic oscillator potential V1x2 = 1\n2 mv2x2 starts out in the ground state\n0 c1t = 029 = 0 09,\n \n where 0 n9 are the normalized eigenstates of the Hamiltonian. Starting at t = 0, a time-\ndependent perturbation is applied given by\nH =1x, t2 = Ax3e -gt.\na) Calculate the probability for the particle to make a transition to an excited state \n0 n91n \u0002 02 after a long time. Deﬁne “long time.”\nb) Are there any selection rules for this transition? If so, what are they?\n 14.10 Consider two possible types of electric dipole transitions: a p S s transition and a p S d tran-\nsition. In each case, choose one allowed set of m quantum numbers and explicitly perform the \nangular integral in Eq. (14.84), then use the Clebsch-Gordan Table 11.5 to conﬁrm your result.\n 14.11 Use the result in Eq. (14.84) and the Clebsch-Gordan Table 11.5 to identify each possible \nelectric dipole transition from an initial p state. Identify the particular Clebsch-Gordan coef-\nﬁcient in Table 11.5 that represents the parity rule /f \u0002 /i.\n 14.12 A hydrogen atom starts in the state n = 4, l = 3, m/ = 3 , where we ignore the spin. What \npossible states will the atom go through as it decays to the ground state? What are the polar-\nizations of the photons that are emitted?\n 14.13 Draw a transition diagram like Fig. 14.10, but for absorption from 1s S 2p rather than emis-\nsion. Explain why the \u0006m values change sign but the s+ and s- labels remain unchanged.\n 14.14 A particle of mass m and charge q is conﬁned in a one-dimensional harmonic oscillator poten-\ntial of natural frequency v.\na) What are the selection rules governing spontaneous emission from excited states?\nb) Which states can decay directly to the ground state?\nc) Find the spontaneous emission rate from the ﬁrst excited state to the ground state.\nd) Calculate the lifetime of the ﬁrst excited state for an electron bound in a potential with \nv = 1015 rad>s.\nRESOURCES \nFurther Reading \nMore details on atom-light interactions can be found in:\nA. Corney, Atomic and Laser Spectroscopy, Oxford: Clarendon Press, 1977.\nC. J. Foot, Atomic Physics, Oxford: Oxford University Press, 2005.\nM. Fox, Quantum Optics, Oxford: Oxford University Press, 2006.\nR. Loudon, Quantum Theory of Light, Oxford: Oxford University Press, 2000.\n",
    " \n469\nC H A P T E R \n15\nPeriodic Systems\nIn this chapter, we explore the energy eigenvalues and eigenstates of a periodic series of potential \nenergy wells, as shown in Fig. 15.1, with the purpose of creating a rudimentary model of a solid. In \nthis model, a single well represents an atom, and the chain of wells represents a molecule or a solid. In \nChapter 5 we explored the energy eigenstates of the ﬁnite square well potential, which is a single ele-\nment of the periodic series shown in Fig. 15.1(a). We found that there were a ﬁnite number of bound \neigenstates and a continuum of unbound eigenstates, and that the shape of the well changed only the \ndetails of the shape of the wave functions and shifted the eigenvalues slightly. The hydrogen atom, \nwhich we studied in Chapter 8, is another example, schematically depicted as an element of the series \nin Fig. 15.1(b). It also has bound states (but an inﬁnite number) and continuum states. It is a three-\ndimensional problem, rather than the one-dimensional problem we will consider here. The similarities \noutweigh the differences, and the basic features of the band structure of a solid appear when we string \nseveral such “one-dimensional atoms” together to model a solid.\nOur model uses an approximate approach that emphasizes the interaction between neighboring \natoms. We will ﬁnd out how the eigenstates of the periodic potential (or molecule or solid) can be con-\nstructed from the eigenstates of the single elements of the periodic potential (or atoms). We will also \nlearn that the eigenstates of a solid are characterized by a wavelength, and that the energies of those \neigenstates form bands centered near the atomic energy eigenvalues. The approximation presented \nhere is a powerful method that is widely used in solid state physics and chemistry, where it goes under \nthe name of tight-binding or LCAO (Linear Combination of Atomic Orbitals). The LCAO approach \nis intuitive, starting from the easily understood atomic orbitals, and building molecular orbitals by \nconsidering how the atoms interact. We introduced the LCAO method in Chapter 13 to model the H +\n2 \nmolecular ion.\nIt is also possible to ﬁnd the energy eigenvalues and eigenstates by directly solving the energy \neigenvalue equation, and we will discuss this approach at the end of the chapter. The problem \npresented here is a single-particle problem whose solution is the possible states of a single electron \nsubject to the periodic potential, so we do not concern ourselves with the identical-particle aspects \nof the problem discussed in Chapter 13. Despite this gross oversimpliﬁcation, the results are surpris-\ningly robust if we simply assume that subsequent electrons would occupy these same states, subject to \nthe Pauli exclusion principle. This independent electron approximation is sufﬁcient to explain the \npresence of energy gaps in the energy-level structure of real solids, and to provide a basis for \nunderstanding the concept of the density of levels and the rudiments of electron transport.\nOur goal is to gain a basic understanding of an energy band diagram and density of states \nplot of a solid, such as depicted in Fig. 15.2 for the semiconductor Si. The plot on the left is simply \nan energy spectrum—a plot of the allowed energies of an electron. The difference between this \nplot and the energy spectrum for atomic hydrogen, say, is that the horizontal axis represents a new \n",
    "470 \nPeriodic Systems\nmomentum variable associated with the eigenstate that arises because of the periodic nature of the \npotential. The graph on the right of Fig. 15.2 is a density of states plot. It represents the relative \nnumber of allowed states (per unit energy) at each energy, regardless of the momentum variable. \nSuch plots help us determine whether materials are metallic or insulating, and tell us something \nabout the optical and electronic transport properties of the solid. We begin by tackling a one-\ndimensional periodic potential, which will lead us to a simple version of one panel of the band \nstructure plot in Fig. 15.2.\n(a)\n(b)\n2a\n1a\n3a\n4a\n5a\nx\nV(x)\nV(x)\nE\n1a\n2a\n3a\n4a\n5a\nx\nE\nFIGURE 15.1 Chain of periodic wells: (a) square wells, (b) Coulomb wells.\ng(E)\nSilicon Band Structure and Density of States\nW\nL\n\f\nX W K\n\u000312\n\u000310\n\u00038\n\u00036\n\u00034\n\u00032\n0\n2\n4\n6\n8\nEnergy (eV)\nFIGURE 15.2 Band structure and density of states of Si.\n",
    "15.1 The Energy Eigenvalues and Eigenstates of a Periodic Chain of Wells \n471\n15.1 \u0002   THE ENERGY EIGENVALUES AND EIGENSTATES OF A PERIODIC \nCHAIN OF WELLS\nOur goal in this section is to use the LCAO method to ﬁnd approximate solutions to the energy \neigenvalue equation for a chain of periodic wells. We’ll study the one-dimensional chain of square \nwells shown in Fig. 15.1(a) as our model system. We’ll solve this problem exactly later in the chapter, \nbut the approximate LCAO method is sufﬁcient to illustrate most of the important features of a peri-\nodic system and is also more revealing. To get started, we’ll study a chain with two square wells, and \nthen we’ll solve the N-well problem.\nIn the LCAO method, we regard each individual well as an “atom.” We assume that we have \nalready solved the energy eigenvalue equation for one isolated well and so we know the energy eigen-\nvalues and the eigenstates, which we refer to as the “atomic” energies and states. For example, in \nChapter 5, we solved the energy eigenvalue equation of the ﬁnite square well. The eigenstates for two \ndifferent square wells are shown in Fig. 15.3. In our discussions, we won’t need more than the lowest \ntwo states in the well, so we’ll label them as ground (g) and excited (e) states to simplify the notation. \nWe will use kets 0 g9 and 0 e9 or wave functions w g1x2 = 8x0 g9 or we1x2 = 8x0 e9 as appropriate.\n15.1.1 \u0002   A Two-Well Chain\nThe simplest system with more than one well is the “chain” of two wells, depicted in Fig. 15.4. We’ll \nmake the problem even simpler and assume that each individual well has just one possible bound state, \nas in Fig. 15.3(a). Our goal is to solve the energy eigenvalue equation\n \nH0 c9 =\n E0 c9, \n(15.1)\nwhere the Hamiltonian H includes the usual kinetic energy of the single electron and the potential \nenergy depicted in Fig. 15.4. The two wells (atoms) are separated by a distance a (the interatomic \nspacing). The ket 0 c9 represents an eigenstate of the two-well Hamiltonian, and E is the corresponding \nenergy. We refer to 0 c9 as a “molecular” eigenstate.\nThe central idea of the LCAO or “interacting atoms” approach is to represent the system state \nvector 0 c9 in the basis of the “atomic” states that are the solutions to the energy eigenvalue problem \nfor a single isolated well. In the simpliﬁed case that we are considering, the only two atomic states in \n(a)\n(b)\n0\n\u0003b/2\nb/2\n0\n\u0003b/2\nb/2\nx\nx\nEg\nEe\nEg\n\rg(x)=\u0006\u0004x\u0002g\u0003\n\re(x)=\u0006\u0004x\u0002e\u0003\n\rg(x)=\u0006\u0004x\u0002g\u0003\nFIGURE 15.3 Finite square well and bound energy eigenstates for cases with (a) one energy \nlevel, and (b) two energy levels. These eigenstates are the basis for the eigenstates of the full \nperiodic Hamilitonian.\n",
    "472 \nPeriodic Systems\nthe system are the ground states of the two wells, centered on wells 1 and 2. We could label them as \n01, g9 and 0 2, g9, but we immediately simplify the notation to 019 and 0 29 because there is only one state \nper well. The wave functions representing these states are identical [as in Fig. 15.3(a)] except that they \nare displaced by a from each other:\n \n 019 \u0003 w g 1x - 1a2  \n \n 029 \u0003 w g 1x - 2a2. \n \n(15.2)\nThe LCAO method assumes that the molecular state is a linear combination of the known atomic \nstates:\n \n0 c9 =\n  c10 19 + c20 29. \n(15.3)\nThe beauty of the LCAO method is that we use the already known atomic wave functions as the \npreferred basis, so we solve the energy eigenvalue equation with the matrix approach rather than \nthe differential equation approach used to ﬁnd the atomic states. This is clearly an approximation \nbecause we expect the spatial wave functions to be altered by the new potential conﬁguration, but the \nresults are quite good in many cases.\nFor the two-atom chain, there are only two atomic states, so the matrix representing the Hamiltonian \nof the system is a 2*2 matrix. This matrix has the form\n \nH \u0003 aa\nb\nb\nab . \n(15.4)\nThe matrix elements of the Hamiltonian are\n \na =\n H11 =\n H22 = 810 H019 = 820 H0 29  \n \nb =\n H12 =\n H21 = 810 H0 29 = 820 H0 19. \n \n(15.5)\nThe two diagonal terms are equal and the two off-diagonal terms are equal because of the symmetry \nof the two-well chain. The parameters a and b are straightforward to calculate given the atomic states \nand will depend on the well depth and the spacing. We can proceed with this problem without actu-\nally calculating a and b—the important features of the band structure will be perfectly clear without \nknowing their values. However, as a physicist, you ought to be very interested in knowing how to \ncalculate them and in knowing what they mean. We’ll pursue these calculations in Section 15.8 and \nas homework problems. It turns out that the diagonal matrix elements a are approximately equal to \nthe energy of the atomic state. The off-diagonal matrix elements b are related to the probability for an \nelectron to move between the wells, and so are referred to as “hopping” matrix elements.\nWe have now reduced the two-well problem to a two-dimensional Hilbert space comprising \nthe ground atomic states, and we proceed to ﬁnd the molecular eigenstates and eigenenergies by \nx\na\n1\n2\nFIGURE 15.4 Two square wells with separation a.\n",
    "15.1 The Energy Eigenvalues and Eigenstates of a Periodic Chain of Wells \n473\ndiagonalizing the Hamiltonian in Eq. (15.4). You have already diagonalized many 2*2 matrices, so \nwe’ll skip the details. The energy eigenvalues are\n \nE+ = a + b\nE- = a - b\n \n(15.6)\nand the energy eigenstates are\n \n0 c+9 =\n1\n12\n 019 +\n1\n12\n 0 29 for E+ = a + b  \n \n0 c-9 =\n1\n12\n 019 -\n1\n12\n 0 29 for E- = a - b . \n \n(15.7)\nThere are two molecular states, one a symmetric and equal superposition of the two atomic states, \nand the other an equal and antisymmetric combination. The energies of the two molecular states are \ndisplaced from the energy a, by an energy b, and the sign of b determines which state has the higher \nenergy. For the two square-well chain, b 6 0 and the lower-energy state (called the bonding orbital) \nis the symmetric combination, and the higher-energy state (called the antibonding orbital) is antisym-\nmetric. Figure 15.5 depicts the level scheme and the wave functions for this two-well system. This \nsolution is reminiscent of degenerate perturbation theory in that the coupling b between the two states \nlifts the degeneracy of the atomic states. The resultant molecular states are similar to the hydrogen \nmolecular ion states we found in Eq. (13.87).\n15.1.2 \u0002 N-Well Chain\nNow consider a system with N one-dimensional wells as depicted in Fig. 15.1. As N increases beyond 2,\nthe molecule contains more and more atoms, and eventually there will be enough to think of it as a \nsolid. In this language, a solid is just a giant molecule, and we’ll continue to refer to the eigenstate of \nthe periodic system as the “molecular state” (as distinct from the “atomic state” of the isolated well). \nLet’s continue to assume that each isolated well has only one bound atomic state, so for the N-atom \nchain, there are N atomic states, which we label with their location as 0 n9. The molecular state is the \nlinear combination of atomic states\n \n0 c9 = a\nN\nn=1\ncn0 n9, \n(15.8)\nΑ\nΑ\u0003Β\nΑ\tΒ\n1a\n2a\nx\n1a\n2a\nx\nFIGURE 15.5 Two atomic states combine to form two molecular states. The bonding \nstate is symmetric and the antibonding state is antisymmetric if b 6 0.\n",
    "474 \nPeriodic Systems\nand the matrix representing the Hamiltonian of the system is an N*N matrix. We now make one addi-\ntional assumption. We assume that the “hopping” matrix elements b are zero unless the two wells are \nadjacent. This nearest-neighbor approximation is easily relaxed, but doing so gives little new physi-\ncal insight and increases the algebraic complexity. With this new assumption, the matrix representing \nthe Hamiltonian is an extension of the two-well Hamiltonian [Eq. (15.4)] with the b terms adjacent to \nthe main diagonal:\n \nH \u0003 ¶\na\nb\n0\n0\ng\nb\na\nb\n0\ng\n0\nb\na\nb\ng\n0\n0\nb\na\ng\nf\nf\nf\nf\nf\n∂ . \n(15.9)\nThe nonzero matrix elements are\n \n a = Hnn\n = 8n0 H0 n9\n \n b = Hn,n{1 = 8n0 H0 n{19. \n(15.10)\nFor small values of N, the Hamiltonian in Eq. (15.9) can be diagonalized either analytically or \nusing a computer to ﬁnd the energy eigenvalues and eigenstates just as we did for the N = 2 case in \nthe last section. We’ll leave that approach to the homework problems. For large N, we use a different \nsolution technique that gets at the heart of the problem. Using the matrix in Eq. (15.9), we express the \nenergy eigenvalue equation H0 c9 =\n E0 c9 as\n \n¶\na\nb\n0\n0\ng\nb\na\nb\n0\ng\n0\nb\na\nb\ng\n0\n0\nb\na\ng\nf\nf\nf\nf\nf\n∂ ¶\nc1\nc2\nc3\nc4\nf\n∂= E ¶\nc1\nc2\nc3\nc4\nf\n∂ . \n(15.11)\nThis leads to the equations\n \nac1 +  bc2 = Ec1 \nbc1 +  ac2 +  bc3 = Ec2 \nbc2 +  ac3 +  bc4 = Ec3 \nbc3 +  ac4 +  bc5 = Ec4 .\n \n \n(15.12)\n \nf \nThe ﬁrst equation and the last equation (not shown) are different, but all the other equations have the \nidentical form\n \nbcp-1 + acp + bcp+1 = Ecp . \n(15.13)\nFor now, we focus on solving this equation and ignore the different endpoint equations that we’ll \ncome back to in the next section.\n",
    "15.1 The Energy Eigenvalues and Eigenstates of a Periodic Chain of Wells \n475\nThe mathematical form of Eq. (15.13) is identical to the equation of motion of a collection of \nmechanical oscillators, each coupled to its two nearest neighbors, as in a beaded string. We use the \ntechnique of normal mode solutions to solve Eq. (15.13) for the coefﬁcients cp and the energy E. The \nnormal-mode approach assumes wavelike solutions of the form\n \ncp = Aeipka. \n(15.14)\nIn Eq. (15.14), p is an integer from 1 to N that labels the atomic state, and each molecular eigenstate \ncorresponds to a different set of N coefﬁcients 1c1, c2 , ... cN2. The parameter a is the separation of the \nﬁnite wells as shown in Fig. 15.1. The values of k and A are unknown for the moment; we have yet to \ndetermine them.\nSubstitute Eq. (15.14) into Eq. (15.13) to get\n \nbAei( p -1)ka + 1a - E2  Aeipka + bAei( p+1)ka = 0 , \n(15.15)\nand factor out eipka to obtain\n \nbe-ika + 1a - E2 + be+ika = 0 . \n(15.16)\nNow solve for the eigenstate energy E, making use of the Euler relation, to ﬁnd the dispersion relation\n \nE = a + 2b cos1ka2. \n(15.17)\nThe dispersion relation is plotted in Fig. 15.6 for k a continuous variable, which is the case when there \nare very many atoms, as we will discuss later.\nNotice that we assumed a form for the coefﬁcients cp in Eq. (15.14) and ended up solving for the\nenergy! Before we return to the coefﬁcients (which amounts to pinning down the values of A and k),\nwe will take some time to discuss the dispersion relation, which contains a great deal of informa-\ntion. First, notice that the energy eigenvalue of the molecular state is determined by k, so k labels \nthe molecular eigenstate. We’ll ﬁnd out in Section 15.2 what values k may have. The energy E is \nperiodic in k with period 2p>a, so clearly there is some redundancy in the information. Second, \nthe values of E are bounded above and below, as indicated by the limits a - 2b and a + 2b in\nFig. 15.6. The fact that there is a band of allowed energies is one of the most important char-\nacteristics of a solid that is replicated by our model. The progression from one atomic energy to \ntwo molecular energies to a band of energies for the one-well, two-well and N S \u0005 well cases, \n\u0003 3Π\na\n0\nk\nΑ\nΑ\u00032Β\nΑ\t2Β\nE(k)\n3Π\na\n\u0003 2Π\na\n2Π\na\n\u0003 Π\na\nΠ\na\nFIGURE 15.6 Energy eigenvalues as a function of wave vector, k, for an \ninﬁnite chain of wells, with b 6 0.\n",
    "476 \nPeriodic Systems\nrespectively, is shown in Fig. 15.7. Third, the band width is 4b, which indicates that the stronger \nthe interaction between neighboring states, the wider the resulting band. Recall that b is the matrix \nelement of the Hamiltonian evaluated between neighboring states and is therefore an indication of the \ninteraction strength. In real solids with the same crystal structure, those with smaller lattice parameters \nhave wider bands because atomic wave functions can overlap more efﬁciently.\nIt is now time to look more closely at k. Is it continuous or discrete, and what values does it take? \nWhat does k represent? Once we know k, we have complete knowledge of E from Eq. (15.17), and \nalmost complete knowledge of the cp [we still need A in Eq. (15.14)] and, hence, of the state vector \nfrom Eqs. (15.8) and (15.14).\n15.2 \u0002   BOUNDARY CONDITIONS AND THE ALLOWED VALUES OF k\nWe introduced the quantity k in Eq. (15.14) cp = Aeipka as an undetermined constant in the coefﬁ-\ncient of the atomic states 0\n p9 that contribute to the molecular state 0 c9. k serves to label the molecular \neigenstate under consideration. In fact, if we had anticipated the need for such a label, we might have \nwritten c k\np, with the interpretation that c k\np is the contribution of the pth atomic state to the kth molecular \neigenstate. Because k appears in the exponential function in combination with the real-space length a, \nit must have dimensions of inverse length. We often refer to the set of k values as “k space,” a recipro-\ncal space to the real space that we are used to. We must apply real-space boundary conditions to deter-\nmine which values k may assume. For solids containing a huge number of atoms 11022 cm-32, it is best \nto use periodic boundary conditions, as illustrated in Fig. 15.8. One can think of bending the linear \n1 well\n2 wells\nN wells\nFIGURE 15.7 The development of a band of energies from a discrete atomic energy as \nthe number of wells increases.\n1\n2\n3\n4\n5\n6\n7\n10\na\n9\n8\nFIGURE 15.8 Periodic boundary conditions for a 10-well chain.\n",
    "15.2 Boundary Conditions and the Allowed Values of k \n477\nchain of atoms into a ring, so that the 1st atom and the Nth atom are now neighbors. The next atom after \nthe Nth is the (N + 1)th, which is the same as the 1st. The physical consequence of this procedure is to \nremove the effect of the boundaries, (i.e., the surface of the solid). This effectively makes the ﬁrst and \nlast equations of the set in Eq. (15.12) identical to all the other equations and justiﬁes the neglect of the \nendpoint equations in the last section.\nThe periodic boundary condition amounts to writing\n \ncn=1 = cn=N+1 , \n(15.18)\nwhich, using Eq. (15.14), is equivalent to\n \neiNka = 1 . \n(15.19)\nThis condition is satisﬁed for\n \nNka = q 2p 1 kq = q\nN\n  2p\na\n , \n(15.20)\nwhere q is yet another integer. It is important that the integer q is not the label of the atomic states. \nIt deﬁnes the allowed values of k, which labels the molecular states. There are N physically distinct \nmolecular states that result from the N different k values corresponding to q = 1, 2, ... N. We get \nthe same set of N molecular states for the set of q values q = N + 1,  N + 2, ... 2N or for the set \nq = -N>2, -N>2 + 1, ... N>2, or indeed for any N consecutive integers. Now we write the disper-\nsion relation as\n \nEkq = a + 2b cos1kq a2, \n(15.21)\nwhich is plotted in Fig. 15.9 for N = 20 and b 6 0, for k corresponding to the set of integers \nq = -  N>2, -N>2 + 1, ... N>2. We’ll see in Section 15.6 that values of k outside this range yield \nexactly the same wave functions and energies, and hence give no new information.\nThe allowed values of k are separated by\n \n\u0006k = kq - kq-1 = 2p\nNa\n . \n(15.22)\n0\nk\nE(k)\n1st Brillouin zone\n\u0003 4Π\n5a\nΑ\nΑ\u00032Β\nΑ\t2Β\n4Π\n5a\n\u0003 Π\n5a\nΠ\n5a\n\u0003 Π\na\nΠ\na\n\u0003 2Π\n5a\n2Π\n5a\n\u0003 3Π\n5a\n3Π\n5a\nFIGURE 15.9 Dispersion relation for a chain of 20 wells. Circles represent k values that give \ndistinct  eigenstates.\n",
    "478 \nPeriodic Systems\nThe quantity Na, the product of the number of atoms and the interatomic spacing, is the (real-space) \nlength of the solid, which we can call L. L usually has macroscopic dimensions (mm, mm, or cm) com-\npared with the Å to nm scale for a. Because\n \n\u0006k = 2p\nL\nV 2p\na\n , \n(15.23)\nthe k spacing is very much smaller than the range of k values, k may be considered a continuous quan-\ntity for most practical purposes in a macroscopic solid, and we can consider the dispersion relation a \ncontinuous function\n \nE 1k2 = a + 2b cos1ka2  . \n(15.24)\n15.3 \u0002  THE BRILLOUIN ZONES\nAny set of N consecutive integers could be used to designate the N distinct k values in Eq. (15.20). The \nset of N integers that gives k values closest to zero deﬁnes the ﬁrst Brillouin zone in k-space. This \nset is q = -N>2, ... N>2, which means that the ﬁrst Brillouin zone extends from -p>a to +p>a in\nk space, and it is this set that is shown in Fig. 15.9. Including the state q = 0, it would seem there are \nN + 1 states, but because of the periodicity, the state at k = -p>a is the same as the one at k = +p>a, \nso there are exactly N distinct states for the N-atom chain. The width of the zone is 2p>a, which illus-\ntrates a fundamental relationship between real space and reciprocal space—if the interatomic spacings \nare large (small) in real space, then the corresponding Brillouin zones are small (large) in k space.\nOne can also deﬁne the set of N integers that give k values larger than any in the ﬁrst Brillouin \nzone, but otherwise closest to zero. This set is q = N>2, ... N and -N>2, … -N and the corresponding \nk values form the second Brillouin zone. The second Brillouin zone is the same size as the ﬁrst, but it \nis not contiguous. A similar procedure deﬁnes higher-order zones. A slightly modiﬁed process deﬁnes \nBrillouin zones in two and three dimensions, where again, higher-order zones have the same area or \nvolume as the ﬁrst zone, but are not contiguous regions of k space.\nWe have not yet speciﬁed what k actually represents, and this will become clearer in Section 15.6. \nThe quantity Uk has the dimensions of momentum, and it is often called the crystal momentum or \nquasimomentum, so called because it deﬁnes the wavelength of the envelope of the molecular wave \nfunction. In this respect, k is similar to the quantity of the same name that we studied in Chapter 5,\n \nkconventional = 22m 1E - V2>U2 , \n(15.25)\nwhich deﬁnes the (local) wavelength of the electron wave function.\nNext, we should use the allowed values of k to ﬁnd the sets of coefﬁcients that determine the speciﬁc \ncontributions of each atomic orbital to each molecular orbital, and draw real-space representations of the \nmolecular orbitals. Before we do this, let’s make a short digression to describe how the LCAO approach \ndescribed above that yielded the dispersion relation [Eq. (15.21) or (15.24)] for a single atomic state per \nwell plays out if we choose well parameters that allow two or more atomic states per well.\n15.4 \u0002  MULTIPLE BANDS FROM MULTIPLE ATOMIC LEVELS\nReal solids have multiple bands of energies, not just one. The dispersion relation Eq. (15.21) resulted \nwhen we considered the interaction between the single levels in neighboring atoms in the periodic \nsystem (these might be considered analogous to the 1s ground state of a hydrogen atom). To model the \neffects of higher-energy atomic states, we must include them in the basis set. The basis would include \nthe ground state 0 g9 and the ﬁrst excited state 0 e9 for example, if we used two atomic levels per atom, \n",
    "15.4 Multiple Bands from Multiple Atomic Levels \n479\nfor a total of 2 * N atomic states in the basis. We would designate them 0 n, g9 and 0 n, e9. The integer \nn labels the atom or individual well, while the second designator labels the state within that well. The \nresult would be the formation of additional energy bands with dispersion relations resembling the \none we calculated in the previous section. In the nearest-neighbor approximation, the simplest case \nwould be represented by the Hamiltonian matrix HA in Fig. 15.10 below, where there is no interaction \nbetween the ground state of one well and the excited state of the adjacent well. In that case, there are \ntwo bands formed with dispersion relations exactly like Eq. (15.24), but with different band centers \n1ag and ae2 and bandwidths 1bg and be2 for each band.\nFigure 15.11 shows the allowed energies for a periodic system with two atomic levels per atom at \nenergies ag = 2 and ae = 10, and b-values of bg = -1 and be = +2 in the same energy units. This \nchoice makes the upper band twice as wide as the lower, and puts the maximum energy of that band \nat k = 0. The allowed k values are the same as before, kq = 2pq>Na, but now there are two possible \nenergy eigenvalues for each value of kq, E (g)\nq  and E (e)\nq , with the upper index labeling the band. In this \nexample, the b-values are smaller than the spacing between the atomic states 1ae - ag2, the bands \nremain separate from one another, and the resulting band structure is a series of branches of E(k) \ncurves deﬁning bands of allowed energies, separated by gaps of forbidden energies. This model quali-\ntatively explains the band gaps we observe in real solids that are so important in semiconductors, for \nexample. Band gaps are discussed further in Section 15.12.\nWhen we discuss the energy band structure of real materials, we often label bands by the atomic \nstates from which they are primarily derived. In Fig. 15.11, we might refer to the lower band as “the \nground state band” and the upper as the “excited state band” to acknowledge that the lower (upper) \nband eigenstates are linear combinations of the ground (excited) atomic states. In real solids, we speak \nof the “1s band,” the “3d band,” and so forth. In the next section, we discuss the composition of the \nmolecular states in more detail.\nIf there is a signiﬁcant interaction 1bge2 between the ground state of one atom with the excited \nstate of its neighbor, the Hamiltonian HB in Fig 15.10 is appropriate. The dispersion relation calcula-\ntion is a nice extension of the example presented here (Problem 15.4). The two bands each contain \nmixtures of both atomic states, rather than being derived exclusively from one or the other atomic state.\nThere are several software packages that ﬁnd the energy eigenvalues and eigenstates of \none-dimensional periodic potentials (see Resources). It is very instructive to use these to examine the \nenergy spectrum and see the bands, and investigate the effect of changing the number of wells, their \nshape, and their separation. Such packages usually plot wave functions and the associated probability \ndensities, too, and this is the topic of the next section.\n1g\n1g\n2g\n2g\n1e\n1e\n2e\n2e\nHA \nΑg\nΒg\n0\n0\nΒg\nΑg\n0\n0\n0\n0\nΑe\nΒe\n0\n0\nΒe\nΑe\nHB \nΑg\nΒg\n0\nΒge\nΒg\nΑg\nΒge\n0\n0\nΒge\nΑe\nΒe\nΒge\n0\nΒe\nΑe\nFIGURE 15.10 Hamiltonian matrices in the nearest-neighbor approximation for a \nperiodic chain of two wells with two states per well. HA describes a situation where the \nstates are well separated in energy and there is no interaction between the upper state of \none well and the lower state of the adjacent well. HB relaxes that assumption.\n",
    "480 \nPeriodic Systems\nFigure 15.11 begins to be reminiscent of the real band structure calculation that we began with in \nFig. 15.2. In Fig. 15.11, if we label the point k = 0 as \u000f and k = p>a as X, we begin to see the simi-\nlarity to the corresponding panel in Fig. 15.2. In Fig. 15.2, \u000f, X, W, and K are simply labels of different\nk values (but in three-dimensional reciprocal space rather than one-dimensional), and the correspond-\ning values of E are plotted for these directions in reciprocal space. There are several bands, some of \nthem overlapping, and some of them with the simple shape that we have found in our rudimentary \nmodel. Nowadays, real band structure calculations are performed with powerful computers and with \nmore sophisticated methods than the LCAO method discussed here, but the LCAO method allows us \nto understand and interpret such pictures rather well.\n15.5 \u0002  BLOCH’S THEOREM AND THE MOLECULAR STATES\nHaving calculated the energy eigenvalues via the dispersion relation Eq. (15.24), we now calculate \nthe molecular eigenstates from Eqs. (15.8) and (15.14). We return to the simple example of one \natomic state per well, where the kth molecular state is represented as a superposition of all the atomic \nstates 0 n9:\n \n0 ck9 = a\nN\nn=1\nAeinka0 n9. \n(15.26)\nWe have almost all the information we need. We know the atomic states 0 n9 and the allowed values of k. \nWe don’t yet know the value of A, nor have we tested that the proposed molecular state has all the prop-\nerties expected of an eigenstate of a periodic potential.\nThe constant A is easy to ﬁnd. We built into the assumption (15.14) that every atomic state \nmakes an equal contribution (in magnitude) to the molecular state. Why? Well, every atom is identi-\ncal (assuming periodic boundary conditions), so how could the magnitude of any one atomic state’s \ncontribution be different than that of any other? The atomic state coefﬁcients, then, must differ only by \na phase factor, and that phase factor is already reﬂected in the exponential in Eq. (15.14). We require \n0\nk\nΑg\nΑe\nE(k)\nEgap\n4Βe\n4Βg\n\u0004 Π\na\nΠ\na\nFIGURE 15.11 Dispersion relation showing two energy bands derived from two atomic \nstates per well. In this example, the parameters are such that the bands do not overlap, and \nthere is an energy gap where no states are allowed for any value of k.\n",
    "15.5 Bloch’s Theorem and The Molecular States \n481\nthat the molecular state be normalized, and because the atomic basis states are orthogonal (approxi-\nmately), the normalization condition is\n \n8ck0 ck9 = a\nN\nn=1\n0 cn0\n2 = a\nN\nn=1\n0 A0\n2 = 1. \n(15.27)\nIt is clear from Eq. (15.27) that the appropriate constant is A = 1> 1N, so that\n \n0 ck9 = a\nN\nn=1\n 1\n2N\n einka0 n9. \n(15.28)\nNow at last we have all the information we need to construct the eigenstate, and we will proceed \nto draw some pictures of the molecular wave functions; they are in Section 15.6, to which you can \nskip immediately if you like. However, we need to check that the eigenstate has all the properties we \nexpect. (It does, of course, otherwise we wouldn’t have gone to all this trouble!)\nThe structure of the solid or molecule is periodic, so the electron probability density, a measur-\nable quantity, must also be periodic. In Dirac notation, this condition is\n \n08x0 ck9 0\n2 = 08x + ma0 ck9 0\n2, \n(15.29)\nwhere m is an integer. In wave function notation, (i.e., the position representation), this condition is\n \n0 ck 1x20\n2 = 0 ck 1x + ma20\n2. \n(15.30)\nYou might be tempted to think that the wave function itself should be periodic, too, but that is too \nstringent a requirement and has no basis in measurement. But if the wave function satisﬁes the condition\n \nck 1x + ma2 = eimkack 1x2  , \n(15.31)\nit is easy to see that Eq. (15.30) is satisﬁed. The condition in Eq. (15.31) is one expression of Bloch’s \ntheorem in one dimension. Bloch’s theorem stems from the translational symmetry of the periodic \nsystem of potential energy wells. Bloch’s theorem can be generalized to two and three dimensions \nand it is a critical part of understanding any periodic system. For our purposes, the one-dimensional \nform will sufﬁce. Now we must ask if the molecular eigenstates of the periodic potential that we have \nconstructed from atomic eigenstates of the individual wells obey Bloch’s theorem. If they do, then we \nhave been successful, and Eq. (15.26) represents the molecular eigenstates. We perform the test with \nthe wave function representation of Eq. (15.26):\n \nck1x2 =\n1\n2N\n a\nN\nn=1\neinkawn1x2, \n(15.32)\nwhere\n \nwn1x2 = 8x0 n9. \n(15.33)\nAll the atomic wave functions have the same shape, but they are displaced from one another by an \ninteger number of well spacings. This statement is represented mathematically by the equation\n \nwn1x + ma2 = wn-m1x2. \n(15.34)\nEquation (15.34) says that if we take the atomic wave function belonging to the nth well and translate \nit backwards by m lattice spacings (that’s the left-hand side), it must look the same as the atomic wave \nfunction corresponding to the (n–m)th well (that’s the right-hand side), which of course is exactly true.\n",
    "482 \nPeriodic Systems\nWith this in mind, start with the left-hand side of Bloch’s theorem and use Eq. (15.32) with \nx - 7 x + ma:\n \n ck (x + ma) =\n1\n2N\n a\nN\nn=1\neinkawn (x + ma)\n \n \n =\n1\n2N\n a\nN\nn=1\ne inkawn  -   m (x)    [from Eq. 15.34] \n(15.35)\n \n =\n1\n2N\n a\nN-m\nn=1-m\ne i(n  +   m) kawn (x)  (n S n + m) \n \n  =\n1\n2N\n a\nN\nn=1\ne i(n + m) kawn (x)  (can start the count anywhere). \nThe last step is possible because, with periodic boundary conditions, we are summing over atomic \nsites around an N-member ring (c0 = cN, c1 = cN+1, etc.). It doesn’t matter where we start the sum as \nlong as we include N consecutive terms. The result is [using Eq. (15.32) again]\n \nck 1x + ma2 = eimka 1\n2N\n a\nN\nn=1\neinkawn1x2 = eimkack 1x2, \n(15.36)\nwhich clearly satisﬁes Bloch’s theorem [Eq. (15.31)] and guarantees that the probability density is \nperiodic, as it must be. In the next section, we’ll draw some pictures to appreciate the patterns in the \nmolecular wave functions.\n15.6 \u0002  MOLECULAR WAVE FUNCTIONS—A GALLERY\nWe can draw the molecular wave functions of Eq. (15.32) if we know the atomic wave functions. For \na chain of ﬁnite square wells, the atomic wave functions are the eigenstates of the ﬁnite well that we \nfound in Chapter 5. The wave functions shown in Fig. 15.12 are drawn using a schematic representa-\ntion of the ground state of a ﬁnite square well. We assume for now that b 6 0; we’ll show it later. We \ncontinue to use the periodic boundary conditions we introduced in Section 15.2. The molecular wave \nfunction corresponding to the lowest energy state has k = 0 and is\n \nc0 1x2 =\n1\n2N\n a\nN\nn=1\nei0awn1x2 =\n1\n2N\n 3w11x2 + w21x2 + w31x2 + w41x2 + ...4. \n(15.37)\nThis is a simple in-phase addition of each of the basis functions, which happens to be real if the basis \nfunctions are real. The plot is shown in Fig. 15.12(a). Notice that there are no nodes in this lowest \nenergy wave function, and in this regard, it is “s-like.”\nThe molecular wave function corresponding to the highest energy state has k = p>a (or  -p>a):\n \ncp>a 1x2 =\n1\n2N\n a\nN\nn=1\neinpwn1x2 =\n1\n2N\n 3w11x2 - w21x2 + w31x2 - w41x2 + ...4. \n(15.38)\nThis antiphase addition of the basis functions, shown in Fig. 15.12(c), also has only a real compo-\nnent. Notice that the envelope (dashed line) of the molecular wave function has a wavelength equal to \n2p>k = 2a, or twice the interatomic spacing. This is the smallest possible wavelength that could have \nphysical meaning. In contrast, the “wavelength” of the k = 0 state in Fig. 15.12(a) is inﬁnite.\n",
    "15.6 Molecular Wave Functions—A Gallery \n483\nFor every other value of k, the system wave function is complex, with both a real and an imagi-\nnary part. One more example will sufﬁce, and further examples for different values of k within the ﬁrst \nBrillouin zone are in the homework problems. For k = p>2a, the molecular wave function is\n \n cp>2a1x2 =\n1\n2N\n a\nN\nn=1\neinp>2wn 1x2\n \n \n =\n1\n2N\n 3w11x2 + iw21x2 - w31x2 - iw41x2 + w51x2 + iw61x2...4\n \n \n =\n1\n2N\n 3w11x2 - w31x2 + w51x2...4 +\ni\n2N\n 3w21x2 - w41x2 + w61x2...4. \n(15.39)\nNotice that the patterns of the real and imaginary parts are the same, and both have a wavelength cor-\nresponding to 2p>k = 4a as evident in Fig. 15.12(b). As a further homework problem, explore the \n(b)\n(c)\nRe[ΨΠ/2a(x)]\nΨΠ/a(x)\nIm[ΨΠ/2a(x)]\n(a)\n1a\n2a\n3a\n4a\n5a\n6a\n7a\n8a\nx\nΨ0(x)\n1a\n2a\n3a\n4a\n5a\n6a\n7a\n8a\nx\n1a\n2a\n3a\n4a\n5a\n6a\n7a\n8a\nx\n1a\n2a\n3a\n4a\n5a\n6a\n7a\n8a\nx\nFIGURE 15.12 Wave functions of a system of periodic wells corresponding to the states (a) k = 0,\n(b) k = p>2a, and (c) k = p>a. The envelopes of the wave functions have wavelength l = 2p>k.\n",
    "484 \nPeriodic Systems\nwave functions obtained when you choose two values of k that differ by 2p>a (i.e., values of k in two \ndifferent Brillouin zones).\nBecause k determines the wavelength of the envelope of the molecular wave function, we often \ncall it the wave vector of the state. In one dimension, k is a “vector with one component,” so it’s not \nobviously distinguishable from a scalar, but in two (three) dimensions, k has two (three) components. \nIn higher dimensions, the molecular wave function is\n \n0 c9k =\n1\n2N\n a\n \nR\neik~R0 R9, \n(15.40)\nand R is the vector that locates the atom or well in real space, and also labels the atomic state associ-\nated with the atom. In one dimension, the equivalent label is na.\nWe have now solved the problem we set out to solve, namely ﬁnding the energies and wave \nfunctions of the allowed states of the periodic potential. The eigenfunctions of the periodic system \ncan be written as linear combinations of the eigenfunctions of the individual wells making up the \nperiodic chain. The different linear combinations are labeled by an index k that describes the modu-\nlation of the envelope of the wave function in terms of a wavelength l = 2p>k. k is restricted to \ndiscrete values, but these values can be so closely spaced for large N that k can be considered a \ncontinuous variable. Unique molecular wave functions result from those k values that lie within the \nﬁrst Brillouin zone. The energy of the eigenstate labeled k is given by the dispersion relation in\nEq. (15.21) or (15.24). The energies are bounded, and are so closely spaced as to form a band. If there \nare multiple states in a single well, there are multiple energy bands in the periodic well, and the bands \nmay be separated by a relatively large energy gap, depending on the strength of the coupling between \nthe states of adjacent wells. The molecular eigenstates in a band are often primarily derived from one \nof the atomic eigenstates.\n15.7 \u0002  THE DENSITY OF STATES\nSome properties of a solid do not depend on the value of the wave vector k of a particular state but \nrather on the number of states in a particular energy range, a quantity that we refer to as the density of \nstates. The density of states g(E ) is easily visualized in the case where the allowed states are discrete, \nas shown in Fig. 15.13(a). Here, the dispersion relation E(k) (see Fig. 15.9) is rotated 90° to make E the \nhorizontal axis. To ﬁnd the density of states from the dispersion plot, slice the energy axis into equal \nintervals, count the number of states (dots) in each energy interval, and plot the result as a histogram, \nas shown in Fig. 15.13(b). The solid line in Fig. 15.13(b) shows the functional form of the density of \nstates in the limit that k becomes a continuous variable.\nIn one dimension, the number of states per unit energy is calculated rather easily from the density \nof states in k space, which we can call gk(k). If there are N wells in the periodic potential, and one state \nper well, then there are N molecular states, whose corresponding k values are evenly spaced between \n-p>a and p>a [Eq. (15.20)]. The state density in k space is\n \ngk1k2 =\nN\n2p>a =\nL\n2p\n , \n(15.41)\nwhere L = Na is the length (one-dimensional volume) of the chain of wells. It makes no difference \nwhether we count states according to their energy label or their k label, so it must be true that\n \ng1E2dE = 2gk1k2dk . \n(15.42)\n",
    "15.7 The Density of States \n485\nThe factor of 2 accounts for the fact that for every state in an interval dk there is another state of the \nsame energy at the opposite value of k. It follows that the density of states (in one dimension) is\n \ng1E2 = 2gk 1k2 dk\ndE = L\np dk\ndE\n  . \n(15.43)\nIn the example considered here, the dispersion relation gives\n \ndE\ndk = 2ba sin 1ka2, \n(15.44)\nresulting in\n \ng1E2 =\nL\n2pba sin 1ka2\n , \n(15.45)\nwhich is plotted in Fig. 15.13(b). The density of states is proportional to the one-dimensional vol-\nume L, and it is often preferable to work with the volume-independent quantity g(E )>L. Strictly, \nbecause g(E) is a function of E rather than k, we should express k in Eq. (15.45) in terms of E via \nEq. (15.17), but the expression becomes cumbersome and is not particularly enlightening.\n(a)\n(b)\nΑ\nΑ\u00072Β\nΑ\u00042Β\nE\n0\nΑ\nΑ\u00072Β\nΑ\u00042Β\nE(k)\n0\nk\ng(E)\n\u0004 Π\na\nΠ\na\nFIGURE 15.13 (a) The dispersion relation E(k) rotated 90° to make E the horizontal axis. The dots \nrepresent the allowed states for small N; the solid line represents the continuous case. (b) The density \nof states g(E) as a function of energy. The histogram corresponds to the small N case; the solid line \nrepresents the continuous case.\n",
    "486 \nPeriodic Systems\nNotice that the density of states diverges at the Brillouin zone boundary. This is a quirk of the \none-dimensional geometry, but it does not cause any unphysical results. For example, if we calculate \nthe total energy of all the states, we arrive at a ﬁnite result (Problem 15.6):\n \nETOT =\nL\nEmax \nEmin \nE g 1E2  dE = Na . \n(15.46)\nIn Fig. 15.2, the real density of states for Si is plotted alongside the band structure. There are no inﬁni-\nties, but there are some sharply peaked features that correspond to local band minima or maxima.\n15.8 \u0002 CALCULATION OF THE MODEL PARAMETERS\nThe parameters a and b were introduced in Eq. (15.5) as the matrix elements of the Hamiltonian in \na periodic potential and they later appeared in the dispersion relation E(k) as the band center (a) and \nthe band width (4b). However, at that time, we did not calculate their values in terms of the atomic \nwell parameters—the width, height, and separation. In this section, we do this calculation for the case \nwhere the individual wells are square wells.\nTo evaluate a and b, we must ﬁnd matrix elements of the Hamiltonian H of the full periodic sys-\ntem in the basis of the eigenstates of the Hamiltonian H0 of an isolated well. Both H and H0 contain the \nsame kinetic energy operator\n \nT = p2\n2m\n , \n(15.47)\nbut the potential energy term in H represents the full periodic potential [Fig. 15.14(a)], while the \npotential energy in H0 represents a single well [Fig. 15.14(b)]. The parameter a is the diagonal ele-\nment of the Hamiltonian matrix\n \na = 8n0 H0 n9. \n(15.48)\nWe rewrite this as\n \na = 8n0 H0 + V\u00040 n9, \n(15.49)\nwhere V\u0004 is the difference [Fig. 15.14(c)] between the full periodic potential and the potential energy of \na single well. That is, V\u0004 is the periodic potential with one well missing—the one corresponding to H0.\nLet’s suppose there is a single eigenstate in the isolated well with energy Eg (calculated according \nto the procedures in Chapter 5). Then\n \nH00 n9 =  Eg0 n9 \n(15.50)\nand\n \na = 8n0 H00 n9 + 8n0 V\u00040 n9 =  Eg + 8n0 V\u00040 n9. \n(15.51)\nThis calculation shows that a is equal to the energy of the isolated well eigenstate plus a term that \nis the matrix element of V\u0004 in the basis of the atomic states:\n \n8n0 V\u00040 n9 =\nL\n\u0005\n- \u0005\nw*\nn1x2V\u00041x2wn1x2dx . \n(15.52)\nThis matrix element is very small because it is the integral of a potential energy that is zero (i.e., missing \na well) exactly where the wave function wn1x2 is nonzero! Where V\u00041x2 is nonzero, the wave func-\ntion is very small. Figure 15.15 shows in graphical form what Eq. (15.52) says in symbolic form. For \nexample, for an electron bound in a well that is V0 = 1 eV deep and b = 0.35  nm wide, the single \nbound energy is 0.6 eV and the difference between a and Eg is -3.6 meV for a well spacing of a = 3b.\n",
    "15.8 Calculation of the Model Parameters \n487\nV\n(a)\n(b)\n(c)\n1a\n2a\n3a\n4a\n5a\n1a\n2a\n3a\n4a\n5a\nx\n0\nV0\nV\n0\nV0\nV\n0\n\u0004V0\nx\n1a\n2a\n3a\n4a\n5a\nx\nFull periodic potential\nSingle well potential\nDifference potential\nFIGURE 15.14 (a) The full periodic potential energy; (b) the atomic potential energy of an \nisolated well located at the position of atom 2; (c) V\u0004, the potential energy difference between \n(a) and (b).\n1a\n2a\n3a\n4a\n5a\nx\n0\n8x039\n8x039\n8x049\nb\u00078n0V'0m9\na\u0006Eg\u00078n0V'0n9\nV'\n\u0006V0\nFIGURE 15.15 Schematic representation of the terms in Eqs. (15.52) and (15.53). The wave \nfunction widths are exaggerated to show the overlap.\n",
    "488 \nPeriodic Systems\nThe evaluation of b is similar, except that n and m correspond to adjacent wells:\n \nb = 8n0 H0 m9 = 8n0 H0 + V\u00040 n { 19. \n(15.53)\nWe assume that the atomic states on adjacent atoms are nearly orthogonal because the wave function \noverlap is small, so the matrix element 8n0 H00 n { 19 is neglected and we ﬁnd\n \n b = 8n0 V\u00040 n { 19\n \n \n =\nL\n\u0005\n- \u0005\nw*\nn1x2V\u00041x2wn{11x2dx . \n \n(15.54)\nThis matrix element of V\u00041x2 is much larger than the one in Eq. (15.52) because where V\u00041x2 is non-\nzero, one of the atomic wave functions is large, and only one is very small. For the same parameters \ngiven above, b is -32 meV. This square-well example is the simplest integral to calculate analytically \nand you should do this for practice. Find the form of wn1x2 from Chapter 5, and perform the calcula-\ntion. A simple Mathematica or Maple program will allow you to generalize to more bands by using \ndeeper wells with more states.\n15.8.1 \u0002 LCAO Summary\n• The LCAO approach to ﬁnding the molecular wave functions ck1x2 and corresponding energies \nof a one-dimensional periodic potential of period a is to begin with the (atomic) wave functions \nwn1x2 of a single element of the potential. If there is one atomic state per well, there are N atomic \nstates in the basis and there are N molecular wave functions, each a different superposition of the \nN atomic states:\n \nck 1x2 =\n1\n2N\n a\nN\nn=1\neinkawn1x2 , \nwhere k labels the molecular state.\n• There are N values of k ranging from -p>a to p>a in steps of 2p>Na, where N is the number of \natoms/elements. This set of k values forms the ﬁrst Brillouin zone.\n• k has dimensions of inverse length and is called the wave vector. The associated wavelength, \nl = 2p>k, is the wavelength of the envelope of the molecular wave function.\n• The periodicity of the potential introduces translational symmetry into the problem. The result \nis that the molecular wave function obeys Bloch’s theorem ck1x + ma2  = eimka c1x2, which \nguarantees that the electron probability distribution is periodic, but does not require that the wave \nfunction itself is periodic.\n• The dispersion relation gives the energy of a molecular state k. In the nearest-neighbor approxi-\nmation, and when there is only one state per well,\n \nEkq = a + 2 b cos1kq a2. \nThese energies are effectively continuous if N is large.\n• The parameters a and b, matrix elements of the Hamiltonian, are\n \na = 8n0 H0 n9\nb = 8n0 H0 n { 19.\n \n",
    "15.9 The Kronig-Penney Model \n489\n• a is the band center and is approximately equal to the atomic state energy. b measures the \nstrength of the interaction between adjacent wells and 4b is the width of the band. Negative b \nputs the energy minimum at k = 0 and the maxima at the Brillouin zone boundaries, and vice \nversa for positive b.\n15.9 \u0002 THE KRONIG-PENNEY MODEL\nThe ﬁnal piece of the picture is to connect the LCAO approximation to the analytical, exact solution, \nwhich is possible for the simple case of a periodic chain of square wells. This example usually goes \nunder the name of the Kronig-Penney model.\nThe LCAO approximation lets us see the progression from the atomic wave functions and the \nenergy spectrum of isolated atoms to the band structure of a solid as the number of atoms becomes \nlarger and the interaction between the atoms becomes stronger. The Kronig-Penney model, on the \nother hand, simply solves the eigenvalue equation for the exact periodic potential. It is a more “cor-\nrect” approach, but lacks the intuitive connections to the atomic system. Moreover, in a real solid, the \nexact periodic potential is unknown, but the electronic energy levels and wave functions of atoms are \nnot too difﬁcult to calculate, so the LCAO model can be a good starting point. Figure 15.16 presents \nthe LCAO dispersion relation, Eq. (15.24), and the exact dispersion relation that we are about to ﬁnd \n[Eq. (15.61)], on the same plot. We see that the LCAO is a good approximation for the energies in the \nperiodic system, especially when the coupling between states in adjacent wells is not too large.\nSeveral excellent texts treat the Kronig-Penney example in great detail, and it is a good example \nto practice solving the energy eigenvalue equation. Here, we’ll present a very broad overview, and \nconcentrate on the energy spectrum rather than the eigenstates. The periodic potential V is sketched \nin Fig. 15.17 and all the relevant lengths and energies are deﬁned. The width of the well is b, the well \nspacing is a, and the well depth is V0. The bottom of the well is located at the zero of energy.\nThe eigenvalue equation is best solved in wave function notation (position representation), just as \nit was in Chapter 5 for the single ﬁnite well. The energy eigenvalue equation is the differential equation\n \n Hc 1x2 = Ec 1x2\n \n -  U2\n2m\n d 2\ndx2  c 1x2 + V 1x2c 1x2 = Ec 1x2. \n(15.55)\n0\nk\nΑ\nΑ\u00042Β\nΑ\u00072Β\nE(k)\n\u0004 Π\na\nΠ\na\nFIGURE 15.16 The dispersion relations for an N-well periodic system as \ncalculated by the LCAO model (solid) and by the Kronig-Penney model (dashed).\n",
    "490 \nPeriodic Systems\nE is the eigenvalue corresponding to the eigenfunction c1x2. The solution that follows is valid for all \nvalues of E 7 0, but in the end, we’ll be interested in the bound states, E 6 V0.\nWe need look only at a single element of the periodic potential, namely that for which \n-b 6 x 6 a -b, because Bloch’s theorem, Eq. (15.31), assures us that once we have found the solu-\ntion ck 1x2 in one element, then we can ﬁnd the solution ck 1x + ma2 for any other element. The \nsolutions to the energy eigenvalue equation in regions I and II are:\n \n cI 1x2 = Aeiqx + Be-iqx; q = 22mE\nU\n \n \n cII 1x2 = Ceikx + De-ikx; k =\n22m1E - V02\nU\n , \n \n(15.56)\nwhere A, B, C, and D are constants. A quick glance at Section 5.5 will refresh your memory if you’ve \nforgotten the procedure.\nThe wave function and its derivative must be continuous, and in particular at x = 0, the boundary \nbetween regions I and II:\n \n cI 102 = cII 102 1\n A + B = C + D\n \n \n c=\nI 102 = c=\nII 102 1\n q 3A - B4 = k 3C - D4. \n \n(15.57)\nThe wave function and its derivative at the edges of the well (one lattice spacing apart) are connected \nby Bloch’s theorem:\n \n eikacI 1-b2 = cII 1-b + a2 1\n Ae-iqb + Beiqb = e-ika  3Ce-ik(a-b) + Deik(a-b)4\n \n(15.58)\n \n eikacI \u00041-b2 = cII \u00041-b + a2 1\n q 3Ae-iqb - Beiqb4 = ke-ika  3Ce-ik(a-b) - Deik(a-b)4\n . \n \nEquations (15.57) and (15.58) are written succinctly in matrix form:\n \n•\n1\n1\n-1\n-1\ne-iqb\neiqb\n-e-ikae-ik1a-b2\n-e-ikaeik1a-b2\nq\n-q\n-k\nk\nqe-iqb\n-qeiqb\n-ke-ikae-ik1a-b2\nke-ikaeik1a-b2\nμ •\nA\nB\nC\nD\nμ = 0 . \n(15.59)\n\u0004b\n0\na\u0004b\nx\nV0\n0\nI\nII\nV\nFIGURE 15.17 Periodic potential parameters for the Kronig-Penney model.\n",
    "15.10 Practical Applications: Metals, Insulators, and Semiconductors \n491\nThere are nontrivial solutions to the set of Eqs. (15.59) only if the determinant of the 4*4 matrix is \nzero. It is an uncomplicated but rather long process to show that the solution to the secular equation is\n \n cos 1qb2cos 1k1a - b22 - q2 + k2\n2qk\n sin 1qb2sin 1k1a - b2 2 = cos 1ka2. \n(15.60)\nEquation (15.60) is valid for any E, but if E 6 V0, then k is imaginary, and it is common to recast it \nexplicitly in terms of real quantities:\n \ncos 1qb2cosh 10 k01a - b22 - q2 - k2\n2qk\n sin 1qb2sinh 10 k01a - b2 2 = cos1ka2.  (15.61)\nThe quantities q and k contain the energy E, so if we pick a value for k, we can invert Eq. (15.61) to \nﬁnd E(k). This task is best assigned to a computer! Figure 15.18 shows a graph of the allowed energies \nfor one particular choice of well parameters, and you can see the gaps in the energy spectrum, just as \nwe found previously, when we employed the LCAO approach. In Fig. 15.16, the lowest band is plotted \ntogether with the LCAO-derived band, to show the good agreement when the bands are not too broad.\nThis example illustrates that it is possible to solve the eigenvalue equation for the one-dimensional \nperiodic chain of potential energy wells without resorting to approximate methods like LCAO. In more \ncomplicated cases in many dimensions with many electrons, exact methods are impossible for practi-\ncal purposes and approximate methods are needed. This example gives a means to assess the degree of \nsuccess of the approximation method in a simple case. The main features are similar in both methods, \nbut the exact shapes of the dispersion relations differ in their details.\n15.10 \u0002  PRACTICAL APPLICATIONS: METALS, INSULATORS, AND SEMICONDUCTORS\nThe purpose of much of the work in this chapter was to produce a rudimentary model of a solid or \nmolecule. Remember though, that the problem that we have solved is for a single electron in a periodic \npotential, while real molecules and solids have very large numbers of electrons! For example, take \nthe case of just two wells—this might be a model of a diatomic molecule, say H2. However, we have \n0\nk\nE(k)\n\u0004 Π\na\nΠ\na\nFIGURE 15.18 Energy spectrum for the Kronig-Penney \nmodel of a periodic system.\n",
    "492 \nPeriodic Systems\nreally modeled H2 \n+, the hydrogen molecule ion, and neglected the effect that the other electron would \nhave had on the energy spectrum. We saw in Chapter 13 how to tackle aspects of this issue, but with-\nout resorting to such detail, a reasonable approximation is to assume that the states of the two-electron \nsystem would be about the same as the simple one-electron system, and that the ground state of the \ntwo-electron system would have both electrons occupying the ground state of the one-electron system, \nbut with opposite spin, so as not to violate the Pauli exclusion principle. If there are many electrons, \nwe would say that the ground state of the system is the conﬁguration where electrons occupy the \nlowest-possible-energy one-electron states, subject to the Pauli exclusion principle, [i.e., two electrons \nwith opposite spin per state (see Fig. 13.5)]. This simple assumption leads to a qualitative explanation \nof the occurrence of metals and insulators. It must be abandoned, though, to explain many interesting \nand important phenomena, like magnetism and superconductivity, where the effects of electron cor-\nrelation are too important to be neglected.\nFigure 15.19 schematically depicts two bands in a one-dimensional 20-atom “solid”. Circles rep-\nresent allowed states and the circles are ﬁlled if electrons occupy the state. Take sodium as an example, \nwhere, in Fig. 15.19(a), the lower band might represent the 3s band, while the upper might represent \nthe 3p band. Because there are 20 3s valence electrons and each state accommodates 2 electrons, only \nthe lowest 10 states in the 3s band are ﬁlled, and the band is half-full. (Don’t worry about the slight \ndifference in ﬁlling that results for the cases of even and odd numbers of wells—it’s not important \nin a large solid.) A half-ﬁlled band is the hallmark of a metal, as we discuss below. Figure 15.19(b) \nmight represent a “solid” of 20 He atoms, where we would need to accommodate 40 electrons in the \n1s band, and all states in the lower band are ﬁlled. A ﬁlled band is characteristic of an insulator. The \nsimple model correctly predicts that solid Na (along with any alkali metal) is metallic and solid He \n(or any solidiﬁed noble gas) is an insulator. This might seem like a trivial conclusion that we could \nhave reached much more simply just by considering the valence shell of the individual atom, but real \nsystems are far more complex.\nAn example of the complexity is given by solid hydrogen, which you might expect to be metal-\nlic similar to the (effectively) one-electron solids Na, K, etc. Normal solid hydrogen is insulating, \nbecause there is a structural distortion of the lattice that causes the 1s band to split in the middle, and \nthe H electrons completely ﬁll the lower band. Another important case where our model is too sim-\nplistic is that of the group IV elements, typiﬁed by silicon and represented in Fig. 15.19(c). A simple \n“valence shell” argument would predict that solid Si consists of ﬁlled 1s, 2s, 2p and 3s bands, and a \none-third ﬁlled 3p band, and hence is metallic. Wrong! If you worked out the a and b parameters for \nthe Si 3s and 3p states (in three dimensions of course), and included all these states in the calculation,\n(a)\n(b)\n(c)\n0\nk\n0\nk\nE(k)\nE(k)\nE(k)\n0\nk\n\u0004 Π\na\n\u0004 Π\na\n\u0004 Π\na\nΠ\na\nΠ\na\nΠ\na\nFIGURE 15.19 Schematic band diagrams: (a) metal, (b) insulator, (c) semiconductor. Circles represent allowed \nstates; they are ﬁlled if the state is occupied by an electron.\n",
    "15.10 Practical Applications: Metals, Insulators, and Semiconductors \n493\nyou would discover that, in fact, the 3s and 3p atomic states of all the atoms combine to form two \ndistinct hybrid bands separated by a small energy gap of about 1 eV. The lower band is completely \noccupied by the Si electrons (we call it the valence band). The upper band is empty (we call it the \nconduction band). In Fig. 15.2, the highest energy of the valence band is (arbitrarily) labeled zero. \nSo Si is an insulator at very low temperatures where electrons ﬁll the states strictly in energy order. At \nroom temperature, the thermal energy of about 0.025 eV is sufﬁcient to deplete the valence band of a \nsmall number of electrons and populate the conduction band. In that case, Si has two partially ﬁlled \nbands, so it is “metallic” (i.e., conducting), but very weakly so, because there are so few current carri-\ners compared to a metal. Si is therefore a semiconductor.\nWhy is it that a partially ﬁlled band is considered the signature of a metal and a ﬁlled band that of \nan insulator? To answer, we have to think about how to represent the motion of an electron in a solid \nunder the inﬂuence of an electric ﬁeld. The eigenstates of energy E(k) that we have derived have the \nproperty that an electron in such a state has an equal probability of being found on any atom in the \ncrystal (see Fig. 15.12). For consideration of the effects of electric ﬁelds on electrons, it is useful to \ntake a more “particle-like” point of view and represent the electron by a wave packet or superposition \nof eigenstates that concentrates the probability of ﬁnding the electron in a more restricted region of \nspace. The Heisenberg uncertainty principle is important here: in “localizing” the electron in a wave \npacket of extent \u0006x, we are conceding an uncertainty in the momentum \u0006p = h>\u0006x. This uncertainty \nis expressed by the range of k values of the Bloch states used to construct the wave packet.\nThe motion of an electron’s wave packet is characterized by a group velocity (see Chapter 6 for a \nreview). This is the velocity of the group of superimposed waves (i.e., the velocity of the envelope of \na pattern of interfering waves). The crests and troughs of individual waves travel at the phase veloc-\nity, which is not necessarily the same as the group velocity. For waves with a dispersion relation v(k), \nthe phase velocity is v>k while the group velocity is dv>dk. These are the same only if the dispersion \nrelation is linear in k, as is the case, for example, for long-wavelength sound waves in a solid.\nFor an electron in a Bloch state 0\n ck9, the electron velocity is the expectation value of p>m \n(momentum/mass), that is,\n \nve = 1\nm\n 8ck 0 p0\n ck9 = 1\nm\n \nL\n\u0005\n- \u0005\nc*\nk  1x2 a-iU d\ndx\n b ck1x2dx \n(15.62)\nin one dimension. If the electron energy dispersion relation is E(k), then the electron’s (group) velocity \nis (because E = U v)\n \nvg = 1\nU \ndE1k2\ndk\n. \n(15.63)\nWe will not carry this out, but it is possible to show that ve and vg are the same if 0\n ck9 are Bloch \nstates.\nNow consider what happens when an electric ﬁeld E = E xn is applied to the solid, for example, \nby attaching electrical leads to opposite ends of the crystal and connecting them to a battery. The elec-\ntrons experience a force F = qE = -eExn. During a short time interval dt in which the force acts, an \nelectron moves a distance vgdt and the work done by the force is\n \n dw = Fdx\n \n \n = -eEvgdt\n \n \n = - aeE\nU b adE1k2\ndk\nb dt . \n \n(15.64)\n",
    "494 \nPeriodic Systems\nAt the same time, that electron’s energy changes by an amount\n \ndE = dE\ndk\n dk . \n(15.65)\nSetting dw = dE, we ﬁnd\n \ndk = -  eE\nU\n dt . \n(15.66)\nIntegrating to get k(t), we have\n \nk 1t2 =\n k 102 -\n eE\nU\n t . \n(15.67)\nThe message here is that application of the electric ﬁeld tends to shift the k values, and hence \nthe energies E(k) of all the electrons in the material. But can this actually happen? It depends on the \noccupation of the states in the band. If the band is full, any change of state of an electron must result in \nanother electron moving into the vacated state, leaving the electron energy and momentum distribu-\ntion unaltered. Under these conditions, no current can ﬂow and the material is an electrical insulator. \nIf the band is partially ﬁlled, plenty of unoccupied states exist within a small energy range (i.e., within \nthe same band) for these electrons to move into to change their k vectors and energies. The net electron \nenergy and momentum distribution changes and a current ﬂows under the inﬂuence of the electric \nﬁeld. This is the signature of electrical conductivity. In the case of a semiconductor, the number of \nthermally excited electrons in the upper band or holes in the lower band (see Section 15.11) is very \nsmall compared to the number in the metal, and the conductivity is weak.\n15.11 \u0002   EFFECTIVE MASS\nThe dispersion relation for a nonrelativistic free particle, one that moves in a region of constant potential, \nis given by\n \nE 1k2 = U2\n2m\n k2. \n(15.68)\nThe free electron dispersion relation simply states mathematically that the energy of a free particle \ncomes entirely from its momentum and that there is no potential energy contribution (except perhaps \nfor a constant). This parabolic or quadratic relation between energy and wave vector is characterized \nby the mass of the particle. Particles with large mass (like protons) are characterized by a parabola \nwith smaller curvature than particles with small mass (like electrons). Now, take another look at the \ndispersion for the one-dimensional chain of atoms, that is, E1k2 = a + 2b cos1ka2, which is plotted \nin Fig. 15.20 for two different values of b. Notice that near k = 0, the band function looks parabolic. \nIndeed, expand the dispersion relation E(k) for small k to ﬁnd\n \n E 1k2 = a + 2b cos1ka2\n \n \n \u0002 a + 2b C1 - 1\n2 1ka2\n2D \n \n \u0002 a + 2b - ba2k2 .\n \n \n(15.69)\nIf b 6 0, we see that near the bottom of the band, the energy is parabolic in k and varies according to\n \nE - Emin = 0 b0 a2k2. \n(15.70)\n",
    "15.11 Effective Mass \n495\nIf we compare Eq. (15.70) to the free particle dispersion relation, Eq. (15.68), we see that the electrons \nin states near the bottom of the band behave like free particles except that U2>2m has been replaced by \n0 b0 a2.  In other words, the electron behaves as if it had an effective mass\n \nm* =\nU2\n20 b0 a2 . \n(15.71)\nThe denominator of this expression is just the curvature of the band function for small k and the effective\nmass can be deﬁned more generally for states anywhere in the band according to\n \nm* = U2  c d 2E\ndk2 d\n-1\n. \n(15.72)\nBy this means, all the effects of the electron’s complicated interactions with the crystal lattice have \nbeen swept into one parameter, the effective mass. Figure 15.20 has the free particle dispersion rela-\ntion with the same curvature at k = 0 superimposed on the exact dispersion relation. We see that the \nupper band has the larger curvature, and hence the smaller effective mass at k = 0.\nNote the inverse dependence of m* on b or d2E>dk2. This means that the weaker the interac-\ntions between atoms (smaller beta), the “heavier” the electron is. Narrow bands (small b) are associ-\nated with high effective masses and wide bands (large b) correspond to relatively “light” electrons. \nThis makes sense intuitively: if b is small, the weak interaction or small overlap between atomic \nwave functions makes it difﬁcult for an electron to move from atom to atom under the inﬂuence of an \napplied electric ﬁeld, and it behaves as if it has a large mass.\nIn general, the effective mass changes at different positions in the band, because for any band \nshape except parabolic, the second derivative of E(k) changes. For states near the top of the band, the \neffective mass is negative! This means that the acceleration of a particle in an electric ﬁeld, a = F>m, \nis in the opposite direction to the force. While a negative mass might seem strange, it is perfectly con-\nsistent. More detailed texts on semiconductors show that when a band is almost completely full, it is \noften easier to think in terms of a small number of empty negative-mass electron states that behave like \nparticles with positive electric charges and positive masses, which we call holes. So in Fig. 15.19(c), \napplication of an applied ﬁeld would cause electrons in the conduction band to move against the ﬁeld \n0\nk\nΑ1\nΑ2\n\u0004 Π\na\nΠ\na\nE(k)\nFIGURE 15.20 The N-square-well E(k) for two values of b 6 0 represented \nby solid lines, and the parabolic free- particle E(k) represented by dashed lines. At \nk = 0, the effective mass is smaller for the more disperse (wider) upper band.\n",
    "496 \nPeriodic Systems\nand holes in the valence band to move in the direction of the ﬁeld. They both result in a current in the \nsame direction, so we add the contributions from the two bands. The number of carriers in each band is \nthe same because the electrons in the upper band originated in the lower band, leaving behind the same \nnumber of holes. But in the example of Fig. 15.20, the response of the holes in the lower band is more \nsluggish because of the larger effective mass. Therefore, the contribution of the electron current to the \ntotal current is larger than the hole current.\n15.12 \u0002  DIRECT AND INDIRECT BAND GAPS\nSemiconductors, particularly Si, are so important in modern technology that it is worthwhile to say a \nlittle more about them, although we will leave details to other texts dedicated to the topic. Semicon-\nductors are characterized by an (almost) full valence band and an (almost) empty conduction band. \nThe difference in energy between the highest energy state in the valence band and the lowest energy \nstate in the conduction band is called “the band gap.” The band gap is labeled in Fig. 15.21. The band \ngap of Si is 1.11 eV, and that of GaAs, another important semiconductor, is 1.43 eV. Of course, there \nare always “gaps” between the energies in different bands associated with a particular allowed value \nof k, but this is not what is meant by “the” band gap.\nAnother important characteristic of the band gap is whether it is a direct band gap or an indirect \nband gap, as illustrated in Fig. 15.21. The band gap is termed direct when the highest energy state \nin the valence band and the lowest energy state in the conduction band occur at the same value of k, \nand indirect when they occur at different values of k. The distinction is signiﬁcant because direct-gap \nsemiconductors absorb and emit light with much higher probability than indirect-gap semiconductors, \nand this is critical for materials selection in optoelectronic devices like light-emitting diodes (LEDs), \nlight sensors (LEDs operating in reverse), and solar cells.\nThe reason that direct-gap semiconductors interact more strongly with light is not hard to \nunderstand. We learned in Chapter 14 how to calculate the probability that an electron makes a tran-\nsition from one quantum state to another, and that this involves both energy and momentum con-\nservation (see Chapter 16 for the momentum aspect). The band gaps in semiconductors are of order \nDirect band gap\nIndirect band gap\n(a)\n(b)\n0\nk\nΑ2\nΑ1\nΑ2\nΑ1\nE(k)\nE(k)\nEgap\nEgap\n0\nk\n\u0004 Π\na\n\u0004 Π\na\nΠ\na\nΠ\na\nFIGURE 15.21 Transitions in (a) a direct-gap and (b) an indirect-gap semiconductor. The vertical arrows \nrepresent photons and the horizontal arrow in (b) represents a phonon.\n",
    "15.13 New Directions—Low-Dimensional Carbon \n497\n1–3 eV, a range that spans the energies of visible photons. Such photons then, have sufﬁcient energy to \ncause electron transitions between bands. In solids, we must also consider the conservation of crystal \nmomentum, represented by Uk:\n \nUke,init + Ukphoton = Uke,fin . \n(15.73)\nIn a direct transition, the electron’s initial and ﬁnal states have the same value of k. How is this pos-\nsible if the photon that induces the transition also has momentum? The momentum of an infrared, \nvisible or even ultraviolet photon is extremely small compared with typical electron momenta, so \nthe photon momentum does not change the electron momentum by any signiﬁcant fraction of the \nBrillouin zone width. Therefore, the transition is extremely close to being direct (a homework problem \nquantiﬁes this). It means that only a photon and an electron are necessary for a direct transition to take \nplace. On the other hand, if the transition is indirect, the electron’s momentum changes by a signiﬁcant \nfraction of the Brillouin zone width, and the photon cannot supply the needed momentum. The neces-\nsary momentum comes from another lattice denizen, the phonon, or lattice vibration. In other words, \nthe lattice changes its mode of vibration to accommodate the electron transition. In probabilistic terms, \nit means that three entities must be present at the same place and time (the electron, the photon and the \nphonon), and this is a far less likely occurrence than a coincidence of just two particles, an electron \nand a photon. The upshot is that direct transitions are far more likely than indirect transitions.\nNow the phonon supplies the necessary momentum for an indirect transition, but it also brings \nalong some energy. However, the phonon energies are rather small compared to the gap energy, so one \nphonon alone is not sufﬁcient to allow an electron to make an interband transition. As a ﬁrst approxi-\nmation, it is the photon that provides the energy and the phonon that provides the momentum for an \nelectron transition across an indirect band gap.\nSi is an example of an indirect-gap semiconductor. You can see in Fig. 15.2 that the valence band \nmaximum occurs at the k-point labeled \u000f, while the conduction band minimum occurs at the k point on \nthe line between \u000f and X. A phonon and a photon are necessary to facilitate this transition, making it \nless probable than if the gap were direct. It might seem strange then that Si is the most widely used \nsemiconductor in solar cells! As it happens, Si is the best material we have, despite the indirect-gap \nproblem. Although Si is not as efﬁcient at absorbing photons close to the band gap energy as a direct-\ngap semiconductor with the same band gap, there is sufﬁcient absorption of photons if the Si is thick \nenough. Its band gap is the perfect size to capture the photon distribution that comprises the solar \nspectrum—it is abundant, it is environmentally benign, and we have huge investments in Si-processing \ntechnology. All this makes Si the best material currently available for large-scale, economic produc-\ntion of photovoltaic cells. Intense efforts are underway to ﬁnd other materials that will do the same job \nmore efﬁciently and more economically. There are some competitors, but Si is still the most widely \nused photovoltaic material. GaAs is a direct-gap semiconductor. Photovoltaic cells made with GaAs \nare more efﬁcient than those made with Si and are used for some high-end applications, such as pow-\nering equipment in space. They are technologically more difﬁcult to produce than Si, and there are \nserious concerns about the abundance of Ga and As and the toxicity of the latter.\n15.13 \u0002 NEW DIRECTIONS—LOW-DIMENSIONAL CARBON\nOne of the most exciting “new materials” under active research at the present time is an old \nmaterial—carbon! Carbon, as diamond, has the same structure as silicon, but its wide band gap makes \nit insulating rather than semiconducting. Carbon, as graphite, has long been used as a lubricant, a \nreasonable conductor and a handy pencil. Graphite consists of weakly bonded layers of graphene, \nand graphene is a one-atom-thick sheet of C atoms strongly bonded to one another in a honeycomb \n",
    "498 \nPeriodic Systems\npattern. It is carbon in this two-dimensional form, as isolated graphene sheets or carbon nanotubes, \nwhich are rolled-up graphene sheets, that is the topic of intense interest. The band structure of gra-\nphene is easy to calculate with the LCAO method, because the interesting part derives from just the \nC 2pz states that are perpendicular to the graphene plane. The dispersion relation reveals that graphene \nis a gapless semiconductor—the top of the valence band and the bottom of the conduction band touch \nat several k points. Moreover, the dispersion relation features a linear dependence of E(k) on k at these \npoints. This linear dispersion relation is just like that of a photon (for which E(k) = Uck), so graphene \nis a playground to study relativity! Carbon nanotubes are particularly interesting from the perspective \nof the material presented in this chapter: nanotubes can be semiconducting or metallic, depending on \nexactly how the graphene sheet is rolled up. Graphene “ribbons” can also be made semiconducting. \nThe nanometer scale of these fascinating forms of carbon make them textbook examples of quantum \nphenomena, such as the fractional quantum Hall effect. On the applications front, graphene and car-\nbon nanotubes show promise as high performance transistors, transparent conductors, super-strong \nﬁbers, biosensors in cells, cages to store atoms, or nano-pipettes to deliver cellular cargo.\nSUMMARY\n• The model of a solid as a periodic array of potential energy wells predicts the existence of \nbands of allowed energies for electrons. This model qualitatively explains solid metals as \nmaterials whose electrons partially ﬁll the state of a band, and insulators and semiconductors \nas materials whose electrons completely ﬁll the band states and have a relatively large band \ngap between the ﬁlled states and the next available empty states. Larger band gaps are char-\nacteristic of insulators and smaller band gaps are characteristic of semiconductors.\n• Metals are good conductors because electron wave packets under the inﬂuence of an electric \nﬁeld may access nearby-energy states and change their momentum. Insulators are poor con-\nductors, because nearby-energy states are occupied by other electrons and no net momentum \nchange can occur. Semiconductors in this model are simply metals (partially ﬁlled bands) \nwith very few charge carriers that are generated thermally.\n• Electron motion in solids is modeled with the use of a wave packet, a superposition of delo-\ncalized Bloch states of different k that peaks at a speciﬁc location. This packet moves with a \nvelocity given by the group velocity (velocity of the envelope of the packet), while individual \nstates that comprise the packet move with a different velocity called the phase velocity.\n• The interactions of an electron in a solid with the lattice cause its response to external forces \nto be different than the response of a free electron. This difference is parameterized by the \neffective mass, which describes the curvature of the E(k) relation. It is especially useful near \nthe maxima and minima of bands, where the dispersion relation is often parabolic, similar to \nthe dispersion relation of a free electron.\n• The density of states g(E) is the number of states per unit energy interval. It is useful when it \nis necessary to quantify the total number of electrons involved in a process, such as optical \nabsorption, or electron transport.\n• The band gap in solids may be termed direct or indirect. A direct (indirect) gap occurs when \nthe highest occupied state in an occupied band is at the same (different) k value as (than) the \nlowest energy state in an empty band. Electrons can absorb photons or emit photons to make \na transition across the gap. Such transitions are more efﬁcient in direct-gap semiconductors.\n",
    "Problems \n499\nPROBLEMS\n 15.1 Write down the matrix representation of the Hamiltonian within the nearest-neighbor approxima-\ntion in terms of a and b for a linear chain of three wells, assuming only one atomic state per well. \nFind the normalized eigenfunctions and eigenvalues. This problem is quite tractable analytically.\n 15.2 Write down the matrix representation of the Hamiltonian within the nearest-neighbor approxi-\nmation in terms of a and b for a linear chain of N wells, assuming only one atomic state per \nwell. Use a computer to ﬁnd the normalized eigenfunctions and eigenvalues. Start with N = 3 \nto repeat the result from the previous problem, and then increase N. Aﬁcionado-code-writers \nmight like to make N much larger.\n 15.3 How would you alter the example presented in Problem 15.1 to ﬁnd the molecular states and ener-\ngies of a linear molecular like carbon dioxide, O=C=O, in the nearest neighbor approximation?\n 15.4 Derive the dispersion relation E(k) for the Hamiltonian HB in Fig. 15.10, which corresponds \nto the case where there are two states per well, and there is an interaction between the upper \nstate of one well and the lower state of the adjacent well in addition to the interactions between \nstates of the same energy. Assume an N-well chain as in Section 15.1.2.\n 15.5 a)  Find the LCAO state that corresponds to k = p>4a, similar to Eq. (15.39). Sketch the real \nand imaginary parts of the wave function, and illustrate that the wavelength is 8a. What is \nthe energy of this state?\nb) Pick another allowed value of k within the ﬁrst Brillouin zone, and repeat.\nc)  Pick a value of k that differs by 2p>a from one you have already chosen, and repeat.\nDiscuss your results.\n 15.6 Explain why the integral 1\nEmax\nEmin  E g1E2dE in Eq. (15.46) does indeed represent the total \n \n energy. Use the density of states expression in Eq. (15.45) to show that the integral  \nevaluates to Na, despite the inﬁnity in g(E ).\n 15.7 a) Find the density of states g(E ) for the case of the free particle in one dimension.\n \n b)  Show that the density of states g(E ) for the free particle dispersion relation in two dimen-\nsions is a constant (challenge problem).\n 15.8 Find the single bound state energy for an electron in an isolated well of depth V0 = 1 eV and \nwidth b = 0.35 nm, as discussed in Section 15.8. Find the matrix elements a and b for a \nperiodic system with well spacing a = 3b and conﬁrm the results given in the text.\n 15.9 a)  Show that the Kronig-Penney dispersion relation, Eq. (15.60), results from Eq. (15.59). \nThis is a straightforward but long calculation, and it’s easy to make mistakes. Be careful, \nand check each step.\n \n b) Show that Eq. (15.61) results from Eq. (15.60) if k is imaginary.\n 15.10 a)  Explore the band structures of C, Si, and Ge, which are all tetrahedrally-bonded solids with \nthe same crystal structure. What trends are evident and how can you explain them?\n \n   b)  In a given solid, effective masses at the extrema of higher bands tend to be lower than effec-\ntive masses at the extrema of lower bands. Is there a plausible physical interpretation of this?\n 15.11  Explain how a simplistic argument that energy bands in solids are entirely derived from the \ncorresponding atomic states might lead to the false conclusion that Mg (or any alkali earth \nelement) is in an electrical insulator. How do you rationalize the observed metallic behavior \nwithin the LCAO model?\n",
    "500 \nPeriodic Systems\n 15.12 a)  What is the energy of a visible photon? What are the band gaps of important semiconduc-\ntors? Are visible photon energies in the right range to facilitate electron transitions across \nthe band gap of a typical semiconductor?\n \n b)  Show that the momentum of a visible photon is insufﬁcient to facilitate electron indirect \ntransitions across the band gap of a typical semiconductor.\n \n c)  Phonons are quantized lattice vibrations. Like photons, they are massless entities, with a \ncharacteristic wavelength that determines the momentum, and a characteristic frequency \nthat determines the energy. If the characteristic wavelength of a phonon is roughly the  \nlattice spacing in a solid, and the characteristic frequency is roughly 1013 Hz, show that the \nmomentum of a phonon is in the right range to facilitate indirect electron transitions across \nthe band gap of a typical semiconductor, but that the energy is too small.\nRESOURCES\nActivities\nPeriodic Systems is a course based on Chapter 15 taught at Oregon State University. The course treats \nboth classical and quantum mechanical periodic systems. The website has a description and activities \nassociated with this course:\nwww.physics.oregonstate.edu/portfolioswiki/courses:home:pphome\nBand Structure: Explore wave functions and probability densities of chains of up to 10 square wells \nor Coulomb potential energy wells. The wells can be adjusted and an electric ﬁeld can be applied:\nhttp://phet.colorado.edu/en/simulation/band-structure \nQuantum Crystal: Explore wave functions and the dispersion relation of several different shapes of \npotential energy wells:\nhttp://www.falstad.com/qm1dcrystal/\nSolid State Physics Simulations (ISBN 0-471-54885-5), by Graham Keeler, Roger Rollins, Steven \nSpicklemire, and Ian Johnston, is one of nine parts of the Consortium for Upper-Level Physics Soft-\nware (CUPS) published by Wiley, edited by Maria Dworzecka, Robert Ehrlich, and William Mac-\nDonald. Solid State Physics Simulations has several useful programs that allow you to explore a \none-dimensional chain of atoms, band structure, dispersion relations, and the LCAO method applied \nto small clusters. There is an accompanying text. The series is out of print, but used copies are listed \nat Amazon.com.\nhttp://physics.gmu.edu/~cups/ss.html\nFurther Reading\nThe Kronig-Penney model is discussed in more detail in several well-known Quantum Mechanics texts:\nD. J. Grifﬁths, Introduction to Quantum Mechanics, 2nd ed., Upper Saddle River, NJ: Prentice \nHall, 2005.\nR. L. Liboff, Introductory Quantum Mechanics, 4th ed., San Francisco: Addison Wesley, 2003.\nA. Goswami, Quantum Mechanics, 2nd ed., Dubuque, IA: William C. Brown, 1996.\n",
    "Resources \n501\nMore advanced references:\nC. Kittel, Introduction to Solid State Physics, 8th ed., New York: John Wiley & Sons, Inc., 2005. \nAn introductory text that treats metals, semiconductors, and insulators, and many of the \nconcepts mentioned in this chapter.\nR. F. Pierret, Semiconductor Device Fundamentals, Reading, MA: Addison Wesley, 1996. Dis-\ncusses the details of carrier transport in semiconductors and modern devices.\nA. K. Geim and A. H. MacDonald, “Graphene: Exploring Carbon Flatland,” Phys. Today 60(8), \n35–41 (2007), http://dx.doi.org/10.1063/1.2774096. Gives a nice introduction to graphene, \nand explains the linear dispersion relation and the fractional quantum Hall effect.\nC. Dekker, “Carbon Nanotubes as Molecular Quantum Wires,” Phys. Today 52(5), 22–28 (1999), \nhttp://dx.doi.org/10.1063/1.882658. Talks about measurements to distinguish the difference \nbetween semiconducting and metallic carbon nanotubes, and discusses some potential uses.\n",
    "C H A P T E R  \n16\nModern Applications  \nof Quantum Mechanics\nTime for some fun! (Not that we weren’t having fun before.) You have now acquired a tool set for \nunderstanding how the microscopic world works. Let’s spend this last chapter using that tool set to \nexamine two current research topics that are extensions of some of the examples of quantum mechan-\nics that you have studied in this text. Quantum mechanical forces on atoms and quantum information \nprocessing both have important connections to Stern-Gerlach spin-1/2 experiments and to resonant \natom-light interactions. These new research ﬁelds can be considered to be quantum engineering in that \nwe understand the quantum mechanics so well that we are now using it for practical applications. The \nresearch is expanding so rapidly that we cannot provide a complete overview in just one chapter. We \nwill focus on a few aspects of these ﬁelds that are directly connected to what you have learned here. \nThe resources at the end of the chapter provide references for you to learn more.\n 16.1 \u0002 MANIPULATING ATOMS WITH QUANTUM MECHANICAL FORCES\nIn the last 30 years, physicists have developed a broad collection of quantum mechanical tools to \nexert forces on atoms. These forces allow us to manipulate the positions and velocities of atoms so \nwell that we can stop atoms and hold them in place for an extended time. We can, therefore, measure \nthem for longer and improve spectroscopic energy measurements that are limited by the energy-time \nuncertainty principle. For example, the standard of time is based upon a microwave transition between \ntwo hyperﬁne states in the cesium atom and the longer the atom can be observed, the better we can \ndeﬁne the second—the basic unit of time. Along the way, researchers have uncovered a host of other \nfun things to do with mechanical forces, and they have even discovered a new form of matter—the \nBose-Einstein condensate discussed in Chapter 13. In the following two subsections, we will discuss \ntwo examples of quantum mechanical forces.\n 16.1.1 \u0002 Magnetic Trapping\nThe ﬁrst example of quantum mechanical forces on atoms is magnetic trapping, where we use mag-\nnetic ﬁelds to conﬁne atoms to a small region of space. Magnetic traps are used to conﬁne atoms at \nvery low temperatures, and have played an important role in Bose-Einstein condensation experiments. \nTo explain how a magnetic trap works, we return to the Stern-Gerlach experiment that we know and \n",
    "16.1 Manipulating Atoms with Quantum Mechanical Forces \n503\nlove. In fact, the ﬁrst equation in this text, Eq. (1.1), told us that a magnetic moment experiences \na force in a magnetic ﬁeld gradient. That introduction was a classical argument, but we discovered \nthe quantum mechanical underpinnings of the Stern-Gerlach experiment in Chapter 12 when we dis-\ncussed the Zeeman effect. The Stern-Gerlach force derives from the potential energy of interaction \nbetween the magnetic moment of the atom and the magnetic ﬁeld:\n \nV = -M~B . \n(16.1)\nThe force on the atom is the negative gradient of this potential energy: F = -\u0002V. The potential \nenergy of the magnetic moment in the magnetic ﬁeld is the Zeeman energy we found from perturba-\ntion theory, which has the general form [see Eq. (12.93)]\n \nVZ = E (1)\nZ\n= g m mB B . \n(16.2)\nFor this discussion, we won’t worry about whether the magnetic moment is associated with a spin (S), \norbital (L), or total angular momentum (J or F), so we leave the subscripts off the Landé g-factor and \nthe magnetic quantum number m.\nIn a typical Stern-Gerlach experiment, the deﬂection angle of the atom is small (Problem 16.1). \nBut what if the Stern-Gerlach force were large enough to signiﬁcantly deﬂect the atom, say by 90°, \nor even 180°, and the magnetic ﬁeld were shaped so that the atom kept on being deﬂected? Then \nyou could imagine constructing a system that contained the atom and didn’t let it escape. That is the \nessence of a magnetic trap.\nTo discuss the mechanics of how a magnetic trap works, it is more instructive to use the energy \napproach rather than the force approach. To trap a particle in general, the potential energy must have \na spatial minimum to form a conﬁning well. For example, the generic potential energy well shown in \nFig. 16.1 has a minimum at x = 0 and will conﬁne or trap particles that have kinetic energies less than \nVmax. As the particles move, they exchange kinetic for potential energy. Such a potential energy well \nis no different in principle from the potential energy wells you have already studied—square well, \nharmonic oscillator, hydrogen atom. We call it a trap when we control the potential energy to conﬁne \nparticles that are otherwise free to move.\nx\nVmax\nV(x)\nFIGURE 16.1 Generic potential energy for a particle trap in one dimension. \nParticles with kinetic energy less than Vmax are trapped in the vicinity of the \norigin, where the potential energy is a minimum.\n",
    "504 \nModern Applications of Quantum Mechanics\nFor a magnetic trap, the potential energy that determines the particle motion is the Zeeman energy \nVZ1r2 = g m mB B1r2. A generic Zeeman energy level diagram is shown in Fig. 16.2. The force on \nthe atom is the negative gradient of the Zeeman energy, so atomic states with positive magnetic quan-\ntum number m are attracted toward regions of low magnetic ﬁeld and are called weak-ﬁeld seeking \nstates. Atom states with negative m are attracted toward regions of high magnetic ﬁeld and are called \n strong-ﬁeld seeking states. A local spatial maximum in the magnetic ﬁeld is not allowed by  Maxwell’s \nequations in free space, so a magnetic trap must rely on a local minimum in the magnetic ﬁeld along \nwith a positive magnetic quantum number m. Hence, a magnetic trap conﬁnes atoms in weak-ﬁeld \nseeking states and ejects atoms in strong-ﬁeld seeking states. An atom in a weak-ﬁeld seeking state has \nits angular momentum aligned with the ﬁeld (positive m), so the magnetic moment is aligned against \nthe ﬁeld.\nIn a three-dimensional magnetic ﬁeld, the magnetic ﬁeld direction is not uniform, especially \naround the local minimum that forms the trap. The changing ﬁeld direction would seem to be problem-\natic because the potential energy VZ1r2 = g m mB B1r2 assumes a given quantization axis along which \nto measure the angular momentum component characterized by the magnetic quantum number m. \nHowever, if the magnetic ﬁeld direction does not change too quickly, then the atom’s Larmor preces-\nsion about the ﬁeld adiabatically follows the changing ﬁeld direction and the atom remains in a weak-\nﬁeld seeking state that is forced toward the origin. This condition holds in most magnetic trapping \nsituations (Problem 16.2). There are some important exceptions, but that is more detail than we need \nfor our brief introduction.\nThe simplest magnetic ﬁeld conﬁguration that produces a magnetic trap is a pair of circular coils \nwith opposing currents. This conﬁguration of anti-Helmholtz coils is shown in Fig. 16.3 with its \nresultant quadrupole magnetic ﬁeld (normal Helmholtz coils have parallel current directions and pro-\nduce a nearly uniform ﬁeld at the center). The magnitude of the magnetic ﬁeld of anti-Helmholtz coils \nis zero of the center of the trap and has a spatial dependence\n \nB1r2 = A2x2 + y2 + 4z2 . \n(16.3)\nThis ﬁeld magnitude increases linearly along any direction from the trap center, but the gradient has \ndifferent values in different directions because of the factor of 4 in Eq. (16.3). The ﬁeld magnitude \nB\nE0\nE\nm \b\t\u00042\nm \b\t\u00041\nm \b\t0\nm \b\t1\nm \b\t2\nweak-field\nseeking states\nstrong-field\nseeking states\nFIGURE 16.2 Zeeman energy levels. States with positive m are attracted to low magnetic ﬁeld \nregions and states with negative m are attracted to high magnetic ﬁeld regions.\n",
    "16.1 Manipulating Atoms with Quantum Mechanical Forces \n505\nalong the x-axis is shown in Fig. 16.4. As noted above, the magnetic ﬁeld direction shown in Fig. 16.3 \nis continuously changing.\nFor the magnetic trap to be useful, it should have enough potential energy depth to conﬁne atoms \nwith a range of kinetic energies, which is determined by the temperature of the ensemble of atoms. The \nthermal spread of energies is Ethermal = kBT, where we ignore factors of order unity (like p, 1>2, etc.). \nThe Landé g-factor and the magnetic quantum number are of order unity, so the potential energy well \ndepth of a magnetic trap is approximately\n \n\u0006Vtrap = mBB max . \n(16.4)\nA typical magnetic trap has a gradient of 100 Gauss>cm and a trapping region of order 1 cm, giving \na maximum ﬁeld of 100 Gauss (atom trappers use Gauss and cm as their standard units, so we follow \nI\nI\nFIGURE 16.3 The opposing currents in a pair of anti-Helmholtz coils produce a \nquadrupole magnetic ﬁeld that traps weak-ﬁeld seeking states at the center of the coils.\n\u00042\n\u00041\n0\n1\n2\nx(cm)\n100\n200\n\u0002B\u0002\u0005(Gauss)\nFIGURE 16.4 The magnitude of the magnetic ﬁeld in a quadrupole \nmagnetic trap increases linearly from the origin of the trap.\n",
    "506 \nModern Applications of Quantum Mechanics\ntheir lead; recall that 1 Gauss = 10-4 Tesla). Equating the trap depth and the thermal energy, we esti-\nmate the temperature of atoms that can be trapped:\n \n T =\n\u0006Vtrap\nkB\n= mB Bmax \nkB\n \n \n =\n1h 1.4 MHz>Gauss21100 Gauss2\n8.62 * 10-5 eV>K\n \n \n =\n0.58 * 10-6 eV\n8.62 * 10-5 eV>K\n \n \n(16.5)\n \n = 7mK .\n \nThat is pretty cold! We could use superconducting coils to provide much higher current. That has been \ndone, but the well depth is still only a few Kelvin. So to trap atoms with magnetic ﬁelds, we must ﬁnd \na way to reduce the temperature (i.e., the translational motion) of the atoms. The force of the magnetic \ntrap itself cannot cool the atoms because it is a conservative force; atoms in the trap speed up and \nslow down (only slightly compared to room temperature motion), but the temperature of the ensemble \nis not reduced. We could use liquid helium to cool the atoms, but that requires expensive cryogenic \ntechniques and cools only into the Kelvin range. A simpler technique, that also allows cooling to the \nmilliKelvin level required for typical magnetic traps, is laser cooling of atoms, which we will discuss \nin the next section.\nThe magnetic trap has become an important research tool in atomic physics. A variety of different \nmagnetic ﬁeld geometries have been designed to optimize the conﬁnement of the atoms, to allow opti-\ncal access of laser beams to the atoms, or to build an array of traps for quantum computing. The best \nknown application is in experiments to achieve Bose-Einstein condensation. The magnetic trap collects \nand conﬁnes atoms that have been cooled with laser cooling (more below). The atoms are then cooled \nfurther by evaporation (like coffee in a mug) in the trap. This slow process takes several seconds, so the \nability to trap the atoms is vital. These experiments are done at very low pressure (high vacuum) so that \nbackground gas atoms do not collide with trapped atoms and knock them out of the trap.\nFinally, it is interesting to note that there are two macroscopic systems that also use magnetic \nﬁelds to trap objects. There is a toy called a Levitron where a spinning magnet is suspended in air \nabove a magnetic base plate. The magnetic ﬁeld is similar to the quadrupole ﬁeld in that there is a \nregion where the ﬁeld is a minimum. The spinning magnet has its magnetic moment aligned against \nthe magnetic ﬁeld of the base plate, much like the weak-ﬁeld seeking states of the atom in the mag-\nnetic trap. The strong magnetic ﬁeld of the base plate tries to ﬂip the spinning magnet over to be \naligned with the ﬁeld, but the torque causes the spinning top to precess about the ﬁeld, like the  Larmor \nprecession of an atom’s magnetic moment. The second macroscopic system is the use of strong \nsuperconducting magnetic ﬁeld gradients to ﬂoat diamagnetic objects, for example, frogs (this was \nannounced in April 1997, but it was not an April Fool’s joke). In a diamagnetic material, an applied \nmagnetic ﬁeld induces a magnetic moment in the material that opposes the applied ﬁeld, again analo-\ngous to the weak-ﬁeld seeking states above.\n 16.1.2 \u0002 Laser Cooling\nOur second example of a quantum mechanical force is the use of lasers to slow down and cool atoms. \nLaser cooling allows us to cool atoms from room temperature or higher down to temperatures below \n1 mK—low enough to be easily conﬁned in a magnetic trap. \n",
    "16.1 Manipulating Atoms with Quantum Mechanical Forces \n507\nThe force that light exerts on matter is known as radiation pressure and comes about because \nlight carries momentum as well as energy. Photons have momentum given by p = h>l = Uk, where \nk = 2p>l is the wave vector. In Chapter 14, we discussed the absorption of a photon by an atom, \nwhereby the energy of the photon causes the electron to be excited to a higher level, and the angular \nmomentum of the photon is taken up by the atom according to the selection rules on the atom’s angular \nmomentum quantum number. We ignored the role of the linear momentum because it is usually quite \nsmall. However, the force of a laser on an atom can be quite large if the right conditions are satisﬁed. \nTo illustrate the conditions required for efﬁcient laser cooling, we use the rubidium atom (Rb) as an \nexample. The relevant parameters for rubidium are shown in Table 16.1.\nWhen an atom of mass M absorbs a photon, the transfer of momentum from the photon to the \natom is\n \n\u0006p = M\u0006v = Uk . \n(16.6)\nThis momentum transfer causes the atom to recoil with a change in velocity of\n \n\u0006v = vr = Uk\nM =\nh\nMl\n . \n(16.7)\nFor a rubidium atom absorbing a 780 nm resonance photon, the recoil velocity vr is 0.6 cm>s, which is \nmuch less than the typical thermal velocity of vT = 280 m>s. So one photon does not impact a rubid-\nium atom signiﬁcantly, just as one mosquito hitting the windshield does not slow down your car. But \nif the atom repeatedly absorbs photons, then the net impact can be large. For a thermal rubidium atom \nto come to rest requires approximately vT>vr \u0005 50,000 recoil kicks. For the atom to absorb this many \nphotons, the atom must return to the same state after each absorption so that it is ready to absorb another \nlaser photon. The best way to achieve this cycle is to start with the atom in the ground state and excite it \nto the ﬁrst excited state so that spontaneous emission returns it to the ground state. Hence, laser cooling \nrequires an atom that behaves like a two-level system and a laser wavelength tuned close to resonance \nwith the primary transition in the atom from the ground state of the atom 0 g9 = 0 19 to the ﬁrst excited \nstate 0 e9 = 0 29. Though no atom is truly a two-level system, there are straightforward laser techniques \nthat allow the two-level model to be applicable in laser cooling experiments, and the atom can be cycled \nthrough the absorption-emission process enough times for radiation pressure to be effective.\nThe cycle of laser absorption and subsequent spontaneous emission that is required for laser cool-\ning of an atom is depicted in Fig. 16.5. The three steps illustrated are: (1) A resonant laser beam is \nincident on an atom in the ground state of the two-level system. (2) The atom absorbs a photon, which \npromotes the electron to the excited state and causes the atom to recoil in the direction of the inci-\ndent laser with momentum change \u0006p = U k. (3) The excited atom decays back down to the ground \nstate via spontaneous emission of another photon. The spontaneous photon is emitted in a random \n direction, so the recoil kick due to the spontaneously emitted photons averages to zero over many \nTable 16.1 Rubidium Laser Cooling Parameters\nResonance Wavelength\nl = 2pc>v21\n780 nm\nResonance Linewidth\n\u0006v = A21\n2p * 6 MHz\nLifetime\nt = 1>A21\n27 ns\nMass\nM\n85 amu = 1.4 * 10-25 kg\nThermal Velocity\nvT = 22kBT>M\n280 m>s\nRecoil Velocity\nvr = Uk>M\n0.6 cm>s\n",
    "508 \nModern Applications of Quantum Mechanics\nabsorption-emission cycles and the average momentum change per complete absorption-emission \ncycle is 8\u0006p9cycle = U\n  k , due only to the momenta of the absorbed photons. Once the atom returns to \nthe ground state, it is ready to absorb another photon and begin the cycle anew. The average absorption-\nemission cycle time is at least as long as the spontaneous emission lifetime of the atom, but that is typi-\ncally nanoseconds, so this process can ﬁnish in much less than one second. Assuming that the minimum \ncycle time is twice the atomic lifetime (e.g., t to absorb a laser photon and t to emit a spontaneous \nphoton), the maximum force on the atom is\n \nFmax = d p\ndt =\n8\u0006p9cycle\n8\u0006t9min \n= U k\n2t\n . \n(16.8)\nThe complete process of photon absorption and emission is called scattering. We refer to the force \ndepicted in Fig. 16.5 as the scattering force to distinguish it from other radiation forces. This force \nis not conservative because the spontaneous emission is an irreversible process. Hence the scattering \nforce differs in a critical way from the magnetic force used to trap atoms described earlier. The good \naspect of this is that the non-conservative nature of the scattering force permits cooling, which is not \npossible with a conservative force. It is important to distinguish slowing from cooling. Individual \natoms are slowed by the scattering force. Cooling requires that we reduce the velocity spread of the \nensemble of atoms, which we’ll explain below.\nThe typical geometry for laser cooling is a laser beam counterpropagating against an atomic beam, \nas shown in Fig. 16.6. The scattering force decelerates the atoms with a maximum acceleration of\n \na\n max = Fmax \nM\n=\nUk\n2 Mt =\nh\n2 Mlt\n . \n(16.9)\n1\n2\n3\n\u0004\np\u0003\u0005\b\t\u0002k\n\np\u0005\b\t\u0002k\nFIGURE 16.5 The laser cooling cycle: (1) A resonant laser beam is incident on a two-\nlevel atom in its ground state. (2) The atom absorbs a photon with the energy going to \nexcite the electron and the momentum causing the atom to recoil. (3) Spontaneous emission \nproduces a photon in a random direction and the atom returns to the ground state.\n",
    "16.1 Manipulating Atoms with Quantum Mechanical Forces \n509\nFor example, the deceleration of a rubidium atom is\n \n a\n max =\nh\n2Mlt\n \n \n =\n16.626 * 10-34  Js2\n2185 amu * 1.66 * 10-27 kg>amu21780 * 10-9 m2127 * 10-9 s2\n \n(16.10)\n \n = 1.11 * 105  m>s2 = 1.14 * 10 4 g . \nEach absorbed photon produces a small momentum change of the atom, but the process is repeated \nso rapidly that the resulting acceleration dominates gravity 1g = 9.8 m>s22 and is sufﬁcient to stop a \nthermal atom within 1 meter (Problem 16.3).\nSo far our description explains only laser slowing or deceleration. Laser cooling requires one \nadditional aspect of the scattering force that we have neglected. The scattering force is velocity depen-\ndent because of the Doppler effect that causes the frequency experienced by a moving atom to be \nshifted from the laser frequency by an amount proportional to the atomic velocity. The Doppler-\nshifted angular frequency of a laser beam with wave vector k as observed by an atom with velocity v is\n \nvAtom = vLaser - k~v . \n(16.11)\nAn atom moving toward the laser source experiences a blue-shifted beam (higher frequency, shorter \nwavelength) and an atom moving away from the laser source experiences a red-shifted beam (lower \nfrequency, longer wavelength), as shown in Fig. 16.7. Because the scattering force relies on the \nOven\nScattered\nphotons\nLaser\nphotons\nv\nk\nFIGURE 16.6 An oven with a small opening produces an atomic beam. The photons \nfrom a counterpropagating resonant laser beam are scattered and the atoms are slowed.\nAtom\nΩ2 \b Ω \u0007 kv\nΩ1 \b Ω \u0004 kv\nv\nFIGURE 16.7 Doppler shifts of copropagating and counterpropagating laser beams. The \nlaser photons are produced in the laboratory with angular frequency v. The moving atom \nobserves these photons shifted up (counterpropagating) or down (copropagating) by kv.\n",
    "510 \nModern Applications of Quantum Mechanics\n resonance of the laser beam with the atomic transition, the motion of the atom has a strong effect on \nthe strength of the scattering force.\nWe quantify the velocity dependence of the scattering force by expressing the force as the \nmomentum change per scattering cycle (absorption-emission cycle) divided by the time for each \ncycle. The cycle time is the inverse of the scattering rate, which is the excitation rate R1S2 we calcu-\nlated in Chapter 14. This results in\n \n Fscatt = d p\ndt =\n8\u0006p9cycle\n8\u0006t9cycle\n \n \n = 1momentum per scattered photon2 * 1scattered photons per second2 \n(16.12)\n \n = U k R1S2 .\n \nSubstituting Eq. (16.11) into the scattering rate from Eq. (14.73), we ﬁnd\n \n R1S2 = 3 I\nc\n B12   f 1vAtom2\n \n \n = 3 I\nc\n B12 \nA21\n2p\n1vLaser - v21 - k~v2\n2 + aA21\n2 b\n2 . \n \n(16.13)\nThe scattering force is then\n \nFscatt1v2 = U k A21\n2\n  I\nI0\n \naA21\n2 b\n2\n1vLaser - v21 - k~v2\n2 + aA21\n2 b\n2 , \n(16.14)\nwhere the characteristic intensity is I0 = 1U v3A21>12pc22. This expression for the scattering force is \nvalid only for incident laser intensities that satisfy I V I0. The valid expression for all intensities is \nthe subject of Problem 16.4.\nThe Doppler shift of the laser beam has two main effects: (1) the laser frequency must be tuned \naway from the resonance frequency v21 to excite moving atoms, and (2) only atoms in a small velocity \nrange experience the radiation pressure. Both of these effects are illustrated in Fig. 16.8, which shows \nthe Maxwellian velocity distribution of rubidium atoms in a thermal beam 1N1v2\f  v3e-v2>v\n 2\nT 2 and the \nvelocity-dependent scattering force for a counterpropagating laser that is tuned 450 MHz below \nthe resonance frequency f21 = v21>2p = c>l21. For this detuning, the laser beam excites rubidium \natoms that are moving toward the laser source at v = 350 m>s (Problem 16.5). The scattering force in \nEq. (16.14) has the same Lorentzian resonance behavior of the excitation rate, with an inherent line-\nwidth \u0006v = A21 = 1>t caused by spontaneous emission. Hence, only atoms in the velocity range \n\u0006v = \u0006v>k about the resonant velocity of 350 m>s experience an appreciable scattering force. For \nrubidium, the spontaneous emission linewidth is \u0006f = \u0006v>2p = 1>2pt = 6 MHz in frequency \nspace, yielding a velocity width\n \n\u0006v = \u0006v\nk\n=\nl\n2pt =\n780 nm\n2p127 ns2 = 4.6 m>s , \n(16.15)\nas indicated in Fig. 16.8. This width is much smaller than the thermal spread of the atomic beam, \nso only a small fraction of the atoms in the beam are decelerated by the scattering force (the force is \nopposite the atomic velocity for a counterpropagating laser beam). For the laser frequency detuning \n",
    "16.1 Manipulating Atoms with Quantum Mechanical Forces \n511\ndepicted in Fig. 16.8, the scattering force decelerates atoms with velocities in the approximate range \n345–355 m>s. These atoms subsequently move at lower velocities and no longer experience the scat-\ntering force, because their new Doppler shift makes the laser photons appear to be off resonance. The \nscattering force thus alters the velocity distribution as shown in Fig. 16.9. The number of atoms in the \nrange 345–355 m>s is depleted and the number of atoms in the range below that is augmented.\nIf our goal is to stop the rubidium atoms in this beam, then we have failed, because the deceleration \ncaused by the scattering force has changed the Doppler shift and taken the atoms away from the initial \nresonance condition. The solution to this problem is straightforward: we change the laser frequency \n0\n200\n400\n600\nv(m/s)\n0\n\u0004200\n\u0004400\n\u0004600\n\u0004800\n\nv\nN(v), Fscatt (v)\nfLaser\u0004f21(MHz)\nN(v)\nFscatt (v)\nFIGURE 16.8 Maxwellian velocity distribution of a rubidium atomic beam at 400°C and the \nmagnitude of the scattering force for a laser tuned 450 MHz below resonance. The narrow width of \nthe scattering force arises from the spontaneous emission line width of the resonance transition.\nfLaser\u0004f21(MHz)\n0\n200\n400\n600\nv(m/s\n0\n\u0004200\n\u0004400\n\u0004600\n\u0004800\nN(v)\nFIGURE 16.9 Atoms in resonance with the detuned laser beam are slowed, depleting the \nnumber of atoms at that velocity and augmenting the number at a slightly lower velocity.\n",
    "512 \nModern Applications of Quantum Mechanics\nto be in resonance with the previously slowed group of atoms (e.g., from fLaser - f21 = -450 MHz \nto fLaser - f21 = -444 MHz). After this group is slowed and falls out of resonance again, we \nrepeat the laser frequency change. In practice, the laser frequency is continuously swept from the \nstarting point toward the resonance frequency f21 to keep the slowing atoms in resonance with the \nlaser beam throughout their journey. This method of compensating for the changing Doppler shift \nis called chirped cooling, in analogy with the changing pitch of a bird’s chirp. From the expression \nfor the scattering force in Eq. (16.14), we see that chirped cooling increases vLaser to keep the term \n1vLaser - v21 + kv2 = 0 as the velocity decreases. The resultant velocity distribution after the fre-\nquency chirp is ﬁnished is shown in Fig. 16.10. Atoms from the initial resonant velocity downward \nare slowed and accumulate near the ﬁnal resonant velocity of the chirp. The ﬁnal velocity distribution \n(at least the part below the initial resonant velocity) is much narrower than the initial distribution, so \nthe atoms have been cooled, not merely decelerated. It is also possible to compensate for the Doppler \nshift and keep slowing atoms in resonance by altering the atomic frequency v21 by applying either a \nspatially varying magnetic or electric ﬁeld that perturbs the atomic energy levels through the Zeeman \neffect or Stark effect, respectively.\nThe laser cooling of an atomic beam illustrated in Fig. 16.10 affects only one of the velocity \ncomponents. Cooling the complete three-dimensional velocity distribution requires scattering forces \nin all three directions. This is achieved with a conﬁguration of six laser beams along the positive and \nnegative Cartesian axes, as shown in Fig. 16.11. This arrangement of laser beams is called optical \nmolasses because it strongly damps the atomic motion, just as molasses damps the motion of a spoon \ndropped into it. At ﬁrst glance, it might appear that the counterpropagating beams of optical molasses \nwould cancel each other out to give no net force. This is true for an atom at rest, but once again the \nDoppler shift of moving atoms plays a key role.\nIn optical molasses, the six laser beams come from the same laser and have the same frequency. \nThe laser is tuned about one line width \u0006v = A21 below the resonance v21 (red detuning). For a mov-\ning atom, the laser beam propagating in the same direction as the atomic velocity is Doppler shifted to \nlower frequencies, taking it farther from resonance, while the laser beam propagating in the opposite \ndirection is Doppler shifted to higher frequencies, bringing it closer to resonance. Hence, the scat-\ntering force from the laser beam counterpropagating to the atom dominates and the atom is slowed \n0\n200\n400\n600\nv(m/s)\n0\n\u0004200\n\u0004400\n\u0004600\n\u0004800\nN(v)\nfLaser\u0004f21(MHz)\nFIGURE 16.10 In chirped laser cooling, the laser frequency is swept from the original detuning \n(–450 MHz) toward the resonance frequency and a wide range of atoms are slowed and accumulate \nnear zero velocity.\n",
    "16.1 Manipulating Atoms with Quantum Mechanical Forces \n513\ndown. The resultant force F+knx1v2 + F-knx1v2 along one of the axes is shown in Fig. 16.12. For the \nlaser frequency detuning shown 1vLaser - v21 \u0005 -A212, the scattering force is approximately a linear \nfunction of velocity for small velocities. The resultant atomic motion in optical molasses is similar to \nthe motion of a particle in a viscous liquid.\nFIGURE 16.11 Optical molasses comprises six laser beams along the Cartesian axes. \nAtoms at the intersection of the six laser beams are strongly cooled in all three dimensions.\n\u00046\n\u00044\n\u00042\n2\n4\n6\nv(m/s)\nFx(v)\nF +x beam\nF\u0004x beam\nFIGURE 16.12 Scattering force as a function of velocity in optical molasses.\n",
    "514 \nModern Applications of Quantum Mechanics\nComparing the scattering force in Fig. 16.12 with the Maxwellian velocity distribution in \nFig. 16.8, we note that the range of velocities that are affected by optical molasses is very small. \nIn a typical experiment, laser cooling of an atomic beam is ﬁrst performed to produce a sample of \natoms with low velocity, as in Fig. 16.10, and then the atoms are further cooled in all three dimen-\nsions in optical molasses. Atoms in optical molasses can be cooled to a temperature of approximately\n100 mK, which provides a sample of atoms that is easily conﬁned in a magnetic trap. This temperature \nlimit, called the Doppler cooling limit, arises from the balance between the cooling force and heat-\ning caused by the random nature of spontaneous emission. The development of these laser cooling \ntechniques resulted in the Nobel Prize for physics in 1997. Laser cooling has been used to improve the \nprecision of atomic clocks, to make precision measurements of gravity, and to create sources of atoms \nthat behave as quantum mechanical waves rather than classical particles. Laser cooling and magnetic \ntrapping were combined in the discovery of Bose-Einstein condensation, which was recognized by the \nNobel Prize for physics in 2001.\n16.2 \u0002 QUANTUM INFORMATION PROCESSING\nOur second example of a modern application of quantum mechanics is quantum information \n processing. We live in the information age. Computers, smart phones, personal digital assistants, GPS \ndevices, and more surround us, whether we want them or not. The explosion of information process-\ning systems has been enabled by the continuing miniaturization of electronic circuits. Every year, \nengineers are able to put more circuits on computer chips. Now that we have entered the nanotechnol-\nogy phase of the information revolution, we are approaching the physical limitation presented by the \natoms that make up the devices. Extrapolation of the miniaturization march would soon have us using \nindividual atoms as memory devices and circuit elements. As we approach the physical size limita-\ntion of the atoms themselves, quantum mechanics must play a role in building and using information \nprocessing devices. This shift is sure to be a disruptive inﬂuence in computing, but it also represents \nan opportunity to take advantage of unique quantum mechanical aspects of information processing.\nThe idea that quantum mechanics could be useful in computing stems in part from a talk and a \npaper by Richard Feynman in the early 1980’s. Feynman asked the question: Can a classical computer \nreliably model a quantum mechanical system? Imagine that we want to model the quantum mechani-\ncal time evolution of a system of 50 spin-1/2 particles. The Hilbert space of this 50-particle system has \n250 states, so the quantum state vector of the system requires 250 \u0005 1015 coefﬁcients to describe a gen-\neral state in the Hilbert space of this system (more details on the numerics later). A 100-particle system \nwould require 2100 \u0005 1030 coefﬁcients and a 300-particle system would require 2300 \u0005 1090 coef-\nﬁcients, which is more than the number of protons in the universe! A computer would have to keep \ntrack of all these coefﬁcients in order to properly account for the particle-particle interactions and their \neffect on the system’s Schrödinger time evolution. So it appears impossible to model the dynamics of \na modestly-sized multiparticle quantum mechanical system because the Hilbert space is so exponen-\ntially large. On the other hand, nature has no trouble managing this large Hilbert space and producing \nthose same dynamics that we are not able to model! This suggests that we let nature, in the form of a \nquantum mechanical system of 50, 100, or 300 particles, be the computer. We let this quantum com-\nputer use its own Schrödinger time evolution to calculate what our classical computer cannot.\nThis conjecture has led to an explosion in the ﬁeld of quantum information processing with \nresearch to uncover the theory of quantum information and to implement some basic experiments to \ndemonstrate the principles. The ﬁeld is too broad and too deep for us to cover thoroughly here, but \nhere is a taste of some of the possibilities, especially as they relate to the ideas you have learned in \nthis text. We’ll introduce the idea of quantum bits to store data and quantum gates to manipulate data. \n",
    "16.2 Quantum Information Processing \n515\nThese elements are required to make a quantum computer, so we’ll brieﬂy discuss some of the quan-\ntum algorithms that make a quantum computer attractive. Then ﬁnally, we’ll discuss how quantum \nteleportation works.\n 16.2.1 \u0002 Quantum Bits—Qubits\nClassical computing relies on binary digits—bits—to store information. Each bit has the value 0 or 1, \nand individual bits are strung together to represent larger binary numbers (for example, 001100101). \nEach binary number represents an actual number or, through coding, some other piece of information \nlike the letter “A.” The job of a classical computer is to store and process bits. Because there are only \ntwo possible states for each bit, many of the tasks required in a classical computer are implemented \nwith simple on-off switches.\nIn quantum information processing, information is stored in quantum bits, or qubits. A qubit is \na quantum system with two possible states, analogous to the 0 and 1 of a classical bit. The canonical \nqubit system is the spin-1/2 system we know and love, with the spin up state 0  +9 and the spin down \nstate 0  -9 playing the roles of the two binary states. But any two-state quantum system can be used as \na qubit. Other common qubit systems include hyperﬁne levels in atoms and polarization states of pho-\ntons. To address all of these diverse systems with the same formalism, we refer to the qubit states as \n0 09 and 0 19, whether the actual states are spin states, atomic states, or photon polarization states. But \nwe will make our discussion concrete when needed by reference to the spin-1/2 system, with the spin \nup state 0  +9 representing 0 09 and the spin down state 0  -9 representing 0 19:\n \n0 09 = 0  +9\n0 19 = 0  -9 . \n(16.16)\nSuperposition states\nThe key difference between bits and qubits is that qubits can exist in superposition states. A general \nqubit superposition is\n \n0 c9 = c00 09 + c10 19 \u0003 ac0\nc1\nb . \n(16.17)\nFor this superposition state, the probability that we measure the system to be in the 0 09 state is\n \nP0 = 0800 c90\n2 = 0 c00\n2, \n(16.18)\nand the probability that we measure the system to be in the 0 19 state is\n \nP1 = 0810 c90\n2 = 0 c10\n2. \n(16.19)\nThis is in stark contrast to a classical bit, which is either 0 or 1 with 100% probability. If that weren’t \nthe case, then our classical computers would not function very well!\nThe probabilistic nature of quantum mechanics doesn’t seem to bode well for the promise of a \nquantum computer. You would not buy a computer if the salesman told you that it would “probably” \nget the right answer. But quantum superposition states are more than simple probability mixtures of \ndifferent possibilities. A quantum superposition state is a coherent combination of states that does \ncontain an aspect of certainty that would be lacking in a classical bit that was only “probably” in the \none state. For example, the spin state\n \n0 c9 = 0  +9x =\n1\n12 0  +9 +\n1\n12 0  -9 \n(16.20)\n",
    "516 \nModern Applications of Quantum Mechanics\nhas 100% probability of being measured to be spin up along the x-axis, even though the probabilities \nof measuring the spin component on the z-axis are 50>50. So whether we view this state as lacking or \nhaving the certainty we expect from our computer depends on our point of view.\nSuperposition states are at the heart of the power of quantum information processing because the \namount of information contained in a quantum system grows exponentially with the number of qubits \nin the system. For example, if we build a system with 2 qubits, labeled A and B, then the basis states of \nthis system are the uncoupled basis states we used in Chapter 11:\n \n 0 009 = 0 09A0 09B\n \n 0 019 = 0 09A0 19B\n \n 0109 = 0 19A0 09B\n \n 0119 = 0 19A0 19B. \n(16.21)\nIn this 2-qubit system, a general superposition state is \n \n0 c9 = c000 009 + c010\n 019 + c100109 + c110119. \n(16.22)\nThis single 2-qubit state contains 22 = 4 pieces of information—the cij coefﬁcients. A classical 2-bit \nstate, such as 01, contains just two pieces of information. For an N-qubit system, a single superposition \nstate contains 2N pieces of information. The classical N-bit system does have 2N possible states, but \nany single state contains just N pieces of information.\nThough the N-qubit superposition state contains 2N pieces of information, it is not possible to mea-\nsure it all. When we measure the state of the system, we destroy much of the information by collapsing \nthe system state vector onto the measured state. For example, if we measure the spin components of \nthe two particles described by Eq. (16.22), we learn which one of the four basis states the system is \nin, just as we would for a classical 2-bit system. Even though there are 2N pieces of information in an \nN-qubit system, it turns out that we can extract only N pieces of classical information through our mea-\nsurements. You might ask whether we can call it information if we cannot know it! This question has \nspawned research into quantum information and how it differs from classical information. The trick of \nquantum computing is to harness the vast store of information that resides in the superposition state, \nbut is hidden from direct measurement. A number of algorithms have been discovered that access the \nhidden quantum information by performing operations that affect many or all of the qubits at once. By \nperforming these multiple operations simultaneously, we achieve quantum parallelism. You can also \nperform parallel computing with classical computers, but you do so by buying more computers!\nEntangled states\nThe power of quantum parallelism relies on the phenomenon of quantum entanglement that we intro-\nduced in Chapter 4. We learned there that entangled quantum states are responsible for the “spooki-\nness” of the Einstein-Podolsky-Rosen paradox. The EPR state 0 c9 =\n1\n12 1 0  +910  -92 - 0  -910  +922 of \nEq. (4.1) is entangled because measurements on one spin are perfectly anti-correlated with measure-\nments on the other spin. The EPR state is a speciﬁc example of the set of 2-qubit entangled states \nknown as Bell states. In terms of the basis states 0\n 009, 0\n 019, 0 109, and 0\n 119 of a 2-qubit system, the \nfour Bell states are\n \n 0\n b009 =\n1\n12 1 0 009 + 01192\n \n 0\n b019 =\n1\n12 1 0 019 + 01092\n \n 0\n b109 =\n1\n12 1 0 009 - 01192\n \n 0\n b119 =\n1\n12 1 0 019 - 01092. \n(16.23)\n",
    "16.2 Quantum Information Processing \n517\nThe EPR state of Chapter 4 is the Bell state 0\n b119. The Bell states comprise an alternate basis to the \nuncoupled and coupled bases we learned in Chapter 11. In quantum computing, we typically use either \nthe Bell basis or the uncoupled basis, which is called the computational basis.\nThe correlations of measurements on the EPR state, and the Bell states in general, show us that \nquantum mechanics is a nonlocal theory. Measuring one of the qubits affects the other, possibly dis-\ntant, qubit instantaneously. Rather than regarding these nonlocal correlations as spooky, we can use \nthem as a resource in quantum information processing. The nonlocal aspect of entangled states is \nuseful because we can act on one part of a system and control another part of the system, and we can \nmeasure one part to learn about another part or about the system as a whole. This is how quantum \nalgorithms are able to process the 2N pieces of information hidden in an N-qubit system. To be useful, \nthe quantum algorithms must be cleverly designed so that the answer we want is contained within the \nN pieces of classical information available through measurements on the system. It is no use having \nmore information available if we cannot access it after the calculation.\nThe importance of entangled states is also evident in our argument about the exponential increase \nin information content of a quantum superposition state. We said that the 2-qubit superposition state \n0\n c9 in Eq. (16.22) contains 22 = 4 pieces of information and that an N-qubit superposition contains \n2N pieces of information. However, there is a caveat to that statement. It turns out that there are some \nsuperposition states that have less information content because they can be expressed as a product of \n1-qubit states. An example of such a 2-qubit product state is\n \n0 c9 = 1a00 09A + a10 19A21b00 09B + b10 19B2. \n(16.24)\nProduct states do not exhibit correlations in measurement and, therefore, they are not entangled states; \nthey behave more like classical states. The 2-qubit state in Eq. (16.24) contains 2 * 2 = 4 pieces \nof information—the ai and bi coefﬁcients. For a general N-qubit system, a superposition state that is \na product state and so is not entangled contains 2 * N pieces of information. Unfortunately, for the \n2-qubit examples we have chosen, 22 = 4 and 2 * 2 = 4 are the same, so the difference between the \n2N exponential information content of general superposition states (which includes entangled states) \nand the 2 * N linear information content of non-entangled states is not immediately evident. We’ll \nleave it to you to explore the N = 3 case in Problem 16.8 and distinguish the difference. The take-\nhome message is that access to the power of quantum parallelism requires the use of entangled states.\nQuantum computing algorithms are designed to process the hidden information in the large Hilbert \nspace in a way that the desired result is brought out in the measured qubits. Two of the most impres-\nsive quantum algorithms are Shor’s factorization algorithm and Grover’s search algorithm. Factoring \na large number into its two prime factors is a difﬁcult task for a classical computer. In 1994, Peter Shor \ndeveloped a quantum algorithm that ﬁnds the prime factors of an integer in a time that is faster than a \nclassical computer by a factor that is exponential in the number of digits of the number being factored. \nBecause of the importance of factoring in encryption, Shor’s algorithm has inspired many to try to build \na quantum computer. Grover’s search algorithm allows a quantum computer to search an unsorted data-\nbase of N entries in a time proportional to 1N, compared to a classical computer that requires a time that \nis proportional to N. Details of these algorithms are available in the resources at the end of the chapter.\nQuantum algorithms are not immune to the probabilistic nature of quantum mechanics. If we run \nthe same program twice on a quantum computer, then we might get two different answers. The power \nof quantum computing is that it can produce answers in many fewer steps than a classical computer. As \nlong as we can easily conﬁrm the answers on a classical computer, then the time advantage overcomes the \nneed to run the program many times. For example, as hard as it is to ﬁnd prime factors of a large number, \nit is trivial to check whether the product of the two proposed factors do in fact yield the original number. \nLikewise, as hard as it is to ﬁnd a needle in a haystack, it is simple to determine if the object you ﬁnd is a \nneedle, so conﬁrming the result of a quantum search algorithm is straightforward on a classical computer.\n",
    "518 \nModern Applications of Quantum Mechanics\n 16.2.2 \u0002 Quantum Gates\nTo process information, a classical computer uses gates that operate on bits. A few typical classical \ngates are shown in Fig. 16.13 along with the truth tables that describe their operation. The NOT gate is \na 1-bit gate with one input bit and one output bit. The AND and OR gates are 2-bit gates with two input \nbits and one output bit. Using a small set of such binary logic gates, albeit a large number of them, \nclassical computers perform a wide range of tasks.\nQuantum computers likewise rely on a small set of 1- and 2-qubit gates to perform their tasks. \nThe measurement devices we have encountered throughout this text, like Stern-Gerlach devices, are \nnot quantum gates. Rather, quantum gates are devices that alter the relative coefﬁcients in a qubit \nsuperposition without destroying the coherence. A 1-qubit gate has an input state 0 cin9 and an output \nstate 0 cout9, which we write as\n \n0 cin9 = c00 09 + c10 19 \n(16.25)\nand\n \n0 cout9 = c =\n00 09 + c =\n10 19. \n(16.26)\nFor any general 1-qubit quantum gate, we represent the transformation from input to output states in \nmatrix notation as\n \n¢\nc =\n0\nc =\n1\n≤= ¢\nU11\nU12\nU21\nU22\n≤¢\nc0\nc1\n≤ . \n(16.27)\nThe transformation matrix U must be a unitary matrix (UU- = 1) to preserve the coherence of the \nqubit. The matrix elements of the transformation tell us how the qubit is changed by the gate. For \nexample, a quantum NOT gate changes 0 09 S 0 19 and also 0 19 S 0 09. The quantum NOT gate is a \nlinear operator, so it also changes a superposition a0 09 + b0 19 S b0 09 + a0 19. The quantum NOT \ngate unitary transformation matrix is\n \nUNOT \u0003 a0\n1\n1\n0b . \n(16.28)\nx    z\n0    1\n1    0\nx    y    z\n0    0    0\n0    1    0\n1    0    0\n1    1    1\nx    y    z\n0    0    0\n0    1    1\n1    0    1\n1    1    1\nx\nz\nNOT gate\nx\ny\nz\nAND gate\nx\ny\nz\nOR gate\nFIGURE 16.13 Classical logic gates.\n",
    "16.2 Quantum Information Processing \n519\nThis unitary operator looks similar to the Sx operator for a spin-1/2 system. That is not a coinci-\ndence. It turns out that all unitary operators for a spin-1/2 system can be expressed as a linear combina-\ntion of the four operators comprising the identity matrix 1 and the three spin-1/2 angular momentum \ncomponent operators, with the factor of U>2 removed. These dimensionless matrices are called the \nPauli matrices and are\n \nsx = a0\n1\n1\n0b     sy = a0\n-i\ni\n0 b     sz = a1\n0\n0\n-1b . \n(16.29)\nThe unitary transformation of a spin-1/2 system also has a convenient geometric interpretation \nas a rotation or a series of rotations of the spin, as affected by the spin precession we discussed in \nChapter 3. For example, the quantum NOT gate is performed by a p rotation about the x-axis, as \ndepicted in Fig. 3.8 for a state that is initially spin up. Let’s now show that this is also true for a \ngeneral initial state.\nExample 16.1 Quantum NOT gate Show that the spin precession transformation of a general \nspin state for a p rotation about the x-axis is equivalent to a quantum NOT gate.\nFor the spin to precess about the x-axis, we apply a magnetic ﬁeld B0 in the x-direction (see \nSection 3.2). The energy states in this applied ﬁeld are 0 {9x and the energies are E{ = {U v0>2, \nwhere v0 = eB0>me is the Larmor precession frequency. To ﬁnd how the state vector is changed by \nthe applied magnetic ﬁeld, we use the Schrödinger time-evolution recipe. The initial general state is\n \n0\n c1029 = c+ 0  +9 + c- 0  -9. \n(16.30)\nWe must write this state in the energy basis, which is the Sx basis in this case:\n \n 0\n c1029 = 1 0  +9x  x8+\n 0 + 0  -9x  x8-  0 20 c1029\n \n \n = c+ 1x8+\n 0  +9 0  +9x + x8-\n 0  +9 0  -9x2 + c-1x8+\n 0  -9 0  +9x + x8-\n 0  -9 0  -9x2 \n \n =\n1\n12 1c+ + c-20  +9x +\n1\n12 1c+ - c-20  -9x .\n \n(16.31)\nTo ﬁnd the time-evolved state, we insert the time-dependent phase factor for each energy basis \nstate:\n \n 0\n c1t29 =\n1\n12 1c+ + c-2e-iE+\n t>U0  +9x +\n1\n12 1c+ - c-2e-iE-\n t>U0  -9x  \n \n =\n1\n12 1c+ + c-2e-iv 0t>20  +9x +\n1\n12 1c+ - c-2e+iv 0t>20  -9x . \n(16.32)\nAs we saw in Eq. (3.35) and Fig. 3.3, the angle of spin precession is v0t, so to have a p rotation \nabout the x-axis requires that the ﬁeld be applied long enough to have v0t = p . Thus the state vec-\ntor after the time evolution is\n \n 0\n c1t29 =\n1\n12 1c+ + c-2e-ip>20  +9x +\n1\n12 1c+ - c-2e+ip>20  -9x\n \n \n =\n-i\n12 1c+ + c-2 1\n12 10  +9 + 0  -92 +\ni\n12 1c+ - c-2 1\n12 10  +9 - 0  -92 \n(16.33)\n \n = -i 1c- 0  +9 + c+ 0  -92,\n \n \nor in matrix notation:\n \nac =\n+\nc =\n-\nb = -i a0\n1\n1\n0bac+\nc-\nb . \n(16.34)\n",
    "520 \nModern Applications of Quantum Mechanics\nThe overall phase e-ip>2 = -i does not produce any measurable effects, so we ignore it in deﬁning \nthe quantum NOT gate transformation matrix:\n \nUNOT \u0003 a0\n1\n1\n0b . \n(16.35)\nA schematic diagram of this spin-precession experiment is shown in Fig. 16.14. The unitary spin \nprecession is performed by the magnet (box with “X ”), while the Stern-Gerlach devices perform \nmeasurements, which are nonunitary transformations. (Recall from SPINS Lab 4 that the number \n“18” in the magnet box rotates the spin by 180°.)\nRotations due to spin precession about the other Cartesian axes produce two more 1-qubit gates. \nThe quantum Z gate is a p rotation around the z-axis, with a transformation matrix (Problem 16.10)\n \nUZ \u0003 a1\n0\n0\n-1b \n(16.36)\nthat is equal to the Pauli sz matrix. The quantum Y gate is a p rotation around the y-axis, with a trans-\nformation matrix (Problem 16.11)\n \nUY \u0003 a0\n-i\ni\n0 b \n(16.37)\nthat is equal to the Pauli sy matrix.\n?\n?\n18\n^n\nX\nZ\ny\nx\nz\nΩ0t\na)\nb)\n\u0004S(0)\u0003\n\u0004S(t)\u0003\nFIGURE 16.14 (a) A Stern-Gerlach spin precession experiment and (b) the resulting precession \nof the spin vector around the x-axis for the case of a p rotation.\n",
    "16.2 Quantum Information Processing \n521\nOne other important 1-qubit gate is the Hadamard gate, with a transformation matrix\n \nUH \u0003\n1\n22\n a1\n1\n1\n-1b . \n(16.38)\nThe Hadamard gate can be made with a p rotation around the z-axis followed by a p>2 rotation around \nthe y-axis (Problem 16.12). The Hadamard gate transforms basis states into superposition states:\n \n UH0\n 09 =\n1\n12 10\n 09 + 0 192  \n \n UH0 19 =\n1\n12 10\n 09 - 0 192 . \n \n(16.39)\nGiven the importance of superposition states in quantum information processing, this is a useful \ngate. Note that the symbol “H ” is used for the Hadamard gate, and it must not be confused with the \nHamiltonian.\nThough we have explained the unitary transformations of 1-qubit gates in terms of the precession \nof a spin-1/2 particle in a magnetic ﬁeld, these same transformations apply to any two-level system. \nThe physical mechanisms for effecting the transformations are different, but the matrices describ-\ning them are the same. For example, pulses of light can transform an atom into a superposition of \nstates to effect a Hadamard gate. Figure 16.15 depicts a general Stern-Gerlach spin precession experi-\nment (a) using our schematic diagram from the SPINS program and (b) using a simpliﬁed schematic \nused for describing quantum information processing in general. The quantum Z gate performs the UZ \ntransformation and the quantum X gate (NOT gate) performs the UNOT = UX transformation. The \nStern-Gerlach measurement devices are not quantum gates because they do not perform a unitary \ntransformation, so we do not depict them in Fig. 16.15(b).\nThe quantum gates we have described so far are all 1-qubit gates, but the power of quantum \ninformation processing resides in entangled superposition states, so multiqubit gates are required. It \n?\n?\n18\n^n\nX\nSpin precession\nState\npreparation\nState\nmeasurement\nLoad\nqubit\nQuantum gates\nRead\nqubit\na) Stern-Gerlach schematic\nb) Quantum computing schematic\n18\nZ\nZ\nX\nq\nin\nq\nout\nY\nFIGURE 16.15 (a) A Stern-Gerlach spin precession experiment and (b) the equivalent experiment \ndepicted with quantum gates.\n",
    "522 \nModern Applications of Quantum Mechanics\nturns out that we can perform all the quantum tasks we need with 1-qubit gates and one type of 2-qubit \ngate. The 2-qubit gate we need is a Controlled-NOT gate (CNOT gate). A CNOT gate has two input \nqubits, referred to as the control and target qubits, and two output qubits. The target qubit is negated \n(by a 1-qubit NOT gate) if the control qubit is in state 0 19C . If the control qubit is in state 0\n 09C , then \nthe target qubit is unchanged. In both cases, the control qubit is unaltered by the gate. We denote the \ntwo-qubit states as 0 i j9 = 0 i9C 0   j9T and the transformations of the CNOT gate are\n \nUCNOT 0 009 = 0 009\nUCNOT 0 019 = 0 019\nUCNOT 0 109 = 0 119\nUCNOT 0 119 = 0 109.\n \n(16.40)\nThe transformation matrix of a CNOT gate is (Problem 16.13)\n \nUCNOT \u0003 ±\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n≤, \n(16.41)\nand the transformation of a general 2-qubit state is\n \nUCNOT 0\n c9 \u0003 §\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n¥ §\nc00\nc01\nc10\nc11\n¥ = §\nc =\n00\nc =\n01\nc =\n10\nc =\n11\n¥ . \n(16.42)\nA schematic diagram of a CNOT gate is shown in Fig. 16.16. The 1-qubit NOT (X) gate acts on the \ntarget bit based upon the condition of the control bit. The conditional connection is depicted by the \nvertical line and node connecting the control qubit with the NOT gate.\nThe physical implementation of a CNOT gate is more complicated than the 1-qubit gates described \nabove. The conditional connection between the two qubits requires an interaction between the two \nphysical qubits. For example, two spin-1/2 particles can interact through their magnetic moments, \ncausing a coupling of the Larmor precession frequencies. \nOne of the most important applications of a CNOT gate is to make entangled states. To make an \nentangled 2-qubit state, like an EPR state, we combine a 1-qubit Hadamard gate and a 2-qubit CNOT \ngate, as shown in Fig. 16.17. The Hadamard gate acts on the input control qubit to place it into a super-\nposition state, then the CNOT gate couples the two qubits together to make an entangled state.\nX\n\u0002j'\u0003Target\n\u0002j \u0003Target\n\u0002i \u0003Control\n\u0002i \u0003Control\nFIGURE 16.16 A 2-qubit controlled-NOT gate has a 1-qubit NOT gate (X ) on \nthe target qubit, which is conditionally activated based upon the control qubit.\n",
    "16.2 Quantum Information Processing \n523\nExample 16.2 Entangled state preparation Show that the combination of a Hadamard gate \nand a CNOT gate (Fig. 16.17) acting on the input state 0 119 produces an entangled Bell state.\nThe input state of the system is\n \n0\n c19 = 0 119 = 0 19C0\n 19T . \n(16.43)\nThe Hadamard gate acts only on the control qubit, with the result\n \n0\n c29 = UHad,C 0\n c19 = 1UHad,C 0 19C2 0 19T . \n(16.44)\nThe transformation of the single control qubit is\n \n UHad,C0\n 19C \u0003\n1\n12\n a1\n1\n1\n-1ba0\n1b  \n \n \u0003\n1\n12\n a 1\n-1b\n \n(16.45)\n \n =\n1\n12\n 1 0\n 09C - 0\n 19C2 . \nThe resultant state of the 2-qubit system before the CNOT gate is\n \n0\n c29 =\n1\n12\n 1 0\n 09C - 0 19C20\n 19T =\n1\n12\n 1 0\n 019 - 0 1192 \u0003\n1\n22\n ±\n0\n1\n0\n-1\n≤ . \n(16.46)\nThe transformation of the CNOT gate is\n \n0\n c39 = UCNOT 0\n c29 \u0003\n1\n22\n ±\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n≤±\n0\n1\n0\n-1\n≤= ±\n0\n1\n-1\n0\n≤ . \n(16.47)\nThe output state is thus\n \n0\n c39 =\n1\n12\n 1 0\n 019 - 0 1092. \n(16.48)\nThis is the entangled Bell state 0 b119 from Eq. (16.23).\nH\nX\n\u0002Ψ1\u0003\n\u0002Ψ2\u0003\n\u0002Ψ3\u0003\n\u0002i \u0003\n\u0002Βij\u0003\n\u0002j \u0003\u0005\nFIGURE 16.17 Preparation of an entangled Bell state by \napplication of a Hadamard gate (H ) and a controlled-NOT gate.\n",
    "524 \nModern Applications of Quantum Mechanics\nNow we see why we labeled the Bell states as we did. The particular Bell state produced in \nExample 16.2 is labeled 0\n b119 because we started with the input state 0 119. The four Bell states are pro-\nduced by using one of the basis states 0\n 009, 0\n 019, 0\n 109, or 0\n 119 as the input into the combined Hadamard \nand CNOT gates (Problem 16.14).\nThe beauty of the 2-qubit CNOT gate is that it lets us transform between the computational basis \nand the Bell basis, when combined with the 1-qubit Hadamard gates. If we reverse the order of the \nHadamard and CNOT gates, as shown in Fig. 16.18, then Bell states are transformed into computa-\ntional basis states (Problem 16.15). Hence, to determine which Bell state a system is in, we perform the \ntransformation in Fig. 16.18 and then measure the single qubits (e.g., the z-components of the spins). \nThe four possible results ij = 00, 01, 10, 11 then correspond to the four Bell states of Eq. (16.23). \nThis is called a Bell-state measurement.\n 16.2.3 \u0002 Quantum Teleportation\nUsing the tools we have described above, we now illustrate the use of entangled states (quantum \nspookiness) as a resource. The problem we want to solve is how to transmit information about an \nunknown quantum state. Imagine that Carol has given Alice a “secret” message in the form of a single \nqubit that she wants Alice to transmit to Bob. Without giving the qubit directly to Bob, how can \nAlice convey the information with the highest probability of success? The answer lies in utilizing \nentangled states, as depicted in Fig. 16.19. In a nutshell, Alice and Bob share an entangled state of \ntwo qubits that was previously prepared and is independent of Carol’s secret message qubit. Alice \nperforms a  Bell-state measurement on the two-state system comprising Carol’s qubit and Alice’s half \nof the entangled state she shares with Bob. Alice than transmits the results of her measurement to Bob \nwho performs a unitary transformation on his half of the entangled state, and voilà, his qubit is in the \nsame state as Carol’s secret message. Let’s see how this works in detail.\nAlice and Bob have previously met and share an entangled state, meaning that each has one of \nthe two qubits of a Bell state, which we assume to be the 0\n b009 state. Using explicit subscripts to dis-\ntinguish the different qubits held by Alice (A), Bob (B), and Carol (C), we denote the entangled state \nshared by Alice and Bob as\n \n0\n b009AB =\n1\n12 10\n 009AB + 0 119AB2 =\n1\n12 0\n 09A0\n 09B +\n1\n12 0\n 19A0\n 19B . \n(16.49)\nThe secret qubit that Carol wants Alice to convey to Bob is in a general, unknown superposition state\n \n0\n csecret9C = a00\n 09C + a10\n 19C . \n(16.50)\nH\nX\n\u0002Βij\u0003\n\u0002Ψ1\u0003\n\u0002Ψ2\u0003\n\u0002Ψ3\u0003\n\u0002i \u0003\n\u0002j\u0003\nFIGURE 16.18 Transformation of a Bell state to the computational \nbasis with a CNOT gate and a Hadamard gate.\n",
    "16.2 Quantum Information Processing \n525\nIf Alice had many copies of this state, she could make repeated measurements and determine the coef-\nﬁcients a0 and a1 with a statistical uncertainty based on the number of copies (as you did in SPINS \nlab 1). But with only one copy of the state, Alice is hard pressed to make a meaningful measurement of \nthe state and send the secret message to Bob.\nAlice’s solution is to make a joint measurement on the system comprising the secret qubit C and \nthe single qubit A of the entangled 0\n b009AB state that she shares with Bob. By a joint measurement, we \nmean that she performs a Bell-state measurement by applying a CNOT gate and a Hadamard gate to \nthe A and C qubits to transform to the computational basis (Fig. 16.18) and then measuring the single \nqubits. To see why Alice’s Bell-state measurement is useful, consider the state vector for the complete \nthree-qubit system\n \n 0\n cABC9 = 0\n  b009AB0\n  csecret9C\n \n \n = A 1\n12 0\n 09A0\n 09B +\n1\n12 0 19A0 19BBAa00\n 09C + a10 19CB\n \n \n =\na0\n12 0\n 09A0\n 09B0\n 09C +\na0\n12 0 19A0 19B0\n 09C +\na1\n12 0\n 09A0\n 09B0 19C +\na1\n12 0 19A0 19B0 19C . \n(16.51)\nThe qubits A and C are not entangled (they have never interacted), but we are free to write the system \nstate vector in terms of the basis of entangled Bell states 0 bij9AC of those two qubits. Some algebra \nreveals that the state vector of the system expressed in this way is (Problem 16.16)\n \n 0\n cABC9 = 1\n2\n 5 0\n b009AC  1a00\n 09B + a10 19B2 \n \n + 0\n b019AC  1a10\n 09B + a00 19B2\n \n \n + 0\n b109AC  1a00\n 09B - a10 19B2\n \n \n + 0\n b119AC  1a10\n 09B - a00 19B26. \n \n(16.52)\n00\n01\n10\n11\n00\n01\n10\n11\nBOB: Unitary Transformation\nALICE:  Bell State Measurement\nCAROL\nC\nA\nB\nClassical\nInformation\n\u0002Ψsecret\u0003B\n\u0002Ψsecret\u0003C\n\u0002Β00\u0003AB\nFIGURE 16.19 Quantum teleportation of a secret qubit from Alice to Bob. Alice and Bob share \nthe entangled qubit pair AB. Alice makes a Bell-state measurement upon the AC qubit pair. Alice \ntransmits the result, 10 for example, to Bob, who applies the appropriate unitary transformation  \n(see Table 16.2) to his qubit B, which is then in the same state as the original secret qubit C.\n",
    "526 \nModern Applications of Quantum Mechanics\nBy expressing the state vector in this Bell basis, we identify a correlation between each Bell state of \nthe qubits A and C and the state of Bob’s qubit B, which turns out to be a superposition state with the \nsecret coefﬁcients from Carol! For example, if Alice’s Bell-state measurement indicates that the A and \nC qubits are in the state 0\n b009AC , then Bob’s qubit B is in the state\n \n0\n c9B = a00\n 09B + a10 19B. \n(16.53)\nThis is exactly the secret state that Carol gave to Alice. There are four possible results of Alice’s \nBell-state measurement, each with a probability of 25%, indicating that the qubits A and C are indeed \nnot entangled. If Alice measures one of the other Bell states, then she communicates her results to Bob \nover a classical channel (she calls him on the phone) and tells him to perform a unitary transformation \non his qubit to change it to the secret state. The transformations that Bob must perform are indicated in \nTable 16.2 (Problem 16.17).\nWith this quantum teleportation scheme, Alice has conveyed Carol’s secret message to Bob using \nonly a classical information channel, and the prearranged Bell state 0\n b009AB. Note that neither Alice \nnor Bob know what the secret state is. Alice has destroyed all her qubits by measuring them, and her \nresults reveal no information to her about the secret message. Bob has not measured anything yet, but \nhas the secret qubit in his possession as long as he does what Alice tells him to do. More precisely, he \nhas a qubit that is in the same state as Carol’s original qubit. The actual physical qubit representing the \nsecret message (e.g., a particle with spin) is still with Alice, or destroyed in detection. Only the quan-\ntum information about the state of the secret qubit has been teleported to Bob.\nThis scheme is made possible by the Bell state that Alice and Bob have set up previously. The \ncorrelations inherent in that entangled state allow Alice to tell Bob what quantum gates he must use to \ntransform his half of their Bell state into the secret message. This is one of many examples that demon-\nstrate the utility of entangled quantum states for information processing.\nSUMMARY\nThese two examples have provided a mere taste of the fun you can have with quantum mechanics. \nMagnetic trapping, laser cooling, and quantum information processing are just a few of the current \nresearch topics that employ the quantum mechanics you have learned in this text. If our brief overview \nhas raised more questions than we have answered, then we have at least planted the seed for you to \ndelve deeper into these subjects. As with any research ﬁeld, there are still more questions to be raised \nand answers to be discovered. Enjoy!\nTable 16.2 Quantum Teleportation of a Secret State from Alice to Bob\nAlice measures\nAlice transmits\nBob applies\nBob transforms\n0b009\n00\n1\n11a0009B + a1019B2 = 0csecret9B\n0b019\n01\nUNOT\nUNOT 1a1009B + a0019B2 = 0csecret9B\n0b109\n10\nUZ\nUZ 1a0009B - a1019B2 = 0csecret9B\n0b119\n11\nUZUNOT\nUZ\n UNOT 1a1009B - a0019B2 = 0csecret9B\n",
    "Problems \n527\nPROBLEMS\n 16.1 Calculate the angular deﬂection of a room temperature rubidium atom traveling through the \nmagnetic ﬁeld gradient of a Stern-Gerlach device. Assume the gradient is 100 G>cm = 1 T>m \nand that the magnetic moment is one Bohr magneton.\n 16.2 Show that for an atom in a typical magnetic trap, the magnetic moment adiabatically follows \nthe changing magnetic ﬁeld direction. That is, show that the Larmor precession frequency \nis much larger than the frequency of motion in the trap. Estimate the motional frequency by \nconsidering the circular motion (radius 1 cm) of a rubidium atom in the trapping potential \nshown in Fig. 16.4.\n 16.3 Find the distance required to stop a room-temperature rubidium atom with the resonant scat-\ntering force. Do the same for a sodium atom.\n 16.4 The general expression for the scattering force that is valid for all intensities is\nFscatt = Uk A21\n2  I\nI0\n \naA21\n2 b\n2\n1vLaser - v21 + kv2\n2 + aA21\n2 b\n2\na1 + I\nI0\nb\n ,\n \n where I0 is a characteristic intensity. Show that this force has the same maximum value given \nby Eq. (16.8) and state the conditions required to achieve that maximum force. Plot the force \nas a function of intensity and suggest a name for I0.\n 16.5 Show that rubidium atoms with velocity v = 350 m>s are resonant with a counterpropagat-\ning laser with a frequency detuning fLaser - f21 = -450 MHz.\n 16.6 Calculate the maximum chirp rate (frequency change per unit time interval) of a laser used in \nchirped laser cooling of rubidium atoms.\n 16.7 Calculate the linear friction coefﬁcient of optical molasses, (i.e., ﬁnd the slope of the force \ncurve in Fig. 16.12 for low velocities).\n 16.8 For an N-qubit system, a general superposition state (which includes entangled states) \n contains 2N pieces of information [see Eq. (16.22)] and a product superposition state \n (nonentangled) contains 2 * N pieces of information [see Eq. (16.24)]. Demonstrate this \nfor N = 3 and N = 4.\n 16.9 Verify the operation of a quantum NOT gate by acting on the computational basis states with \nthe unitary matrix UNOT. Demonstrate that the transformation matrix is unitary by showing \nthat the norm of a general superposition state is unchanged by the transformation.\n 16.10 Show that the transformation matrix for a p rotation about the z-axis is the Pauli matrix sz. \n(Hint: as in Example 16.1, ignore an overall phase.)\n 16.11 Show that the transformation matrix for a p rotation about the y-axis is the Pauli matrix sy . \n(Hint: as in Example 16.1, ignore an overall phase.)\n 16.12 Show that the Hadamard gate for a spin-1/2 system can be made with a p rotation about the \nz-axis followed by a p>2 rotation about the y-axis.\n 16.13 Using the transformation equations of the CNOT gate in Eq. (16.40), derive the transforma-\ntion matrix in Eq. (16.41).\n 16.14 Show that the combination of a Hadamard gate and a CNOT gate (see Fig. 16.17) transforms \nthe computational basis states 0\n 009, 0\n 019, and 0 109 into Bell states.\n",
    "528 \nModern Applications of Quantum Mechanics\n 16.15 Show that the combination of a CNOT gate and a Hadamard gate (see Fig. 16.18) transforms a \nBell state into a computational basis state, for each of the possible Bell states.\n 16.16 Show that the complete state vector for Alice, Bob, and Carol’s qubits can be written as in\nEq. (16.52).\n 16.17 Show that the transformations that Alice asks Bob to do (see Table 16.2) produce the secret \nstate.\nRESOURCES\nFurther Reading\nThese references provide further details on laser cooling and magnetic traps for atoms:\nH. J. Metcalf and P. van der Straten, Laser Cooling and Trapping, New York: Springer \n University Press, 1999.\n \nC. J. Foot, Atomic Physics, Oxford: Oxford University Press, 2005.\n \nM. Fox, Quantum Optics: An Introduction, Oxford: Oxford University Press, 2006.\nB. H. Bransden and C. J. Joachain, Physics of Atoms and Molecules, 2nd ed., Harlow, England: \nPrentice Hall, 2003.\nC. Wieman, G. Flowers, and S. Gilbert, “Inexpensive laser cooling and trapping experiment for \nundergraduate laboratories,” Am. J. Phys. 63, 317–330 (1995).\n \nP. Gould, “Laser cooling of atoms to the Doppler limit,” Am. J. Phys. 65, 1120–1123 (1997).\nThe Nobel Prize in Physics 1997, Nobelprize.org: \n \nhttp://nobelprize.org/nobel_prizes/physics/laureates/1997/\nThese references provide further details on magnetic traps for macroscopic objects:\n \nA. Geim, “Everyone’s Magnetism,” Phys. Today 51(9), 36–39 (1998).\nM. D. Simon, L. O. Heﬂinger, and S. L. Ridgway, “Spin stabilized magnetic levitation,” Am. J. \nPhys. 65, 286–292 (1997).\n \nM. V. Berry and A. K. Geim, “Of ﬂying frogs and levitrons,” Eur. J. Phys. 18, 307–313 (1997).\nM. V. Berry, “The LevitronTM: an Adiabatic Trap for Spins,” Proc. R. Soc. Lond. A 452, \n 1207–1220 (1996).\nT. B. Jones, M. Washizu, and R. Gans, “Simple theory for the Levitron,” J. Appl. Phys. 82, \n883–888 (1997).\nThese references provide further details on quantum information processing:\n \nR. P. Feynman, “Simulating Physics with Computers,” Int. J. Theor. Phys. 21, 467–488 (1982).\n \nS. M. Barnett, Quantum Information, Oxford: Oxford University Press, 2009.\nP. Kaye, R. Laﬂamme, and M. Mosca, An Introduction to Quantum Computing, Oxford: Oxford\nUniversity Press, 2007.\nM. A. Nielsen and I. L. Chuang, Quantum Computation and Quantum Information, Cambridge: \nCambridge University Press, 2000.\n",
    " \n529\nA P P E N D I X\nA\n \nProbability\nQuantum mechanics is inherently a probabilistic theory, so we present here a brief review of some \nimportant concepts in probability theory. We distinguish between discrete probabilities, encountered \nin spin measurements, and continuous probabilities, encountered in position measurements.\nA.1 \u0002 DISCRETE PROBABILITY DISTRIBUTION\nImagine collecting together all the grades that students received in your English class last term. You \nﬁnd that the students received 8 A’s, 14 B’s, 7 C’s and 1 D. Though these are not random events, you \ncould still ask, what is the probability of receiving an A? A classmate received an A grade nA = 8 \ntimes out of the 30 total students, so the probability is the ratio\n \nPA =\nnA\nnA + nB + nC + nD\n= 8\n30. \n(A.1)\nYou calculate all four probabilities and represent them in a histogram, such as shown in Fig. A.1. \nThis set of probabilities is a discrete probability distribution. In this case, the distribution has been \ndetermined by experiment. In some cases, such as throwing dice, the probability distribution can be \ncalculated theoretically and compared to experiment.\nIn the general case, we label the possible results (e.g., grades) xi, and if there are N possible \nresults that can occur, then the probability of any one result is\n \nPxi =\nnxi\na\nN\ni=1\nnxi\n. \n(A.2)\nA\nB\nC\nD\nGrade\n0.5\n1.0\nPxi\nPA\nPB\nPC\nPD\nFIGURE A.1 The histogram of grades received in an English class.\n",
    "530 \nProbability\nThe sum of the individual probabilities must be one because you are certain to get some result. The \ngeneral statement of this condition is\n \na\nN\ni=1\nPxi = 1. \n(A.3)\nOf course, the most obvious use of the grade probability distribution is for calculating a grade \npoint average (GPA). Given the standard assignment of grade points A = 4, etc., the class GPA is\n \n GPA = 4nA + 3nB + 2nC + 1nD\nnA + nB + nC + nD\n \n \n = 4PA + P3PB + 2PC + 1PD. \n(A.4)\nIn the general case, we calculate the average or mean of the possible results using\n \n8x9 = a\nN\ni=1\nxi Pxi  , \n(A.5)\nwhere we use the angled brackets 89 to denote the average. In quantum mechanics, the average is \nreferred to as the expectation value, which is a bit misleading because it is not the value you expect to \nget. In fact, the expectation value is in general not one of the possible results. The class GPA may be \n3.14, but no student received that value as a grade.\nWe also quantify probability distributions by the spread of the distribution. The most used measure \nof the spread is the standard deviation s, deﬁned as the square root of the average of the squares of the \ndeviations from the average! Once more, slowly: (1) ﬁnd the deviation of each result xi from the average \nvalue 8x9; (2) square the deviations (to avoid negative values); (3) average all possible squared devia-\ntions, weighted by the probabilities of each result, as in Eq. (A.5); and (4) take the square root. This is also \ncalled the root-mean-square deviation, or rms deviation. Mathematically, the standard deviation is\n \ns = 281x - 8x9229 = B a\nN\ni=1\n1xi - 8x92\n2\n Pxi. \n(A.6)\nThe variance s2 is the square of the standard deviation.\nThere is a useful shortcut for evaluating the standard deviation of a probability distribution. Con-\nsider the variance:\n \ns2 = H1x - 8x92\n2I = a\nN\ni=1\n1xi - 8x92\n2 Pxi. \n(A.7)\nExpand the square\n \n s2 = a\nN\ni=1\nAx2\ni - 2 xi8x9 + 8x9\n2BPxi\n \n \n = a\nN\ni=1\nx2\ni  Pxi - a\nN\ni=1\n2xi8x9Pxi + a\nN\ni=1\n8x9\n2Pxi \n(A.8)\n \n = a\nN\ni=1\nx2\ni  Pxi - 28x9 a\nN\ni=1\nxi Pxi + 8x92 a\nN\ni=1\nPxi \n",
    "and use the deﬁnition of the average in Eq. (A.5) and the normalization condition in Eq. (A.3) to get\n \n s2 = 8x29 - 28x98x9 + 8x9\n2 \n \n = 8x29 - 8x9\n2.\n \n(A.9)\nSo the variance is also the difference between the average of the squares and the square of the average, \nwhere the average of the squares of the possible results is\n \n8x29 = a\nN\ni=1\nx 2\ni  Pxi. \n(A.10)\nNote that the square 8x9\n2 of the average and the average 8x29 of the squares are not generally equal. In \nfact, Eq. (A.9) implies that the 8x9\n2 = 8x29 only if the variance is zero, which happens only if there \nis no spread in the distribution, that is, there is only one possible result. Using Eq. (A.9), we write the \nstandard deviation as\n \ns = 28x29 - 8x92  . \n(A.11)\nIn quantum mechanics, we use the standard deviation for the uncertainty, and we use the symbol \u0006x \nor \u0006p or \u0006Sz instead of s.\nA.2 \u0002 CONTINUOUS PROBABILITY DISTRIBUTION\nIf the possible results of an experiment form a continuum rather than a discrete set, then we must \nmodify some of the deﬁnitions from the last section. Rather than speaking of a probability for a spe-\nciﬁc result, we must speak of the probability for a range of results within some interval. For example, \nif you were a product tester and were charged with specifying how long the battery lasts on a laptop \ncomputer, then you might make a series of measurements of the time it takes for the laptop to drain the \nbattery. Time is a continuum, but if you made measurements to the nearest minute, then the histogram \nof probability results would have bins of 1 minute on the time axis, as indicated in Fig. A.2(a) where \nwe use the nontraditional convention of labeling time with x to follow our notation in the last section. \nFor small enough intervals, you would expect that the probability Pxi6x6xi+\u0006x of obtaining a result \nx (min)\n3550\n3600\n3650\n0.00\n0.01\n0.02\n(a)\nPxi\nFIGURE A.2 (a) The discrete probability distribution of battery lifetimes expressed as a histogram, \nand (b) the continuous probability distribution of lifetimes expressed as a function.\nx\n3550\n3600\n3650\nx (min)\n0.00\n0.01\n0.02\n(b)\nP\nA.2 Continuous Probability Distribution \n531\n",
    "532 \nProbability\nwithin the time interval xi 6 x 6 xi + \u0006x is proportional to the width of the interval and to a factor \ntelling you the likelihood of results in that interval. We express this as\n \nPxi6x6xi+\u0006x = P1xi2\u0006x, \n(A.12)\nwhere P1xi2 is the likelihood factor. Because the probability Pxi6x6xi+\u0006x is a dimensionless number \nand \u0006x has dimensions (x could be time, height, velocity, etc.), the likelihood factor P1xi2 must have \ndimensions of 1>x. We call this the probability density P1x2 because it is the probability per unit \ntime (or height or velocity, etc.). We distinguish the probability density from a probability by denot-\ning it as a function rather a subscripted value. The probability density is a continuous probability \n distribution, in contrast to the discrete probability distribution in the previous section. For the battery \nexperiment, the continuous probability distribution is shown in Fig. A.2(b).\nFor a continuous probability distribution, the condition that the sum of the individual probabili-\nties must be one becomes an integral\n \nL\n\u0005\n- \u0005\nP1x2  dx = 1. \n(A.13)\nThe average or expectation value is\n \n8x9 =\nL\n\u0005\n- \u0005\nx P1x2  dx \n(A.14)\nand the expectation value of any other function of the measurement variable is\n \n8 f 1x29 =\nL\n\u0005\n- \u0005\nf  1x2P1x2  dx. \n(A.15)\nThe standard deviation is still deﬁned by Eq. (A.11)\n \ns = 28x29 - 8x92, \n(A.16)\nwith the new deﬁnition of the average in Eq. (A.14).\n",
    " \n533\nA P P E N D I X \nB\nComplex Numbers\nComplex numbers are a critical component of the mathematics of quantum mechanics, so we provide \na brief review here. Complex numbers are an extension of the real numbers to include an additional \nimaginary part. The imaginary number i is the square root of –1:\n \ni = 2-1. \n(B.1)\nA complex number has a real part and imaginary part and is written as\n \nz = a + ib, \n(B.2)\nwhere this form assumes that a and b are real values. We refer to a as the real part of z and b as the \nimaginary part of z, and denote them as\n \n a = Re 1z2  \n \n b = Im 1z2. \n \n(B.3)\nWhen we add two complex numbers together, we must keep the real and imaginary parts separate:\n \nz1 + z2 = 1a1 + ib12 + 1a2 + ib22 = 1a1 + a22 + i1b1 + b22. \n(B.4)\nThis makes it clear that the real and imaginary parts are the “apples and oranges” that you are often \ntold not to mix together. In fact, a complex number contains two independent pieces of information, \nmuch like the components of a vector. We even represent a complex number in a similar way, as \nshown in Fig. B.1.\nVisualization of complex numbers in this “complex plane” can be very powerful. The horizontal \naxis in Fig. B.1 corresponds to the real part of a complex number and the vertical axis corresponds to \nthe imaginary part. Expressing the complex number as z = a + ib corresponds to using the Cartesian \nrepresentation. Figure B.1 also suggests that a polar representation is useful and that the radius r and \nthe angle u could also characterize a complex number.\nHow do we connect the Cartesian and polar representations mathematically? Consider the expo-\nnential of a complex number. The Taylor series expansion of a complex exponential is\n \neiu = 1 + 1iu2 + 1\n2!\n 1iu2\n2 + 1\n3!\n 1iu2\n3 + 1\n4!\n 1iu2\n4 + 1\n5!\n 1iu2\n5 + ... . \n(B.5)\nEvaluating the powers of the imaginary number i results in half of the terms of the expansion being \nreal (the even powers) and half being imaginary (the odd powers). Moreover, alternating signs arise \nfrom i 2 = -1 and i 4 = +1, yielding\n \neiu = a1 - 1\n2!\n u2 + 1\n4!\n u4 - ...b + i au - 1\n3!\n u3 + 1\n5!\n u5 + ...b . \n(B.6)\n",
    "534 \nComplex Numbers\nThe bracketed terms are the Taylor series expansion for the cosine and sine functions, giving the \nfamous Euler’s formula:\n \neiu = cos u + i sin u  . \n(B.7)\nUsing Euler’s formula, the polar representation of a point in the complex plane is\n \nz = reiu = r cos u + ir sin u, \n(B.8)\nas depicted in Fig. B.1. We refer to r as the modulus or magnitude and u as the phase or argument. \nWe connect the Cartesian and polar viewpoints by equating the real and imaginary parts of Eqs. (B.2) \nand (B.8), giving\n \n a = r cos u \n \n b = r sin u, \n(B.9)\nwhich agrees with trigonometry. The inverse relations are\n \n r = 2a2 + b2 \n \n u = tan-1ab\nab . \n(B.10)\nCare must be exercised when ﬁnding the polar angle because the inverse tangent function is multi-\nvalued. However, the real and imaginary parts of a complex number separately determine the cosine \nand sine of the polar angle, so the correct quadrant is determined by using Eq. (B.10) in conjunction \nwith Eq. (B.9). You should practice converting numbers between Cartesian and polar forms, and use \nwhichever form is most convenient. For example, the complex number i is one unit along the imagi-\nnary axis in the Cartesian form. In the polar form, i is the number 1 rotated by p>2 from the real axis, \nReal Axis\nImaginary Axis\nr\nΘ\na\nb\nFIGURE B.1 Complex plane.\n",
    "Complex Numbers \n535\nthus i = eip>2. Likewise, the complex number \u00111 is one unit along the negative real axis in Cartesian \nform. In polar form, \u00111 has magnitude 1 and is rotated (has phase) p from the real axis, thus -1 = eip.\nThe polar representation of complex numbers makes multiplication and division easy:\n \n z1z 2 = r1eiu1r2eiu2 = r1r2e i(u1+u2) \n \n z1\nz2\n= r1eiu1\nr2eiu2 = r1\nr2\n e i (u1-u2). \n \n(B.11)\nAddition and subtraction are easier in the Cartesian representation [Eq. (B.4)].\nComplex numbers have a unique operation known as complex conjugation, which is deﬁned by \nchanging i S -i. We say that z* is the complex conjugate of z:\n \nz* = a - ib = re -iu. \n(B.12)\nIn the complex plane, this operator corresponds to reﬂection through the real axis. A complex number \nmultiplied by its own complex conjugate yields the square of its modulus or magnitude:\n \n \u0004 z \u0004\n2 = z z* = 1a + ib21a - ib2 = a2 + b2 \n \n = 1reiu21re -iu2 = r 2,\n \n \n(B.13)\nsometimes called the complex square. The resultant modulus from Eq. (B.13) agrees with the geo-\nmetric result in Eq. (B.10). The complex square is also a handy device to express a complex fraction \nin standard Cartesian form. For example, multiplying the numerator and denominator by the complex \nconjugate of the denominator places all the imaginary numbers in the numerator:\n \n w =\n1\na + ib = a\n1\na + ibb aa - ib\na - ibb = a - ib\na2 + b2 \n \n =\na\na2 + b2 - i \nb\na2 + b2 .\n \n(B.14)\nThis is standard form with\n \n Re 1w2 =\na\na2 + b2  \n \n Im 1w2 =\nb\na2 + b2 . \n(B.15)\nA particularly useful case is a = 0, b = 1, which gives 1>i = -i.\nComplex notation can also be used to make trigonometric manipulations much easier, even when \ncomplex numbers are not really needed. Euler’s formula can be inverted to express trigonometric \nfunctions in terms of complex exponentials\n \n  cos u = eiu + e -iu\n2\n \n \n  sin u = eiu - e -iu\n2i\n \n(B.16)\nthat are very handy. For example, consider the trigonometric identity\n \nsin 1a + b2 = sin a cos b + cos a sin b. \n(B.17)\n",
    "536 \nComplex Numbers\nIt can be derived using Eq. (B.16):\n \n sin 1a + b2 = ei(a+b) - e -i(a+b)\n2i\n \n \n = eiaeib - e -iae -ib\n2i\n \n \n = 1\n2i\n 51cos a + i sin a21cos b + i sin b2 - 1cos a - i sin a21cos b - i sin b26 (B.18)\n \n = 1\n2i\n 52i sin a cos b + 2i cos a sin b6\n \n \n =  sin a cos b + cos a sin b . \nThat is a useful trick when you can’t ﬁnd your trigonometry book!\n",
    " \n537\nA P P E N D I X\nC\n \nMatrices\nWe present some of the basic deﬁnitions and properties of matrices necessary to implement the matrix \nformulation of quantum mechanics. We adopt the Dirac bra-ket notation used throughout the text. We \nadopt the quantum mechanical viewpoint that matrices are representations of operators or states and so \nwe use the \u0003 notation where appropriate to mean “is represented by.”\nA matrix is an ordered array of numbers:\n \nA \u0003 •\nA11\nA12\nA13\ng\nA21\nA22\nA23\ng\nA31\nA32\nA33\ng\nf\nf\nf\nf\nμ , \n \n(C.1)\nwhere the subscript labels the rows and columns:\n \nAij =  Matrix element in the ith row and jth column. \n \n(C.2)\nA vector is a special case of a matrix with only one column or row. A column vector\n \n0 a9 \u0003 •\na1\na2\na3\nf\nμ \n \n(C.3)\nrequires only one subscript to label its elements. A row vector has its elements arranged in a row\n \n8b0 \u0003 1b1\nb2\nb3\n g\n 2. \n \n(C.4)\nTo add matrices, we add the corresponding elements:\n \nCi j = Ai j + Bi j. \n \n(C.5)\nFor example, given the two matrices\n \nA \u0003 ¢a\nb\nc\nd≤,   B \u0003 ¢e\nf\ng\nh≤, \n(C.6)\n",
    "538 \nMatrices\ntheir sum is\n \nA + B \u0003 ¢a\nb\nc\nd≤+ ¢e\nf\ng\nh≤= ¢a + e\nb + f\nc + g\nd + h≤ . \n \n(C.7)\nFor addition, the two matrices must have the same size and shape and the result is the same size and shape.\nMatrix multiplication is more complicated. If we multiply two matrices A and B to form a third \nmatrix C, then the elements of the matrix C are\n \nCi j = a\nn\nk=1\nAi k Bk j . \n \n(C.8)\nFor example, given the two matrices\n \nA \u0003 ¢a\nb\nc\nd≤ ,   B \u0003 ¢e\nf\ng\nh≤ , \n(C.9)\ntheir product is\n \nAB \u0003 ¢a\nb\nc\nd≤¢e\nf\ng\nh≤= ¢ae + bg\na f + bh\nce + dg\nc f + dh≤ . \n \n(C.10)\nIn general, Eq. (C.8) tells us that to ﬁnd the matrix element Cij in the ith row and jth column of C, take \nthe ith row of the matrix A and overlay it on top of the jth column of the matrix B. Multiply each pair of \noverlaid numbers and sum the products. For this to make any sense, the number of elements in a row \nof A must equal the number of elements in a column of B, which means that the number of columns in \nA must equal the number of rows in B. Thus, if A is an / * n matrix and B is an n * m matrix, then the \nproduct C = AB is an / * m matrix. Matrix multiplication is not commutative, that is\n \nAB \u0002 BA \n \n(C.11)\nin general.\nThe rules of matrix multiplication make it clear that multiplication of a column vector by a matrix \nyields another column vector\n \nA0 a9 = 0 b9 \n \n(C.12)\nand multiplication of a row vector and a matrix yields another row vector\n \n8c0 A = 8d0 , \n \n(C.13)\nbut each must occur in the order shown. The product of a row vector and a column vector in the \n“proper” bra-ket order is an inner product\n \n18b0 21 0 a92 = 8b0 a9 \n \n(C.14)\nor a scalar product because the result is a scalar. For example,\n \n8b 0\n a9 = 1r\ns\nt2°\nu\nv\nw\n¢ = r u + s v + t w. \n \n(C.15)\nThe product of a row vector and a column vector in the “wrong” ket-bra order is an outer product\n \n1 0 a9218b0 2 = 0 a98b0 , \n \n(C.16)\n",
    "which is a matrix. For example,\n \n@ a98b @ \u0003 °\nu\nv\nw\n¢ 1r\ns\nt2 = °\nur\nus\nut\nvr\nvs\nvt\nwr\nws\nwt\n¢ . \n \n(C.17)\nThe transpose of a matrix is obtained by interchanging rows and columns. In component nota-\ntion this means that\n \n1AT2ij = A ji. \n \n(C.18)\nFor example, if the matrix A is\n \nA \u0003 ¢a\nb\nc\nd≤ , \n \n(C.19)\nthen the transpose AT is\n \nAT \u0003 ¢a\nc\nb\nd≤ . \n(C.20)\nA matrix is called symmetric if it is equal to its transpose, A = AT. The transpose of a column vector \nis a row vector.\nThe Hermitian conjugate (or adjoint) of a matrix is obtained by transposing the matrix and \ncomplex conjugating each element. We denote the Hermitian conjugate with a dagger -. In compo-\nnent notation, the Hermitian conjugate is\n \n1A-2i j = A* \nj i . \n(C.21)\nFor example, if the matrix A is\n \nA \u0003 ¢a\nb\nc\nd≤ , \n(C.22)\nthen the Hermitian conjugate A- is\n \nA- \u0003 ¢a*\nc*\nb*\nd*≤ . \n(C.23)\nA matrix is called Hermitian (or self-adjoint) if it is equal to its Hermitian conjugate, A = A-. In \nquantum mechanics, all operators that correspond to physical observables are Hermitian operators.\nThe determinant of a matrix is deﬁned as the sum of the products of the elements of any row (or \ncolumn) with the cofactors of those elements. The cofactor of an element Aij of a matrix is the product \nof the factor (-1)i+j and the determinant of the submatrix obtained by striking out the row and column \ncontaining Aij. For example, the determinant of the 2 * 2 matrix A in Eq. (C.22) is\n \n det (A) = ` a\nb\nc\nd ` = ad - bc. \n(C.24)\nThe determinant of a 3 * 3 matrix is\n \n  det (A) = †\na\nb\nc\nd\ne\nf\ng\nh\ni\n† = a1-121+1 ` e\nf\nh\ni\n ` + b1-121+2 ` d\nf\ng\ni\n ` + c1-121+3 ` d\ne\ng\nh `  \n \n = a 1ei - f  h2 - b 1di - fg2 + c 1dh - eg2.\n \n(C.25)\nMatrices \n539\n",
    "540 \nMatrices\nThe eigenvalues and eigenvectors of a matrix are found by solving the eigenvalue problem:\n \nA0 a9 = l0 a9, \n(C.26)\nwhere l are the eigenvalues and 0 a9 are the eigenvectors. The eigenvalue equation has a solution \nwhen the determinant of the coefﬁcients of the homogeneous equations is zero:\n \n det 1A - l12 = 0 , \n(C.27)\nthat is\n \n det •   \nA11 - l\nA12\nA13\ng\nA21\nA22 - l\nA23\ng\nA31\nA32\nA33 - l\ng\nf\nf\nf\nf\nμ = 0 . \n(C.28)\nThe resulting equation is the characteristic equation and its solution yields the eigenvalues of the \nmatrix. For an n * n matrix, the characteristic equation is an nth order equation and yields n solutions, \nthough some may be degenerate or equal. To ﬁnd the eigenvectors of the matrix, we substitute each \neigenvalue in turn into the eigenvalue equation (C.26) and solve for the corresponding eigenvector.\nRESOURCES\nActivities\nMath Primer Course: A weeklong course that reviews matrix algebra and frames the discussion of \nmatrices in the context of vectors spaces and linear transformations. The course includes several stu-\ndent activities.\nwww.physics.oregonstate.edu/portfolioswiki/courses:home:prhome\n",
    " \n541\nA P P E N D I X\nD\nWaves and Fourier Analysis\nD.1 \u0002 CLASSICAL WAVES\nA classical wave in one dimension is represented by a function f 1x, t2 that is a solution of the classical \nwave equation\n \n0 2 f 1x, t2\n0 x 2\n= 1\nv2 \n0 2 f 1x, t2\n0 t2\n, \n(D.1)\nwhere v is the wave speed. This equation is applicable to water waves, waves on a string, electro-\nmagnetic waves, and other types of classical waves. Any function of the form f 1x { vt2 satisﬁes \nthis equation and represents a wave moving in the positive (for x - vt argument) or negative (for \nx + vt argument) x direction. The wave equation obeys the linear superposition principle, so any two \nsolutions can be added to form another valid solution. Because of this, we typically focus on the har-\nmonic or sinusoidal solutions and then use the Fourier principle to construct any general solution when \nneeded.\nA sinusoidal wave is periodic in space and in time, as shown in Fig. D.1, and is characterized by \nthe spatial period, or wavelength, l, and by the temporal period T. We write the sinusoidal wave as\n \nf 1x, t2 = A sin c 2p a x\nl - t\nTb + dd , \n(D.2)\nwhere A and d are the amplitude and phase constant, respectively, required to produce a general solu-\ntion to the second-order differential wave equation. It is standard practice to write the sinusoidal wave \nin a simpler form by using the wave vector k and the angular frequency v, given by\n k = 2p\nl\n \n v = 2p\nT . \n(D.3)\nThus we get\n \nf 1x, t2 = A sin1kx - vt + d2. \n(D.4)\nThe velocity of a point of ﬁxed phase on this harmonic wave is found by the condition\n \n d1phase2 = 0\n \n d1kx - vt + d2 = 0\n \n kdx - vdt = 0 , \n(D.5)\n",
    "542 \nWaves and Fourier Analysis\nyielding the phase velocity\n \nvphase = dx\ndt\n2\nfixed phase\n= v\nk = l\nT\n . \n(D.6)\nIn many cases, the phase velocity is referred to simply as the velocity of the wave.\nThe relation between the wave vector k and the angular frequency v\n \nv = v1k2 \n(D.7)\nis called the dispersion relation. We typically treat the wave vector k as the independent variable. If \nthe (phase) velocity is constant, independent of the wave vector, then we say that there is no dispersion \nin the system. If the velocity is not constant, then waves with different wave vectors (i.e., different \nwavelengths) move at different speeds and a general wave composed of different harmonic solutions \nwill disperse as it propagates. In that case, the motion of the superposition or wave packet is character-\nized by the group velocity\n \nvg =\ndv1k2\ndk\n2\nk0\n, \n(D.8)\nwhere k0 is the peak of the wave-vector distribution comprising the wave packet. The group velocity is \nthe same as the phase velocity if there is no dispersion.\nFor mathematical convenience, we often use the complex form of the sinusoidal wave\n \nei(kx-vt) = cos1kx - vt2 + i sin1kx - vt2, \n(D.9)\nnoting that we must take the real part at the end of the classical calculation, because we measure only \nreal quantities. Quantum mechanics uses complex numbers, so we focus on the complex form of the \nclassical wave.\nx\nf(x,t)\nf(x,t)\nΛ\nt\nT\n(a)\n(b)\nFIGURE D.1 A classical wave, showing (a) the wavelength \nin space and (b) the period in time, for the choice of the phase \nconstant d = 0.\n",
    "D.2 \u0002 FOURIER ANALYSIS\nFourier analysis is the decomposition of a general wave or oscillation into harmonic components. \nBecause we treat the wave vector as the independent variable of a wave, the Fourier decomposition \nis typically done in terms of wave vectors. A Fourier series is a sum of sinusoidal functions, each of \nwhich is a harmonic of some fundamental wave vector or spatial frequency. A Fourier transform is an \nintegral over a continuous distribution of sinusoidal functions.\nA Fourier series is appropriate when the system has boundary conditions that limit the allowed \nwave vectors to a discrete set. For a system where the spatial periodicity is 2L, the Fourier decomposi-\ntion of a general periodic function is the series\n \nf 1x2 =\na\n\u0005\nn= - \u0005\ncneikn\n x, \n(D.10)\nwhere the allowed wave vectors are\n \nkn = np\nL\n . \n(D.11)\nThe expansion coefﬁcients cn in Eq. (D.10) are complex. The real version of the Fourier expansion is\n \nf 1x2 = a0\n2 + a\n\u0005\nn=1\nc an cosanpx\nL b + bn sinanpx\nL b d . \n(D.12)\nThe expansion coefﬁcients an\n , bn\n , cn are obtained by calculating the overlap integrals (i.e., projections \nor inner products) of the desired function with the harmonic basis functions\n \n an = 1\nL L\n2L\n0\nf 1x2 cosanpx\nL b  dx \n \n bn = 1\nL L\n2L\n0\nf 1x2 sinanpx\nL b  dx \n(D.13)\n \n cn = 1\n2L L\n2L\n0\nf 1x2e-ikn\n x dx . \nA Fourier transform is appropriate when the system has no boundary conditions that limit the allowed \nwave vectors. In this case, the Fourier decomposition is an integral over a continuum of wave vectors:\n \nf 1x2 =\n1\n12p L\n\u0005\n- \u0005\na1k2eik x dk, \n(D.14)\nwhere the expansion function a1k2 is complex. To obtain the expansion function a1k2 for a given\nspatial function f 1x2 requires the inverse Fourier transform\n \na1k2 =\n1\n12p L\n\u0005\n- \u0005\nf 1x2e-ikx\n dx, \n(D.15)\nwhich is a projection of the spatial function f 1x2 onto the harmonic basis functions eikx> 12p. The \nbasis functions are orthogonal and normalized in the Dirac sense, which means their projections onto \neach other are Dirac delta functions\n \n1\n2p L\n\u0005\n- \u0005\neik\u0004xe-ikx\n dx = d1k - k\u00042 \n \n1\n2p L\n\u0005\n- \u0005\neikx\u0004e-ikx\n dk = d1x - x\u00042, \n(D.16)\nwhether viewed in the position representation or the wave-vector representation.\nD.2 Fourier Analysis \n543\n",
    "544 \nWaves and Fourier Analysis\nSome typical Fourier transform pairs are shown in Fig. D.2 and are listed here (without proper \nscale factors):\n \n f 1x2 = eik0\n x 3 a1k2 = d1k - k02\n \nsinusoid \ndelta function\n \n f 1x2 = eik0\n xe-x 2>2s2 3 a1k2 = e-s2(k-k0)2>2\n \nGaussian  \nGaussian\n \n f 1x2 = eik0\n xe-0  x 0>s 3 a1k2 =\n1\n1 + s21k - k02\n2  \n(D.17)\n \nexponential\n \nLorentzian\n \n  f 1x2 = eik0\n x;0 x0 6s 3 a1k2 =\nsin5s1k - k026\ns1k - k02\n. \n \nsquare pulse\n \nsinc \n(a)\n(b)\n(c)\n(d)\nk0\nk0\nk0\nk0\nk\nx\nInfinite wave\nDelta function\n\u000e\nGaussian\nGaussian\n\u000e\nExponential\nLorentzian\n\u000e\nSquare pulse\nSinc function\n\u000e\nFIGURE D.2 Fourier transform pairs: (a) Inﬁnite wave 3  delta \nfunction, (b) Gaussian 3  Gaussian, (c) exponential 3  Lorentzian, \n(d) square pulse 3  sinc function.\n",
    "In each case, a1k2 and f 1x2 are Fourier transforms of each other following Eqs. (D.14) and (D.15). \nIn Fig. D.2, only the real part of the function f 1x2 is plotted and each wave has a central wavelength \nl0 = 2p>k0.\nThe spatial extent \u0006x of a function f 1x2 and the width \u0006k of the Fourier transform a1k2 in wave-\nvector space are inversely related through the uncertainty relation\n \n\u0006k\u0006x Ú 1. \n(D.18)\nThis relation tells us that if want to make a wave that is conﬁned to a small region of space, we need to \nuse a wide range of wave vectors. In quantum mechanics, this concept is the Heisenberg uncertainty \nrelation. To describe a wave f 1x, t2, we replace x with x - vt in the Fourier decomposition of the func-\ntion f 1x2, as long as there is no dispersion. This means that the wave retains its initial shape as it moves. \nWhen a system has dispersion, this replacement is no longer valid. The different speeds of the different \nwave-vector components of the superposition f 1x, t2 cause the shape of the wave to change as it propa-\ngates. The expansion function a1k2 remains the same and the time dependent wave is represented by\n \nf 1x, t2 =\n1\n22p L\n\u0005\n- \u0005\na1k2ei(kx-v(k)t) dk . \n(D.19)\nWe must recalculate the integral at each time to learn how the wave shape evolves.\nParseval’s theorem says that the power is the same whether calculated in position space or wave-\nvector space:\n \nL\n\u0005\n- \u0005\n0  f 1x2 0\n2dx =\nL\n\u0005\n- \u0005\n0 a 1k2 0\n2\n dk . \n(D.20)\nD.3 \u0002 QUANTUM MECHANICS\nIn quantum mechanics, we describe systems using momentum as the variable rather than wave vector, \nbut the Fourier ideas are similar. Converting from wave-vector space to momentum space requires \nsome care with the units:\n \n  f 1x2 =\n1\n12p L\n\u0005\n- \u0005\na1k2eik x  dk =\n1\n12p L\n\u0005\n- \u0005\na1 p>U2ei( p>U)x d1 p>U2 \n \n =\n1\n12ph L\n\u0005\n- \u0005\n1\n1U\n a1 p>U2ei(p>U)x dp . \n(D.21)\nThis tells us that the amplitudes in wave-vector space and momentum space are related by\n \nf1 p2 =\n1\n1U\n a1 k = p>U2. \n(D.22)\nHence, we arrive at the quantum mechanical version of the Fourier transform that connects wave func-\ntions in position space and momentum space:\n \n c1x2 =\n1\n22p U L\n\u0005\n- \u0005\nf\n  1 p2  eipx>U dp, \n(D.23)\n \n f1 p2 =\n1\n22p U L\n\u0005\n- \u0005\nc\n 1x2e-ipx>U dx. \n(D.24)\nD.3 Quantum Mechanics \n545\n",
    "546 \nWaves and Fourier Analysis\nParseval’s theorem applied to quantum mechanics says that the probability normalization condi-\ntion is the same whether calculated in position space or momentum space:\n \n1 =\nL\n\u0005\n- \u0005\n0 c1x2 0\n2\n dx =\nL\n\u0005\n- \u0005\n0 f\n 1 p2 0\n2\n  dp. \n(D.25)\nThe Heisenberg uncertainty relation relates the spatial extent \u0006x of a probability density 0 c1x2 0\n2 \nand the width \u0006p of the momentum space probability distribution 0 f1p2 0 2 in a manner analogous to \nEq. (D.18):\n \n\u0006p\u0006x Ú U\n2. \n(D.26)\nThis relation tells us that if want to make a wave function that is conﬁned to a small region of space, \nthen we must use a wide range of momenta. Hence, we cannot speak of a quantum mechanical system \nwith a well-deﬁned position and a well-deﬁned momentum.\n",
    " \n547\nA P P E N D I X \nE\nSeparation of Variables\nThe separation of variables procedure permits us to simplify a partial differential equation by \nseparating out the dependence on the different independent variables and creating multiple ordinary \ndifferential equations. To illustrate the method, we apply a six-step process to the classical wave equa-\ntion to show how the time dependence of the wave function can be found through a separate ordinary \ndifferential equation. The scalar wave equation is:\n \n\u00022u1r, t2 - 1\nv2 \n02u1r, t2\n0t 2\n= 0, \n(E.1)\nwhere v is the wave speed.\nTo separate the time dependence from the spatial dependence, the six steps are:\nStep 1: Write the partial differential equation in an appropriate coordinate system. For the wave \nequation, we choose Cartesian coordinates (this is not crucial in this example because we are separat-\ning only the time dependence):\n \n0 2u\n0x 2 + 0 2u\n0y2 + 0 2u\n0z 2 - 1\nv2 0 2u\n0t 2 = 0. \n(E.2)\nStep 2:  Assume that the solution u 1x, y, z, t2 can be written as the product of functions, at least \none of which depends on only one variable, in this case t. The other function(s) must not depend at all \non this variable, that is, assume\n \nu1x, y, z, t2 = S1x, y, z2T 1t2. \n(E.3)\nPlug this assumed solution into the partial differential equation Eq. (E.2). Because of the special \nform for u1x, y, z, t2, the partial derivatives each act on only one of the functions in u1x, y, z, t2.\n \nT1t20 2S1x, y, z2\n0 x 2\n+ T 1t20 2S1x, y, z2\n0 y2\n+ T 1t20 2 S1x, y, z2\n0 z 2\n- 1\nv 2 S1x, y, z2d 2T 1t2\ndt 2\n= 0. \n(E.4)\nAny partial derivatives that act only on a function of a single variable may be rewritten as total \nderivatives.\nStep 3: Divide by u1x, y, z, t2 in the form of Eq. (E.3):\n \n1\nS1x, y, z2 e\n0 2S1x, y, z2\n0x 2\n+\n0 2S1x, y, z2\n0y2\n+\n0 2S1x, y, z2\n0z 2\nf - 1\nv2 1\nT(t) d 2T1t2\ndt 2\n= 0. \n(E.5)\n",
    "548 \nSeparation of Variables\nStep 4: Isolate all of the dependence on the chosen separation variable (t) on one side of the\nequation. Do as much algebra as you need to do to achieve this. In our example, this is straightforward:\n \n1\nS1x, y, z2\n e 0 2 S1x, y, z2\n0 x 2\n+ 0 2 S1x, y, z2\n0 y2\n+ 0 2 S1x, y, z2\n0z 2\nf = 1\nv2 1\nT 1t2 d 2T 1t2\nd t 2\n. \n(E.6)\n \n(1111111111111111111111111)111111111111111111111111* \n(11111)11111*\n \nfunction of space only \nfunction of time only\nStep 5: Now imagine changing the isolated variable t by a small amount. In principle, the right-\nhand side of Eq. (E.6) could change as t changes, but nothing on the left-hand side would because there \nis no time dependence. Therefore, if the equation is to be true for all values of t, the particular combi-\nnation of t dependence on the right-hand side must be constant. We call this constant -k 2 (because we \nalready know what the answer is):\n \n1\nS1x, y, z2\n e\n0 2S1x, y, z2\n0 x 2\n+\n0 2S1x, y, z2\n0 y2\n+\n0 2S1x, y, z2\n0 z 2\nf = 1\nv2 1\nT 1t2 \nd 2T 1t2\ndt2\n = -k 2. \n(E.7)\nIn this way we have broken our original partial differential equation up into a pair of equations, one of \nwhich is an ordinary differential equation involving only t, the other is a partial differential equation \ninvolving only the three spatial variables:\n \n \n1\nS1x, y, z2\n e 0 2S1x, y, z2\n0 x 2\n+\n0 2S1x, y, z2\n0 y2\n+\n0 2S1x, y, z2\n0 z 2\nf = -k 2, \n(E.8)\n \n1\nv2 1\nT 1t2 \nd 2T 1t2\ndt 2\n = -k 2. \n(E.9)\nThe separation constant -k 2 appears in both equations.\nStep 6: Write each equation in standard form by multiplying each equation by its unknown \nfunction to clear it from the denominator:\n \n \n0 2S1x, y, z2\n0 x 2\n+\n0 2S1x, y, z2\n0 y2\n+\n0 2S1x, y, z2\n0 z2\n = -k2S1x, y, z2, \n(E.10)\n \n1\nv2 \nd 2T1t2\ndt 2  = -k 2 T 1t2. \n(E.11)\nWe have now separated the time dependence from the spatial dependence. Equation (E.11) is \nan ordinary differential equation for the time dependent part T1t2 of the complete wave function \nu1x, y, z, t2 = S1x, y, z2T 1t2. Equation (E.10) is still a partial differential equation for the space depen-\ndent part S1x, y, z2 of the complete wave function u1x, y, z, t2 = S1x, y, z2T 1t2. The six steps of this \nprocedure can be applied again to separate the different spatial parts of S1x, y, z2 into three separate \nordinary differential equations.\n",
    " \n549\nA P P E N D I X\nF\nIntegrals\nA small collection of useful integrals is listed below. You may already be accustomed to using Maple \nor Mathematica to do integrals, which is not too different than looking up an integral in this table. But \nbe careful to not become too reliant on the computer. For example, if the computer tells you the answer \nis zero, then maybe that should have been obvious from examining the symmetry of the integrand.\n \n \nL\nsin mx sin  nx dx =\n sin 1m - n2x\n2 1m - n2\n-\n sin 1m + n2x\n2 1m + n2\n, 1m2 \u0002 n22 \n(F.1)\n \n \nL\ncos mx cos nx dx =\nsin 1m - n2x\n2 1m - n2\n+\nsin 1m + n2x\n2 1m + n2 , 1m2 \u0002 n22 \n(F.2)\n \n \nL\nsin mx cos nx dx = - \ncos 1m - n2x\n2 1m - n2\n-\ncos 1m + n2x\n2 1m + n2\n, 1m2 \u0002 n22 \n(F.3)\n \n \nL\nsin2 ax dx = 1\n2\n  x - 1\n2a\n  sin ax cos ax \n(F.4)\n \n \nL\ncos2 ax dx = 1\n2\n x + 1\n2a\n  sin ax cos ax \n(F.5)\n \n \nL\nsin ax cos ax dx = 1\n2a\n  sin2 ax \n(F.6)\n \nL\nsin ax cosm ax dx = - cos m +  1 ax\n1m + 12a \n(F.7)\n \nL\nsinm ax cos ax dx = sinm+1 ax\n1m + 12a \n(F.8)\n \nL\nx sin ax dx = 1\na2\n  sin ax - x\na\n  cos ax \n(F.9)\n \nL\nx cos ax dx = 1\na2\n  cos ax + x\na\n  sin ax \n(F.10)\n \nL\nx2 sin ax dx = 2x\na2\n  sin ax - a2x 2 - 2\na3\n  cos ax \n(F.11)\n",
    "550 \nIntegrals\n \nL\nx2 cos ax  dx = 2x\na2 cos ax + a2x 2 - 2\na3\n sin ax \n(F.12)\n \nL\nx sin2 ax  dx = x 2\n4 - x\n4a\n sin 2ax -\n1\n8a2 cos 2ax \n(F.13)\n \nL\nx cos2 \n ax dx = x 2\n4 + x\n4a\n sin 2ax +\n1\n8a2 cos 2 ax \n(F.14)\n \nL\nx2 sin2 ax dx = x3\n6 - a x2\n4a -\n1\n8a3bsin 2 ax -\nx\n4a2 cos 2 ax \n(F.15)\n \nL\nx2 cos2 ax dx = x3\n6 + a x2\n4a -\n1\n8a3bsin 2 ax +\nx\n4a2 cos 2 ax \n(F.16)\n \nL\nxe-x dx = -xe-  x - e-  xa \n(F.17)\n \nL\nx 2e-x dx = -x 2e-x - 2xe-x - 2e-x \n(F.18)\n \nL\nx3e-x dx = -x 3e-x - 3x 2e-x - 6xe-x - 6e-x \n(F.19)\n \nL\nx4e-x dx = -x4e-x - 4x 3e-x - 12x 2e-x - 24xe-x - 24e-x \n(F.20)\n \nL\n\u0005\n0\nx ne-ax  d x =\nn!\nan +1 \n(F.21)\n \nL\n\u0005\n0\ne-a2\n x 2 d x = 1\n2a1p \n(F.22)\n \nL\n\u0005\n- \u0005\ne-a2x2+bx  dx = 1p\na\n eb2>4a2 \n(F.23)\n \nL\n\u0005\n0\nxe-  x 2  dx = 1\n2 \n(F.24)\n \nL\n\u0005\n0\nx 2e-x 2  dx = 1p\n4  \n \n(F.25)\n \nL\n\u0005\n0\nx 2ne-x 2\n dx = 1p 12n2!\nn!\n  \n1\n22 n +1 \n(F.26)\n \nL\n\u0005\n0\nx 2n+1e-x 2\n dx = n!\n2  \n(F.27)\n",
    " \n551\nA P P E N D I X\nG\nPhysical Constants\nThese values are taken from: “CODATA recommended values of the fundamental physical constants: \n2006,” P. J. Mohr, B. N. Taylor, and D. B. Newell, Rev. Mod Phys. 80, 633–730 (2008). Experimental \nuncertainties are shown in parentheses.\nQuantity \nSymbol \nValue\nSpeed of light in vacuum   \nc \n299 792 458 m >s 1Exact2 \nPermeability of free space \nm0 \n4p * 10-7 N # s2>C2 1Exact2 \nPermittivity of free space \ne0 = 1>m0c2 \n8.854 187 817... * 10-12 C2>N # m2 1Exact2\nPlanck>s constant \nU \n6.582 118 99 1162 * 10-16 eV # s\n \n \n1.054 571 628 1532 * 10-34  J # s\n \nh = 2pU \n4.135 667 33 1102 * 10-15 eV # s\n \n \n6.626 068 96 1332 * 10-34 J # s\nElementary charge\ne\n1.602 176 487 1402 * 10-19 C\nElectron mass \nme\n0.510 998 910 1132 MeV>c2\n \n \n9.109 382 15 1452 * 10-31 kg\nProton mass \nmp \n938.272 013 1232 MeV>c2\n \n \n1.672 621 637 1832 * 10-27 kg\nFine structure constant \na =\ne2\n4pe0  Uc \n1\n137.035 999 679 1942 \nRydberg constant \nR \u0005 = a2me c\n2h   \n10 973 731.568 527 1732 m-1\n \nR \u0005c \n3.289 841 960 361 1222 * 1015 Hz\nRydberg energy \nRyd = R\u0005hc \n13.605 691 93 1342 eV\nBohr radius \na0 = 4pe0  U2\nme e2  \n0.529 177 208 59 1362 * 10-10 m\nBohr magneton \nmB =\neU\n2me\n \n9.274 009 15 1232 * 10-24 J>T\n \nmB>h \n1.399 624 604 1352 MHz>Gauss\nNuclear magneton \nmN =\neU\n2mp\n \n5.050 783 24 1132 * 10-27 J>T\nBoltzmann constant \nkB \n1.380 650 4 1242 * 10-23 J>K\n",
    "552 \nPhysical Constants\n Conversion factors \n1 J = 107 erg = 6.24151 * 1018 eV\n1 eV = 1.60218 * 10-19 J\n1 eV corresponds to 1E = hf = hc>l = hcn2\n2.41799 * 1014 Hz 1 f = E>h2\n1239.84 nm 1l = hc>E2\n8065.54 cm-1 1n = E>hc2\n1 cm-1 corresponds to\n29.9792458 GHz 1 f = c n 2\n107 nm 1l = 1>  n 2\n1.23984 * 10-4 eV 1E = hcn2\nhc = 1240 eV # nm\n1 amu = 931.494 MeV>c2 = 1.66054 * 10-27 kg\n",
    " \n553\nA\nAbsorption, 91, 451–454\nAbsorption spectrum, 108\nAddition of angular momenta, 355\nAdiabatic theorem, 450\nAlgebraic method, 277–284\nAlgorithms, quantum computing, 517\nAmplitude, 10, 15\nAnalyzer, Stern-Gerlach device as, 4\nAngular integral, 463\nAngular momentum, 210–215, 357–359\nin atoms and spectroscopic  \nnotation, 377\nclassical, 210\ngeneralized, 357–359\nintrinsic, 2–3\nmotion of a particle on a ring, 218–227\nmotion on a sphere, 227–244\norbital, 2, 3, 211–212, 357\nof photon, 507\nquantum mechanical, 210–215\nspherical coordinates, 215–218\nspin, 211–212, 357\nAngular momentum basis, 359\nAngular momentum ladder operators,  \n359–360, 362–363, 378\ncommutation relations for, 359–360\nAngular momentum operators. See also \nSpherical harmonics\nL2, 211–212, 214, 228, 239, 245, 263, 272\nLz, 211–213, 214, 220–221, 239, 245, 263, \n272, 397, 399\nSz, 397, 399\nAngular momentum quantum number\neffective potential for, 250–251\nfor hydrogen, 255–256, 261\nAngular momentum states, ladder of, 360\nAnnihilation operators, 284\nAnomalous Zeeman effect, 396, 405\nAntibonding orbital, 439, 442, 473\nAnti-Helmholtz coils, 504–505\nAntisymmetric states, 411–413\nfermions and, 419, 421, 442\nsinglet, 416\nApplications of quantum mechanics, 502–528\nlaser cooling, 506–514\nmagnetic trapping, 502–506\nmanipulating atoms, 502–514\nquantum bits (qubits), 515–517\nquantum gates, 518–524\nquantum information processing, 514–526\nquantum teleportation, 524–526\nApproximation, nearest-neighbor, 474\nAssociated Laguerre polynomials, 262–263\nAssociated Legendre equation, 229\nAssociated Legendre functions, 233–235\npolar plots, 234, 235\nproperties of, 234\nAsymmetric square well, 147–150\nperturbation and, 312\nAsymptotic solutions to radial eigenvalue  \nequation, 252–253\nAtomic beam, 508–509\nAtomic clocks, 514\nAtomic levels, multiple bands from multiple, \n478–480\nAtomic number, 434\nAtomic parity violation experiments, 348\nAtom interferometry, 192–196\nAtom-light interactions, Einstein model of, 456–460\nAtoms\nangular momentum in, 377\nblackbody radiation and, 455–460\nclassical, 202\nlaser cooling of, 506–514\nmagnetic traps and, 502–506\nmanipulating with quantum mechanical forces, \n502–514\nquantum, 202\nAuto-ionization, 431\nAzimuthal angle, spin component in general  \ndirection and, 41\nAzimuthal eigenvalue equation, 217, 218–222\nB\nBalmer series, 259\nBand diagrams, 492\nIndex\n",
    "554 \nIndex\nBand gaps, 479\ndirect, 496–497\nindirect, 496–497\nBand of allowed energies, 475\nBand structure, 500\nBand width, 476\nBarrier penetration, 133\nBarrier potential, 188\nBarriers, 182\ntunneling through, 188–192\nBasis states, 14, 16, 32, 44, 45, 63\nharmonic oscillator, 293, 311\nproperties of, 165\nsuperposition of, 166\nBell, John, 7, 99–101\nBell inequality, 101\nBell state, 516–517\nentangled, 523\nquantum teleportation and, 525–526\ntransformation to computational basis, 524\nBell-state measurement, 524, 525–526\nBeta decay problem, 84\nBethe, Hans, 393\nBinnig, Gerd, 192\nBiot-Savart law, 389\nBits, 515\nBlackbody radiation, Einstein model and, 455–460\nBloch’s theorem, 480–482, 488, 490\nBlue-shifted beam, 509\nBohm, David, 98\nBohr, Niels, 103\nBohr energies, 259, 383, 385\nBohr energy levels of hydrogen, 392–393, 394, 402, \n403, 407\nBohr frequency, 71, 302\ntime-dependent perturbation and, 448, 449, \n450–451\nBohr magneton, 355, 393\nZeeman effect and, 394\nBohr oscillation, 90\nBohr radius, 257, 272\nBoltzmann’s constant, 458\nBoltzmann thermal distribution law, 457\nBonding\ncovalent, 441\nionic, 441\nBonding orbital, 439, 442, 473\nBorn-Oppenheimer approximation, 439\nBose-Einstein condensation, 422–423, 502, 514\nBosons, 412–413\nexchange interaction and, 420–421\ninteracting in one-dimensional potential energy \nwell, 423–427\nin many-particle system, 421–422\nsymmetric states and, 419, 421, 442\nin two-particle excited state, 416–420\nin two-particle ground state, 415–420\nBoundary condition at inﬁnity, 130\nﬁnite square well, 130\nBoundary conditions, 476–478\non wave function, 122, 128, 156\nBound eigenstates, 469\nBound particles, free particles vs., 162\nBound states, 120\nin potential energy well, 155\nBound systems, energy states of, 107\nBra-ket formulae, translating to wave function \n formulae, 116, 154\nBras (bra vectors), 11–13, 17–18. See also Dirac \nnotation\nmatrix notation and, 23\nscalar product and, 12\nBrillouin zones, 478\nband gaps in semiconductors and, 497\ndensity of states and, 485\nBroadband excitation, 455, 456–460\nC\nCarbon\nangular momentum in, 377\nas graphite, 497–498\nlow-dimensional, 497–498\nCarbon nanotubes, 498\nCarrier wave, of wave packet, 170\nCavity QED, 460\nCenter-of-mass, separating relative motion and, \n204–208\nCentral potential, 204\nCentrifugal barrier, 250\nCesium, hyperﬁne transition in, 365\nChain of periodic wells. See Periodic chain \nof wells\nCharacteristic (secular) equation, 39–40\nChemical shift, 324\nChirped cooling, 512\nClassical angular momentum, 210\nClassical atom, 202\nClassical harmonic oscillator, 275–276\nClassically allowed region, 120\nClassically forbidden regions, 120\nClassical turning points, 120\nClebsch-Gordan coefﬁcients, 369, 373, 374–376, 378\n",
    " Index \n555\nselection rules and, 463–464\nZeeman effect and, 397\nClosure (completeness) relation, 45\nharmonic oscillator eigenstates and, 291, 292\nCoefﬁcients\nClebsch-Gordan, 369, 373, 374–376, 378, 397, \n463–464\nEinstein A, 457–459, 466\nEinstein B, 457, 458, 461, 466\nreﬂection, 186, 190–191\nof spherical harmonic expansion, 243\ntransmission, 185–186, 191\nCoherent state, of harmonic oscillator, 303–304\nCoherent superposition, 20, 50\nCollapse (reduction, projection), of quantum state \nvector, 46\nColumn vector, 22–23\nCommutation relations\nfor angular momentum ladder operators, 359–360\nquantum mechanical angular momentum and, \n210–211\nCommutators, 54–56, 63\nCommute, 54–55\nCompatible observables, 55–56\nCompleteness, 11, 12, 26, 29, 137\nof basis states, 165\nenergy eigenstates and, 156\nspherical harmonics and, 238\nCompleteness relation (closure), 45, 291, 292\nComplete vectors, 11\nComplex numbers, 10, 11, 278\nComputational basis, 517\nConduction band, 493\nConstants, normalization, 14\nContinuous, discrete vs., 113, 114\nContinuous basis representation, 113\nContinuous superposition, 171–176\nContinuum states, 469\nControlled-NOT gate (CNOT gate), 522–524\nCopenhagen interpretation of quantum  \nmechanics, 103\nCoulomb interaction, 272, 432\nbetween identical charged particles, 426\nCoulomb potential energy\nof diatomic molecule, 305\nfor hydrogen atom, 251\nCoulomb wells, 470\nCoupled angular momentum quantum number, 378\nCoupled basis, 355, 361, 365–370, 378\nClebsch-Gordan coefﬁcients and, 372–376\neigenstates, 366–369\nidentical particles and, 410–412, 415–416\nspin-orbit coupling and, 390\nZeeman effect and, 398, 401, 405–406, 409\nCoupled basis operators, 390\nCoupled basis vectors, 372, 378\nCoupled magnetic quantum number, 378\nCovalent bonding, 441\nCreation operators, 284\nCross section, 461–462\nCrystal momentum, 478\nD\nDarwin term, 392–393\nde Broglie relation, 163, 197\nde Broglie wavelength, 163\natom interferometer and, 194, 196\nDecoherence, 105\nDegeneracy\nof particle-on-a-ring system, 222\nparticle on a sphere and, 239\nDegenerate energy state, 164\nDegenerate perturbation theory, 336–343\nhydrogen and, 346–351\nhyperﬁne interaction of hydrogen and, 361–365\nDegenerate subspace, 339\nDelta-function potential, 135\nDensity of states, 460, 469–470, 484–486\nDetailed balance, principle of, 457\nDeuterium, as fermion, 413\nDiagonalization\nof hyperﬁne perturbation, 361–365\nof operators, 38–41\nof perturbation Hamiltonian, 339–343,  \n348–350, 351\nDiagonal matrix, 36\nDiamagnetic material, magnetic traps and, 506\nDiatomic molecule, as example of rigid rotor, 237\nDifferential volume element, 209\nDirac, Paul A. M., 4\nDirac bra-ket notation, for ﬁrst-order energy  \ncorrection, 327\nDirac delta function, 165, 166, 453\nDirac normalization, 166\nDirac notation, 114, 115\nharmonic oscillator problem and, 289–293\nDirect band gaps, 496–497\nDirect integral, 424–425, 426\nin helium, 432–433\nDiscrete, continuous vs., 113, 114\nDiscrete basis representation, 113\nDiscrete superposition, 168–171\n",
    "556 \nIndex\nDispersion relation, 475, 478–479, 480, 484–485, \n488, 489\nDisplaced Gaussian superposition, time dependence \nof spatial probability density for, 303, 304\nDoppler cooling limit, 514\nDoppler effect, laser cooling and, 509–510, 512\nDot product term, 463\nDot (scalar) product, 11–12\nDouble ionization level, 429\nDouble-slit interference experiment, 10\natom interferomery and, 193–196\nE\nEffective mass, 494–496\nEffective potential energy (Veff), 250–251\nEhrenfest’s theorem, 77, 146, 303\nEigenstates. See also Energy eigenstates\ncoupled basis, 366–369\nenergy, 161–163\nﬁrst-order correction, 324–329\nfree particle, 161–167\nmass, 85\nmolecular, 471\nmomentum, 163–167, 180, 197\nfor particle on a ring, 224\nof perturbation Hamiltonian, 314\nposition, 113–114, 180\nprojection operators and, 45–46\nsimultaneous sets of, 55\nuncoupled basis, 366–369\nEigenvalue equations, 35. See also Energy \neigenvalue equations\nangular momenta and, 358, 377\nhydrogenic atom, 263\nin matrix form, 35\nin matrix notation, 38, 39–40\nfor spin-1/2 operator Sz, 63\nEigenvalues, 34–36. See also Energy eigenvalues\ndiagonalization of matrix and, 38–41\nof inﬁnite square well, 121, 125–126\npostulate 3 and, 34, 35\nEigenvectors, 34–36\ndiagonalization of matrix and, 38–41\nof Hermitian matrix, 44\nof time-independent Hamiltonian, 69\nas unit vectors in their own basis, 36\nEinstein, Albert, 97, 98\nbroadband excitation and, 455–460\nEinstein A coefﬁcient, 457–459, 466\nEinstein B coefﬁcient, 457, 458, 461, 466\nEinstein-Podolsky-Rosen paradox. See EPR \n (Einstein-Podolsky-Rosen) paradox\nEinstein’s locality principle, 99\nElectrical conductivity, 492–494\nElectric dipole approximation, 455\nElectric dipole Hamiltonian, 454\nElectric dipole interaction, 454–462\nElectric dipole moment\noscillating, 270–271\nStark effect in hydrogen and, 347–351\nstatic, 271\nElectric dipole transitions, selection rules and, \n462–465, 466\nElectric ﬁeld, application and effect on k values, \n493–494\nElectromagnetic coupling constant, 257, 385\nElectromagnetic ﬁeld, energy density of, 456\nElectron\nhyperﬁne interaction between proton and, 365–366\nin spin-orbit coupling, 388–389\nElectron diffraction experiments, 192\nElectron distribution of hydrogen, 265, 266\nElectronic conﬁgurations, 434, 435, 436\nElectron magnetic moment, 355–356, 357\nElectron mass (me), 256\nElectron rest mass energy, of hydrogen, 384\nElectron velocity, relativistic correction caused by, \n386–388\nElements of reality, 97\nEmission, 91, 451, 452\nspontaneous, 457–458\nstimulated, 452\nEmission spectrum, 108\nEncryption, factorization algorithm and, 517\nEnergy. See also Quantized energies\nCoulomb interaction, 432\nFermi, 423\nEnergy band diagram, 469–470\nEnergy bands, multiple, from multiple atomic levels, \n478–480\nEnergy basis, 69\nharmonic oscillator, 294\nEnergy conserving delta function, 453–454\nEnergy density\nblackbody, 458\nof electromagnetic ﬁeld, 456\nEnergy eigenstates, 70, 161–164\nasymmetric square well, 149–150\nenergy basis and, 137\nharmonic oscillator, 284–288\n",
    " Index \n557\nin harmonic oscillator well, 278\nin hydrogen atom, 278\ninﬁnite and ﬁnite wells, 134–135\ninﬁnite square well, 156\nin inﬁnite square well, 278\nof inﬁnite square well, 121, 123–125, 127, 143–145\nnumerical solutions, 151–153\nof periodic chain of wells, 471–476\nproperties of, 156\nqualitative (eyeball) solutions, 150–151\ntime evolution of, 162–163\nEnergy eigenstate wave functions\nﬁnite square well, 131–133\nof hydrogenic atom, 263–269\nEnergy eigenvalue equations, 68, 110–112, 156\nﬁnite square well, 128–133, 159\nharmonic oscillator, 277, 281, 283, 284\ninﬁnite square well, 119, 121–128\nKronig-Penney model and, 489–490\norbital angular momentum and, 214\nquantum mechanical tunneling, 189–190\nsolving numerically, 152–153\nin spherical coordinates, 208–209\nfor two-body system, 207\nunbound states and, 161\nusing LCAO method to solve, 471–473\nzeroth-order problem, 319\nEnergy eigenvalues, 203\nof hydrogen atom, 202–204\nof hydrogenic atom, 256\nof inﬁnite square well, 142\nof periodic chain of wells, 471–476\nof rigid rotor, 236–237\nEnergy estimation, using uncertainty principle, \n180–181\nEnergy ﬁngerprint, of microscopic systems, 107\nEnergy levels\nin bound systems, 107–108\nin GaAs quantum well, 147\nhydrogen, 256–259, 382–386\nhydrogenic atom, 263–264\nperiodic table and, 434–435\nperturbation and, 312\nin second-order perturbation theory, 332\nspin-1/2 particle in uniform magnetic ﬁeld, 313\nEnergy measurements\nspectroscopy, 107–109\nspherical harmonics, 242–244\nEnergy spectrum, 108\nof harmonic oscillator, 283\nof helium, 433\nhydrogen, 258–259\nof inﬁnite square well, 123\nfor Kronig-Penney model, 491\nfor particle on a ring, 222\nof rigid rotor, 236\nof scattering states, 184\nEntangled states, 98–102\nCNOT gate and, 522–523\nquantum bits and, 516–517\nSchrödinger cat paradox and, 104\nEnvelope, of wave packet, 170\nEPR (Einstein-Podolsky-Rosen) paradox, 97–102\nentangled quantum states and, 516\nEuler relation, 475\nEvanescent wave, 188\nEven parity, 136\nExchange force/exchange interaction, 420–421\nExchange integral, 425, 426\nin helium, 432–433\nExchange operator, 411\nExchange symmetry, 438\nhydrogen molecule, 442\nExcited state band, 479\nExcited states, 108\nexponential time decay of population of, 459\nhelium atom, 431–433\ntwo-particle, 416–420\nExpectation values, 51–52\nof harmonic oscillator, 295–296, 301–302\nof hydrogen molecule, 438\nof inﬁnite square well, 125, 126, 142–143, 146\nof the perturbation, 322–323\nof radial position, 267–269\nof wave function, 117–118\nEyeball solutions, to energy eigenstates, 150–151\nF\nFactorization algorithm, 517\nFermi, Enrico, 84\nFermi contact interaction, 356\nFermi energy, 423\nFermions, 412–413\nantisymmetric states and, 419, 421, 442\nexchange interaction, 420–421\ninteracting, 426–427\nin many-particle system, 422, 423\nin two-particle excited state, 416–420\nin two-particle ground state, 415–420\nFermi’s golden rule, 454, 460\n",
    "558 \nIndex\nFeynman, Richard, 8, 104, 514\nFine structure, 382\nof hydrogen, 382–393\nFine-structure constant, 257\nof hydrogen, 384, 385\nFine structure of hydrogen\nrelativistic correction, 386–388\nspin-orbit coupling, 388–393\nFinite square barrier, 188\nFinite square well, 128–133, 155, 469\nbarrier penetration, 135–136\nbound states and, 182, 183\ncompleteness, 137\nenergy eigenvalue equation, 159\ninversion symmetry and parity, 136\nnodes, 135\northonormality, 136\nschematic diagram of, 132\ntransmission and reﬂection coefﬁcients for, 186\ntransmission coefﬁcient for, 185–186\nwave function curvature, 133–135\nwaves incident upon, reﬂected from and \n transmitted through, 187\nFirst Brillouin zone, 478\nFirst-order energy correction, 320–324\nFirst-order state vector correction, 324–329,  \n334, 335\nForbidden transitions, 93, 136, 462\nFormulas\nhydrogen energy, 388\nPlanck blackbody radiation, 458\nRabi’s, 81–84, 90, 449\nRodrigues’, 231\nFourier energy-time uncertainty relation, 453\nFourier frequency-time uncertainty relation,  \n452–453\nFourier integral, 449\nFourier transform, 167, 197, 448\nof delta function, 178–179\nof emitted electromagnetic ﬁeld, 460\nof Gaussian function, 177–178\ninverse, 197\nmomentum space wave function and, 297\ntime-dependent generalization of, 172\nFourier wave packet, uncertainty principle and, \n177–178\nFree particle eigenstates, 161–167\nenergy eigenstates, 161–164\nmomentum eigenstates, 163–167\nFree particles, bound particles vs., 162\nFrequency\ngeneralized Rabi, 91\nLarmor, 333\nRabi, 91\nFull width at half maximum (FWHM), 91\nFunctions, activities, 159\nFWHM. See Full width at half maximum (FWHM)\nG\nGaAs\nband gap of, 496–497\nas direct-gap semiconductor, 497\nGaAs quantum well, 146–147\nGates\nlogic, 518\nquantum, 518–524\nGaussian function, probability analysis and, 298\nGaussian integral, 173\nGaussian momentum distribution, 172\nGaussian momentum space wave function,  \n172–174\nGaussian perturbation, 449–450\nGaussian wave packet, 172–176, 197, 201\ntime evolution of, 201\nuncertainty principle and, 177–178\nGedanken experiments, 97, 196\nEPR paradox, 97–102\nSchrödinger cat paradox, 102–105\nGeneralized angular momentum (J), 357–359\naddition of, 370–376\nmathematical rules for, 358\nGeneralized Rabi frequency, 91\nGeneral quantum state (c), 11\nGeneral quantum systems, 25–27, 62–63\nGerade state, 438–440, 442\nGerlach, Walther, 1\nGraphene, 497–498\nGraphite, 497–498\nGravity measurement, 514\nGrayscale density plots, for hydrogen energy  \neigenstates, 265, 267\nGround state, 108\nhelium atom, 428–430\ntwo-particle, 415–420\nGround state band, 479\nGroup velocity, 170, 493\nGrover’s search algorithm, 517\nGyromagnetic ratio, 72, 77, 356\nZeeman effect and, 394–395, 398, 401\nGyroscopic ratio, 3\n",
    " Index \n559\nH\nHadamard gate, 521, 523–524\nHamiltonian\ncenter-of-mass, 205–206\nchanges in energy eigenstates and, 445\nenergy eigenvalue equation and, 110\nharmonic oscillator, 277, 307\nperturbation, 313–319\nfor three-dimensional system of two  \nparticles, 204\ntime-dependent perturbation theory and, 445–462\nHamiltonian operators\nlight-matter interactions and, 92–93\ntime-dependent, 87–93\ntime-independent, 68–71\nHamiltonians. See also individual types\nHänsch, Theodor, 382\nHarmonic oscillator, 155, 275–311\nbasis states, 293, 311\nclassical, 275–276\nDirac notation, 289–293\nmanifestations of quantum mechanical  \npostulates, 308\nmatrix representations, 293–296\nmolecular vibrations, 305–307\nmomentum space wave function, 296–298\nperturbation theory and, 343–346\nquantum mechanical, 277–284\ntime dependence and, 300–304\nuncertainty principle and, 298–299\nwave functions, 284–289\nHarmonic oscillator Hamiltonian, 278–284\nHarmonic perturbation, 450–454\nHeisenberg, Werner, 103\nHeisenberg uncertainty principle, 56–57, 63\nharmonic oscillator and, 298–299\nmotion of electron’s wave packet and, 493\nposition and momentum and, 176–181\nHelicity, 465\nHelium atom\nexcited states, 431–433\nground state, 428–430\nsymmetrization postulate and, 427–433\nHelium Hamiltonian, 427\nHermite polynomials, 287–288, 297\nHermitian adjoint, 44\nHermitian matrices, 44\nHermitian operators, 44\nHamiltonian as, 68\nHidden variables, 99–100\nHilbert space, 10–11, 320, 368\nquantum information processing and, 514, 517\nHoles, 495–496\nHooke’s law, 275\nHopping matrix elements, 472, 474\nHybrids, of angular momentum eigenstates, 241\nHydrogen atom\nangular momentum in, 377, 378–379\nasymptotic solutions to radial equation, 252–253\nas boson, 413\nenergy eigenstates, 278\nenergy levels, 107–108, 382–386\nﬁne structure, 382–393\nfull hydrogen wave functions, 263–269\nground state energy, 384\nhydrogen energies and spectrum, 256–260\nhyperﬁne Hamiltonian for ground state of, 356–357\nhyperﬁne interaction effect on, 361–365\nhyperﬁne structure, 384–385\norbital angular momentum and, 207, 208, 209, 214\nperturbation of, 346–351\nprobability densities, 265–267, 274\n2p A 1s transition of, 465\nradial eigenvalue equation, 250–252\nradial probability integrand for 1s ground state, \n267–269\nradial wave functions, 261–263, 266–269\nrelative motion Hamiltonian and, 206–207, 208\nseparation of center-of-mass motion from relative \nmotion, 204–208\nseparation of variables, 215–218\nseries solution to radial equation, 253–256\nsolving energy eigenvalue problem for, 202–204\nspectroscopy of, 382\nStark effect in, 346–351\nsuperposition states, 270–271\ntransitions between states in, 260\ntransition wavelengths of, 109\nZeeman effect, 393–406\nZeeman structure of ground state, 406\nHydrogen chloride molecule\nenergy eigenvalues, 237\nmolecular vibration in, 306–307\nHydrogen energy eigenstates, radial wave  \nfunctions for, 262\nHydrogen energy formula, 388\nHydrogenic atoms, 251\nenergy eigenstate wave functions of, 263–269\nenergy eigenvalues of, 256\nradial wave functions of, 262\n",
    "560 \nIndex\nHydrogen molecule, 437–442\nhydrogen molecule H2, 440–442\nhydrogen molecule ion H2\n+, 438–440\nHydrogen wave functions, full, 263–269\nHyperﬁne interaction Hamiltonian, 356–357\nHyperﬁne perturbation Hamiltonian, 369–370\ndiagonalization of, 362–365\nHyperﬁne structure\naddition of generalized angular momenta, 355, \n370–376\ncoupled basis, 355, 365–370\ndiagonalization of hyperﬁne perturbation, 361–365\nof hydrogen, 355, 364–365, 378–379, 384–385\nhyperﬁne interaction, 355–357\nZeeman perturbation of 1s, 405–406\nI\nIdentical particles, 410–444\nhelium atom, 427–433\nhydrogen molecule, 437–442\ninteracting particles, 423–427\nin one dimension, 414–423\nperiodic table and, 434–437\ntwo spin-1/2 particles, 410–413\nIdentity matrix, 39\nIdentity operator 1, 45\nIncompatible observables, 8, 56\nIndependent electron approximation, 469\nIndirect band gaps, 496–497\nInﬁnite square well, 119–128, 155\nallowed energies, 156\nbarrier penetration, 135–136\ncompleteness, 137\nenergy eigenstates in, 278\nenergy spectrum of, 123\nﬁrst-order correction to perturbed,  \n328–329\ninversion symmetry and parity, 136\nnodes, 135\northonormality, 136\nschematic diagram of problem and solution, 127\nsuperposition states and time dependence  \nof, 140–145\ntime evolution of, 160\ntwo particles bound in, 417–421\nwave function curvature, 133–135\nInner product, 12, 13, 28\nmatrix notation and, 23\nprobability amplitude and, 15\nInput state, normalized, 21–22\nInstruction sets, 99–100\nInsulators, 491–494\nInterference, example of, 49–50\nInterference terms, 49\nIntrinsic angular momentum (S), 2–3\nInverse Fourier transform, 197\ntime-dependent generalization of, 172\nInversion symmetry, 136\nIonic bonding, 441\nIonization level, 429–430\ndouble, 429\nsingle, 430\nIonization limit, 256\nK\nKets (ket vectors), 4, 10, 11–14, 17–19, 28, 29. See \nalso Dirac notation; Eigenvectors\nenergy eigenvalue equation and, 110–111\nmatrix notation and, 22–25\nscalar product and, 12\nKinetic energy, 386\nKronecker delta, 26, 138, 139, 165, 166, 291, 294\nKronig-Penney model, 489–491\nk space, 478, 484\nallowed values of, 476–478\nL\nLadder operators, 281–284, 302, 307, 344\naddition of generalized angular momenta and, \n371–373\nangular momentum, 359–360, 362–363, 378\nmatrices for, 294–295\nLadder termination condition, 282\nLaguerre polynomials, 263\nassociated, 262–263\nLamb, Willis, 393\nLamb shift, 284, 384–385, 393, 464\nLandé g factor, 401\nmagnetic trapping and, 505\nLaplace series, 238\nLarmor frequency, 76–77, 79, 82–83, 333\nperturbing magnetic ﬁelds and, 313–314\ntime-dependent Hamiltonians and, 88, 91\nLarmor precession, 76–77, 82–83, 519\nLaser cooling, 506–514\nLaser excitation, 460–462\nLattice-matched growth, 146–147\nLCAO. See Linear combination of atomic orbitals \n(LCAO)\nLectures on Physics (Feynman), 8\n",
    " Index \n561\nLEDs. See Light-emitting diodes (LEDs)\nLegendre functions, 228, 233\nassociated, 233–235\nLegendre polynomials, 231–232, 233\nLegendre’s equation\nassociated, 229\nseries solution of, 228–233\nLeptons, 84–85\nLevitron, 506\nLight-emitting diodes (LEDs), 496\nLight-matter interactions, 92–93\nLight sensors, 496\nLinear combination of atomic orbitals (LCAO),  \n438, 469\nband structure of graphene and, 498\nenergy eigenvalues and eigenstates of N-well \nchain, 473–476\nenergy eigenvalues and eigenstates of two-well \nchain and, 471–473\nKronig-Penney method and, 489, 491\nsummary, 488–489\nLinear potential, 155\nLiquid helium, 422–423\nLocal hidden variable theory, 7, 99\nLocality principle, 99\nLogic gates, 518\nLorentzian curve, 91\nLorentzian frequency dependence, 461\nLorentzian function, 460\nLow-dimensional carbon, 497–498\nLowering operators, 278, 280–284\nLyman-a, 259\nLyman series, 259\nM\nMagnetic ﬁeld\nin a general direction, 78–84\nin magnetic trap, 504–505\nspin-orbit coupling and, 388–389\nin the z-direction, 72–78\nZeeman effect and, 394, 396–405\nZeeman effect with intermediate, 403–405\nZeeman effect with strong, 402–403\nZeeman effect with weak, 396–401\nMagnetic ﬁeld gradient, 3\nMagnetic moment (μ), 355–356, 357\nin Stern-Gerlach experiment, 1–3\nMagnetic quantum number (m), 62, 358, 378\nClebsch-Gordan coefﬁcient and, 464\nZeeman effect and, 394–395\nMagnetic resonance, 87–92\nMagnetic resonance imaging (MRI), 87\nMagnetic trapping, 502–506\nManifold of magnetic quantum number states, 358\nMaple software, 151, 243, 488\nMass eigenstates, 85\nMathematica software, 151, 243, 488\nMatLab, 151, 243\nMatrices\ncommutation relations and, 56\nrepresenting spin-1/2 operators, 63\nMatrix elements, 37–38\nhopping, 472, 474\nMatrix notation, 22–25, 29\neigenvalue equation in, 38, 39–40\noperators and, 35–36\nMatrix representations\nharmonic oscillator problem and, 293–296\nof operators, 37–38\nMaxwellian velocity distribution, in laser cooling, \n510–511\nMBE. See Molecular beam epitaxy (MBE)\nMean, 51\nMeasurement, 50–54\nBell-state, 524, 525–526\nenergy, 107–109, 242–244\nMermin, N. David, 98\nMesoscopic system, 104\nMetal-organic chemical vapor deposition (MOCVD), \n146\nMetals, 491–494\nMicroscopic systems, energy ﬁngerprint of, 107\nMinimum uncertainty state, 177–178\nMixed states, 19–20\nstatistical, 50\nin Stern-Gerlach experiment, 49–50\nsuperposition state vs., 19–20\nMixing angle, 85\nMOCVD. See Metal-organic chemical vapor deposi-\ntion (MOCVD)\nModulation envelope, of wave packet, 170\nMolecular beam epitaxy (MBE), 146\nMolecular eigenstate, 471\nMolecular orbitals, 478\nMolecular states, 473, 475, 480–482\nMolecular vibrations, harmonic oscillator and, \n305–307\nMolecular wave functions, 482–484\nMomentum, 110\nof center of mass, 204\n",
    "562 \nIndex\nMomentum, continued\ncomplementary to position, 180\ncrystal, 478\nof superposition state of harmonic oscillator, \n302–303\nuncertainty principle and, 176–181\nMomentum eigenstates, 163–167, 169, 180, 197\nMomentum eigenvalue equation, 163\nMomentum operators, 156\nmatrix representation, 295\nMomentum space wave function, 167, 171\nharmonic oscillator, 296–298\nMonochromatic excitation, 455\nMorse oscillator, 306\nMorse potential, 305–306\nMotion\nof a particle on a ring, 218–227\non a sphere, 227–244\nMRI. See Magnetic resonance imaging (MRI)\nMuon, decay to electron, 84\nN\nNearest-neighbor approximation, 474\nNeutrino mixing, 85\nNeutrino oscillations, 84–86\nNeutrinos\ndiscovery of, 84\nsolar neutrino problem, 84\nNewton’s second law, 151–152\nclassical harmonic oscillator and, 276\nNMR. See Nuclear magnetic resonance (NMR)\nNodes, 135\nNoether’s theorem, 210\nNondegenerate perturbation theory, 319–329, 351\nﬁrst-order energy correction, 320–324\nﬁrst-order state vector correction, 324–329\nsecond-order, 329–335\nNormalization, 11, 12–14\nof basis states, 165\nDirac, 166\nenergy eigenstates and, 156\nfor full hydrogen wave function, 264\nharmonic oscillator and, 291–292\nof input state, 21–22\nof perturbed state, 317\nof spin-1/2 basis vectors, 12\nof state vector, 26\nNormalization constant, 14\nNormalized vectors, 11\nNormalizing wave function, 114–115, 116\nNormal mode solutions, 475\nNuclear magnetic moments, 406\nNuclear magnetic resonance (NMR), 87\nNumber operator, 283–284\nNumerical solutions, to energy eigenstates, 151–153\nN-well chain, energy eigenvalues and eigenstates \nof, 473–476\nO\nObservable physical quantity, 4\nObservables\ncommuting, 54–56\ncompatible, 55–56\nincompatible, 8, 56\nsimultaneous, 55\nOdd parity, 136\nOperator F2, 366, 367, 368\nOperator J2, 358, 360, 390\nOperator L2, 390\nOperator method, 277–284\nOperators, 34–36. See also Angular momentum \noperators; Ladder operators\nactivities, 159\ncommuting, 54–56\ndiagonal in their own basis, 36\ndiagonalization of, 38–41\nHamiltonian, 68–71, 87–93\nHermitian, 44\nmatrix representation of, 37–38\nnew, 41–50\nprojection, 44–47\nspin component in general direction, 41–43\nOperator S2, 57–59, 390\nOperator S · I, 362–363, 369\nOperator Sn, 41\nOperator Sx, 41, 56, 57\nOperator Sy, 39–40, 41, 56, 57\nOperator Sz, 41, 46, 47, 50, 53–54, 59–61, 63\nidealized measurement of, 53–54\nmagnetic ﬁeld in the z-direction and, 72–73, \n75–76\nmatrix representation of, 36\nOptical molasses, 512–514\nOptical spectrum, 108\nOptical transitions, in helium, 433\nOptics interference analogy, 187\nOrbital angular momentum (L), 2, 3, 210–215, 357\nOrbital angular momentum quantum number, 212\nOrbital magnetic ﬁeld, spin-orbit coupling and, 388\nOrbital magnetic quantum number, 212\n",
    " Index \n563\nOrbitals\nantibonding, 473\nbonding, 473\nmolecular, 478\nOrthogonality, 11, 12, 136\naddition of generalized angular momenta and, \n371–372\nof basis states, 165\nenergy eigenstates and, 156\nharmonic oscillator and, 290–291\nOrthogonal vectors, 11\nOrthohelium, 433\nOrthonormality, 11, 26, 28, 136\nfor basis set of momentum states, 165–166\nharmonic oscillator and, 291\nspherical harmonics and, 238\nOscillating electric dipole moment, 270–271\nOscillator frequency, 277\nOuter product, 45\nOverall phase, 225\nP\nParahelium, 433\nParameters\na and b, 472–479, 486–489\ncalculation of, 486–489\nParity, 136\neven, 136\nof full hydrogen wave function, 267\nodd, 136\nrestriction on matrix elements and, 464\nspherical harmonics and, 239\nParity violation, 348\nParticle in a box, 120, 155\nParticle on a ring\nenergy spectrum for, 222\nmotion of, 218–227\nquantum measurements on, 223–224\nin superposition state, 223–227\nParticles. See Bound particles; Free particles; \n Identical particles\nPaschen series, 259\nPauli, Wolfgang, 84\nPauli exclusion principle, 410, 412, 418, 434, 442\nPauli matrices, 519\nPeriodic boundary conditions, 476–478\nPeriodic chain of wells, 469, 470\nenergy eigenvalues and eigenstates of, 471–476\nN-well chain, 473–476\ntwo-well chain, 471–473\nPeriodic systems, 469–501\napplications, 491–494\nBloch’s theorem, 480–482\nboundary conditions and allowed values of k, \n476–478\nBrillouin zones, 478\ncalculation of the model parameters, 486–489\ndensity of states, 484–486\ndirect and indirect band gaps, 496–497\neffective mass, 494–496\nenergy eigenvalues and eigenstates of periodic \nchain of wells, 471–476\nKronig-Penney model, 489–491\nlow-dimensional carbon, 497–498\nmolecular states, 480–482\nmolecular wave functions, 482–484\nmultiple bands from multiple atomic levels, \n478–480\nPeriodic table, 434–437\nPerturbation Hamiltonian, 313–319, 333\ndiagonalizing, 339–343, 348–350, 351\nPerturbations, 147–150\nPerturbation theory, 312–354\ndegenerate, 336–343\nharmonic oscillator and, 343–346\nhydrogen and, 346–351, 382–409\ninteracting particles and, 423–426\nnondegenerate. See Nondegenerate perturbation \ntheory\nspin-1/2 example, 313–317\nStark effect and, 346–351\ntime-dependent. See Time-dependent perturbation \ntheory\ntwo-level example, 317–319\nZeeman, of 1s hyperﬁne structure, 405–406\nPhase velocity, 162–163, 493\nPhonon, 496, 497\nPhotons, 194–195, 284\nband gaps in semiconductors and, 496–497\nEinstein model and, 455–460\nhelicity and, 465\nlaser cooling and, 507\nPhysical observables, 34–35\nPion, decay to muon, 84\n\u000b-pulse, 91\nPlanck blackbody radiation formula, 458\nPlanck’s constant, 3, 279\nPodolsky, Boris, 97\nPolar angle, spin component in general direction \nand, 41\n",
    "564 \nIndex\nPolar angle eigenvalue equation, 217, 218, 227–235\nPolar plots\nof associated Legendre functions, 234, 235\nof spherical harmonics, 240, 241, 242\nPolynomials\nassociated Laguerre, 262–263\nHermite, 287–288, 297\nLaguerre, 263\nLegendre, 231–232, 233\nPosition, 110\ncomplementary to momentum, 180\nuncertainty principle and, 176–181\nwave function and, 113–115\nPosition eigenstate, 180\nPosition operators, 156\nmatrix representation, 295\nPosition representation, 111\nPostulates of quantum mechanics, 27–28\n1 (one), 4–5, 27\nharmonic oscillator and, 289, 308\nhydrogen atom problem and, 308\nspin-1/2 system and, 308\n2 (two), 27, 34\nharmonic oscillator and, 289, 308\nhydrogen atom problem and, 308\nspin-1/2 system and, 308\n3 (three), 27\nharmonic oscillator and, 289, 308\nhydrogen atom problem and, 308\nspin-1/2 system and, 308\n4 (four), 14–15, 28, 29\nharmonic oscillator and, 293, 308\nhydrogen atom problem and, 308\nspin-1/2 system and, 308\n5 (ﬁve), 28, 46–47, 63\nharmonic oscillator and, 293, 308\nhydrogen atom problem and, 308\nspin-1/2 system and, 308\n6 (six), 28, 68\nharmonic oscillator and, 293, 308\nhydrogen atom problem and, 308\nspin-1/2 system and, 308\nmanifestations of, 308\nsymmetrization. See Symmetrization postulate\nPotential energy, 110. See also Finite square well; \nInﬁnite square well\ndouble-slit atom interferometer for measuring, 196\neffective, 250\nof electric dipole, 347\nof GaAs quantum well, 146–147\nharmonic oscillator, 277, 305\nmagnetic trapping and, 503, 505\nPotential energy diagram, 119\nPotential energy function, general, 276\nPotential energy shelf, 328\nPotential wells, 119–120\nbound and unbound states in, 182\ngeneral, 154, 155\nwaves incident upon, reﬂected from and  \ntransmitted through, 184\nPrecession\nLarmor, 76–77\nspin, 72–84\nPrincipal quantum number, hydrogen, 255, 261\nPrinciple of detailed balance, 457\nProbability\nprojection operators and, 46\nquantum mechanical, 14–15\nProbability amplitude, 15, 138\nwave function and, 116–118\nProbability cloud, 202\nProbability density, 154\nfor detecting particle on screen, 194\nof energy eigenstates of inﬁnite square  \nwell, 125–126\nof full hydrogen wave function, 265–267\nof Gaussian wave packet, 174–175\nof harmonic oscillator, 288–289, 301, 302, 304\nof hydrogen, 274\nof inﬁnite square well, 145\nof many-particle system, 421–422\nof momentum eigenstate, 165\nof momentum space, 298\none-particle, 421\nof position of particle on the ring, 224–226\ntwo-particle, 417–419, 421\nwave function and, 114, 118\nProbability function\nwave function and, 114\nProbability postulate (postulate 4), 14–15, 28, 29\nharmonic oscillator and, 293, 308\nhydrogen atom problem and, 308\nspin-1/2 system and, 308\nProduct\ndot (scalar), 11–12\ninner, 12, 13, 28\nouter, 45\nProduct state, 517\nProjection, 12\nof quantum state vector, 46\n",
    " Index \n565\nProjection operators, 44–47\nStern-Gerlach experiments and, 47–50\nProjection postulate (postulate 5), 28, 46–47, 63\nharmonic oscillator and, 293, 308\nhydrogen atom problem and, 308\nspin-1/2 system and, 308\nProjection theorem, 399–400\nProton\nhyperﬁne interaction between electron and, \n365–366\nin spin-orbit coupling, 388–389\nProton magnetic moment, 355–356, 357\nQ\nQED. See Quantum electrodynamics (QED)\nQualitative solutions, to energy eigenstates, 150–151\nQuantization, of spin angular momentum, 3–4\nQuantization condition, 122–123, 124\nQuantized energies, 107–160\nasymmetric square well, 147–150\nenergy eigenvalue equation, 110–112\nﬁnite square well, 128–137\nﬁtting energy eigenstates by computer, 151–154\nﬁtting energy eigenstates by eye, 150–151\ngeneral potential wells, 154\ninﬁnite square well, 119–128, 133–137\nquantum wells and dots, 146–147\nspectroscopy, 107–109\nsuperposition states and time dependence, 137–146\nwave function, 112–119\nQuantum atom, 202\nQuantum bits (qubits), 515–517\nentangled states, 516–517\nquantum teleportation and, 524–526\nsuperposition states, 515–516\nQuantum bound states, 160\nQuantum computer, 514\nQuantum computing algorithms, 517\nQuantum crystal, 500\nQuantum dots, 147\nQuantum electrodynamics (QED), 393, 459–460\ncavity, 460\nQuantum entanglements, 516–517\nQuantum fuzziness, 59\nQuantum gates, 518–524\ncontrolled-NOT gate, 522–524\nHadamard gate, 521, 523–524\nquantum NOT gate, 519–520\nQuantum information processing, 514–526\nquantum bits (qubits), 515–517\nquantum gates, 518–524\nquantum teleportation, 524–526\nQuantum measurements, on particle conﬁned to ring, \n223–224\nQuantum mechanical angular momentum, 210–215\nQuantum mechanical harmonic oscillator, 277–284\nQuantum mechanical tunneling, 188–192, 197, 201\nQuantum mechanics\nmodern applications. See Applications of quantum \nmechanics\nas nonlocal theory, 517\npostulates of. See Postulates of quantum mechanics\nQuantum NOT gate, 519–520\nQuantum number, 122–123\norbital angular momentum, 212\norbital magnetic, 212\nprincipal, 255\nQuantum parallelism, 516\nQuantum particles, behavior of, 1\nQuantum spookiness, 97–106\nEinstein-Podolsky-Rosen paradox, 97–102\nSchrödinger cat paradox, 102–105\nQuantum state vectors, 10–22, 154\nseparation of, 206–207\nspatial vectors and, 325\nsuperposition states, 19–22\nsymmetrization postulate and, 413\nwave function and, 112–113\nQuantum teleportation, 524–526\nQuantum wells, 146–147\nQuantum wires, 147\nQuasimomentum, 478\nR\nRabi, I. I., 87, 406\nRabi ﬂopping, 90–91, 92\nRabi frequency, 91\nRabi’s formula, 81–84, 90, 449\nRadial eigenvalue equation, 217, 218, 250–252\nasymptotic solutions to, 252–253\nseries solution to, 253–256\nRadial integrals, 463\nin relativistic energy correction, 388\nRadial position\nexpectation values of, 267–269\nRadial wave functions, 274\nhydrogen, 261–263, 266–269, 272\nRadiation pressure, 507\nRaising operators, 278, 280–284, 285, 286\nRamsauer-Townsend effect, 188\n",
    "566 \nIndex\nRate equation, 458\nReality, elements of, 97\nReciprocal space, 476\nRecoil velocity, 507\nRecurrence relation, 230–231, 254\nRed-shifted beam, 509\nReduced mass, 205\nReduction, of quantum state vector, 46\nReﬂection coefﬁcient, 186\nfor scattering from square barrier, 190–191\nRelative motion, separating center-of-mass and, \n204–208\nRelative phase, 225\nRelativistic energy, 386\nRelativistic energy correction, 386–388, 393\nRepresentation, 23\nResonance condition, 188\nRest mass energy, 386\nRetherford, Robert, 393\nRigid rotor, 227\nenergy eigenvalues of, 236–237\nRodrigues’ formula, 231\nRohrer, Heinrich, 192\nRoot-mean-square deviation (r.m.s. deviation), 52\nRosen, Nathan, 97\nRotational constant, 237\nRotation-vibration coupling, 306–307\nRow vector, 23\nRubidium laser cooling, 507–512\nRussell-Saunders notation, 377\nRydberg constant (R'), 259–260\nRydberg energy, 365, 383–384\nRydberg (Ryd), 260\nS\nScalar (dot) product, 11–12\nScanning tunneling microscope, 192\nScattering, 508–514\nunbound states and, 181–188\nScattering force, 508–514\nScattering states, 182–186, 197\nSchawlow, Arthur, 382\nSchrödinger, Erwin, 68\nSchrödinger cat paradox, 102–105\nSchrödinger equation, 28, 68–71\ntime-dependent Hamiltonians and, 88–90\ntime-dependent perturbation and, 446\ntime-dependent solution to, 137\nSchrödinger time evolution, 68–96\nharmonic oscillator and, 300\nmagnetic ﬁeld in general direction, 78–84\nmagnetic ﬁeld in the z-direction, 72–78\nneutrino oscillations, 84–86\nSchrödinger equation, 68–71\nspin precession, 72–84\nsuperposition states and, 225\ntime-dependent Hamiltonians, 87–93\ntime evolution of hydrogen atom and, 270–271\ntime-independent Hamiltonians, 68–71\nSearch algorithm, 517\nSecond-order energy correction, 344\nSecond-order nondegenerate perturbation theory, \n329–335\nSecond-order perturbation equation, 329\nSecular (characteristic) equation, 39–40\nSelection rules, 93, 462–465, 466\nSemiclassical method, atom-light interaction, 459\nSemiconductor quantum wells, 146–147\nSemiconductors, 491–494. See also GaAs; \n Silicon (Si)\nband gaps and, 496–497\nband structure and density of states of Si, 469–470\ngraphene as, 498\nSeparation constant, 217\nSeries solution of Legendre’s equation, 228–233\nSeries solution to radial eigenvalue equation, 253–256\nShell, 434\nShell number, 256\nShooting method model, 160\nShor, Peter, 517\nShor’s factorization algorithm, 517\nSilicon (Si), 492–493\nband gap of, 496–497\nband structure and density of states of, 469–470\ndensity of states for, 470\nas indirect-gap semiconductor, 497\nSilver atom, magnetic moment of, 3\nSimultaneous observables, 55\nSimultaneous sets of eigenstates, 55\nSingle ionization level, 430\nSingle-particle problem, 469\nSingle-slit diffraction, 10\nSinglet state, 368, 369\nantisymmetric, 416\nSodium, band states of, 492\nSodium nucleus\nﬁrst-order energy shifts due to perturbation of, \n323–324\nsecond-order energy shifts due to perturbation of, \n332–335\n",
    " Index \n567\nSoftware\nMaple, 151, 243, 488\nMathematica, 151, 243, 488\nMatLab, 151, 243\nSPINS. See SPINS software\nSolar cells, 496\nSolar neutrino problem, 84\nSpatial integrals, in relativistic energy correction, 388\nSpatial symmetry, 438\nSpatial vectors, 11\nproperties of, 11\nquantum state vectors and, 325\nSpectroscopic experiments, 445\nSpectroscopic notation, 377\nSpectroscopy, 93, 107–109\nof hydrogen atom, 382\nSphere, motion on, 227–244\nSpherical coordinates, 215–218\nenergy eigenvalue equation in, 208–209\nSpherical harmonic expansion, 243–244\nSpherical harmonics, 227, 237–240, 463\ncoefﬁcients of spherical harmonic  \nexpansion, 243\ncompleteness and, 238\northonormality and, 238\nparity and, 239\nproperties of, 238–239\nvisualization of, 240–244\nsp3 hybrid orbitals, angular dependence of, 214, 215\nSpin, 2\nin Stern-Gerlach experiment, 2–3\nZeeman effect with, 396\nZeeman effect without, 394–396\nSpin angular momentum quantum number (spin \nquantum number) (s), 62\nSpin angular momentum (S), 211–212, 357\nSpin component in general direction, 41–43\nSpin component measurement, 6–9, 10–11, 15–17, \n22, 24\nafter state preparation in new direction, 42–43\nSpin component quantum number (magnetic  \nquantum number) (m), 62\nSpin down, 4\nSpin ﬂip, 80–82, 90–92\nSpin-1/2 operators, matrices representing, 63\nSpin-1/2 particles, system of two, 381,  \n410–413\nClebsch-Gordon coefﬁcients for, 369\ncoupled basis in terms of uncoupled basis, \n365–370, 379\nSpin-1/2 systems, 4\nexpectation values and, 51–52\nket as basis, 11\nmeasurement in, 50–51\nperturbation and, 313–317\npostulate 4, 14–15\nSchrödinger time evolution and, 72–84\nspin angular momentum, 211–212\nvector model of, 58\nSpin-1 system, 59–62\nSpin-orbit coupling, 388–393\nSpin-orbit energy correction, 391, 392–393\nSpin-orbit Hamiltonian, 390\nSpin precession, 72–84\nexperiments, 77–78, 80–81\nmagnetic ﬁeld in general direction, 78–84\nmagnetic ﬁeld in the z-direction, 72–78\ntime-dependent Hamiltonians and, 90\nSpin-singlet fermions, interacting, 426\nSPINS software, 32–33, 521\nmagnetic ﬁeld in the z-direction, 74\nmatrix calculations in, 23\nmixed state and, 21\nStern-Gerlach experiment and, 5, 8\nWhich Path experiments and, 50\nSpin-statistics theorem, 412\nSpin-triplet fermions interacting, 426–427\nSpin up, 4\nSpin up state, eigenvalue equation for, 35\nSpin vector, S2 operator and magnitude of, \n57–59\nSpontaneous emission, 457–458\nSpontaneous emission rate, 458\ns-p superposition, 271\nSquare potential energy barrier, 189\nSquare wells, 470\nasymmetric, 147–150\nﬁnite. See Finite square well\ninﬁnite. See Inﬁnite square well\ns-states, Darwin term and, 392\nStandard deviation, 51, 52–54\ndeﬁned, 52\nStark effect, 271\nin hydrogen, 346–351\nlaser cooling and, 512\nState density, 484–486\nState vectors\nﬁrst-order correction to, 324–329\nnormalization of, 12–14, 15, 26\nStatic electric dipole moment, 271\n",
    "568 \nIndex\nStationary states, 70\nStatistical mixed state, 50\nStern, Otto, 1\nStern-Gerlach device, 2, 4\nStern-Gerlach experiment(s), 1–33\nwith applied uniform magnetic ﬁeld, 74\nexperiment 1, 5–6\nexperiment 2, 6–7, 60\nexperiment 3, 7–8, 47–50\nexperiment 4, 8–10, 47–50\ngeneral quantum systems, 25–27\nmagnetic trapping and, 502–503\nmatrix notation, 22–25\npostulates, 27–28\nprojection operator and, 47\nquantum state vectors, 10–22\nschematic of, 4\nsimulation, 33\nspin-1 system, 59–62\nZeeman effect and, 395–396, 406\nStern-Gerlach spin precession experiment,  \nquantum computing and, 520, 521\nStimulated emission, 452\nStong-ﬁeld seeking states, 504\nStretched state, 371\nSubshell, 434–437\nSuperﬂuid, 422–423\nSuperposition states, 19–22, 137–146\ncoherent, 50\ncoherent superposition and, 20\ncontinuous, 171–176\ndiscrete, 168–171\nof harmonic oscillator, 300–304\nhydrogen atom, 270–271\nmixed state vs., 19–20\nof momentum eigenstates, 165\nparticle on a ring in, 223–227\nprobability distribution of, 145\nquantum bits and, 515–516\nin Schrödinger cat experiment, 103\nin Stern-Gerlach experiment, 49\nSymmetric states, 411–413\nbosons and, 419, 421, 442\nSymmetric triplet states, 416\nSymmetrization postulate, 412–413, 418\nconsequences for many-particle system, 421–423\nhelium atom and, 427–433, 434\nperiodic table and, 434\nSz basis, 11\ndiagonalization of matrix and, 40–41\nSz representation, 23\nT\nTau, 84\nTaylor series expansion, 275\nTerm notation, 377\nThomas precession, 390\nTime dependence, 137–146\nin harmonic oscillator, 300–304\nof momentum eigenstate, 164\nTime-dependent Hamiltonians, 87–93\nlight-matter interactions, 92–93\nmagnetic resonance, 87–92\nTime-dependent perturbation theory, 445–468\nconstant perturbation, 449\nelectric dipole interaction, 454–462\nGaussian perturbation, 449–450\nharmonic perturbation, 450–454\nselection rules, 462–465\ntransition probability, 445–450\nTime-dependent problems, solving, 93\nTime evolution\nof energy eigenstates, 162–163\nof Gaussian wave packet, 201\nof harmonic oscillator, 311\nof inﬁnite square well solutions, 160\nwave packet and, 168–169\nTotal angular momentum, 503\naddition of angular momenta and, 372\ncoupled basis and, 366–367, 369, 371\nspectroscopic notation of, 377, 378\nZeeman effect and, 390, 391, 399\nTranscendental equations\nasymmetric square well, 150\nﬁnite square well, 130–131, 132\nTransition probability, 445–450\nas function of time, 452\nTransition rate, 453\nTransitions, 87, 445\nforbidden, 462\nharmonic oscillator and, 302, 306, 307\nin helium excited states, 433\nin hydrogen, 260\nTransmission coefﬁcient, 185–186\nfor scattering from square barrier, 191\nTransmission probability, for quantum mechanical \ntunneling, 190\nTriplet state, 368, 369\nsymmetric, 416\nTunneling, quantum mechanical, 188–192, 197, 201\n",
    " Index \n569\nTwo-body system, center-of-mass for, 204–205\nTwo-level system, perturbation and, 317–319\nTwo-particle Hamiltonian, 415\nTwo-particle probability density, 414\nTwo particles in one dimension, 414–423\nexchange interaction, 420–421\nsymmetrization postulate, consequences of, 421–423\ntwo-particle excited state, 416–417\ntwo-particle ground state, 415–416\nvisualization of states, 417–420\nTwo-particle wave function, 414\nTwo-state spin-1/2 quantum system, properties of \nnormalization, orthogonality and completeness \nin, 1\nTwo-well chain, energy eigenvalues and eigenstates \nof, 471–473\nU\nUnbound eigenstates, 469\nUnbound states, 120, 161–201\natom interferometry, 192–196\nfree particle eigenstates, 161–167\nscattering and, 181–188\ntunneling through barriers and, 188–192\nuncertainty principle, 176–181\nwave packets, 168–176\nUncertainty principle. See Heisenberg uncertainty \nprinciple\nUncoupled basis, 355, 361, 366, 378\nClebsch-Gordon coefﬁcients and, 374, 375\neigenstates, 366–369\nidentical particles and, 410–411, 415\nperturbation of hydrogen and, 390–391\nquantum bits and, 516, 517\nZeeman effect and, 397, 402–403, 404–405, 406\nUngerade state, 438–440, 442\nUnit vectors i, j, k ( n over letters), 11\nUpdate equations, 152\nV\nValence band, 493\nValence bond method, 441–442\nVector model, 58\nVectors. See also Bras (bra vectors); Eigenvectors; \nKets (ket vectors)\ncolumn, 22–23\ncomplete, 11\ncoupled basis, 378\nnormalized, 11\northogonal, 11\nquantum state. See Quantum state vectors\nrow, 23\nspatial, 11, 325\nspin, 57–59, 121, 123, 129, 484\nstate, 12–14, 15, 26, 324–329\nunit, 11\nwave, 121, 123, 129, 484\nVector space, Hilbert space, 10–11\nVelocity\nelectron, 386–388\ngroup, 170, 493\nphase, 162–163, 493\nrecoil, 507\nVelocity Verlet algorithm, 152\nVisualization of spherical harmonics, 240–244\nW\nWave function curvature, 133–135\nWave function formulae, translating bra-ket formulae \nto, 116, 154\nWave functions, 111, 112–119\nantisymmetric, 418\nboundary conditions on, 122, 156\nDirac expression for, 166\nof energy eigenstates of inﬁnite square well, 124\nGaussian momentum space, 172–176\nof harmonic oscillator, 284–289\nhydrogen, 263–269\nmolecular, 482–484\nmomentum space, 167, 171, 296–298\nnormalizing, 114–115, 116\nfor particle conﬁned to sphere, 227–228\nfor particle on a ring, 224–225\nof particle tunneling through barrier, 191\nradial, 261–263\nspherical harmonics, 244\nWave interference, 10\nWave packets, 165, 168–176, 197\ncontinuous superposition, 171–176\ndiscrete superposition, 168–171\nenvelope of, 170\nquantum tunneling and, 201\nuncertainty principle and, 176–181\nWave-particle duality, 125\nWave vector, 121, 484\nﬁnite square well, 129\ninﬁnite square well, 123, 129\nWave vector quantization condition, 123\nWeak-ﬁeld seeking states, 504\nWeak force/weak interaction, 84\n",
    "570 \nIndex\n“Welcher Weg” experiment, 50\n“Which Path” experiment, 50\nWigner-Eckhart theorem, 399\nX\nx-axis\nmagnetic ﬁeld component along, 78–79\nprobability for measuring spin component along, 75\nin Stern-Gerlach experiment, 6–7\nx-z plane, density plots in, 265, 267\nY\nYoung, Thomas, 10, 194\nZ\nz-axis\nprobability for measuring spin component  \nalong, 75\nin Stern-Gerlach experiment, 2–4, 5–8\nz-component, in Stern-Gerlach experiment, 3\nz-direction\nmagnetic ﬁeld in, 72–78\nmagnetic resonance and, 87–88\nZeeman effect, 382, 388, 393–406\nanomalous, 396, 405\nﬁrst-order energy correction, 400\nintermediate magnetic ﬁeld, 403–406, 407\nlaser cooling and, 512\nwith spin, 396\nstrong magnetic ﬁeld, 402–403, 405–406, 407\nweak magnetic ﬁeld, 396–401, 405–406, 407\nwithout spin, 394–396\nZeeman perturbation of 1s hyperﬁne structure, \n405–406\nZeeman energy levels, magnetic trapping and, 504\nZeeman Hamiltonian, 398\nZeeman perturbation Hamiltonian, 394\nZeroes, in Clebsch-Gordon tables, 375\nZero-point energy, 282\nZitterbewegung, 392\n",
    "Useful Definitions and Equations\nState vector, wave function:\t\n0 c9 \u001f c(x) = 8x\u001fc9\nNormalization:\t\n8c \u001f c9 =\nL\n\u001f\n-\u001f\n0\n c(x)02 dx = 1 \nMeasurement probability:\t\nan = 08an\u001fc902 = `\nL\n\u001f\n-\u001f\nw*\nan(x)c(x)dx `\n2\nExpectation value:\t\n8A9 = 8c0 A 0c9 = a\nn\nanan\nProbability density:\t\n (x) = 0 c(x)0 2\nPosition probability:\t\na  6  x 6  b =\nL\nb\na\n0c(x)0 2 dx\nPosition representation:\t\nxn \u001f  x,   pn \u001f -iU d\ndx\nEnergy eigenvalue equation:\t\nH0En9 = En0En9,  Hwn(x) = Enwn(x)\nOrthogonality:\t\n8En\u001fEm9 =\nL\n\u001f\n-\u001f\nw*\nn(x)wm(x)dx = dnm\nCompleteness:\t\n0c9 = a\nn\ncn0En9,  c(x) = a\nn\ncnwn(x)\n",
    "Useful Definitions and Equations\nSchrödinger equation:\t\niU d\ndt 0 c(t)9 = H(t)0 c(t)9\nSchrödinger time evolution:\t\n0 c(t)9 = a\nn\ncne-iEnt>U0En9\nPosition-momentum commutator:\t\n3xn ,  pn4 = iU\nMomentum space wave function:\t\nf(p) = 8p@ c9 =\n1\n22pU L\n\u001f\n-\u001f\n c(x)e-i px>Udx\nMomentum eigenstate:\t\n0 p9 \u001f wp(x) =\n1\n22pU\n eipx>U\nde Broglie wavelength:\t\nldeBroglie = h\np\nHeisenberg uncertainty relation: \t\n\u001ex\u001ep Ú U\n2\nPerturbation corrections:\t\nE(1)\nn\n= H\u001dnn = 8n(0)0H\u001d0n(0)9\n\t\nE(2)\nn\n= a\nm\u001fn\n08n(0)0H\u001d0m(0)902\n1E(0)\nn\n- E(0)\nm 2\nTransition probability:\t\ni S f(t) = 1\nU2 `\nL\nt\n0 8f\u001f H\u001d(t\u001d)\u001f i9ei1Ef -Ei2t\u001d>Udt\u001d `\n2\n",
    "Spin and Angular Momentum Relations\nSpin eigenvalue equations:\t\nSz 0+9 = U\n2 0+9, Sz0 -9 = - U\n2 0 -9\nSpin-1/2 eigenstates:\t\n\u001f+9 \u001f a1\n0b  \u001f+9x \u001f\n1\n22\n a1\n1b  \u001f+9y \u001f\n1\n22\n a1\ni b\n\t\n\u001f-9 \u001f a0\n1b  \u001f-9x \u001f\n1\n22\n a 1\n-1b \u001f-9y \u001f\n1\n22\n a 1\n-ib\nSpin-1/2 matrices:\n\t\nSx \u001f U\n2 a0\n1\n1\n0b\t\nSy \u001f U\n2 a0\n-i\ni\n0 b\n\t\nSz \u001f U\n2 a1\n0\n0\n-1b    S2 \u001f 3U2\n4  a1\n0\n0\n1b\nSpin-1 matrices:\n\t\nSx \u001f\nU\n12 °\n0\n1\n0\n1\n0\n1\n0\n1\n0\n¢    Sy \u001f\nU\n12 °\n0\n-i\n0\ni\n0\n-i\n0\ni\n0\n¢\n\t\nSz \u001f U °\n1\n0\n0\n0\n0\n0\n0\n0\n-1\n¢\t\nS2 \u001f 2U2 °\n1\n0\n0\n0\n1\n0\n0\n0\n1\n¢\nAngular momentum:\n\t\nJ 20 jmj9 =  j(j + 1)U20 jmj9\n\t\nJz0 jmj9 = mjU0 jmj9\n\t\nJ{0  jmj9 = U 3j( j + 1) - mj(mj { 1)4\n1>20\n  j, mj { 19\nOrbital angular momentum:\n L2Y/\nm(u, f) = /(/ + 1) U2Y/ m(u, f),  / = 0,1,2,3, ...\n\t\nLzY/ m(u, f) = m  UY / m(u, f),       m = -/,...,/\nAngular momentum commutators:\t 3Jx,Jy4 = iUJz,  3Jy,Jz4 = iUJx,  3Jz, Jx4 = iU Jy\n\t\n3J 2, Jx4 = 3J 2, Jy4 = 3J 2,Jz4 = 0 \n",
    "Bound State Systems\nInfinite square well:\t\n En = n2p2U2\n2mL2 , n = 1,2,3, ...\n\t\n wn(x) = A\n2\nL\n sin npx\nL\nHydrogen atom:\t\nEn = -  1\nn2 m\n2U2 a e2\n4pe0\nb\n2\n= -  1\nn2 1\n2\n a2mc2 = -  1\nn2 13.6 eV\nHarmonic oscillator:\t\n En = Uv an + 1\n2b, n = 0,1,2,3, ...\n\t\na = A\nmv\n2U axn + i pn\nmvb\n\t\na† = A\nmv\n2U axn - i pn\nmvb\n\t\na0 n9 = 2n0 n - 19\n\t\na†0 n9 = 2n + 10 n + 19\nFundamental Constants \nPlanck’s constant:\t\nU = 6.582 * 10-16 eVs\nSpeed of light:\t\nc = 299  792  458 m>s\nElectron mass:\t\nme c2 = 511 keV\nProton mass:\t\nmp c2 = 938 MeV\nFine-structure constant:\t\na =\ne2\n4pe0 Uc \u001e\n1\n137\nBohr radius:\t\na0 = 0.0529 nm\nBohr magneton:\t\nmB\nh = 1.40 MHz>Gauss\n"
  ],
  "full_text": "\n\nQUANTUM MECHANICS\nA Paradigms Approach\n\nThis page intentionally left blank \n\nBoston   Columbus   Indianapolis   New York   San Francisco   Upper Saddle River  \nAmsterdam   Cape Town   Dubai   London   Madrid   Milan   Munich   Paris   Montréal   Toronto  \nDelhi   Mexico City   São Paulo   Sydney   Hong Kong   Seoul   Singapore   Taipei   Tokyo\nQUANTUM MECHANICS\nA Paradigms Approach\nDavid H. McIntyre\nOregon State University\nwith contributions from Corinne A. Manogue, Janet Tate  \nand the Paradigms in Physics group at Oregon State University\n\nPublisher: Jim Smith\nEditorial Manager: Laura Kenney\nSenior Project Editor: Katie Conley\nAssistant Editors: Peter Alston and Steven Le\nSenior Marketing Manager: Kerry McGinnis\nManaging Editor: Corinne Benson\nProduction Project Manager: Mary O’Connell\nProduction Management and Composition: \nElement LLC\nCover Design: Mark Ong\nManufacturing Buyer: Kathy Sleys\nManager, Rights and Permissions: Zina Arabia\nManager, Cover Visual Research & \nPermissions: Karen Sanatar\nPrinter and Binder: Courier, Westford\nCover Printer: Courier, Westford\nCover Images: David H. McIntyre\nCopyright © 2012 Pearson Education, Inc., publishing as Pearson Addison-Wesley, 1301 Sansome St., San Francisco, \nCA 94111. All rights reserved. Manufactured in the United States of America. This publication is protected by Copyright \nand permission should be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, \nor transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise. To obtain \npermission(s) to use material from this work, please submit a written request to Pearson Education, Inc., Permissions \nDepartment, 1900 E. Lake Ave., Glenview, IL 60025. For information regarding permissions, call (847) 486-2635.\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. Where \nthose designations appear in this book, and the publisher was aware of a trademark claim, the designations have been printed \nin initial caps or all caps.\nLibrary of Congress Cataloging-in-Publication Data\nMcIntyre, David H.\n Quantum mechanics : a paradigms approach / David H. McIntyre ; with  \ncontributions from Corinne A. Manogue, Janet Tate, and the Paradigms in \nPhysics group at Oregon State University.\n  p. cm.\n Includes bibliographical references and index.\n ISBN-13: 978-0-321-76579-6\n ISBN-10: 0-321-76579-6\n 1. Quantum theory. 2. Mechanics.  I. Manogue, Corinne A. II. Tate, Janet.\nIII. Oregon State University. IV. Title.\n QC174.12.M3785 2012\n 530.12--dc23\n                2011039322\nISBN 10: 0-321-76579-6  \nISBN 13: 978-0-321-76579-6\n1 2 3 4 5 6 7 8 9 10 —CRW—16 15 14 13 12 11\nwww.pearsonhighered.com\n\n \nv\n Brief Contents\n 1 Stern-Gerlach Experiments \n1\n 2 Operators and Measurement \n34\n 3 Schrödinger Time Evolution \n68\n 4 Quantum Spookiness \n97\n 5 Quantized Energies: Particle in a Box \n107\n 6 Unbound States \n161\n 7 Angular Momentum \n202\n 8 Hydrogen Atom \n250\n 9 Harmonic Oscillator \n275\n 10 Perturbation Theory \n312\n 11 Hyperﬁne Structure and the Addition of Angular Momenta \n355\n 12 Perturbation of Hydrogen \n382\n 13 Identical Particles \n410\n 14 Time-Dependent Perturbation Theory \n445\n 15 Periodic Systems \n469\n 16 Modern Applications of Quantum Mechanics \n502\nAppendices \n529\nIndex \n553\n\nThis page intentionally left blank \n\n \nvii\nContents\nPreface \nxiii\nPrologue \nxix\n \n1 \u0002 Stern-Gerlach Experiments \n1\n1.1 \nStern-Gerlach Experiment 1\n1.1.1 \nExperiment 1 5\n1.1.2 \nExperiment 2 6\n1.1.3 \nExperiment 3 7\n1.1.4 \nExperiment 4 8\n1.2 \nQuantum State Vectors 10\n1.2.1 \nAnalysis of Experiment 1 16\n1.2.2 \nAnalysis of Experiment 2 16\n1.2.3 \nSuperposition States 19\n1.3 \nMatrix Notation 22\n1.4 \nGeneral Quantum Systems 25\n1.5 \nPostulates  27\nSummary 28\nProblems 29\nResources 32\nActivities 32\nFurther Reading 33\n \n2 \u0002 Operators and Measurement \n34\n2.1 \nOperators, Eigenvalues, and Eigenvectors 34\n2.1.1 \nMatrix Representation of Operators 37\n2.1.2 \nDiagonalization of Operators 38\n2.2 \nNew Operators 41\n2.2.1 \nSpin Component in a General Direction 41\n2.2.2 \nHermitian Operators 44\n2.2.3 \nProjection Operators 44\n2.2.4 \nAnalysis of Experiments 3 and 4 47\n2.3 \nMeasurement 50\n2.4 \nCommuting Observables 54\n2.5 \nUncertainty Principle 56\n2.6 \nS2 Operator 57\n2.7 \nSpin-1 System 59\n\nviii \nContents \n2.8 \nGeneral Quantum Systems 62\nSummary 63\nProblems 64\nResources 67\nActivities 67\n \n3 \u0002 Schrödinger Time Evolution \n68\n3.1 \nSchrödinger Equation 68\n3.2 \nSpin Precession 72\n3.2.1 \nMagnetic Field in the z-Direction 72\n3.2.2 \nMagnetic Field in a General Direction 78\n3.3 \nNeutrino Oscillations 84\n3.4 \nTime-Dependent Hamiltonians 87\n3.4.1 \nMagnetic Resonance 87\n3.4.2 \nLight-Matter Interactions 92\nSummary 93\nProblems 94\nResources 96\nActivities 96\nFurther Reading 96\n \n4 \u0002 Quantum Spookiness \n97\n4.1 \nEinstein-Podolsky-Rosen Paradox 97\n4.2 \nSchrödinger Cat Paradox 102\nProblems 105\nResources 106\nFurther Reading 106\n \n5 \u0002 Quantized Energies: Particle in a Box \n107\n5.1 \nSpectroscopy 107\n5.2 \nEnergy Eigenvalue Equation 110\n5.3 \nThe Wave Function 112\n5.4 \nInﬁnite Square Well 119\n5.5 \nFinite Square Well 128\n5.6 \nCompare and Contrast 133\n5.6.1 \nWave Function Curvature 133\n5.6.2 \nNodes 135\n5.6.3 \nBarrier Penetration 135\n5.6.4 \nInversion Symmetry and Parity 136\n5.6.5 \nOrthonormality 136\n5.6.6 \nCompleteness 137\n5.7 \nSuperposition States and Time Dependence 137\n5.8 \nModern Application: Quantum Wells and Dots 146\n5.9 \n Asymmetric Square Well: Sneak Peek at  \nPerturbations 147\n5.10  Fitting Energy Eigenstates by Eye or by  \nComputer 150\n5.10.1 Qualitative (Eyeball) Solutions 150\n\nContents  \n \nix\n5.10.2 Numerical Solutions 151\n5.10.3 General Potential Wells 154\nSummary 154\nProblems 156\nResources 159\nActivities 159\nFurther Reading 160\n \n6 \u0002 Unbound States \n161\n6.1 \nFree Particle Eigenstates 161\n6.1.1 \nEnergy Eigenstates 161\n6.1.2 \nMomentum Eigenstates 163\n6.2 \nWave Packets 168\n6.2.1 \nDiscrete Superposition 168\n6.2.2 \nContinuous Superposition 171\n6.3 \nUncertainty Principle 176\n6.3.1 \nEnergy Estimation 180\n6.4 \nUnbound States and Scattering 181\n6.5 \nTunneling Through Barriers 188\n6.6 \nAtom Interferometry 192\nSummary 197\nProblems 197\nResources 201\nActivities 201\nFurther Reading 201\n \n7 \u0002 Angular Momentum \n202\n7.1 \n Separating Center-of-Mass and Relative  \nMotion 204\n7.2 \n Energy Eigenvalue Equation in Spherical  \nCoordinates 208\n7.3 \nAngular Momentum 210\n7.3.1 \nClassical Angular Momentum 210\n7.3.2 \n Quantum Mechanical Angular  \nMomentum 210\n7.4 \nSeparation of Variables: Spherical Coordinates 215\n7.5 \nMotion of a Particle on a Ring 218\n7.5.1 \nAzimuthal Solution 220\n7.5.2 \n Quantum Measurements on a Particle  \nConﬁned to a Ring 223\n7.5.3 \nSuperposition States 224\n7.6 \nMotion on a Sphere 227\n7.6.1 \nSeries Solution of Legendre’s Equation 228\n7.6.2 \nAssociated Legendre Functions 233\n7.6.3 \nEnergy Eigenvalues of a Rigid Rotor 236\n7.6.4 \nSpherical Harmonics 237\n7.6.5 \nVisualization of Spherical Harmonics 240\n\nx \nContents \nSummary 245\nProblems 245\nResources 249\nActivities 249\n \n8 \u0002 Hydrogen Atom \n250\n8.1 \nThe Radial Eigenvalue Equation 250\n8.2 \nSolving the Radial Equation 252\n8.2.1 \n Asymptotic Solutions to the Radial  \nEquation 252\n8.2.2 \nSeries Solution to the Radial Equation 253\n8.3 \nHydrogen Energies and Spectrum 256\n8.4 \nThe Radial Wave Functions 261\n8.5 \nThe Full Hydrogen Wave Functions 263\n8.6 \nSuperposition States 270\nSummary 272\nProblems 272\nResources 274\nActivities 274\nFurther Reading 274\n \n9 \u0002 Harmonic Oscillator \n275\n9.1 \nClassical Harmonic Oscillator 275\n9.2 \nQuantum Mechanical Harmonic Oscillator 277\n9.3 \nWave Functions 284\n9.4 \nDirac Notation 289\n9.5 \nMatrix Representations 293\n9.6 \nMomentum Space Wave Function 296\n9.7 \nThe Uncertainty Principle 298\n9.8 \nTime Dependence 300\n9.9 \nMolecular Vibrations 305\nSummary 307\nProblems 308\nResources 311\nActivities 311\nFurther Reading 311\n \n10 \u0002 Perturbation Theory \n312\n10.1 Spin-1/2 Example 313\n10.2 General Two-Level Example 317\n10.3 Nondegenerate Perturbation Theory 319\n10.3.1 First-Order Energy Correction 320\n10.3.2 First-Order State Vector Correction 324\n10.4  Second-Order Nondegenerate Perturbation  \nTheory 329\n10.5 Degenerate Perturbation Theory 336\n10.6 More Examples 343\n\nContents  \n \nxi\n10.6.1 Harmonic Oscillator 343\n10.6.2 Stark Effect in Hydrogen 346\nSummary 351\nProblems 352\n \n11 \u0002  Hyperﬁne Structure and the Addition of \nAngular Momenta \n355\n11.1 Hyperﬁne Interaction 355\n11.2 Angular Momentum Review 357\n11.3 Angular Momentum Ladder Operators 359\n11.4 Diagonalization of the Hyperﬁne Perturbation 361\n11.5 The Coupled Basis 365\n11.6 Addition of Generalized Angular Momenta 370\n11.7  Angular Momentum in Atoms and Spectroscopic  \nNotation 377\nSummary 377\nProblems 379\nResources 381\nActivities 381\nFurther Reading 381\n \n12 \u0002 Perturbation of Hydrogen \n382\n12.1 Hydrogen Energy Levels 382\n12.2 Fine Structure of Hydrogen 386\n12.2.1 Relativistic Correction 386\n12.2.2 Spin-Orbit Coupling 388\n12.3 Zeeman Effect 393\n12.3.1 Zeeman Effect without Spin 394\n12.3.2 Zeeman Effect with Spin 396\n12.3.2.1 Weak magnetic ﬁeld 396\n12.3.2.2 Strong magnetic ﬁeld 402\n12.3.2.3 Intermediate magnetic ﬁeld 403\n12.3.3  Zeeman Perturbation of the 1s \nHyperﬁne Structure 405\nSummary 407\nProblems 407\nResources 409\nActivities 409\nFurther Reading 409\n \n13 \u0002 Identical Particles \n410\n13.1 Two Spin-1/2 Particles 410\n13.2 Two Identical Particles in One Dimension 414\n13.2.1 Two-Particle Ground State 415\n13.2.2 Two-Particle Excited State 416\n13.2.3 Visualization of States 417\n13.2.4 Exchange Interaction 420\n\nxii \nContents \n13.2.5  Consequences of the Symmetrization  \nPostulate 421\n13.3 Interacting Particles 423\n13.4 Example: The Helium Atom 427\n13.4.1 Helium Ground State 428\n13.4.2 Helium Excited States 431\n13.5 The Periodic Table 434\n13.6 Example: The Hydrogen Molecule 437\n13.6.1 The Hydrogen Molecular Ion H2\n+ 438\n13.6.2 The Hydrogen Molecule H2 440\nSummary 442\nProblems 442\nResources 444\nFurther Reading 444\n \n14 \u0002 Time-Dependent Perturbation Theory \n445\n14.1 Transition Probability 445\n14.2 Harmonic Perturbation 450\n14.3 Electric Dipole Interaction 454\n14.3.1 Einstein Model: Broadband Excitation 456\n14.3.2 Laser Excitation 460\n14.4 Selection Rules 462\nSummary 466\nProblems 467\nResources 468\nFurther Reading 468\n \n15 \u0002 Periodic Systems \n469\n15.1  The Energy Eigenvalues and Eigenstates of a  \nPeriodic Chain of Wells 471\n15.1.1 A Two-Well Chain 471\n15.1.2 N-Well Chain 473\n15.2  Boundary Conditions and the Allowed Values  \nof k 476\n15.3 The Brillouin Zones 478\n15.4 Multiple Bands from Multiple Atomic Levels 478\n15.5 Bloch’s Theorem and the Molecular States 480\n15.6 Molecular Wave Functions—a Gallery 482\n15.7 The Density of States 484\n15.8 Calculation of the Model Parameters 486\n15.8.1 LCAO Summary 488\n15.9 The Kronig-Penney Model 489\n15.10  Practical Applications: Metals, Insulators, and  \nSemiconductors 491\n15.11 Effective Mass 494\n15.12 Direct and Indirect Band Gaps 496\n15.13 New Directions—Low-Dimensional Carbon 497\n\nContents  \n \nxiii\nSummary 498\nProblems 499\nResources 500\nActivities 500\nFurther Reading 500\n \n16 \u0002 Modern Applications of Quantum Mechanics \n502\n16.1  Manipulating Atoms with Quantum  \nMechanical Forces 502\n16.1.1 Magnetic Trapping 502\n16.1.2 Laser Cooling 506\n16.2 Quantum Information Processing 514\n16.2.1 Quantum Bits—Qubits 515\n16.2.2 Quantum Gates 518\n16.2.3 Quantum Teleportation 524\nSummary 526\nProblems 527\nResources 528\nFurther Reading 528\nAppendix A: Probability \n529\nAppendix B: Complex Numbers \n533\nAppendix C: Matrices \n537\nAppendix D: Waves and Fourier Analysis \n541\nAppendix E: Separation of Variables \n547\nAppendix F: Integrals \n549\nAppendix G: Physical Constants \n551\nIndex \n553\n\nThis page intentionally left blank \n\n \nxv\nPreface\nThis text is designed to introduce undergraduates at the junior and senior levels to quantum mechan-\nics. The text is an outgrowth of the new physics major curriculum developed by the Paradigms in \nPhysics program at Oregon State University. This new curriculum distributes material from the sub-\ndisciplines throughout the two upper-division years and provides students with a more gradual tran-\nsition between introductory and advanced levels. We have also incorporated and developed modern \npedagogical strategies to help improve student learning. This text covers the quantum mechanical \naspects of our curriculum in a way that can also be used in traditional curricula, but that still pre-\nserves the advantages of the Paradigms approach to the ordering of materials and the use of student \nengagement activities.\nPARADIGMS PROGRAM\nThe Paradigms project began in 1997, when the Department of Physics at Oregon State University \nbegan an extensive revision of the upper-division physics major. In an effort to encourage students \nto draw connections between the subdisciplines of physics, the structure of the Paradigms has been \ncrafted to mimic the organization of expert physics knowledge. Students are presented with a model \nof how physicists organize their understanding of physical phenomena and problem solving. Each \nof the nine short junior-year Paradigms courses focuses on a speciﬁc paradigm or class of physics \nproblems that serves as the centerpiece of the course and on which different tools and skills are built. \nIn the senior year, students resume a more traditional curriculum, taking six capstone courses in \nthe traditional disciplines. This curriculum incorporates a diverse set of student activities that allow \nstudents to stay actively engaged in the classroom and to work together in constructing their under-\nstanding of physics. Computer resources are used frequently to help students visualize the systems \nthey are studying.\nCONTENT AND APPROACH\nQuantum mechanics is integrated into four of the junior-year Paradigms courses and one senior-year \ncapstone course at Oregon State University. This text includes all the quantum mechanics topics \ncovered in those ﬁve courses. We adopt a “spins-ﬁrst” approach by introducing quantum mechanics \nthrough the analysis of sequential Stern-Gerlach spin measurements. This approach is based upon \nprevious presentations of spin systems by Feynman, Leighton, and Sands; Cohen-Tannoudji, Diu, \nand Laloe; Sakurai; and Townsend. The aim of the spins-ﬁrst approach is twofold: (1) To imme-\ndiately immerse students in the inherently quantum mechanical aspects of physics by focusing on \nsimple measurements that have no classical explanation, and (2) To give students early and extensive \nexperience with the mechanics of quantum mechanics in the forms of Dirac and matrix notation. \n\nxvi \nPreface \nThe  simplicity of the spin-1/2 and spin-1 systems allows the students to focus on these new features, \nwhich run counter to classical mechanics.\nThe ﬁrst three chapters of this text deal exclusively with spin systems and extensions to general \ntwo- and three-state quantum mechanical systems. The basic postulates of quantum mechanics are \nillustrated through their manifestation in the Stern-Gerlach experiments. After these three chapters, \nstudents have the tools to tackle any quantum mechanical problem presented in Dirac or matrix \nnotation. After a brief interlude into quantum spookiness (the EPR Paradox and Schrödinger’s cat) \nin Chapter 4, we tackle the traditional wave function aspects of quantum mechanics. We present \nseveral quantum systems—a particle in a box, on a ring, on a sphere, the hydrogen atom, and the \nharmonic oscillator—and emphasize their common features and their connections to the basic pos-\ntulates. The differential equations of angular momentum and the hydrogen atom radial problem are \nsolved in detail to expose students to the rigor of series solutions, though we stress that these are \nagain eigenvalue equations, no different in principle from the spin eigenvalue equations. Whenever \npossible, we continue the use of Dirac notation and matrix notation learned in the spin chapters, \nemphasizing the importance of ﬂuency in multiple representations. We build upon the spins-ﬁrst \napproach by using the spin-1/2 example to introduce perturbation theory, the addition of angular \nmomentum, and identical particles.\nUSAGE\nAt Oregon State University, the content of this text is taught in ﬁve courses as shown below.\nJunior-Year Paradigms Courses\nSpin and Quantum \nMeasurement\nWaves\nCentral Forces\nPeriod Systems\n1.  Stern-Gerlach  \nExperiments\n2.  Operators and  \nMeasurement\n3.  Schrödinger Time \nEvolution\n4. Quantum Spookiness\nMechanical waves \nand EM waves\n5.  Quantized Energies:  \nParticle in a Box\n6. Unbound States\nPlanetary orbits\n7.  Angular  \nMomentum\n8. Hydrogen Atom\nCoupled \nOscillations\n15.  Periodic  \nSystems\nSenior-Year Quantum Mechanics Capstone Course\n 9. Harmonic Oscillator\n10. Perturbation Theory\n11.  Hyperﬁne Structure \nand the Addition of \nAngular Momentum\n12.  Perturbation of \nHydrogen\n13. Identical Particles\n14.  Time-Dependent \nPerturbation \nTheory\n16.  Modern \nApplications\nFor a traditional curriculum, the content of this text would cover a full-year course, either two \nsemesters or three quarters. A proposed weekly outline for two 15-week semesters or three 10-week \nquarters is shown below.\n\nPreface  \nxvii\nWeek\nChapter\nTopics\n1\n1\nStern-Gerlach experiment, Quantum State Vectors, Bra-ket notation \n2\n1\nMatrix notation, General Quantum Systems\n3\n2\nOperators, Measurement, Commuting Observables\n4\n2\nUncertainty Principle, S2 Operator, Spin-1 System\n5\n3\nSchrödinger Equation, Time Evolution\n6\n3\nSpin Precession, Neutrino Oscillations, Magnetic Resonance\n7\n4\nEPR Paradox, Bell’s Inequalities, Schrödinger’s Cat\n8\n5\nEnergy Eigenvalue Equation, Wave Function\n9\n5\nOne-Dimensional Potentials, Finite Well, Inﬁnite Well\n10\n6\nFree Particle, Wave Packets, Momentum Space\n11\n6\nUncertainty Principle, Barriers\n12\n7\nThree-Dimensional Energy Eigenvalue Equation, Separation of Variables\n13\n7\nAngular Momentum, Motion on a Ring and Sphere, Spherical Harmonics\n14\n8\nHydrogen Atom, Radial Equation, Energy Eigenvalues\n15\n8\nHydrogen Wave Functions, Spectroscopy\n16\n9\n1-D Harmonic Oscillator, Operator Approach, Energy Spectrum\n17\n9\nHarmonic Oscillator Wave Functions, Matrix Representation\n18\n9\nMomentum Space Wave Functions, Time Dependence, Molecular Vibrations\n19\n10\nTime-Independent Perturbation Theory: Nondegenerate, Degenerate\n20\n10\nPerturbation Examples: Harmonic Oscillator, Stark Effect in Hydrogen\n21\n11\nHyperﬁne Structure, Coupled Basis\n22\n11\nAddition of Angular Momenta, Clebsch-Gordan Coefﬁcients\n23\n12\nHydrogen Atom: Fine Structure, Spin-Orbit, Zeeman Effect\n24\n13\nIdentical Particles, Symmetrization, Helium Atom\n25\n14\nTime-Dependent Perturbation Theory, Harmonic Perturbation\n26\n14\nRadiation, Selection Rules\n27\n15\nPeriodic Potentials, Bloch’s Theorem\n28\n15\nDispersion Relation, Density of States, Semiconductors\n29\n16\nModern Applications of Quantum Mechanics, Laser Cooling and Trapping\n30\n16\nQuantum Information Processing\n\nxviii \nPreface \nAUDIENCE AND EXPECTED BACKGROUND\nThe intended audience is junior and senior physics majors, who are expected to have taken intermediate-\nlevel courses in modern physics and linear algebra. No other upper-level physics or mathematics courses \nare required. For our own students, we review matrix algebra in a seven contact hour “preface” course \nthat precedes the Paradigms courses that teach quantum mechanics. The material for that preface course \nis in Appendix C. The material in Appendix B summarizes an earlier Paradigms course on oscillations, \nand the material in Appendix D summarizes the classical wave part of the Paradigms course on waves.\nSTUDENT ACTIVITIES AND WEBSITE\nStudent engagement activities are an integral part of the Paradigms curriculum. All of the activities \nthat we have developed are freely available on our wiki website:\nhttp://physics.oregonstate.edu/portfolioswiki\nThe wiki contains a wealth of information about the Paradigms project, the courses we teach, and the \nmaterials we have developed. Details about individual activities include descriptions, student handouts, \ninstructor’s guides, advice about how to use active engagement strategies, videos of classroom prac-\ntice, narratives of classroom activities, and comments from users—both internal and external to Oregon \nState University. This is a dynamic website that is continually updated as we develop new activities and \nimprove existing ones. We encourage you to visit the website and join the community. E-mail us with \ncorrections, additions, and suggestions.\nEach of the quantum mechanics activities that we use in our ﬁve courses is referenced in the \nresource section at the end of the appropriate chapter in the text. The quantum mechanics activities are \ncollected within the wiki website with a direct link: \nwww.physics.oregonstate.edu/qmactivities \nThese activities include different types of activities such as computer-based activities, group activities, \nand class response activities. The most extensive activity is a computer simulation of Stern-Gerlach \nexperiments. This SPINS software is a full-featured, menu-driven application that allows students to \nsimulate successive Stern-Gerlach measurements and explore incompatible observables, eigenstate \nexpansions, interference, and quantum dynamics. The use of the SPINS software facilitates our spins-\nﬁrst approach. The beauty of the simulation is that students steeped in classical physics perform a foun-\ndational quantum experiment and learn the most fascinating and counterintuitive aspects of quantum \nmechanics at an early stage.\nACKNOWLEDGMENTS\nThis work is the product of a broad and energetic community of educators and students within the \nParadigms in Physics program. I thank all of our students for their hard work, insights, and innu-\nmerable suggestions. My colleagues Corinne Manogue and Janet Tate have developed some of the \ncourses upon which this text is based. They have worked with me throughout the writing of this text \nand I am indebted to them for their valuable contributions. I gratefully acknowedge my fellow faculty \nwho have developed and taught in the new curriculum: Dedra Demaree, Tevian Dray, Tomasz Gieb-\nultowicz, Elizabeth Gire, William Hetherington, Henri Jansen, Kenneth Krane, Yun-Shik Lee, Victor \nMadsen, Ethan Minot, Oksana Ostroverkhova, David Roundy, Philip Siemens, Albert Stetz, William \n\nPreface   \nxix\nWarren, and Allen Wasserman. I would also like to acknowledge the important contributions of early \nteaching assistants Kerry Browne, Jason Janesky, Cheryl Klipp, Katherine Meyer, Steve Sahyun, and \nEmily Townsend—their expertise, dedication, and enthusiasm were above and beyond the call of \nduty. The many subsequent teaching assistants have also been enthusiastic and valued contributors. \nI also thank those who have contributed in various ways to the development of activities: Mario Bel-\nloni, Tim Budd, Wolfgang Christian, Paco Esquembre, Lichun Jia, and Shannon Mayer. I particularly \nthank Daniel Schroeder for sharing his original SPINS software. I acknowledge useful and construc-\ntive feedback from Jeffrey Dunham, Joshua Folk, Rubin Landau, Edward (Joe) Redish, Joseph Roth-\nberg, Homeyra Sadaghiani, Daniel Schroeder, Chandralekha Singh, and Daniel Styer. The Paradigms \nadvisory committee has also provided valuable feedback and I acknowledge David Grifﬁths, Bruce \nMason, William McCallum, Harriett Platsek, and Michael Wittmann for their help. I am grateful to the \nsuccessive Physics Department chairs, Kenneth Krane and Henri Jansen, and Deans Fred Horne and \nSherman Bloomer at Oregon State University for their endorsement of the Paradigms project.\nThis material is based on work supported by the National Science Foundation under Grant Nos. \n9653250, 0231194, and 0618877. Any opinions, ﬁndings, and conclusions or recommendations \nexpressed in this material are those of the authors and do not necessarily reﬂect the views of the \nNational Science Foundation. I thank Duncan McBride and Jack Hehn for their encouragement and \nsupport of our endeavor.\nJim Smith at Addison Wesley has been enthusiastic about this project from the early stages. Peter \nAlston has navigated me through the editorial process with skill and patience. I am grateful to them \nand also to Katie Conley, Steven Le, and the rest of the staff at Addison Wesley for their work to pro-\nduce this text.\nDavid H. McIntyre\nCorvallis, Oregon  \nNovember 2011\n\nThis page intentionally left blank \n\n \nxxi\nPrologue\nIt was a dark and stormy night. Erwin huddled under his covers as he had done numerous times that \nsummer. As the wind and rain lashed at the window, he feared having to retreat to the storm cellar \nonce again. The residents of Erwin’s apartment building sought shelter whenever there were threats of \ntornadoes in the area. While it was safe down there, Erwin feared the ridicule he would face once again \nfrom the other school boys. In the rush to the cellar, Erwin seemed to always end up with a random \npair of socks, and the other boys teased him about it mercilessly.\nNot that Erwin hadn’t tried hard to solve this problem. He had a very simple collection of \nsocks—black or white, for either school or play; short or long, for either trousers or lederhosen. \nAfter the ﬁrst few teasing episodes from the other boys, Erwin had sorted his socks into two sepa-\nrate drawers. He placed all the black socks in one drawer and all the white socks in another drawer. \nErwin ﬁgured he could determine an individual sock’s length in the dark of night simply by feel-\ning it, but he had to have them presorted into white and black because the apartment generally lost \npower before the call to the shelter.\nUnfortunately, Erwin found that this presorting of the socks by color was ineffective. Whenever \nhe reached into the white sock drawer and chose two long socks, or two short socks, there was a 50% \nprobability of any one sock being black or white. The results from the black sock drawer were the \nsame. The socks seemed to have “forgotten” the color that Erwin had determined previously.\nErwin also tried sorting the socks into two drawers based upon their length, without regard to \ncolor. When he chose black or white socks from these long and short drawers, the socks had also “for-\ngotten” whether they were long or short.\nAfter these fruitless attempts to solve his problem through experiments, Erwin decided to save \nhimself the fashion embarrassment, and he replaced his sock collection with a set of medium length \nbrown socks. However, he continued to ponder the mysteries of the socks throughout his childhood.\nAfter many years of daydreaming about the mystery socks, Erwin Schrödinger proposed his the-\nory of “Quantum Socks” and become famous. And that is the beginning of the story of the quantum \nsocks.\nThe End.\nFarfetched?? You bet. But Erwin’s adventure with his socks is the way quantum mechanics works. \nRead on.\n\nThis page intentionally left blank \n\n \n1\nC H A P T E R \n1\nStern-Gerlach Experiments\nIt was not a dark and stormy night when Otto Stern and Walther  Gerlach performed their now famous \nexperiment in 1922. The Stern-Gerlach experiment demonstrated that measurements on microscopic \nor quantum particles are not always as certain as we might expect. Quantum particles behave as mys-\nteriously as Erwin’s socks—sometimes forgetting what we have already measured. Erwin’s adven-\nture with the mystery socks is farfetched because you know that everyday objects do not behave like \nhis socks. If you observe a sock to be black, it remains black no matter what other properties of the \nsock you observe. However, the Stern- Gerlach experiment goes against these ideas. Microscopic or \nquantum particles do not behave like the classical objects of your everyday experience. The act of \nobserving a quantum particle affects its measurable properties in a way that is foreign to our classical \nexperience.\nIn these ﬁrst three chapters, we focus on the Stern-Gerlach experiment because it is a conceptu-\nally simple experiment that demonstrates many basic principles of quantum mechanics. We discuss \na variety of experimental results and the quantum theory that has been developed to predict those \nresults. The mathematical formalism of quantum mechanics is based upon six postulates that we will \nintroduce as we develop the theoretical framework. (A complete list of these postulates is in Section 1.5.) \nWe use the Stern-Gerlach experiment to learn about quantum mechanics theory for two primary reasons: \n(1) It demonstrates how quantum mechanics works in principle by illustrating the postulates of quan-\ntum mechanics, and (2) it demonstrates how quantum mechanics works in practice through the use \nof Dirac notation and matrix mechanics to solve problems. By using a simple example, we can focus \non the principles and the new mathematics, rather than having the complexity of the physics obscure \nthese new aspects.\n1.1 \u0002 STERN-GERLACH EXPERIMENT\nIn 1922 Otto Stern and Walther Gerlach performed a seminal experiment in the history of quantum \nmechanics. In its simplest form, the experiment consisted of an oven that produced a beam of neu-\ntral atoms, a region of space with an inhomogeneous magnetic ﬁeld, and a detector for the atoms, as \ndepicted in Fig. 1.1. Stern and Gerlach used a beam of silver atoms and found that the beam was split \ninto two in its passage through the magnetic ﬁeld. One beam was deﬂected upwards and one down-\nwards in relation to the direction of the magnetic ﬁeld gradient.\nTo understand why this result is so at odds with our classical expectations, we must ﬁrst analyze \nthe experiment classically. The results of the experiment suggest an interaction between a neutral parti-\ncle and a magnetic ﬁeld. We expect such an interaction if the particle possesses a magnetic moment M.\nThe potential energy of this interaction is E = -M~B, which results in a force F = \u00021M~B2. In the \n\n2 \nStern-Gerlach Experiments\nStern-Gerlach experiment, the magnetic ﬁeld gradient is primarily in the z-direction, and the resulting \nz-component of the force is\n \n Fz =\n0\n0z\n 1M~B2 \n(1.1)\n \n \u0002 mz \n0Bz\n0z\n .\n \nThis force is perpendicular to the direction of motion and deﬂects the beam in proportion to the com-\nponent of the magnetic moment in the direction of the magnetic ﬁeld gradient.\nNow consider how to understand the origin of the atom’s magnetic moment from a classical view-\npoint. The atom consists of charged particles, which, if in motion, can produce loops of current that give \nrise to magnetic moments. A loop of area A and current I produces a magnetic moment\n \nm = IA \n(1.2)\nin MKS units. If this loop of current arises from a charge q traveling at speed v in a circle of radius r, \nthen\n \n m =\nq\n2pr>v\n pr 2 \n \n = qrv\n2\n \n(1.3)\n \n =\nq\n2m\n L ,\n \nwhere L = mrv is the orbital angular momentum of the particle. In the same way that the earth \nrevolves around the sun and rotates around its own axis, we can also imagine a charged particle in \nan atom having orbital angular momentum L and a new property, the intrinsic angular momen-\ntum, which we label S and call spin. The intrinsic angular momentum also creates current loops, \nso we expect a similar relation between the magnetic moment M and S. The exact calculation \nx\nOven\nCollimator\nMagnet\nDetector\nMagnet\nCross-Section\ny\nz\nS\nS\nN\nN\nFIGURE 1.1 Stern-Gerlach experiment to measure the spin component of neutral \nparticles along the z-axis. The magnet cross section at right shows the inhomogeneous \nﬁeld used in the experiment.\n\n1.1 Stern-Gerlach Experiment \n3\ninvolves an integral over the charge distribution, which we will not do. We simply assume that we \ncan relate the magnetic moment to the intrinsic angular momentum in the same fashion as Eq. (1.3), \ngiving\n \nM = g q\n2m\n S , \n(1.4)\nwhere the dimensionless gyroscopic ratio g contains the details of that integral.\nA silver atom has 47 electrons, 47 protons, and 60 or 62 neutrons (for the most common isotopes). \nThe magnetic moments depend on the inverse of the particle mass, so we expect the heavy protons and \nneutrons (\u00032000 me) to have little effect on the magnetic moment of the atom and so we neglect them. \nFrom your study of the periodic table in chemistry, you recall that silver has an electronic conﬁgura-\ntion 1s22s22p63s23p64s23d104p64d105s1, which means that there is only the lone 5s electron outside \nof the closed shells. The electrons in the closed shells can be represented by a spherically symmetric \ncloud with no orbital or intrinsic angular momentum (unfortunately we are injecting some quantum \nmechanical knowledge of atomic physics into this classical discussion). That leaves the lone 5s elec-\ntron as a contributor to the magnetic moment of the atom as a whole. An electron in an s state has no \norbital angular momentum, but it does have spin. Hence the magnetic moment of this electron, and \ntherefore of the entire neutral silver atom, is\n \nM = -g e\n2me\n S , \n(1.5)\nwhere e is the magnitude of the electron charge. The classical force on the atom can now be written as\n \nFz \u0002 -g e\n2me\n Sz \n0Bz\n0z\n . \n(1.6)\nThe deﬂection of the beam in the Stern-Gerlach experiment is thus a measure of the component (or pro-\njection) Sz of the spin along the z-axis, which is the orientation of the magnetic ﬁeld gradient.\nIf we assume that the 5s electron of each atom has the same magnitude 0 S0  of the intrinsic angular \nmomentum or spin, then classically we would write the z-component as Sz = 0 S0 cos u, where u is \nthe angle between the z-axis and the direction of the spin S. In the thermal environment of the oven, \nwe expect a random distribution of spin directions and hence all possible angles u. Thus we expect \nsome continuous distribution (the details are not important) of spin components from Sz = - 0 S0  to \nSz = + 0 S0 , which would yield a continuous spread in deﬂections of the silver atomic beam. Rather, \nthe experimental result that Stern and Gerlach observed was that there are only two deﬂections, indi-\ncating that there are only two possible values of the z-component of the electron spin. The magnitudes \nof these deﬂections are consistent with values of the spin component of\n \nSz = { U\n2\n ,  \n(1.7)\nwhere U is Planck’s constant h divided by 2p and has the numerical value\n \n U = 1.0546 * 10-34  J~s \n \n = 6.5821 * 10-16  eV~s. \n \n(1.8)\nThis result of the Stern-Gerlach experiment is evidence of the quantization of the electron’s \nspin angular momentum component along an axis. This quantization is at odds with our classical \n\n4 \nStern-Gerlach Experiments\nexpectations for this measurement. The factor of 1/2 in Eq. (1.7) leads us to refer to this as a \nspin-1/2 system.\nIn this example, we have chosen the z-axis along which to measure the spin component, but there \nis nothing special about this direction in space. We could have chosen any other axis and we would \nhave obtained the same results.\nNow that we know the ﬁne details of the Stern-Gerlach experiment, we simplify the experiment \nfor the rest of our discussions by focusing on the essential features. A simpliﬁed schematic representa-\ntion of the experiment is shown in Fig. 1.2, which depicts an oven that produces the beam of atoms, a \nStern-Gerlach device with two output ports for the two possible values of the spin component, and two \ncounters to detect the atoms leaving the output ports of the Stern-Gerlach device. The Stern-Gerlach \ndevice is labeled with the axis along which the magnetic ﬁeld is oriented. The up and down arrows \nindicate the two possible measurement results for the device; they correspond respectively to the \nresults Sz = {U>2 in the case where the ﬁeld is oriented along the z-axis. There are only two possible \nresults in this case, so they are generally referred to as spin up and spin down. The physical quantity \nthat is measured, Sz in this case, is called an observable. In our detailed discussion of the experiment \nabove, we chose the ﬁeld gradient in such a manner that the spin up states were deﬂected upwards. \nIn this new simpliﬁcation, the deﬂection itself is not an important issue. We simply label the output \nport with the desired state and count the particles leaving that port. The Stern-Gerlach device sorts \n(or ﬁlters, selects or analyzes) the incoming particles into the two possible outputs Sz = {U>2 in the \nsame way that Erwin sorted his socks according to color or length. We follow convention and refer to \na Stern-Gerlach device as an analyzer.\nIn Fig. 1.2, the input and output beams are labeled with a new symbol called a ket. We use the \nket 0  +9 as a mathematical representation of the quantum state of the atoms that exit the upper port \ncorresponding to Sz = +U>2. The lower output beam is labeled with the ket 0  -9, which corresponds \nto Sz = -U>2, and the input beam is labeled with the more generic ket 0  c9. The kets are representa-\ntions of the quantum states. They are used in mathematical expressions and they represent all the \ninformation that we can know about the state. This ket notation was developed by Paul A. M. Dirac \nand is central to the approach to quantum mechanics that we take in this text. We will discuss the \nmathematics of these kets in full detail later. With regard to notation, you will ﬁnd many different \nways of writing the same ket. The symbol within the ket brackets is any simple label to distinguish \nthe ket from other different kets. For example, the kets 0  +9, 0  +U>29, 0 Sz = +U>29, 0  +zn9, and 0 c9 \nare all equivalent ways of writing the same thing, which in this case signiﬁes that we have measured \nthe z-component of the spin and found it to be +U>2 or spin up. Though we may label these kets in \ndifferent ways, they all refer to the same physical state and so they all behave the same mathemati-\ncally. The symbol 0 {9 refers to both the 0  +9 and 0  -9 kets. The ﬁrst postulate of quantum mechanics \ntells us that kets in general describe the quantum state mathematically and that they contain all the \ninformation that we can know about the state. We denote a general ket as 0  c9.\nZ\n50\n50\n\u0002\u0002\u0003\n\u0002\u0003\u0003\n\u0002Ψ\u0003\nFIGURE 1.2 Simpliﬁed schematic of the Stern-Gerlach experiment, \ndepicting a source of atoms, a Stern-Gerlach analyzer, and two counters.\n\n1.1 Stern-Gerlach Experiment \n5\nPostulate 1\nThe state of a quantum mechanical system, including all the information you \ncan know about it, is represented mathematically by a normalized ket 0  c9.\nWe have chosen the particular simpliﬁed schematic representation of the Stern-Gerlach \nexperiment shown in Fig. 1.2, because it is the same representation used in the SPINS software \nprogram that you may use to simulate these experiments. The SPINS program allows you to per-\nform all the experiments described in this text. This software is freely available, as detailed in \nResources at the end of the chapter. In the SPINS program, the components are connected with \nsimple lines to represent the paths the atoms take. The directions and magnitudes of deﬂections \nof the beams in the program are not relevant. That is, whether the spin up output beam is drawn \nas deﬂected upwards, downwards, or not at all, is not relevant. The labeling on the output port is \nenough to tell us what that state is. Thus the extra ket label 0  +9 on the spin up output beam in Fig. \n1.2 is redundant and will be dropped soon.\nThe SPINS program permits alignment of Stern-Gerlach analyzing devices along all three axes \nand also at any angle f measured from the x-axis in the x-y plane. This would appear to be difﬁcult, if \nnot impossible, given that the atomic beam in Fig. 1.1 is directed along the y-axis, making it unclear \nhow to align the magnet in the y-direction and measure a deﬂection. In our depiction and discussion of \nStern-Gerlach experiments, we ignore this technical complication.\nIn the SPINS program, as in real Stern-Gerlach experiments, the numbers of atoms detected \nin particular states can be predicted by probability rules that we will discuss later. To simplify \nour schematic depictions of Stern-Gerlach experiments, the numbers shown for detected atoms \nare those obtained by using the calculated probabilities without any regard to possible statistical \nuncertainties. That is, if the theoretically predicted probabilities of two measurement possibilities \nare each 50%, then our schematics will display equal numbers for those two possibilities, whereas \nin a real experiment, statistical uncertainties might yield a 55%>45% split in one experiment and \na 47%>53% split in another, etc. The SPINS program simulations are designed to give statistical \nuncertainties, so you will need to perform enough experiments to convince yourself that you have a \nsufﬁciently good estimate of the probability (see SPINS Lab 1 for more information on statistics).\nNow let’s consider a series of simple Stern-Gerlach experiments with slight variations that help to \nillustrate the main features of quantum mechanics. We ﬁrst describe the experiments and their results \nand draw some qualitative conclusions about the nature of quantum mechanics. Then we introduce the \nformal mathematics of the ket notation and show how it can be used to predict the results of each of \nthe experiments.\n1.1.1 \u0002 Experiment 1\nThe ﬁrst experiment is shown in Fig. 1.3 and consists of a source of atoms, two Stern-Gerlach ana-\nlyzers both aligned along the z-axis, and counters for the output ports of the analyzers. The atomic \nbeam coming into the ﬁrst Stern-Gerlach analyzer is split into two beams at the output, just like the \noriginal experiment. Now instead of counting the atoms in the upper output beam, the spin compo-\nnent is measured again by directing those atoms into the second Stern-Gerlach analyzer. The result of \nthis experiment is that no atoms are ever detected coming out of the lower output port of the second \nStern-Gerlach analyzer. All atoms that are output from the upper port of the ﬁrst analyzer also pass \n\n6 \nStern-Gerlach Experiments\nthrough the upper port of the second analyzer. Thus we say that when the ﬁrst Stern-Gerlach analyzer \nmeasures an atom to have a z-component of spin Sz = +U>2, then the second analyzer also measures \nSz = +U>2 for that atom. This result is not surprising, but it sets the stage for results of experiments \nto follow.\nThough both Stern-Gerlach analyzers in Experiment 1 are identical, they play different roles in \nthis experiment. The ﬁrst analyzer prepares the beam in a particular quantum state 10  +92 and the \nsecond analyzer measures the resultant beam, so we often refer to the ﬁrst analyzer as a state prepa-\nration device. By preparing the state with the ﬁrst analyzer, the details of the source of atoms can be \nignored. Thus our main focus in Experiment 1 is what happens at the second analyzer because we \nknow that any atom entering the second analyzer is represented by the 0  +9 ket prepared by the ﬁrst \nanalyzer. All the experiments we will describe employ a ﬁrst analyzer as a state preparation device, \nthough the SPINS program has a feature where the state of the atoms coming from the oven is deter-\nmined but unknown, and the user can perform experiments to determine the unknown state using only \none analyzer in the experiment.\n1.1.2 \u0002 Experiment 2\nThe second experiment is shown in Fig. 1.4 and is identical to Experiment 1 except that the sec-\nond Stern-Gerlach analyzer has been rotated by 90° to be aligned with the x-axis. Now the second \nanalyzer measures the spin component along the x-axis rather the z-axis. Atoms input to the second \nanalyzer are still represented by the ket 0  +9 because the ﬁrst analyzer is unchanged. The result of this \nexperiment is that atoms appear at both possible output ports of the second analyzer. Atoms leaving \nthe upper port of the second analyzer have been measured to have Sx = +U>2, and atoms leaving \nZ\nZ\n50\n50\n0\n\u0002\u0002\u0003\n\u0002\u0002\u0003\n\u0002\u0003\u0003\n\u0002\u0003\u0003\n\u0002Ψ\u0003\nFIGURE 1.3 Experiment 1 measures the spin component along the z-axis twice in succession.\nX\nZ\n50\n25\n25\n\u0002\u0003\u0003\n\u0002\u0003\u0003x\n\u0002\u0002\u0003x\n\u0002\u0002\u0003\n\u0002Ψ\u0003\nFIGURE 1.4 Experiment 2 measures the spin component along the z-axis and then along the x-axis.\n\n1.1 Stern-Gerlach Experiment \n7\nthe lower port have Sx = -U>2. On average, each of these ports has 50% of the atoms that left the \nupper port of the ﬁrst analyzer. As shown in Fig. 1.4, the output states of the second analyzer have \nnew labels 0  +9x and 0  -9x, where the x subscript denotes that the spin component has been measured \nalong the x-axis. We assume that if no subscript is present on the quantum ket 1e.g., 0  +92, then the \nspin component is along the z-axis. This use of the z-axis as the default is a common convention \nthroughout our work and also in much of physics.\nA few items are noteworthy about this experiment. First, we notice that there are still only two \npossible outputs of the second Stern-Gerlach analyzer. The fact that it is aligned along a different axis \ndoesn’t affect the fact that we get only two possible results for the case of a spin-1/2 particle. Second, \nit turns out that the results of this experiment would be unchanged if we used the lower port of the ﬁrst \nanalyzer. That is, atoms entering the second analyzer in state 0  -9 would also result in half the atoms \nin each of the 0 {9x output ports. Finally, we cannot predict which of the second analyzer output ports \nany particular atom will come out. This can be demonstrated in actual experiments by recording the \nindividual counts out of each port. The arrival sequences at any counter are completely random. We \ncan say only that there is a 50% probability that an atom from the second analyzer will exit the upper \nanalyzer port and a 50% probability that it will exit the lower port. The random arrival of atoms at the \ndetectors can be seen clearly in the SPINS program simulations.\nThis probabilistic nature is at the heart of quantum mechanics. One might be tempted to say that \nwe just don’t know enough about the system to predict which port the atom will exit. That is to say, \nthere may be some other variables, of which we are ignorant, that would allow us to predict the results. \nSuch a viewpoint is known as a local hidden variable theory. John Bell proved that such theories are \nnot compatible with the experimental results of quantum mechanics. The conclusion to draw from this \nis that even though quantum mechanics is a probabilistic theory, it is a complete description of reality. \nWe will have more to say about this in Chapter 4.\nNote that the 50% probability referred to above is the probability that an atom input to the second \nanalyzer exits one particular output port. It is not the probability for an atom to pass through the whole sys-\ntem of Stern-Gerlach analyzers. It turns out that the results of this experiment (the 50>50 split at the sec-\nond analyzer) are the same for any combination of two orthogonal axes of the ﬁrst and second analyzers.\n1.1.3 \u0002 Experiment 3\nExperiment 3, shown in Fig. 1.5, extends Experiment 2 by adding a third Stern-Gerlach analyzer aligned \nalong the z-axis. Atoms entering the third analyzer have been measured by the ﬁrst Stern-Gerlach \nanalyzer to have spin component up along the z-axis, and by the second analyzer to have spin component \nup along the x-axis. The third analyzer then measures how many atoms have spin component up or down \n125\n125\n500\nX\nZ\nZ\n250\n\u0002\u0002\u0003\n\u0002\u0002\u0003\n\u0002\u0003\u0003\n\u0002\u0003\u0003\n\u0002Ψ\u0003\n\u0002\u0003\u0003x\n\u0002\u0002\u0003x\nFIGURE 1.5 Experiment 3 measures the spin component three times in succession.\n\n8 \nStern-Gerlach Experiments\nalong the z-axis. Classically, one would expect that the ﬁnal measurement would yield the result spin \nup along the z-axis, because that was measured at the ﬁrst analyzer. That is to say: classically the ﬁrst \ntwo analyzers tell us that the atoms have Sz = +U>2 and Sx = +U>2, so the third measurement must \nyield Sz = +U>2. But that doesn’t happen, as Erwin learned with his quantum socks in the Prologue. \nThe quantum mechanical result is that the atoms are split with 50% probability into each output port at \nthe third analyzer. Thus the last two analyzers behave like the two analyzers of Experiment 2 (except \nwith the order reversed), and the fact that there was an initial measurement that yielded Sz = +U>2 is \nsomehow forgotten or erased.\nThis result demonstrates another key feature of quantum mechanics: a measurement disturbs the \nsystem. The second analyzer has disturbed the system such that the spin component along the z-axis \ndoes not have a unique value, even though we measured it with the ﬁrst analyzer. Erwin saw this \nwhen he sorted, or measured, his socks by color and then by length. When he looked, or measured, \na third time, he found that the color he had measured originally was now random—the socks had \nforgotten about the ﬁrst measurement. One might ask: Can I be more clever in designing the experi-\nment such that I don’t disturb the system? The short answer is no. There is a fundamental incompat-\nibility in trying to measure the spin component of the atom along two different directions. So we say \nthat Sx and Sz are incompatible observables. We cannot know the measured values of both simul-\ntaneously. The state of the system can be represented by the ket 0  +9 = 0 Sz = +U>29 or by the ket \n0  +9x = 0 Sx = +U>29, but it cannot be represented by a ket 0 Sz = +U>2, Sx = +U>29 that speciﬁes \nvalues of both components. Having said this, it should be said that not all pairs of quantum mechanical \nobservables are incompatible. It is possible to do some experiments without disturbing some of the \nother aspects of the system. We will see in Section 2.4 that whether two observables are compatible or \nnot is very important in how we analyze a quantum mechanical system.\nNot being able to measure both the Sz and Sx spin components is clearly distinct from the classi-\ncal case where we can measure all three components of the spin vector, which tells us which direction \nthe spin is pointing. In quantum mechanics, the incompatibility of the spin components means that we \ncannot know which direction the spin is pointing. So when we say “the spin is up,” we really mean \nonly that the spin component along that one axis is up (vs. down). The quantum mechanical spin vec-\ntor cannot be said to be pointing in any given direction. As is often the case, we must check our classi-\ncal intuition at the door of quantum mechanics.\n1.1.4 \u0002 Experiment 4\nExperiment 4 is depicted in Fig. 1.6 and is a slight variation on Experiment 3. Before we get into the \ndetails, note a few changes in the schematic drawings. As promised, we have dropped the ket labels on \nthe beams because they are redundant. We have deleted the counters on all but the last analyzer and \ninstead simply blocked the unwanted beams and given the average number of atoms passing from one \nanalyzer to the next. The beam blocks are shown explicitly in Fig. 1.6 but will not be shown after this to \nbe consistent with the SPINS program. Note also that in Experiment 4c two output beams are combined \nas input to the following analyzer. This is simple in principle and in the SPINS program but can be \ndifﬁcult in practice. The recombination of the beams must be done properly so as to avoid “disturbing” \nthe beams. If you care to read more about this problem, see Feynman’s Lectures on Physics, volume 3. \nWe will have more to say about the “disturbance” later in Section 2.2. For now we simply assume that \nthe beams can be recombined in the proper manner.\nExperiment 4a is identical to Experiment 3. In Experiment 4b, the upper beam of the second ana-\nlyzer is blocked and the lower beam is sent to the third analyzer. In Experiment 4c, both beams are \ncombined with our new method and sent to the third analyzer. It should be clear from our previous \nexperiments that Experiment 4b has the same results as Experiment 4a. We now ask about the results of \n\n1.1 Stern-Gerlach Experiment \n9\nExperiment 4c. If we were to use classical probability analysis, then Experiment 4a would indicate that \nthe probability for an atom leaving the ﬁrst analyzer to take the upper path through the second analyzer \nand then exit through the upper port of the third analyzer is 25%, where we are now referring to the total \nprobability for those two steps. Likewise, Experiment 4b would indicate that the total probability to \ntake the lower path through the second  analyzer and exit through the upper port of the third analyzer is \nalso 25%. Hence the total probability to exit from the upper port of the third analyzer when both paths \nare available, which is Experiment 4c, would be 50%, and likewise for the exit from the lower port.\nHowever, the quantum mechanical result in Experiment 4c is that all the atoms exit the upper \nport of the third analyzer and none exits the lower port. The atoms now appear to “remember” that \nthey were initially measured to have spin up along the z-axis. By combining the two beams from \nthe second analyzer, we have avoided the quantum mechanical disturbance that was evident in \nExperiments 3, 4a, and 4b. The result is now the same as Experiment 1, which means it is as if the \nsecond analyzer is not there.\nTo see how odd this is, look carefully at what happens at the lower port of the third analyzer. In \nthis discussion, we refer to percentages of atoms leaving the ﬁrst analyzer, because that analyzer is \nthe same in all three experiments. In Experiments 4a and 4b, 50% of the atoms are blocked after the \nmiddle analyzer and 25% of the atoms exit the lower port of the third analyzer. In Experiment 4c, \n100% of the atoms pass from the second analyzer to the third analyzer, yet fewer atoms come out \nof the lower port. In fact, no atoms make it through the lower port! So we have a situation where \n25\n25\nX\nZ\nZ\n100\n50\n25\n25\nX\nZ\nZ\n100\n(a)\n(b)\n(c)\n50\n100\n0\nX\nZ\nZ\n100\n100\nFIGURE 1.6 Experiment 4 measures the spin component three times in succession \nand uses (a and b) one or (c) two beams from the second analyzer.\n\n10 \nStern-Gerlach Experiments\nallowing more ways or paths to reach a counter results in fewer counts. Classical probability theory \ncannot explain this aspect of quantum mechanics. It is as if you opened a second window in a room to \nget more sunlight and the room went dark!\nHowever, you may already know of a way to explain this effect. Imagine a procedure whereby \ncombining two effects leads to cancellation rather than enhancement. The concept of wave interfer-\nence, especially in optics, comes to mind. In the Young’s double-slit experiment, light waves pass \nthrough two narrow slits and create an interference pattern on a distant screen, as shown in Fig. 1.7. \nEither slit by itself produces a nearly uniform illumination of the screen, but the two slits combined \nproduce bright and dark interference fringes, as shown in Fig. 1.7(b). We explain this by adding \ntogether the electric ﬁeld vectors of the light from the two slits, then squaring the resultant vector to \nﬁnd the light intensity. We say that we add the amplitudes and then square the total amplitude to ﬁnd \nthe resultant intensity. See Section 6.6 or an optics textbook for more details about this experiment.\nWe follow a similar prescription in quantum mechanics. We add together amplitudes and then \ntake the square to ﬁnd the resultant probability, which opens the door to interference effects. Before \nwe discuss quantum mechanical interference, we must explain what we mean by an amplitude in \nquantum mechanics and how we calculate it.\n1.2 \u0002 QUANTUM STATE VECTORS\nPostulate 1 of quantum mechanics stipulates that kets are to be used for a mathematical description of a \nquantum mechanical system. These kets are abstract entities that obey many of the rules you know about \nordinary spatial vectors. Hence they are called quantum state vectors. As we will show in Example 1.3, \nthese vectors must employ complex numbers in order to properly describe quantum mechanical systems. \nQuantum state vectors are part of a vector space that we call a Hilbert space. The dimensionality of \nthe Hilbert space is determined by the physics of the system at hand. In the Stern-Gerlach example, \nthe two possible results for a spin  component measurement dictate that the vector space has only two \nPinhole\nSource\nDouble\nSlit\n(a)\n(b)\nScreen\nSingle Slit\nPatterns\nDouble Slit\nPattern\nFIGURE 1.7 (a) Young’s double-slit interference experiment and (b) resultant intensity patterns \nobserved on the screen, demonstrating single-slit diffraction and double-slit interference.\n\n1.2 Quantum State Vectors \n11\ndimensions. That makes this problem mathematically as simple as it can be, which is why we have chosen \nto study it. Because the quantum state vectors are abstract, it is hard to say much about what they are, \nother than how they behave mathematically and how they lead to physical predictions.\nIn the two-dimensional vector space of a spin-1/2 system, the two kets 0 {9 form a basis, just like \nthe unit vectors in  , jn , and kn form a basis for describing vectors in three-dimensional space. However, \nthe analogy we want to make with these spatial vectors is only mathematical, not physical. The spatial \nunit vectors have three important mathematical properties that are characteristic of a basis: the basis \nvectors in , jn , and kn are normalized, orthogonal, and complete. Spatial vectors are normalized if their \nmagnitudes are unity, and they are orthogonal if they are geometrically perpendicular to each other. \nThe basis is complete if any general vector in the space can be written as a linear superposition of the \nbasis vectors. These properties of spatial basis vectors can be summarized as follows:\n \n in~in = jn~jn = kn~kn = 1   normalization\n \n in~jn = in~kn = jn~kn = 0   orthogonality  \n(1.9)\n \n A = axin + ay jn + azkn   completeness,\nwhere A is a general vector. Note that the dot product, also called the scalar product, is central to the \ndescription of these properties.\nContinuing the mathematical analogy between spatial vectors and abstract vectors, we require that \nthese same properties (at least conceptually) apply to quantum mechanical basis vectors. For the Sz \nmeasurement, there are only two possible results, corresponding to the states 0  +9 and 0  -9, so these \ntwo states comprise a complete set of basis vectors. This basis is known as the Sz basis. We focus on \nthis basis for now and refer to other possible basis sets later. The completeness of the basis kets 0 {9 \nimplies that a general quantum state vector 0  c9 is a linear combination of the two basis kets:\n \n0  c9 = a0  +9 + b0  -9, \n(1.10)\nwhere a and b are complex scalar numbers multiplying each ket. This addition of two kets yields \nanother ket in the same abstract space. The complex scalar can appear either before or after the ket \nwithout affecting the mathematical properties of the ket 1i.e., a0  +9 = 0  +9a2. It is customary to use \nthe Greek letter c (psi) for a general quantum state. You may have seen c1x2 used before as a quan-\ntum mechanical wave function. However, the state vector or ket 0  c9 is not a wave function. Kets do \nnot have any spatial dependence as wave functions do. We will study wave functions in Chapter 5.\nTo discuss orthogonality and normalization (known together as orthonormality) we must ﬁrst \ndeﬁne scalar products as they apply to these new kets. As we said above, the machinery of quantum \nmechanics requires the use of complex numbers. You may have seen other ﬁelds of physics use com-\nplex numbers. For example, sinusoidal oscillations can be described using the complex exponential \neivt rather than cos(vt). However, in such cases, the complex numbers are not required, but are rather \na convenience to make the mathematics easier. When using complex notation to describe classical \nvectors like electric and magnetic ﬁelds, the deﬁnition of the dot product is generalized slightly, such \nthat one of the vectors is complex conjugated. A similar approach is taken in quantum mechanics. The \nanalog to the complex conjugated vector of classical physics is called a bra in the Dirac notation of \nquantum mechanics. Thus corresponding to a general ket 0  c9, there is a bra, or bra vector, which is \nwritten as 8c 0 . If a general ket 0  c9 is speciﬁed as 0  c9 = a0  +9 + b0  -9, then the corresponding bra \n8c 0  is deﬁned as\n \n8c 0 = a*8+ 0 + b*8- 0  , \n(1.11)\n\n12 \nStern-Gerlach Experiments\nwhere the basis bras 8  +  0  and 8  -  0  correspond to the basis kets 0  +9 and 0  -9, respectively, and the \ncoefﬁcients a and b have been complex conjugated.\nThe scalar product in quantum mechanics is deﬁned as the product of a bra and a ket taken in the \nproper order—bra ﬁrst, then ket second:\n \n18bra0210 ket92. \n(1.12)\nWhen the bra and ket are combined together in this manner, we get a bracket (bra ket)—a little physics \nhumor—that is written in shorthand as\n \n8bra0 ket9. \n(1.13)\nThus, given the basis kets 0  +9 and 0  -9, one inner product, for example, is written as\n \n18 + 0210  - 92 = 8 + 0  -9 \n(1.14)\nand so on. Note that we have eliminated the extra vertical bar in the middle. The scalar product in \nquantum mechanics is generally referred to as an inner product or a projection.\nSo how do we calculate the inner product 8+  0  +9? We do it the same way we calculate the dot \nproduct in~ in. We deﬁne it to be unity because we like basis vectors to be unit vectors. There is a little \nmore to it than that, because in quantum mechanics (as we will see shortly) using normalized basis \nvectors is more rooted in physics than in our personal preferences for mathematical cleanliness. But \nfor all practical purposes, if someone presents a set of basis vectors to you, you can probably assume \nthat they are normalized. So the normalization of the spin-1/2 basis vectors is expressed in this new \nnotation as 8+  0  +9 = 1 and 8-  0  -9 = 1.\nNow, what about orthogonality? The spatial unit vectors in, jn, and kn used for spatial vectors are \northogonal to each other because they are at 90° with respect to each other. That orthogonality is \nexpressed mathematically in the dot products in~jn = in~kn = jn~kn = 0. For the spin basis kets 0  +9 and \n0  -9, there is no spatial geometry involved. Rather, the spin basis kets 0  +9 and 0  -9 are orthogonal in \nthe mathematical sense, which we express with the inner product as 8+  0  -9 = 0. Again, we do not \nprove to you that these basis vectors are orthogonal, but we assume that a well-behaved basis set obeys \northogonality. Though there is no geometry in this property for quantum mechanical basis vectors, \nthe fundamental idea of orthogonality is the same, so we use the same language—if a general vector \n“points” in the direction of a basis vector, then there is no component in the “direction” of the other \nunit vectors.\nIn summary, the properties of normalization, orthogonality, and completeness can be expressed \nin the case of a two-state spin-1/2 quantum system as:\n \n8+  0  +9 = 1\n8-  0  -9 = 1 r    normalization\n \n \n8+ 0  -9 = 0\n8- 0  +9 = 0r    orthogonality\n \n \n(1.15)\n \n0  c9 = a0  +9 + b0  -9    completeness    . \nNote that a product of kets 1e.g., 0  +9 0  +92 or a similar product of bras 1e.g., 8 + 08 + 02 is meaningless \nin this new notation, while a product of a ket and a bra in the “wrong” order 1e.g., 0  + 98 + 02 has a \nmeaning that we will deﬁne in Section 2.2.3. Equations (1.15) are sufﬁcient to deﬁne how the basis \n\n1.2 Quantum State Vectors \n13\nkets behave mathematically. Note that the inner product is deﬁned using a bra and a ket, though it is \ncommon to refer to the inner product of two kets, where it is understood that one is converted to a bra \nﬁrst. The order does matter, as we will see shortly.\nUsing this new notation, we can learn a little more about general quantum states and derive some \nexpressions that will be useful later. Consider the general state vector 0  c9 = a0  +9 + b0  -9. Take the \ninner product of this ket with the bra 8 + 0  and obtain\n \n 8 + 0  c9 = 8 + 0  1a0  +9 + b0  -92  \n \n = 8 + 0 a0  +9 + 8 + 0 b0  -9 \n(1.16)\n \n = a 8 + 0  +9 + b8 + 0  -9  \n \n = a ,\n \nusing the properties that inner products are distributive and that scalars can be moved freely through \nbras or kets. Likewise, you can show that 8- 0 c9 = b. Hence the coefﬁcients multiplying the basis \nkets are simply the inner products or projections of the general state 0 c9 along each basis ket, albeit in \nan abstract complex vector space rather than the concrete three-dimensional space of normal vectors. \nUsing these results, we rewrite the general state as\n \n 0  c9 = a0  +9 + b0  -9\n \n \n = 0  +9a + 0  -9b\n \n(1.17)\n \n = 0  +958 + 0  c96 + 0  -958 - 0  c96, \nwhere the rearrangement of the second equation again uses the property that scalars 1e.g., a = 8 + 0  c92 \ncan be moved through bras or kets.\nFor a general state vector 0  c9 = a0  +9 + b0  -9, we deﬁned the corresponding bra to be \n8c 0 = a*8 + 0  +b*8 - 0 . Thus, the inner product of the state 0  c9 with the basis ket 0  +9 taken in the \nreverse order compared to Eq. (1.16) yields\n \n 8c 0  +9 = 8 + 0 a*0  +9 + 8 - 0 b*0  +9 \n \n = a*8 +\n 0  +9 + b*8 - 0  +9  \n \n = a*.\n \n \n(1.18)\nThus, we see that an inner product with the states reversed results in a complex conjugation of the \ninner product:\n \n8 + 0  c 9 = 8c 0  +9*. \n(1.19)\nThis important property holds for any inner product. For example, the inner product of two general \nstates is\n \n8f0  c9 = 8c 0 f 9*  . \n(1.20)\nNow we come to a new mathematical aspect of quantum vectors that differs from the use of vec-\ntors in classical mechanics. The rules of quantum mechanics (postulate 1) require that all state vectors \ndescribing a quantum system be normalized, not just the basis kets. This is clearly different from \nordinary spatial vectors, where the length or magnitude of a vector means something and only the unit \nvectors in, jn, and kn are normalized to unity. This new rule means that in the quantum mechanical state \n\n14 \nStern-Gerlach Experiments\nspace only the direction—in an abstract sense—is important. If we apply this normalization require-\nment to a general state 0  c9, then we obtain\n \n8c 0  c9 = 5a*8 + 0 + b*8- 0 65a 0  +9 + b0  -96 = 1 \n \n 1 a*a 8+ 0  +9 + a*b 8+ 0  -9 + b*a 8 - 0  +9 + b*b 8 - 0  -9 = 1\n \n 1 a*a + b*b = 1\n \n \n(1.21)\n \n 1 0 a0\n2 + 0 b0\n2 = 1 ,\nor using the expressions for the coefﬁcients obtained above,\n \n08 + 0  c9 0\n2 + 08 -\n 0  c9 0\n2 = 1. \n(1.22)\nExample 1.1 Normalize the vector 0  c9 = C110  +9 + 2i0  -92. The complex constant C is often \nreferred to as the normalization constant.\nTo normalize 0  c9, we set the inner product of the vector with itself equal to unity and then \nsolve for C—note the requisite complex conjugations\n \n 1 = 8c 0  c9\n \n \n = C*518 + 0 - 2i8- 0 6C510  +9 + 2i0  -96\n \n \n = C*C518 + 0  +9 + 2i8 + 0  -9 - 2i8- 0  +9 + 48 - 0  -96 \n(1.23)\n \n = 50 C0\n2\n \n \n 1 0 C0 =\n1\n25\n .\n \nThe overall phase of the normalization constant is not physically meaningful (Problem 1.3), so \nwe follow the standard convention and choose it to be real and positive. This yields C = 1> 15. \nThe normalized quantum state vector is then\n \n0  c9 =\n1\n25\n 110  +9 + 2i0  -92. \n(1.24)\nNow comes the crucial element of quantum mechanics. We postulate that each term in the sum \nof Eq. (1.22) is equal to the probability that the quantum state described by the ket 0  c9 is measured \nto be in the corresponding basis state. Thus\n \nPSz=+ U>2 = 08+  0  c9 0\n2 \n(1.25)\nis the probability that the state 0  c9 is found to be in the state 0  +9 when a measurement of Sz is made, \nmeaning that the result Sz = +U>2 is obtained. Likewise,\n \nPSz=- U>2 = 08-  0  c9 0\n2 \n(1.26)\nis the probability that the measurement yields the result Sz = -U>2. The subscript on the probability \nindicates the measured value. For the spin component measurements, we will usually abbreviate this \nto, for example, P+ for an Sz = +U>2 result or P-y for an Sy = -U>2 measurement.\n\n1.2 Quantum State Vectors \n15\nWe now have a prescription for predicting the outcomes of the experiments we have been dis-\ncussing. For example, the experiment shown in Fig. 1.8 has the state 0  c9 = 0  +9 prepared by the \nﬁrst Stern-Gerlach device and then input to the second Stern-Gerlach device aligned along the z-axis. \nTherefore the probabilities of measuring the input state 0  c9 = 0  +9 to have the two output values are \nas shown. Because the spin-1/2 system has only two possible measurement results, these two prob-\nabilities must sum to unity—there is a 100% probability of recording some value in the experiment. \nThis basic rule of probabilities is why the rules of quantum mechanics require that all state vectors \nbe properly normalized before they are used in any calculation of probabilities. The experimental \npredictions shown in Fig. 1.8 are an example of the fourth postulate of quantum mechanics, which is \npresented below.\n50\n0\nP\u0002\u0005\u0006\u0005\u0002\u0004\u0002\u0002\u0002\u0003\u00022 \u0006\u00051\nP\u0003\u0005\u0006\u0005\u0002\u0004\u0003\u0002\u0002\u0003\u00022 \u0006\u00050\nZ\nZ\n\u0002\u0002\u0003\n\u0002\u0002\u0003\n\u0002\u0003\u0003\nFIGURE 1.8 Probabilities of spin component measurements.\nPostulate 4 (Spin-1/2 system)\nThe probability of obtaining the value {U>2 in a measurement of the observ-\nable Sz on a system in the state 0  c9 is\nP{ = 08{ 0  c9 0\n2,\nwhere 0 {9 is the basis ket of Sz corresponding to the result {U>2.\nThis is labeled as the fourth postulate because we have written this postulate using the language of the \nspin-1/2 system, while the general statement of the fourth postulate presented in Section 1.5 requires \nthe second and third postulates of Section 2.1. A general spin component measurement is shown in \nFig. 1.9, along with a histogram that compactly summarizes the measurement results.\nBecause the quantum mechanical probability is found by squaring an inner product, we refer to \nan inner product, 8+ 0  c9 for example, as a probability amplitude or sometimes just an amplitude; \nmuch like a classical wave intensity is found by squaring the wave amplitude. Note that the conven-\ntion is to put the input or initial state on the right and the output or ﬁnal state on the left: 8out0 in9, so \none would read from right to left in describing a problem. Because the probability involves the com-\nplex square of the amplitude, and 8out0 in9 = 8in0 out9*, this convention is not critical for calculat-\ning probabilities. Nonetheless, it is the accepted practice and is important in situations where several \namplitudes are combined.\nArmed with these new quantum mechanical rules and tools, let’s continue to analyze the experi-\nments discussed earlier. Using the experimental results and the new rules we have introduced, we can \nlearn more about the mathematical behavior of the kets and the relationships among them. We will \nfocus on the ﬁrst two experiments for now and return to the others in the next chapter.\n\n16 \nStern-Gerlach Experiments\n1.2.1 \u0002 Analysis of Experiment 1\nIn Experiment 1, the ﬁrst Stern-Gerlach analyzer prepared the system in the 0  +9 state and the sec-\nond analyzer later measured this state to be in the 0  +9 state and not in the 0  -9 state. The results of \nthe experiment are summarized in the histogram in Fig. 1.10. We can use the fourth postulate to pre-\ndict the results of this experiment. We take the inner product of the input state 0  +9 with each of the \npossible output basis states 0  +9 and 0  -9. Because we know that the basis states are normalized and \northogonal, we calculate the probabilities to be\n \nP+ = 08+  0  +9 0\n2 = 1  \n \nP- = 08  - 0  +9 0\n2 = 0 . \n \n(1.27)\nThese predictions agree exactly with the histogram of experimental results shown in Fig. 1.10. A 0  +9 \nstate is always measured to have Sz = +U>2.\n1.2.2 \u0002 Analysis of Experiment 2\nIn Experiment 2, the ﬁrst Stern-Gerlach analyzer prepared the system in the 0  +9 state and the sec-\nond analyzer performed a measurement of the spin component along the x-axis, ﬁnding 50% prob-\nabilities for each of the two possible states 0  +9x and 0  -9x, as shown in the histogram in Fig. 1.11(a). \nFor this experiment, we cannot predict the results of the measurements, because we do not yet have \n\u0002Ψ\u0003\nZ\nP\u0002\u0005\u0006\u0005\u0002\u0004\u0002\u0002Ψ\u0003\u00022\nP\u0003\u0005\u0006\u0005\u0002\u0004\u0003\u0002Ψ\u0003\u00022\n(a)\n(b)\nSz\n1\nP\nP\u0003\nP\u0002\n\u0003\u0002\n2\n\u0002\n2\n\u0002\u0002\u0003\n\u0002\u0003\u0003\nFIGURE 1.9 (a) Spin component measurement for a general input state and \n(b) histogram of measurement results.\n1\nP\nP\u0003\nP\u0002\n\u0002Ψin\u0003\u0005\u0006\u0005\u0002\u0002\u0003\nSz\n\u0003\u0002\n2\n\u0002\n2\nFIGURE 1.10 Histogram of Sz spin component measurements \nfor Experiment 1 with 0 cin9 = 0  + 9.\n\n1.2 Quantum State Vectors \n17\nenough information about how the states 0  +9x and 0  -9x behave mathematically. Rather, we will use \nthe results of the experiment to determine these states. Recalling that the experimental results would \nbe the same if the ﬁrst analyzer prepared the system to be in the 0  -9 state [see Fig. 1.11(b)], we have \nfour results for the two experiments:\n \nP1,+x = 0 x8+  0  +9 0\n2 = 1\n2  \n \nP1,-x = 0 x8 - 0  +9 0\n2 = 1\n2  \n \nP2,+x = 0 x8+  0  -9 0\n2 = 1\n2  \n \n(1.28)\n \nP2,-x = 0 x8 - 0  -9 0\n2 = 1\n2. \nBecause the kets 0  +9 and 0  -9 form a complete basis, the kets describing the Sx measurement, 0  +9x \nand 0  -9x, can be written in terms of them. We do not yet know the speciﬁc coefﬁcients of the 0 {9x \nstates, so we use general expressions\n \n0  +9x = a0  +9 + b0  -9  \n \n0  -9x = c0  +9 + d0  -9, \n(1.29)\nand now our task is to use the results of Experiment 2 to determine the coefﬁcients a, b, c, and d. The \nﬁrst measured probability in Eq. (1.28) is\n \nP1,+x = 0 x8+  0  +9 0\n2 = 1\n2. \n(1.30)\nUsing the general expression for 0  +9x in Eq. (1.29), we calculate the probability that the 0  +9 input \nstate is measured to be in the 0  +9x output state, that is, to have Sx = +U>2:\n \n P1,+x = 0 x8+  0  +9 0\n2\n \n \n = 05a*8 + 0 + b*8  - 0 6 0  +9 0\n2 \n \n(1.31)\n \n = 0 a*0\n2 = 0 a0\n2 ,\n \nwhere we convert the 0  +9x ket to a bra x8 + 0  in order to calculate the inner product. Equating the \nexperimental result in Eq. (1.30) and the prediction in Eq. (1.31), we ﬁnd\n \n0 a0\n2 = 1\n2. \n(1.32)\n(a)\nP\u0003x\n1\nP\nSx\n(b)\nP\u0002x\nP\u0003x\n1\nP\nSx\n\u0003\u0002\n2\n\u0002\n2\n\u0003\u0002\n2\n\u0002\n2\nP\u0002x\n\u0002Ψin\u0003\u0005\u0006\u0005\u0002\u0003\u0003\n\u0002Ψin\u0003\u0005\u0006\u0005\u0002\u0002\u0003\nFIGURE 1.11 Histograms of Sx spin component measurements for Experiment 2 \nfor different input states (a) 0 cin9 = 0  + 9 and (b) 0 cin9 = 0  -9.\n\n18 \nStern-Gerlach Experiments\nSimilarly, one can calculate the other three probabilities to arrive at 0 b0\n2 = 0 c0\n2 = 0 d0\n2 = 1\n2 . (Prob-\nlem 1.4) Because each coefﬁcient is complex, each has an amplitude and phase. However, the overall \nphase of a quantum state vector is not physically meaningful (see Problem 1.3). Only the relative \nphase between different components of the state vector is physically measurable. Hence, we are free to \nchoose one coefﬁcient of each vector to be real and positive without any loss of generality. This allows \nus to write the desired states as\n \n 0  +9x =\n1\n12 3 0  +9 + eia0  -94   \n \n 0  -9x =\n1\n12 3 0  +9 + eib0  -94,\n \n \n(1.33)\nwhere a and b are relative phases that we have yet to determine. Note that these states are already nor-\nmalized because we used all of the experimental results, which reﬂect the fact that the probability for \nall possible results of an experiment must sum to unity.\nWe have used all the experimental results from Experiment 2, but the 0 {9x kets are still not deter-\nmined. We need some more information. If we perform Experiment 1 with both analyzers aligned \nalong the x-axis, the results will be as you expect—all 0  +9x states from the ﬁrst analyzer will be mea-\nsured to have Sx = +U>2 at the second analyzer, that is, all atoms exit in the 0  +9x state and none in the \n0  -9x . The probability calculations for this experiment are\n \nP+x = 0 x8+  0  +9x0\n2 = 1  \n \nP-x = 0 x8  - 0  +9x0\n2 = 0, \n \n(1.34)\nwhich tell us mathematically that the 0 {9x states are orthonormal to each other, just like the 0 {9 \nstates. This also implies that the 0 {9x kets form a basis, the Sx basis, which you might expect because \nthey correspond to the distinct results of a different spin component measurement. The general expres-\nsions we used for the 0 {9x kets are already normalized but are not yet orthogonal. That is the new \npiece of information we need. The orthogonality condition leads to\n \nx8  - 0  +9x = 0\n \n \n1\n12 38+  0 + e-ib8  - 0 4 1\n12 3 0 +9 + eia0  -94 = 0 \n \n1\n2 31 + ei1a-b24 = 0\n \n \n(1.35)\n \nei1a-b2 = -1\n \n \neia = -eib,\n \nwhere the complex conjugation of the second coefﬁcient of the x8  - 0  bra should be noted.\nWe now have an equation relating the remaining coefﬁcients a and b, but we need some more \ninformation to determine their values. Unfortunately, there is no more information to be obtained, so \nwe are free to choose the value of the phase a. This freedom comes from the fact that we have required \nonly that the x-axis be perpendicular to the z-axis, which limits the x-axis only to a plane rather than to \na unique direction. We follow convention here and choose the phase a \u0003 0. Thus we can express the \nSx basis kets in terms of the Sz basis kets as\n \n 0  +9x =\n1\n12 3 0  +9 + 0  -94  \n \n 0  -9x =\n1\n12 3 0  +9 - 0  -94. \n \n(1.36)\n\n1.2 Quantum State Vectors \n19\nWe generally use the Sz basis as the preferred basis for writing general states, but we could use \nany basis we choose. If we were to use the Sx basis, then we could write the 0 {9 kets as general states \nin terms of the 0 {9x kets. This can be done by solving Eq. (1.36) for the 0 {9 kets, yielding\n \n 0  +9 =\n1\n12 3 0  +9x + 0  -9x4  \n \n 0  -9 =\n1\n12 3 0  +9x - 0  -9x4. \n \n(1.37)\nWith respect to the measurements performed in Experiment 2, Eq. (1.37) tells us that the 0  +9 \nstate is a combination of the states 0  +9x and 0  -9x. The coefﬁcients tell us that there is a 50% probabil-\nity for measuring the spin component to be up along the x-axis, and likewise for the down possibility, \nwhich is in agreement with the histogram of measurements shown in Fig. 1.11(a). We must now take \na moment to describe carefully what a combination of states, such as in Eqs. (1.36) and (1.37), is and \nwhat it is not.\n1.2.3 \u0002 Superposition States\nA general spin-1/2 state vector 0  c9 can be expressed as a combination of the basis kets 0  +9 and 0  -9\n \n0  c9 = a0  +9 + b0  -9. \n(1.38)\nWe refer to such a combination of states as a superposition state. To understand the importance of a \nquantum mechanical superposition state, consider the particular state\n \n0  c9 =\n1\n12 10  +9 + 0  -92 \n(1.39)\nand measurements on this state, as shown in Fig. 1.12(a). Note that the state 0  c9 is none other \nthan the state 0  +9x that we found in Eq. (1.36), so we already know what the measurement results \nare. If we measure the spin component along the x-axis for this state, then we record the result \nSx = +U>2 with 100% probability (Experiment 1 with both analyzers along the x-axis). If we mea-\nsure the spin component along the orthogonal z-axis, then we record the two results Sz = {U>2 \nwith 50% probability each (Experiment 2 with the ﬁrst and second analyzers along the x- and \nz-axes, respectively). Based upon this second set of results, one might be tempted to consider the \nstate 0  c9 as describing a beam that contains a mixture of atoms with 50% of the atoms in the 0  +9 \nstate and 50% in the 0  -9 state. Such a state is called a mixed state and is very different from a \nsuperposition state.\nTo clarify the difference between a mixed state and a superposition state, let’s carefully exam-\nine the results of experiments on the proposed mixed-state beam, as shown in Fig. 1.12(b). If \nwe measure the spin component along the z-axis, then each atom in the 0  +9 state yields the result \nSz = +U>2 with 100% certainty and each atom in the 0  -9 state yields the result Sz = -U>2 with \n100% certainty. The net result is that 50% of the atoms yield Sz = +U>2 and 50% yield Sz = -U>2. \nThis is exactly the same result as that obtained with all atoms in the 0  +9x state, as seen in Fig. 1.12(a). \nIf we instead measure the spin component along the x-axis, then each atom in the 0  +9 state yields the \ntwo results Sx = {U>2 with 50% probability each (Experiment 2 with the ﬁrst and second analyzers \nalong the z- and x-axes, respectively). The atoms in the 0  -9 state yield the same results. The net result \nis that 50% of the atoms yield Sx = +U>2 and 50% yield Sx = -U>2. This is in stark contrast to the \nresults of Experiment 1, which tell us that once we have prepared the state to be 0  +9x, then subsequent \nmeasurements yield Sx = +U>2 with certainty, as seen in Fig. 1.12(a).\n\n20 \nStern-Gerlach Experiments\nHence we must conclude that the system described by the 0  c9 = 0  +9x state is not a mixed \nstate with some atoms in the 0  +9 state and some in the 0  -9 state. Rather, each atom in the 0  +9x \nbeam is in a state that itself is a superposition of the 0  +9 and 0  -9 states. A superposition state is \noften called a coherent superposition because the relative phase of the two terms is important. For \nexample, if the input beam were in the 0  -9x state, then there would be a relative minus sign between \nthe two coefﬁcients, which would result in an Sx = -U>2 measurement but would not affect the Sz \nmeasurement.\nZ\n50\n50\n\u0002Ψ\u0003\u0005\u0006\u0005\u0002\u0002\u0003x  \u0006\u0005(\u0002\u0002\u0003\u0005\u0002\u0005\u0002\u0003\u0003)\u0007\b2\nX\n100\n0\nZ\n50\n50\n50% \u0002\u0002\u0003\n50% \u0002\u0003\u0003\n50% \u0002\u0002\u0003\n50% \u0002\u0003\u0003\nX\n50\n50\n\u0002Ψ\u0003\u0005\u0006\u0005\u0002\u0002\u0003x \u0006\u0005(\u0002\u0002\u0003\u0005\u0002\u0005\u0002\u0003\u0003)\u0007\b2\n(b)\n(a)\nFIGURE 1.12 (a) Superposition state measurements and (b) mixed state measurements.\n\n1.2 Quantum State Vectors \n21\nWe will not have any further need to speak of mixed states, so any combination of states we use \nis a superposition state. Note that we cannot even write down a ket describing a mixed state. So if \n someone gives you a quantum state written as a ket, then it must be a superposition state and not a \nmixed state. The random option in the SPINS program produces a mixed state, while the unknown \nstates are all superposition states.\nExample 1.2 Consider the input state\n \n0 cin9 = 30  +9 + 40  -9.  \n(1.40)\nNormalize this state vector and ﬁnd the probabilities of measuring the spin component along the \nz-axis to be Sz = {U>2.\nTo normalize this state, introduce an overall complex multiplicative factor and solve for this \nfactor by imposing the normalization condition:\n \n0 cin9 = C 330  +9 + 40  -94\n \n \n8cin0 cin9 = 1\n \n \n5C* 338  +  0 + 48 - 0465C 330  +9 + 40  -946 = 1\n \n(1.41)\n \nC*C 398  +  0  +9 + 128  +  0  -9 + 128  - 0  +9 + 168  - 0  -94 = 1 \n \nC*C 3254 = 1\n \n \n0 C0\n2 = 1\n25.\n \nBecause an overall phase is physically meaningless, we choose C to be real and positive: C = 1>5. \nHence the normalized input state is\n \n@ cin9 = 3\n5 @  +9 + 4\n5 @  -9. \n(1.42)\nThe probability of measuring Sz = +U>2 is\n \n P+ = @8+ @cin9@\n2\n \n \n = @8  +  @33\n5 @  +9 + 4\n5 @  -94 @\n2 \n(1.43)\n \n = @ 3\n5 8  +  @  +9 + 4\n5 8  +  @  -9@\n2 \n \n = @ 3\n5 @\n2 =\n9\n25.\n \nThe probability of measuring Sz = -U>2 is\n \n P- = @8- @ cin9@\n2\n \n \n = @8- @33\n5 @  +9 + 4\n5 @  -94 @\n2 \n \n = @ 3\n5 8- @  +9 + 4\n5 8- @  -9@\n2 \n \n = @ 4\n5 @\n2 = 16\n25.\n \n(1.44)\n\n22 \nStern-Gerlach Experiments\nNote that the two probabilities add to unity, which indicates that we normalized the input state \nproperly. A histogram of the predicted measurement results is shown in Fig. 1.13.\n 1.3 \u0002 MATRIX NOTATION\nUp to this point, we have deﬁned kets mathematically in terms of their inner products with other kets. \nThus, in the general case we write a ket as\n \n0  c9 = 8+  0  c9 0  +9 + 8  - 0  c9 0  -9, \n(1.45)\nor in a speciﬁc case, we write\n \n 0  +9x = 8+  0  +9x 0  +9 + 8  - 0  +9x 0  -9 \n \n =\n1\n12 0  +9 +\n1\n12 0  -9.\n \n \n(1.46)\nIn both of these cases, we have chosen to write the kets in terms of the 0  +9 and 0  -9 basis kets. If we \nagree on that choice of basis as a convention, then the two coefﬁcients 8+  0  +9x and 8  - 0  +9x uniquely \nspecify the quantum state, and we can simplify the notation by using just those numbers. Thus, we \nrepresent a ket as a column vector containing the two coefﬁcients that multiply each basis ket. For \nexample, we represent 0  +9x as\n \n0  +9x \u0003\n1\n22\n ¢1\n1≤ , \n(1.47)\nwhere we have used the new symbol \u0003 to signify “is represented by,” and it is understood that we \nare using the 0  +9 and 0  -9 basis or the Sz basis. We cannot say that the ket equals the column vector, \nbecause the ket is an abstract vector in the state space and the column vector is just two complex num-\nbers. If we were to choose a different basis for representing the vector, then the complex coefﬁcients \nwould be different even though the vector is unchanged. We need to have a convention for the order-\ning of the amplitudes in the column vector. The standard convention is to put the spin up amplitude \nﬁrst (at the top). Thus, the representation of the 0  -9x state in Eq. (1.36) is\n \n0  -9x \u0003\n1\n22\n ¢ 1\n-1≤ d 0  +9\nd 0  -9, \n \n(1.48)\n1\nP\nSz\n\u0003\u0002\n2\n\u0002Ψin\u0003\u0005\u0006\u0005\u0005\u0005\u0005\u0005\u0002\u0002\u0003\u0005\u0002\u0005\u0005\u0005\u0005\u0005\u0002\u0003\u0003\n3\n5\n4\n5\n\u0002\n2\nP\u0003\nP\u0002\nFIGURE 1.13 Histogram of Sz spin component measurements.\n\n1.3 Matrix Notation \n23\nwhere we have explicitly labeled the rows according to their corresponding basis kets. Using this con-\nvention, it should be clear that the basis kets themselves are written as\n \n0  +9 \u0003 a1\n0b  \n \n0  -9 \u0003 a0\n1b. \n \n(1.49)\nThis demonstrates the important feature that basis kets are unit vectors when written in their own basis.\nThis new way of expressing a ket simply as the collection of coefﬁcients that multiply the basis \nkets is referred to as a representation. Because we have assumed the Sz kets as the basis kets, this is \ncalled the Sz representation. It is always true that basis kets have the simple form shown in Eq. (1.49) \nwhen written in their own representation. A general ket 0  c9 is written as\n \n0  c9 \u0003 ¢ 8+  0  c9\n8- 0  c9 ≤. \n(1.50)\nThis use of matrix notation simpliﬁes the mathematics of bras and kets. The advantage is not so evident for \nthe simple two-dimensional state space of spin-1/2 systems, but it is very evident for larger dimensional \nproblems. This notation is indispensable when using computers to calculate quantum mechanical results. \nFor example, the SPINS program employs matrix calculations coded in the Java computer language to \nsimulate the Stern-Gerlach experiments using the same probability rules you are learning here.\nWe saw earlier [Eq. (1.11)] that the coefﬁcients of a bra are the complex conjugates of the coef-\nﬁcients of the corresponding ket. We also know that an inner product of a bra and a ket yields a single \ncomplex number. In order for the matrix rules of multiplication to be used, a bra must be represented \nby a row vector, with the entries being the coefﬁcients ordered in the same sense as for the ket. For \nexample, if we use the general ket\n \n0  c9 = a0  +9 + b0  -9, \n(1.51)\nwhich is represented as\n \n0  c9 \u0003 aa\nbb, \n(1.52)\nthen the corresponding bra\n \n8c 0 = a*8 +  0 + b*8  - 0  \n(1.53)\nis represented by a row vector as\n \n8c 0 \u0003 1a* b*2. \n(1.54)\nThe rules of matrix algebra can then be applied to ﬁnd an inner product. For example,\n \n 8c 0  c9 = 1a* b*2aa\nbb \n \n = 0 a0\n2 + 0 b0\n2.  \n \n(1.55)\nSo a bra is represented by a row vector that is the complex conjugate and transpose of the column vec-\ntor representing the corresponding ket.\n\n24 \nStern-Gerlach Experiments\nExample 1.3 To get some practice using this new matrix notation, and to learn some more about \nthe spin-1/2 system, use the results of Experiment 2 to determine the Sy basis kets using the matrix \napproach instead of the Dirac bra-ket approach.\nConsider Experiment 2 in the case where the second Stern-Gerlach analyzer is aligned along \nthe y-axis. We said before that the results are the same as in the case shown in Fig. 1.4. Thus, we \nhave\n \n P1,+y = @ y8+  @  +9@\n2 = 1\n2  \n \n P1,-y = @ y8-  @  +9@\n2 = 1\n2  \n \n P2,+y = @ y8+  @  -9@\n2 = 1\n2  \n(1.56)\n \n P2,-y = @ y8-  @  -9@\n2 = 1\n2, \nas depicted in the histograms of Fig. 1.14.\nThese results allow us to determine the kets 0 {9y corresponding to the spin component up and \ndown along the y-axis. The argument and calculation proceeds exactly as it did earlier for the 0 {9x \nstates up until the point [Eq. (1.35)] where we arbitrarily chose the phase a to be zero. Having done \nthat for the 0 {9x states, we are no longer free to make that same choice for the 0 {9y states. Thus \nwe use Eq. (1.35) to write the 0 {9y states as\n \n 0  +9y =\n1\n22\n 3 0  +9 + eia0  -94 \u0003\n1\n22\n a 1\neiab\n \n \n 0  -9y =\n1\n22\n 3 0  +9 - eia0  -94 \u0003\n1\n22\n a 1\n-eiab. \n(1.57)\nTo determine the phase a, we use some more information at our disposal. Experiment 2 could be \nperformed with the ﬁrst Stern-Gerlach analyzer aligned along the x-axis and the second analyzer \nalong the y-axis. Again the results would be identical (50% at each output port), yielding\n \nP+y = @ y8+  @  +9x@\n2 = 1\n2 \n(1.58)\n(a)\nP\u0002y\nP\u0003y\n1\nP\nSy\n\u0003\u0002\n2\n\u0002\n2\n(b)\nP\u0002y\nP\u0003y\n1\nP\nSy\n\u0003\u0002\n2\n\u0002\n2\n\u0002Ψin\u0003\u0005\u0006\u0005\u0002\u0003\u0003\n\u0002Ψin\u0003\u0005\u0006\u0005\u0002\u0002\u0003\nFIGURE 1.14 Histograms of Sy spin component measurements for input states (a) 0 cin9 = 0  +9 \nand (b) 0 cin9 = 0  -9.\n\n1.4 General Quantum Systems \n25\nas one of the measured quantities. Now use matrix algebra to calculate this:\n \n  y8 + 0  +9x =\n1\n12 11 e-ia2 1\n12 a1\n1b\n \n \n = 1\n2 11 + e-ia2\n \n \n @ y8 + 0  +9x@\n2 = 1\n2 11 + e-ia21\n2 11 + eia2 \n(1.59)\n \n = 1\n4 11 + eia + e-ia + 12 \n \n \n = 1\n2 11 + cos a2 = 1\n2.\n \nThis result requires that cos a = 0, or that a = {p>2. The two choices for the phase correspond \nto the two possibilities for the direction of the y-axis relative to the already determined x- and z-axes. \nThe choice a = +p>2 can be shown to correspond to a right-handed coordinate system, which is the \nstandard convention, so we choose that phase. We thus represent the 0 {9y kets as\n \n 0  +9y \u0003\n1\n22\n a1\ni b\n \n \n 0  -9y \u0003\n1\n22\n a 1\n-ib. \n(1.60)\nNote that the imaginary components of these kets are required. They are not merely a mathemati-\ncal convenience as one sees in classical mechanics. In general, quantum mechanical state vectors \nhave complex coefﬁcients. But this does not mean that the results of physical measurements are \ncomplex. On the contrary, we always calculate a measurement probability using a complex square, \nso all quantum mechanics predictions of probabilities are real.\n 1.4 \u0002 GENERAL QUANTUM SYSTEMS\nThe machinery we have developed for spin-1/2 systems can be generalized to other quantum systems. \nFor example, if an observable A yields quantized measurement results an for some ﬁnite range of n, \nthen we generalize the schematic depiction of a Stern-Gerlach measurement to a measurement of the \nA\n\u0002a1\u0003\n\u0002a2\u0003\n\u0002a3\u0003\n\u0002Ψin\u0003\na2\na1\na3\nFIGURE 1.15 Generic depiction of the quantum mechanical measurement of observable A.\n\n26 \nStern-Gerlach Experiments\nobservable A, as shown in Fig. 1.15. The observable A labels the measurement device and the possible \nresults a1, a2, a3, etc. label the output ports. The basis kets corresponding to the results an are then 0 an9. \nThe mathematical rules about kets in this general case are\n \n 8ai@ aj9 = dij         orthonormality \n \n 0  c9 = a\ni\n8ai0  c9 0 ai9\n completeness,  \n \n(1.61)\nwhere we use the Kronecker delta\n \ndij = e0\n1 i \u0002 j\ni = j \n(1.62)\nto express the orthonormality condition compactly. In this case, the generalization of postulate 4 says \nthat the probability of a measurement of one of the possible results an is\n \nPan = 08an0 cin9 0\n2.  \n(1.63)\nExample 1.4 Imagine a quantum system with an observable A that has three possible measure-\nment results: a1, a2, and a3. The three kets 0 a19, 0 a29, and 0 a39 corresponding to these possible \nresults form a complete orthonormal basis. The system is prepared in the state\n \n0  c9 = 20 a19 - 30 a29 + 4i0 a39. \n(1.64)\nCalculate the probabilities of all possible measurement results of the observable A.\nThe state vector in Eq. (1.64) is not normalized, so we must normalize it before calculating \nprobabilities. Introducing a complex normalization constant C, we ﬁnd\n \n 1 = 8c 0  c9\n \n \n = C*128a10 - 38a20 - 4i8a302C120 a19 - 30 a29 + 4i0 a392 \n \n = 0 C0\n2548a10 a19 - 68a10 a29 + 8i8a10 a39\n \n \n- 68a20 a19 + 98a20 a29 - 12i8a20 a39\n \n(1.65)\n \n \n- 8i8a30 a19 + 12i8a30 a29 + 168a30 a396\n \n \n = 0 C0\n254 + 9 + 166 = 0 C0\n2 29\n \n \n 1 C =\n1\n129.\n \nThe normalized state is\n \n0  c9 =\n1\n129 120 a19 - 30 a29 + 4i0 a392. \n(1.66)\n\n1.5 Postulates \n27\nThe probabilities of measuring the results a1, a2, and a3 are\n \n Pa1 = 08a10  c9 0\n2 \n \n = @8a10\n1\n129520 a19 - 30 a29 + 4i0 a396@\n2\n \n \n =\n1\n29 0 28a10 a19 - 38a10 a29 + 4i8a10 a39 0\n2 =\n4\n29 \n(1.67)\n \n Pa2 = 0 a20  c9 0\n2 = @8a2@\n1\n12952@ a19 - 3@ a29 + 4i@ a396@\n2\n=\n9\n29 \n \nPa3 = 08a30  c9 @\n2 = @8a3@\n1\n12952@ a19 - 3@ a29 + 4i@ a396@\n2\n= 16\n29 . \nA schematic of this experiment is shown in Fig. 1.16(a) and a histogram of the predicted probabili-\nties is shown in Fig. 1.16(b).\n 1.5 \u0002 POSTULATES\nWe have introduced two of the postulates of quantum mechanics in this chapter. The postulates \nof quantum mechanics dictate how to treat a quantum mechanical system mathematically and \nhow to interpret the mathematics to learn about the physical system in question. These postulates \ncannot be proven, but they have been successfully tested by many experiments, and so we accept \nthem as an accurate way to describe quantum mechanical systems. New results could force us \nto reevaluate these postulates at some later time. All six postulates are listed below to give you \nan idea where we are headed and a framework into which you can place the new concepts as we \nconfront them.\n \nPostulates of Quantum Mechanics\n \n1. The state of a quantum mechanical system, including all the information you can know \nabout it, is represented mathematically by a normalized ket 0 c9.\n \n2. A physical observable is represented mathematically by an operator A that acts on kets.\n \n3. The only possible result of a measurement of an observable is one of the eigenvalues an of \nthe corresponding operator A. \nA a2\na1\na3\n(a)\n(b)\nPa2 \u0006\nPa1\nPa1\nPa2\nPa3\n\u0006\nPa3 \u0006\nA\n1\nP\n4\n29\n9\n29\n16\n29\na3\na2\na1\n\u0002Ψin\u0003\nFIGURE 1.16 (a) Schematic diagram of the measurement of observable A and (b) histogram of the \npredicted measurement probabilities.\n\n28 \nStern-Gerlach Experiments\n \n4. The probability of obtaining the eigenvalue an in a measurement of the observable A on the \nsystem in the state 0  c9 is\n \nPan = 08an0  c9 0\n2, \n \n where 0 an9 is the normalized eigenvector of A corresponding to the eigenvalue an.\n \n5. After a measurement of A that yields the result an, the quantum system is in a new state that \nis the normalized projection of the original system ket onto the ket (or kets) corresponding \nto the result of the measurement:\n \n0  c\u00049 =\nPn0  c9\n28c 0 Pn0  c9\n. \n \n6. The time evolution of a quantum system is determined by the Hamiltonian or total energy \noperator H(t) through the Schrödinger equation\n \niU d\ndt\n 0  c 1t29 = H 1t2 0  c 1t29. \nAs you read these postulates for the ﬁrst time, you will undoubtedly encounter new terms and \nconcepts. Rather than explain them all here, the plan of this text is to continue to explain them through \ntheir manifestation in the Stern-Gerlach spin-1/2 experiment. We have chosen this example because it \nis inherently quantum mechanical and forces us to break away from reliance on classical intuition or \nconcepts. Moreover, this simple example is a paradigm for many other quantum mechanical systems. \nBy studying it in detail, we can appreciate much of the richness of quantum mechanics.\nSUMMARY\nThrough the Stern-Gerlach experiment we have learned several key concepts about quantum mechan-\nics in this chapter.\n• Quantum mechanics is probabilistic. \n \nWe cannot predict the results of experiments precisely. We can predict only the probability \nthat a certain result is obtained in a measurement.\n• Spin measurements are quantized. \n \nThe possible results of a spin component measurement are quantized. Only these discrete \nvalues are measured.\n• Quantum measurements disturb the system. \n \nMeasuring one physical observable can “destroy” information about other observables.\nWe have learned how to describe the state of a quantum mechanical system mathematically using \na ket, which represents all the information we can know about that state. The kets 0  +9 and 0  -9 result \nwhen the spin component Sz along the z-axis is measured to be up or down, respectively. These kets \nform an orthonormal basis, which we denote by the inner products\n \n 8+  0  +9 = 1  \n \n 8- 0  -9 = 1  \n(1.68)\n \n 8+  0  -9 = 0. \n\nProblems \n29\nThe basis is also complete, which means that it can be used to express all possible kets as superposi-\ntion states\n \n0  c9 = a0  +9 + b0  -9. \n(1.69)\nFor spin component measurements, the kets corresponding to spin up or down along the three \nCartesian axes are\n \n 0  +9   0  +9x =\n1\n12 3 0  +9 + 0  -94   0  +9y =\n1\n12 3 0  +9 + i0  -94\n \n 0  -9   0  -9x =\n1\n12 3 0  +9 - 0  -94   0  -9y =\n1\n12 3 0  +9 - i0  -94. \n(1.70)\nWe also found it useful to introduce a matrix notation for calculations. In this matrix language the kets \nin Eq. (1.70) are represented by\n \n0  +9 \u0003 ¢1\n0≤ \n0  +9x \u0003\n1\n22\n ¢1\n1≤ \n0  +9y \u0003\n1\n22\n ¢1\ni ≤ \n \n0  -9 \u0003 ¢0\n1≤ \n0  -9x \u0003\n1\n22\n ¢ 1\n-1≤ \n0  -9y \u0003\n1\n22\n ¢ 1\n-i≤. \n(1.71)\nThe most important tool we have learned so far is the probability postulate (postulate 4). To \ncalculate the probability that a measurement on an input state 0 cin9 will yield a particular result, for \nexample Sz = U>2, we complex square the inner product of the input state with the ket corresponding \nto the measured result, 0  +9 in this case:\n \nP+ = 08 + 0 cin9 0\n2. \n(1.72)\nThis is generalized to other systems where a measurement yields a particular result an corresponding \nto the ket 0 an9 as:\n \nPan = 08an0 cin9 0\n2.  \n(1.73)\nPROBLEMS\n 1.1 Consider the following state vectors:\n \n 0 c19 = 30  +9 + 40  -9\n \n \n 0 c29 = 0  +9 + 2i0  -9\n \n \n 0 c39 = 30  +9 - eip>30  -9. \na) Normalize each state vector.\nb) For each state vector, calculate the probability that the spin component is up or down \nalong each of the three Cartesian axes. Use bra-ket notation for the entire calculation.\nc) Write each normalized state in matrix notation.\nd) Repeat part (b) using matrix notation for the entire calculation.\n\n30 \nStern-Gerlach Experiments\n 1.2 Consider the three quantum states:\n \n 0 c19 =\n1\n13 0  +9 + i 12\n13 0  -9\n \n \n 0 c29 =\n1\n15 0  +9 -\n2\n15 0  -9\n \n \n 0 c39 =\n1\n12 0  +9 + eip>4 1\n12 0  -9. \n \n Use bra-ket notation (not matrix notation) to solve the following problems. Note  \nthat 8+  0  +9 = 1, 8- 0  -9 = 1, and 8+  0  -9 = 0.\na) For each of the 0 ci9 above, ﬁnd the normalized vector 0 fi9 that is orthogonal to it.\nb) Calculate the inner products 8ci0 cj9 for i and j = 1, 2, 3.\n 1.3 Show that a change in the overall phase of a quantum state vector does not change \nthe probability of obtaining a particular result in a measurement. To do this, consider  \nhow the probability is affected by changing the state 0  c9 to the state eid0  c9.\n 1.4 Show by explicit bra-ket calculations using the states in Eq. (1.29) that the four \nexperimental results in Eq. (1.28) lead to the results 0 b0\n2 = 0 c0\n2 = 0 d0\n2 = 1\n2.\n 1.5 A beam of spin-1/2 particles is prepared in the state\n0  c9 =\n2\n113 0  +9 + i 3\n113 0  -9.\na) What are the possible results of a measurement of the spin component Sz, and with \nwhat probabilities would they occur?\nb) What are the possible results of a measurement of the spin component Sx, and with \nwhat probabilities would they occur?\nc) Plot histograms of the predicted measurement results from parts (a) and (b).\n 1.6 A beam of spin-1/2 particles is prepared in the state\n0  c9 =\n2\n113 0  +9x + i 3\n113 0  -9x.\na) What are the possible results of a measurement of the spin component Sz, and with \nwhat probabilities would they occur?\nb) What are the possible results of a measurement of the spin component Sx, and with \nwhat probabilities would they occur?\nc) Plot histograms of the predicted measurement results from parts (a) and (b).\n 1.7 A classical coin is thrown in the air and lands on the ground, where a measurement is \nmade of its state.\na) What are the possible results of this measurement?\nb) What are the predicted probabilities for these possible outcomes?\nc) Plot a histogram of the predicted measurement results.\n 1.8 A classical cubical die is thrown onto a table and comes to rest, where a measurement \nis made of its state.\na) What are the possible results of this measurement?\nb) What are the predicted probabilities for these possible outcomes?\nc) Plot a histogram of the predicted measurement results.\n\nProblems \n31\n 1.9 A pair of dice (classical cubes) are thrown onto a table and come to rest, where a \nmeasurement is made of the state of the system (i.e., the sum of the two dice).\na) What are the possible results of this measurement?\nb) What are the predicted probabilities for these possible outcomes?\nc) Plot a histogram of the predicted measurement results.\n 1.10 Consider the three quantum states:\n \n 0 c19 = 4\n5 0  +9 + i 3\n5 0  -9\n \n \n 0 c29 = 4\n5 0  +9 - i 3\n5 0  -9\n \n \n 0 c39 = -  4\n5 0  +9 + i 3\n5 0  -9. \na) For each of the 0 ci9 above, calculate the probabilities of spin component measurements \nalong the x-, y-, and z-axes.\nb) Use your results from (a) to comment on the importance of the overall phase and of the \nrelative phases of the quantum state vector.\n 1.11  A beam of spin-1/2 particles is prepared in the state\n0  c9 =\n3\n134 0  +9 + i 5\n134 0  -9.\na) What are the possible results of a measurement of the spin component Sz, and with what \nprobabilities would they occur?\nb) Suppose that the Sz measurement yields the result Sz = -U>2. Subsequent to that result \na second measurement is performed to measure the spin component Sx. What are the \npossible results of that measurement, and with what probabilities would they occur?\nc) Draw a schematic diagram depicting the successive measurements in parts (a) and (b).\n 1.12  Consider a quantum system with an observable A that has three possible measurement \nresults: a1, a2, and a3. Write down the orthogonality, normalization, and completeness \nrelations for the three kets comprising the basis corresponding to the possible results of the  \nA measurement.\n 1.13  Consider a quantum system with an observable A that has three possible measurement \nresults: a1, a2, and a3.\na) Write down the three kets 0 a19, 0 a29, and 0 a39 corresponding to these possible results \nusing matrix notation.\nb) The system is prepared in the state\n0  c9 = 10 a19 - 20 a29 + 50 a39.\n \nWrite this state in matrix notation and calculate the probabilities of all possible measurement \nresults of the observable A. Plot a histogram of the predicted measurement results.\nc) In a different experiment, the system is prepared in the state\n0  c9 = 20 a19 + 3i0 a29.\n \nWrite this state in matrix notation and calculate the probabilities of all possible measurement \nresults of the observable A. Plot a histogram of the predicted measurement results.\n\n32 \nStern-Gerlach Experiments\n 1.14  Consider a quantum system in which the energy E is measured and there are four possible \nmeasurement results: 2 eV, 4 eV, 7 eV, and 9 eV. The system is prepared in the state\n0  c9 =\n1\n139 530 2 eV9 - i0 4 eV9 + 2eip>70 7 eV9 + 50 9 eV96.\n \n Calculate the probabilities of all possible measurement results of the energy E. Plot a \nhistogram of the predicted measurement results.\n 1.15  Consider a quantum system described by a basis 0 a19, 0 a29, and 0 a39. The system is initially \nin a state\n0\n ci9 =\ni\n13 0 a19 + 4\n2\n3 0 a29.\n \n Find the probability that the system is measured to be in the ﬁnal state\n@\n cf9 = 1+i\n13 0 a19 +\n1\n16 0 a29 +\n1\n16 0 a39.\n 1.16  The spin components of a beam of atoms prepared in the state 0 cin9 are measured and the fol-\nlowing experimental probabilities are obtained:\n \nP+ = 1\n2 \nP+x = 3\n4 \nP+y = 0.067\n \nP- = 1\n2 \nP-x = 1\n4 \nP-y = 0.933. \n \n From the experimental data, determine the input state.\n 1.17  In part (1) of SPINS Lab #2, you measured the probabilities of all the possible spin compo-\nnents for each of the unknown initial states 0 ci9 (i = 1, 2, 3, 4). Using your data from that \nlab, ﬁnd the unknown states 0 c19, 0 c29, 0 c39, and 0 c49. Express each of the unknown states \nas a linear superposition of the Sz basis states 0  +9 and 0  -9. For each state, use your result \nto calculate the theoretical values of the probabilities for each component measurement and \ncompare these theoretical predictions with your experimental results.\nRESOURCES\nActivities \nSPINS: A software program to simulate Stern-Gerlach spin experiments. The Java software runs on \nall platforms and can be downloaded in two forms:\nOpen Source Physics framework\nwww.physics.oregonstate.edu/~mcintyre/ph425/spins/index_SPINS_OSP.html\nor\nStandalone Java\nwww.physics.oregonstate.edu/~mcintyre/ph425/spins\nThe bulleted activities are available at\nwww.physics.oregonstate.edu/qmactivities\n\nResources \n33\n• SPINS Lab 1: An introduction to successive Stern-Gerlach spin-1/2 measurements. The random-\nness of measurements is demonstrated and students use statistical analysis to deduce probabilities \nfrom measurements.\n• SPINS Lab 2: Students deduce unknown quantum state vectors from measurements of spin projec-\ntions (part 3 requires material from Chapter 2 to do the calculations).\nStern-Gerlach simulation: A different simulation of the Stern-Gerlach experiment from the PHET \ngroup at the University of Colorado (somewhat Flashier version):\nhttp://phet.colorado.edu/en/simulation/stern-gerlach\nFurther Reading\nThe history of the Stern-Gerlach experiment and how a bad cigar helped are chronicled in  \na Physics Today article:\nB. Friedrich and D. Herschbach, “Stern and Gerlach: How a Bad Cigar Helped Reorient  \nAtomic Physics,” Phys. Today 56(12), 53–59 (2003). \n \nhttp://dx.doi.org/10.1063/1.1650229\nA different spin on the quantum mechanics of socks is discussed by John S. Bell in this article:\nJ. S. Bell, “Bertlmann’s socks and the nature of reality, ” J. Phys. Colloq. 42, C22 \nC2.41-C2.62 (1981).  \n \nhttp://cdsweb.cern.ch/record/142461\nNature has published a supplement on the milestones in spin physics. An extensive timeline \nof historical events, review articles, and links to original articles are included.\nNature Phys. 4, S1–S43 (2008). \n \nwww.nature.com/milestones/spin\nThe SPINS lab software is described in this pedagogical article:\nD. V. Schroeder and T. A. Moore, “A computer-simulated Stern-Gerlach laboratory,”  \nAm. J. Phys. 61, 798–805 (1993). \n \nhttp://dx.doi.org/10.1119/1.17172\nSome other textbooks that take a spins-ﬁrst approach or have an extensive treatment  \nof Stern-Gerlach experiments:\nR. P. Feynman, R. B. Leighton, and M. Sands, The Feynman Lectures on Physics, \nVolume 3, Quantum Mechanics, Reading, MA: Addison-Wesley Publishing Company, \nInc., 1965.\nJ. J. Sakurai, Modern Quantum Mechanics, Redwood City, CA: Addison-Wesley \nPublishing Company, Inc., 1985.\nJ. S. Townsend, A Modern Approach to Quantum Mechanics, New York: McGraw \nHill, Inc., 1992.\nC. Cohen-Tannoudji, B. Diu, and F. Laloë, Quantum Mechanics, New York: John Wiley & \nSons, 1977.\nD. F. Styer, The Strange World of Quantum Mechanics, Cambridge: Cambridge University \nPress, 2000.\n\nC H A P T E R \n2\nOperators and Measurement\nIn Chapter 1 we used the results of experiments to deduce a mathematical description of the spin-1/2 \nsystem. The Stern-Gerlach experiments demonstrated that spin component measurements along the \nx-, y-, or z-axes yield only {U>2 as possible results. We learned how to predict the probabilities of \nthese measurements using the basis kets of the spin component observables Sx, Sy, and Sz, and these \npredictions agreed with the experiments. However, the real power of a theory is its ability to predict \nresults of experiments that you haven’t yet done. For example, what are the possible results of a mea-\nsurement of the spin component Sn along an arbitrary direction nn and what are the predicted probabili-\nties? To make these predictions, we need to learn about the operators of quantum mechanics.\n2.1 \u0002 OPERATORS, EIGENVALUES, AND EIGENVECTORS\nThe mathematical theory we developed in Chapter 1 used only quantum state vectors. We said that \nthe state vector represents all the information we can know about the system and we used the state \nvectors to calculate probabilities. With each observable Sx, Sy, and Sz we associated a pair of kets \n corresponding to the possible measurement results of that observable. The observables themselves are \nnot yet included in our mathematical theory, but the distinct association between an observable and its \nmeasurable kets provides the means to do so.\nThe role of physical observables in the mathematics of quantum theory is described by the two \npostulates listed below. Postulate 2 states that physical observables are represented by mathematical \noperators, in the same sense that physical states are represented by mathematical vectors or kets (postu-\nlate 1). An operator is a mathematical object that acts or operates on a ket and transforms it into a new \nket, for example A0 c9 = 0 f9. However, there are special kets that are not changed by the operation \nof a particular operator, except for a possible multiplicative constant, which we know does not change \nanything measurable about the state. An example of a ket that is not changed by an operator would be \nA0 c9 = a0 c9. Such kets are known as eigenvectors of the operator A and the multiplicative constants \nare known as the eigenvalues of the operator. These are important because postulate 3 states that the only \npossible result of a measurement of a physical observable is one of the eigenvalues of the corresponding \noperator.\nPostulate 2\nA physical observable is represented mathematically by an operator A \nthat acts on kets.\n\n2.1 Operators, Eigenvalues, and Eigenvectors \n35\nWe now have a mathematical description of that special relationship we saw in Chapter 1 between \na physical observable, Sz say, the possible results {U>2, and the kets 0{9 corresponding to those \nresults. This relationship is known as the eigenvalue equation and is depicted in Fig. 2.1 for the case \nof the spin up state in the z-direction. In the eigenvalue equation, the observable is represented by an \noperator, the eigenvalue is one of the possible measurement results of the observable, and the eigen-\nvector is the ket corresponding to the chosen eigenvalue of the operator. The eigenvector appears on \nboth sides of the equation because it is unchanged by the operator.\nThe eigenvalue equations for the Sz operator in a spin-1/2 system are:\n \n Sz0  +9 = +  U\n2 0  +9  \n \n Sz0  -9 = -  U\n2 0  -9. \n \n(2.1)\nThese equations tell us that +U>2 is the eigenvalue of Sz corresponding to the eigenvector 0  +9 and \n-U>2 is the eigenvalue of Sz corresponding to the eigenvector 0  -9. Equations (2.1) are sufﬁcient to \ndeﬁne how the Sz operator acts mathematically on kets. However, it is useful to use matrix notation \nto represent operators in the same sense that we used column vectors and row vectors in Chapter 1 to \nrepresent bras and kets, respectively. For Eqs. (2.1) to be satisﬁed using matrix algebra with the kets \nrepresented as column vectors of size 1*  2, the operator Sz must be represented by a 2 *  2 matrix. The \neigenvalue equations (2.1) provide sufﬁcient information to determine this matrix.\nTo determine the matrix representing the operator Sz, assume the most general form for a 2 *  2 matrix\n \nSz \u0003 aa\nb\nc\ndb, \n(2.2)\nwhere we are again using the \u0003 symbol to mean “is represented by.” Now write the eigenvalue equa-\ntions in matrix form:\n \n aa\nb\nc\ndb a1\n0b = +  U\n2\n a1\n0b  \n \n aa\nb\nc\ndb a0\n1b = -  U\n2\n a0\n1b. \n \n(2.3)\nPostulate 3\nThe only possible result of a measurement of an observable is one of the \neigenvalues an of the corresponding operator A.\neigenvalue\neigenvector\noperator\n\u0002\n2\nSz \u0002\u0002\u0003\u0005\u0006\u0005\u0005\u0005\u0005\u0005\u0005\u0002\u0002\u0003\u0005\nFIGURE 2.1 Eigenvalue equation for the spin up state.\n\n36 \nOperators and Measurement\nNote that we are still using the convention that the 0{9 kets are used as the basis for the representation. \nIt is crucial that the rows and columns of the operator matrix are ordered in the same manner as used \nfor the ket column vectors; anything else would amount to nonsense. An explicit labeling of the rows \nand columns of the operator and the basis kets makes this clear:\n \nSz\n0  +9\n0  -9\n8+ 0\na\nb\n8- 0\nc\nd\n \n0  +9\n8+ 0\n1\n8- 0\n0\n \n0  -9\n8+ 0\n0\n8- 0\n1\n . \n(2.4)\nCarrying through the multiplication in Eqs. (2.3) yields\n \n aa\ncb = +  U\n2\n  a1\n0b  \n \n ab\ndb = -  U\n2\n a0\n1b, \n \n(2.5)\nwhich results in\n \na = + U\n2 \nb = 0 \n \nc = 0 \nd = -  U\n2. \n \n(2.6)\nThus the matrix representation of the operator Sz is\n \n Sz \u0003 aU>2\n0\n0\n-U>2b \n \n \u0003 U\n2\n a1\n0\n0\n-1b.\n \n \n(2.7)\nNote two important features of this matrix: (1) it is a diagonal matrix—it has only diagonal elements—\nand (2) the diagonal elements are the eigenvalues of the operator, ordered in the same manner as the \ncorresponding eigenvectors. In this example, the basis used for the matrix representation is that formed \nby the eigenvectors 0{9 of the operator Sz. That the matrix representation of the operator in this case \nis a diagonal matrix is a necessary and general result of linear algebra that will prove valuable as we \nstudy quantum mechanics. In simple terms, we say that an operator is always diagonal in its own \nbasis. This special form of the matrix representing the operator is similar to the special form that the \neigenvectors 0{9 take in this same representation—the eigenvectors are unit vectors in their own \nbasis. These ideas cannot be overemphasized, so we repeat them:\nAn operator is always diagonal in its own basis.  \nEigenvectors are unit vectors in their own basis.\nLet’s also summarize the matrix representations of the Sz operator and its eigenvectors:\n \nSz \u0003 U\n2\n a1\n0\n0\n-1b \n0  +9 \u0003 a1\n0b \n0  -9 \u0003 a0\n1b. \n(2.8)\n\n2.1 Operators, Eigenvalues, and Eigenvectors \n37\n2.1.1 \u0002 Matrix Representation of Operators\nNow consider how matrix representation works in general. Consider a general operator A describ-\ning a physical observable (still in the two-dimensional spin-1/2 system), which we represent by the \ngeneral matrix\n \nA \u0003 aa\nb\nc\ndb \n(2.9)\nin the Sz basis. The operation of A on the basis ket 0  +9 yields\n \nA0  +9 \u0003 aa\nb\nc\ndba1\n0b = aa\ncb. \n(2.10)\nThe inner product of this new ket A0  +9 with the ket 0  +9 (converted to a bra following the rules) results in\n \n8+ 0 A0  +9 = 11\n02aa\ncb = a , \n(2.11)\nwhich serves to isolate one of the elements of the matrix. Hence an individual element such as \n8+ 0 A0  +9 or 8+ 0 A0  -9 is generally referred to as a matrix element. This “sandwich” of a bra, an \noperator, and a ket\n \n8bra0 OPERATOR0 ket9 \n(2.12)\nplays an important role in many quantum mechanical calculations. Even in cases where the bra and ket \nare not basis kets, such as in 8c0 A0 f9, we still refer to this as a matrix element. A schematic diagram \nof a generic matrix element is depicted in Fig. 2.2(a).\nAll four elements of the matrix representation of A can be determined in the same manner as \nEq. (2.11), with the ﬁnal result\n \nA \u0003 ¢8+ 0 A0  +9\n8+ 0 A0  -9\n8- 0 A0  +9\n8- 0 A0  -9≤. \n(2.13)\nTo emphasize the structure of the matrix, let’s write it with explicit labeling of the rows and columns:\n \nA\n0  +9\n0  -9\n8+ 0\n8+ 0 A0 +9\n8+ 0 A0  -9\n8- 0\n8- 0 A0 +9\n8- 0 A0  -9\n . \n(2.14)\n(a) bra\nket\noperator\n\u0004bra\u0002OPERATOR\u0002ket\u0003\n(b) row\ncolumn\noperator\n\u0004n\u0002A\u0002m\u0003\n\u0004Φ\u0002A\u0002Ψ\u0003\nFIGURE 2.2 (a) Schematic diagram of a generic matrix element. (b) Schematic diagram \nof the row and column labeling convention for matrix elements.\n\n38 \nOperators and Measurement\nIn a more general problem with more than two dimensions in the complex vector space, the matrix \nrepresentation of an operator is\n \nA \u0003 •\nA11\nA12\nA13\ng\nA21\nA22\nA23\ng\nA31\nA32\nA33\ng\nf\nf\nf\nf\nμ, \n(2.15)\nwhere the matrix elements are\n \nAij = 8i0\n A0  j9 \n(2.16)\nand the basis is assumed to be the states labeled 0 i9, with the subscripts i and j labeling the rows and \ncolumns respectively, as depicted in Fig. 2.2(b). Using this matrix representation, the action of this\noperator on a general ket 0 c9 = a\ni\nci0 i9 is\n \nA0 c9 \u0003 •\nA11\nA12\nA13\ng\nA21\nA22\nA23\ng\nA31\nA32\nA33\ng\nf\nf\nf\nf\nμ•\nc1\nc2\nc3\nf\nμ = •\nA11c1 + A12c2 + A13c3 + g\nA21c1 + A22c2 + A23c3 + g\nA31c1 + A32c2 + A33c3 + g\nf\nμ. \n(2.17)\nIf we write the new ket 0  f9 = A0 c9 as 0 f9 = a\ni\nbi0 i9, then from Eq. (2.17) the coefﬁcients bi are\n \nbi = a\nj\nAij\n cj \n(2.18)\nin summation notation.\n 2.1.2 \u0002 Diagonalization of Operators\nIn the case of the operator Sz above, we used the experimental results and the eigenvalue equations to \nﬁnd the matrix representation of the operator in Eq. (2.7). It is more common to work the other way. \nThat is, one is given the matrix representation of an operator and is asked to ﬁnd the possible results of \na measurement of the corresponding observable. According to the third postulate, the possible results \nare the eigenvalues of the operator, and the eigenvectors are the quantum states representing them. In \nthe case of a general operator A in a two-state system, the eigenvalue equation is\n \nA0 an9 = an0 an9, \n(2.19)\nwhere we have labeled the eigenvalues an and we have labeled the eigenvectors with the correspond-\ning eigenvalues. In matrix notation, the eigenvalue equation is\n \n¢A11\nA12\nA21\nA22\n≤¢cn1\ncn2\n≤= an ¢cn1\ncn2\n≤, \n(2.20)\nwhere cn1 and cn2 are the unknown coefﬁcients of the eigenvector 0 an9 corresponding to the eigen-\nvalue an. This matrix equation yields the set of homogeneous equations\n \n 1A11 - an2cn1 + A12\n cn2 = 0  \n \n A21cn1 + 1A22 - an2cn2 = 0. \n \n(2.21)\n\n2.1 Operators, Eigenvalues, and Eigenvectors \n39\nThe rules of linear algebra dictate that a set of homogeneous equations has solutions for the unknowns \ncn1 and cn2 only if the determinant of the coefﬁcients vanishes:\n \n` A11 - an\nA12\nA21\nA22 - an\n` = 0. \n(2.22)\nIt is common notation to use the symbol l for the eigenvalues, in which case this equation is\n \ndet 1A - lI 2 = 0, \n(2.23)\nwhere I is the identity matrix\n \nI = a1\n0\n0\n1b. \n(2.24)\nEquation (2.23) is known as the secular or characteristic equation. It is a second order equation in the \nparameter l and the two roots are identiﬁed as the two eigenvalues a1 and a2 that we are trying to ﬁnd. \nOnce these eigenvalues are found, they are then individually substituted back into Eqs. (2.21), which \nare solved to ﬁnd the coefﬁcients of the corresponding eigenvector.\nExample 2.1 Assume that we know (e.g., from Problem 2.1) that the matrix representation for \nthe operator Sy is\n \nSy \u0003 U\n2\n a0\n-i\ni\n0 b . \n(2.25)\nFind the eigenvalues and eigenvectors of the operator Sy.\nThe general eigenvalue equation is\n \nSy0 l9 = l0 l9, \n(2.26)\nand the possible eigenvalues l are found using the secular equation\n \ndet0 Sy - lI0 = 0. \n(2.27)\nThe secular equation is\n \n∞\n-l\n-i U\n2\ni U\n2\n-l\n∞= 0, \n(2.28)\nand solving yields the eigenvalues\n \n l2 + i 2 a U\n2b\n2\n= 0 \n \n l2 - a U\n2b\n2\n= 0\n \n \n l2 = a U\n2b\n2\n \n \n l = { U\n2,\n \n(2.29)\n\n40 \nOperators and Measurement\nwhich was to be expected, because we know that the only possible results of a measurement of any \nspin component are {U>2.\nAs before, we label the eigenvectors 0{9y. The eigenvalue equation for the positive eigenvalue is\n \nSy0  +9y = +  U\n2 0  +9y, \n(2.30)\nor in matrix notation\n \nU\n2\n a0\n-i\ni\n0 b aa\nbb = + U\n2\n aa\nbb, \n(2.31)\nwhere we must solve for a and b to determine the eigenvector. Multiplying through and canceling \nthe common factor yields\n \na-ib\nia b = aa\nbb. \n(2.32)\nThis results in two equations, but they are not linearly independent, so we need some more infor-\nmation. The normalization condition provides what we need. Thus we have two equations that \ndetermine the eigenvector coefﬁcients:\n \n b = ia\n \n \n 0 a0\n2 + 0 b0\n2 = 1. \n(2.33)\nSolving these yields\n \n 0 a0\n2 + 0 ia0\n2 = 1 \n \n 0 a0\n2 = 1\n2.\n \n(2.34)\nAgain we follow the convention of choosing the ﬁrst coefﬁcient to be real and positive, resulting in\n \n a =\n1\n12  \n \n b = i 1\n12. \n(2.35)\nThus the eigenvector corresponding to the positive eigenvalue is\n \n0  +9y \u0003\n1\n12\n a1\ni b. \n(2.36)\nLikewise, one can ﬁnd the eigenvector for the negative eigenvalue to be\n \n0  -9y \u0003\n1\n12\n a 1\n-ib . \n(2.37)\nThese are, of course, the same states we found in Chapter 1 (Eq. 1.60).\nThis procedure of ﬁnding the eigenvalues and eigenvectors of a matrix is known as diagonaliza-\ntion of the matrix and is the key step in many quantum mechanics problems. Generally, if we ﬁnd a \nnew operator, the ﬁrst thing we do is diagonalize it to ﬁnd its eigenvalues and eigenvectors. However, \nwe stop short of the mathematical exercise of ﬁnding the matrix that transforms the original matrix to \nits new diagonal form. This would amount to a change of basis from the original basis to a new basis \nof the eigenvectors we have just found, much like a rotation in three dimensions changes from one \ncoordinate system to another. We don’t want to make this change of basis. In the example above, the \nSy matrix is not diagonal, whereas the Sz matrix is diagonal, because we are using the Sz basis. It is \n\n2.2 New Operators \n41\ncommon practice to use the Sz basis as the default basis, so you can assume that is the case unless you \nare told otherwise.\nIn summary, we now know three operators and their eigenvalues and eigenvectors. The spin com-\nponent operators Sx, Sy, and Sz all have eigenvalues {U>2. The matrix representations of the opera-\ntors and eigenvectors are (see Problem 2.1)\n \nSx \u0003 U\n2\n a0\n1\n1\n0b \n0  +9x \u0003\n1\n12\n a1\n1b \n0  -9x \u0003\n1\n12\n a 1\n-1b \n \nSy \u0003 U\n2\n a0\n-i\ni\n0 b \n0  +9y \u0003\n1\n12\n a1\ni b \n0  -9y \u0003\n1\n12\n a 1\n-ib    \n.\n \n \nSz \u0003 U\n2\n a1\n0\n0\n-1b \n0  +9 \u0003 a1\n0b \n0  -9 \u0003 a0\n1b \n(2.38)\n 2.2 \u0002 NEW OPERATORS\n 2.2.1 \u0002 Spin Component in a General Direction\nNow that we know the three operators corresponding to the spin components along the three Cartesian \naxes, we can use them to ﬁnd the operator Sn for the spin component along a general direction nn. This \nnew operator will allow us to predict results of experiments we have not yet performed. The direction \nnn is speciﬁed by the polar and azimuthal angles u and f as shown in Fig. 2.3. The unit vector nn is\n \nn = in sin u cos f + jn sin u sin f + kn cos u. \n(2.39)\nThe spin component along this direction is obtained by projecting the spin vector S onto this new unit \nvector\n \n Sn = S~nn\n \n \n = Sx sin u cos f + Sy sin u sin f + Sz cos u. \n \n(2.40)\nThe matrix representations we found for Sx, Sy, and Sz lead to the matrix representation of the spin \ncomponent operator Sn (Problem 2.6):\n \nSn \u0003 U\n2\n acos u\nsin u  e-if\nsin u  eif\n-cos u\nb. \n(2.41)\nn\nz\nx\ny\nΦ\nΘ\n\u0006\nFIGURE 2.3 General direction along which to measure the spin component.\n\n42 \nOperators and Measurement\nWe have found a new operator, so to learn about its properties, we diagonalize it. Following \nthe diagonalization procedure outlined in Section 2.1.2, we ﬁnd that the eigenvalues of Sn are {U>2 \n(Problem 2.7). So if we measure the spin component along any direction, we get only two possible \nresults. This is to be expected from the experiments in Chapter 1. The eigenvectors for these two pos-\nsible measurements are (Problem 2.7):\n \n0  +9n = cos u\n2 0  +9 + sin u\n2\n eif0  -9  \n \n0  -9n = sin u\n2 0  +9 - cos u\n2\n eif0  -9, \n \n(2.42)\nwhere we again use the convention of choosing the ﬁrst coefﬁcient to be real and positive. It is important \nto point out that the 0  +9n eigenstate (or equivalently the 0  -9n eigenstate) can be used to represent any \npossible ket in a spin-1/2 system, if one allows for all possible angles 0 … u 6 p and 0 … f 6 2p.\nWe generally write the most general state as 0 c9 = a0  +9 + b0  -9, where a and b are complex. Requir-\ning that the state be normalized and using the freedom to choose the ﬁrst coefﬁcient real and positive \nreduces this to\n \n0 c9 = 0 a0 0  +9 + 41 - 0 a0\n2\n eif0  -9. \n(2.43)\nIf we change the parametrization of 0 a0  to cos 1u>22, we see that 0  +9n is equivalent to the most general \nstate 0 c9. This correspondence between the 0  +9n eigenstate and the most general state is only valid in a \ntwo-state system such as spin 1/2. In systems with more dimensionality, it does not hold because more \nparameters are needed to specify the most general state than are afforded by the two angles u and f.\nExample 2.2 Find the probabilities of the measurements shown in Fig. 2.4, assuming that the \nﬁrst Stern-Gerlach analyzer is aligned along the direction nn deﬁned by the angles u = 2p>3 and \nf = p>4.\nThe measurement by the ﬁrst Stern-Gerlach analyzer prepares the system in the spin up state \n0  +9n along the direction nn. This state is then the input state to the second Stern-Gerlach analyzer. \nThe input state is\n \n 0 cin9 = 0  +9n = cos u\n2 0  +9 + sin u\n2\n eif0  -9 \n \n = cos p\n3 0  +9 + sin p\n3\n eip/40  -9\n \n \n = 1\n2 0  +9 + 23\n2\n eip/40  -9.\n \n(2.44)\nX\n?\n?\n^n\nP x\n2\nP x\nx\nn\n2\nx\nn\nFIGURE 2.4  Measurement of the spin component after state preparation in a new direction.\n\n2.2 New Operators \n43\nThe second analyzer is aligned along the x-axis, so the probabilities are\n \n P+x = 0 x8+ 0 cin9 0\n2 = 0 x8+ 0  +9n0\n2  \n \n P-x = 0 x8- 0 cin9 0\n2 = 0 x8- 0  +9n0\n2. \n(2.45)\nLet’s calculate the ﬁrst probability using bra-ket notation, recalling that 0  +9x =\n1\n12 30  +9 + 0  -94:\n \n P+x = 0 x8+ 0  +9n0\n2\n \n \n = @ 1\n12 38+ 0   +  8- 0 4 1\n2 3 0  +9 + 13eip/40  -94@\n2\n \n \n = @\n1\n212 31 + 13eip/44@\n2\n \n \n = 1\n8 31 + 13eip/4431 + 13e-ip/44\n \n \n = 1\n8 31 + 131eip/4 + e-ip/42 + 34\n \n \n = 1\n8 34 + 213 cos 1p>424\n \n \n = 1\n8 34 + 213> 124 \u0002 0.806.\n \n(2.46)\nLet’s calculate the second probability using matrix notation, recalling that 0  -9x =\n1\n12 30  +9 - 0  -94:\n \n P-x = 0 x8- 0  +9n0\n2\n \n \n = ` 1\n12 11\n-12 1\n2 a\n1\n13eip>4b `\n2\n \n \n = @\n1\n212 31 - 13eip/44@\n2\n \n \n = 1\n8 34 - 213 cos 1p>424\n \n \n = 1\n8 34 - 213> 124 \u0002 0.194. \n(2.47)\nThe two results sum to unity as they must. A histogram of the measured results is shown in Fig. 2.5.\n1\nP\nP\u0003x\nP\u0002x\n\u0002Ψin\u0003 \u0006\u0005\u0002\u0002\u0003n\nSx\n\u0003\u0002\n2\n\u0002\n2\nFIGURE 2.5 Histogram of spin component Sx measurement.\n\n44 \nOperators and Measurement\n2.2.2 \u0002 Hermitian Operators\nSo far we have deﬁned how operators act upon kets. For example, an operator A acts on a ket 0 c9 to \nproduce a new ket 0 f9 = A0 c9. The operator acts on the ket from the left; if the operator is on the \nright of the ket, the result is not deﬁned, which is clear if you try to use matrix representation. Simi-\nlarly, an operator acting on a bra must be on the right side of the bra\n \n8j0 = 8c0 A \n(2.48)\nand the result is another bra. However, the bra 8j0 = 8c0 A is not the bra 8f0  that corresponds to the \nket 0 f9 = A0 c9. Rather the bra 8f0  is found by deﬁning a new operator A† that obeys\n \n8f0 = 8c0 A†. \n(2.49)\nThis new operator A† is called the Hermitian adjoint of the operator A. We can learn something about the \nHermitian adjoint by taking the inner product of the state 0 f9 = A0 c9 with another (unspeciﬁed) state 0 b9\n \n 8f0 b9 = 8b0 f9*\n \n \n 38c0 A+4 0 b9 = 58b03A0 c946* \n \n 8c0 A+ 0 b9 = 8b0 A0 c9*,\n \n(2.50)\nwhich relates the matrix elements of A and A†. Equation (2.50) tells us that the matrix representing the \nHermitian adjoint A† is found by transposing and complex conjugating the matrix representing A. This \nis consistent with the deﬁnition of Hermitian adjoint used in matrix algebra.\nAn operator A is said to be Hermitian if it is equal to its Hermitian adjoint A†. If an operator is \nHermitian, then the bra 8c0 A is equal to the bra 8f0  that corresponds to the ket 0 f9 = A0 c9. That is, a \nHermitian operator can act to the right on a ket or to the left on a bra with the same result. In quantum \nmechanics, all operators that correspond to physical observables are Hermitian. This includes the spin \noperators we have already encountered as well as the energy, position, and momentum operators that \nwe will introduce in later chapters. The Hermiticity of physical observables is important in light of two \nfeatures of Hermitian matrices: (1) Hermitian matrices have real eigenvalues, which ensures that results \nof measurements are always real; and (2) the eigenvectors of a Hermitian matrix comprise a complete \nset of basis states, which ensures that we can use the eigenvectors of any observable as a valid basis.\n 2.2.3 \u0002 Projection Operators\nFor the spin-1/2 system, we now know four operators: Sx, Sy, Sz, and Sn. Let’s look for some other \noperators. Consider the ket 0 c9 written in terms of its coefﬁcients in the Sz basis\n \n 0 c9 = a0  +9 + b0  -9\n \n \n = 18+ 0 c92 0  +9 + 18- 0 c92 0  -9. \n \n(2.51)\nLooking for the moment only at the ﬁrst term, we can write it as a number times a ket, or as a ket times \na number:\n \n18+ 0 c92 0  +9 = 0  +918+ 0 c92 \n(2.52)\nwithout changing its meaning. Using the second form, we can separate the bra and ket that form the \ninner product and obtain\n \n0  +918+ 0 c92 = 10  +98+ 02 0 c9. \n(2.53)\n\n2.2 New Operators \n45\nThe new term in parentheses is a product of a ket and a bra but in the opposite order compared to the \ninner product deﬁned earlier. This new object must be an operator because it acts on the ket 0 c9 and\nproduces another ket: 18+ 0 c92 0  +9. This new type of operator is known as an outer product.\nReturning now to Eq. (2.51), we write 0 c9 using these new operators:\n \n 0 c9 = 8+ 0 c9 0  +9 + 8- 0 c9 0  -9  \n \n = 0  +98+ 0 c9 + 0  -98- 0 c9  \n \n = 10  +98+ 0 + 0  -98- 02 0 c9. \n \n(2.54)\nThe term in parentheses is a sum of two outer products and is clearly an operator because it acts on a \nket to produce another ket. In this special case, the result is the same as the original ket, so the operator \nmust be the identity operator 1. This relationship is often written as\n \n0  +98+ 0 + 0  -98- 0 = 1 \n(2.55)\nand is known as the completeness relation or closure. It expresses the fact that the basis states 0 {9 \ncomprise a complete set of states, meaning any arbitrary ket can be written in terms of them. To make \nit obvious that outer products are operators, it is useful to express Eq. (2.55) in matrix notation using \nthe standard rules of matrix multiplication:\n \n 0  +98+ 0 + 0  -98- 0 \u0003 a1\n0b11\n02 + a0\n1b10\n12 \n \n \u0003 a1\n0\n0\n0b + a0\n0\n0\n1b\n \n \n \n \u0003 a1\n0\n0\n1b.\n \n \n(2.56)\nEach outer product is represented by a matrix, as we expect for operators, and the sum of these two \nouter products is represented by the identity matrix, which we expected from Eq. (2.54).\nNow consider the individual operators 0  +98+ 0  and 0  -98- 0 . These operators are called projec-\ntion operators, and for spin 1/2 they are given by\n \n P+ = 0  +98+ 0 \u0003 a1\n0\n0\n0b  \n \n P- = 0  -98- 0 \u0003 a0\n0\n0\n1b. \n \n(2.57)\nIn terms of these new operators the completeness relation can also be written as\n \nP+ + P- = 1. \n(2.58)\nWhen a projection operator for a particular eigenstate acts on a state 0 c9, it produces a new ket that is \naligned along the eigenstate and has a magnitude equal to the amplitude (including the phase) for the \nstate 0 c9 to be in that eigenstate. For example,\n \nP+ 0 c9 = 0  +98+ 0 c9 = 18+ 0 c92 0  +9  \n \nP- 0 c9 = 0  -98- 0 c9 = 18- 0 c92 0  -9. \n \n(2.59)\n\n46 \nOperators and Measurement\nNote also that a projector acting on its corresponding eigenstate results in that eigenstate, and a projec-\ntor acting on an orthogonal state results in zero:\n \n P+ 0  +9 = 0  +98+ 0  +9 = 0  +9 \n \n P- 0  +9 = 0  -98- 0  +9 = 0.  \n \n(2.60)\nBecause the projection operator produces the probability amplitude, we expect that it must be inti-\nmately tied to measurement in quantum mechanics.\nWe found in Chapter 1 that the probability of a measurement is given by the square of the inner \nproduct of initial and ﬁnal states (postulate 4). Using the new projection operators, we rewrite the \nprobability as\n \n P+ = 08+ 0 c9 0\n2\n \n \n = 8+ 0 c9*8+ 0 c9 \n \n = 8c0  +98+ 0 c9  \n \n = 8c0 P+ 0 c9.\n \n \n(2.61)\nThus we say that the probability of the measurement Sz = U>2 can be calculated as a matrix element \nof the projection operator, using the input state 0 c9 and the projector P+ corresponding to the result.\nThe other important aspect of quantum measurement that we learned in Chapter 1 is that a mea-\nsurement disturbs the system. That is, if an input state 0 c9 is measured to have Sz = +U>2, then the \noutput state is no longer 0 c9 but is changed to 0  +9. We saw above that the projection operator does this \noperation for us, with a multiplicative constant of the probability amplitude. Thus, if we divide by this \namplitude, which is the square root of the probability, then we can describe the abrupt change of the \ninput state as\n \n0 c\u00049 =\nP+ 0 c9\n2\n 8c0 P+ 0 c9\n= 0  +9, \n(2.62)\nwhere 0 c\u00049 is the output state. This effect is described by the ﬁfth postulate, which is presented below \nand is often referred to as the projection postulate.\nPostulate 5\nAfter a measurement of A that yields the result an, the quantum system is in a \nnew state that is the normalized projection of the original system ket onto the \nket (or kets) corresponding to the result of the measurement:\n0 c\u00049 =\nPn0 c9\n2\n 8c0 Pn0 c9\n.\nThe projection postulate is at the heart of quantum measurement. This effect is often referred to as the \ncollapse (or reduction or projection) of the quantum state vector. The projection postulate clearly states \nthat quantum measurements cannot be made without disturbing the system (except in the case where the \ninput state is the same as the output state), in sharp contrast to classical measurements. The collapse of \n the quantum state makes quantum mechanics irreversible, again in contrast to classical mechanics.\n\n2.2 New Operators \n47\nWe can use the projection postulate to make a model of quantum measurement, as shown in the \nrevised depiction of a Stern-Gerlach measurement system in Fig. 2.6. The projection operators act on \nthe input state to produce output states with probabilities given by the squares of the amplitudes that \nthe projection operations yield. For example, the input state 0 cin9 is acted on the projection operator \nP+ = 0  +98+ 0 , producing an output ket 0 cout9 = 0  +918+ 0 cin92 with probability P+ = 08+ 0 cin9 0 2. \nThe output ket 0 cout9 = 0  +918+ 0 cin92 is really just a 0  +9 ket that is not properly normalized, so we \nnormalize it for use in any further calculations. We do not really know what is going on in the mea-\nsurement process, so we cannot explain the mechanism of the collapse of the quantum state vector. \nThis lack of understanding makes some people uncomfortable with this aspect of quantum mechan-\nics and has been the source of much controversy surrounding quantum mechanics. Trying to better \nunderstand the measurement process in quantum mechanics is an ongoing research problem. How-\never, despite our lack of understanding, the theory for predicting the results of experiments has been \nproven with very high accuracy.\n 2.2.4 \u0002 Analysis of Experiments 3 and 4\nWe can now return to Experiments 3 and 4 from Chapter 1 and analyze them with these new tools. \nRecall that Experiment 3 is the same as Experiment 4a, and Experiments 4a and 4b are similar in that \nthey each use only one of the output ports of the second Stern-Gerlach analyzer as input to the third \nanalyzer. Figure 2.7 depicts these experiments again, with Fig. 2.7(a) showing a hybrid experiment \nthat is essentially Experiment 4a in its upper half and Experiment 4b in its lower half, and Fig. 2.7(b) \nshowing Experiment 4c. In this problem, we discuss the probability that an atom leaving the ﬁrst \nanalyzer in the 0  +9 state is detected in one of the counters connected to the output ports of the third \nanalyzer. Such a probability involves two measurements at the second and third analyzers. The total \nprobability is the product of the individual probabilities of each measurement.\nFor the hybrid experiment shown in Fig. 2.7(a), the probability of measuring an atom at the top-\nmost counter is the probability of measuring Sx = +U>2 at the second analyzer, 0 x8+ 0  +9 0\n2, times the \nprobability of measuring Sz = +U>2 at the third analyzer, 08+ 0  +9x0\n2, giving\n \nPupper, + = 08+ 0  +9x0\n2\n 0 x8+ 0  +9 0\n2. \n(2.63)\nLikewise the probability of measuring the atom to have Sx = +U>2 and then Sz = -U>2 is\n \nPupper, - = 08- 0  +9x0\n2\n 0 x8+ 0  +9 0\n2, \n(2.64)\nZ\n\u0002\u0002\u0003 \u0004\u0002\u0002\n\u0002\u0003\u0003 \u0004\u0003\u0002\n\u0002\u0002\u0003\n\u0002\u0003\u0003\nProject\nNormalize\n\u0002Ψ\u0003\n\u0002\u0002\u0003\u0004\u0002\u0002Ψ\u0003\n\u0002\u0003\u0003\u0004\u0003\u0002Ψ\u0003\nFIGURE 2.6 Schematic diagram of the role of the projection operator in a Stern-Gerlach spin measurement.\n\n48 \nOperators and Measurement\nwhere we have written the product so as to be read from right to left as is the usual practice with \nquantum mechanical amplitudes and probabilities. For atoms that take the lower path from the second \nanalyzer, the ﬁnal probabilities are\n \nPlower, + = 08+ 0  -9x0\n2\n 0 x8- 0  +9 0\n2  \n \nPlower, - = 08- 0  -9x0\n2\n 0 x8- 0  +9 0\n2. \n \n(2.65)\nFor Experiment 4c, shown in Fig. 2.7(b), we have a new situation at the second analyzer. Both \noutput ports are connected to the third analyzer, which means that the probability of an atom from \nthe ﬁrst analyzer being input to the third analyzer is 100%. So we need only calculate the probability \nof passage through the third analyzer. The crucial step is determining the input state, for which we \nuse the projection postulate. Because both states are used, the relevant projection operator is the sum \nof the two projection operators for each port, P+x + P-x, where P+x = 0  +9x x8+ 0  and P-x = 0  -9x x8- 0 .\nThus the state after the second analyzer is\n \n 0 c29 =\n1P+x + P-x2 0 c19\n2\n 8c101P+x + P-x2 0 c19\n \n \n =\n1P+x + P-x2 0  +9\n2\n 8+ 01P+x + P-x2 0  +9\n . \n \n(2.66)\n100\n0\nX\nZ\nZ\n100\n100\n25\n25\nZ\nX\nZ\n100\n(a)\n(b)\n50\n25\n25\nZ\n50\nFIGURE 2.7 (a) Hybrid Experiment 4a and 4b, and (b) Experiment 4c.\n\n2.2 New Operators \n49\nIn this simple example, the projector P+x + P-x is equal to the identity operator because the two states \nform a complete basis. This clearly simpliﬁes the calculation, giving 0 c29 = 0  +9, but to illustrate our \npoint, let’s simplify only the denominator (which equals one), giving\n \n 0 c29 = 10  +9x x8+ 0 + 0  -9x x8- 02 0  +9  \n \n = 0  +9x x8+ 0  +9 + 0  -9x x8- 0  +9. \n \n(2.67)\nThus the beam entering the third analyzer can be viewed as a coherent superposition of the eigenstates \nof the second analyzer. Now calculate the probability of measuring spin up at the third analyzer:\n \n P+ = 08+ 0 c29 0\n2\n \n \n = 08+ 0  +9x x8+ 0  +9 + 8+ 0  -9x x8- 0  +9 0\n2. \n \n(2.68)\nThe probability of measuring spin down at the third analyzer is similarly\n \nP - = 08- 0 c29 0\n2\n \n \n = 08- 0  +9x x8+ 0  +9 + 8- 0  -9x x8- 0  +9 0\n2. \n \n(2.69)\nIn each case, the probability is a square of a sum of amplitudes, each amplitude being the amplitude \nfor a successive pair of measurements. For example, in P- the amplitude 8- 0  +9x x8+ 0  +9 refers to the \nupper path that the initial 0  +9 state takes as it is ﬁrst measured to be in the 0  +9x state and then mea-\nsured to be in the 0  -9 state (read from right to left). This amplitude is added to the amplitude for the \nlower path because the beams of the second analyzer are combined, in the proper fashion, to create the \ninput beam to the third analyzer. When the sum of amplitudes is squared, four terms are obtained, two \nsquares and two cross terms, giving\n \n P- = 08- 0  +9x x8+ 0  +9 0\n2 + 08- 0  -9x x8- 0  +9 0\n2 \n \n +8- 0  +9*\nx x8+ 0  +9*8- 0  -9x x8- 0  +9\n \n \n +8- 0  +9x x8+ 0  +98- 0  -9*\nx x8- 0  +9*\n \n \n = Pupper, - + Plower, - + interference terms. \n \n(2.70)\nThis tells us that the probability of detecting an atom to have spin down when both paths are used is the \nsum of the probabilities for detecting a spin down atom when either the upper path or the lower path is \nused alone plus additional cross terms involving both amplitudes, which are commonly called interference \nterms. It is these additional terms, which are not complex squares and so could be positive or negative, that \nallow the total probability to become zero in this case, illustrating the phenomenon of interference.\nThis interference arises from the nature of the superposition of states that enters the third analyzer. \nTo illustrate, consider what happens if we change the superposition state to a mixed state, as we dis-\ncussed previously in Section 1.2.3. Recall that a superposition state implies a beam with each atom in \nthe same state, which is a combination of states, while a mixed state implies that the beam consists of \natoms in separate states. As we have described it so far, Experiment 4c involves a superposition state \nas the input to the third analyzer. We can change this to a mixed state by “watching” to see which of \nthe two output ports of the second analyzer each atom travels through. There are a variety of ways to \nimagine doing this experimentally. The usual idea proposed is to illuminate the paths with light and \nwatch for the scattered light from the atoms. With proper design of the optics, the light can be localized \n\n50 \nOperators and Measurement\nsufﬁciently to determine which path the atom takes. Hence, such experiments are generally referred to \nas “Which Path” or “Welcher Weg” experiments. Such experiments can be performed in the SPINS \nprogram by selecting the “Watch” feature. Once we know which path the atom takes, the state is not \nthe superposition 0 c29 described above, but is either 0  +9x or 0  -9x, depending on which path produces \nthe light signal. To ﬁnd the probability that atoms are detected at the spin down counter of the third \nanalyzer, we add the probabilities for atoms to follow the path 0  +9 S 0  +9x S 0  -9 to the probability \nfor other atoms to follow the path 0  +9 S 0  -9x S 0  -9 because these are independent events, giving\n \n Pwatch,  - = 08- 0  +9x x8+ 0  +9 0\n2 + 08- 0  -9x x8- 0  +9 0\n2 \n \n = Pupper, - + Plower, - ,\n \n \n(2.71)\nin which no interference terms are present.\nThis interference example illustrates again the important distinction between a coherent superpo-\nsition state and a statistical mixed state. In a coherent superposition, there is a deﬁnite relative phase \nbetween the different states, which gives rise to interference effects that are dependent on that phase. In a \nstatistical mixed state, the phase relationship between the states has been destroyed and the interference \nis washed out. Now we can understand what it takes to have the beams “properly” combined after the \nsecond analyzer of Experiment 4c. The relative phases of the two paths must be preserved. Anything that \nrandomizes the phase is equivalent to destroying the superposition and leaving only a statistical mixture. \nIf the beams are properly combined to leave the superposition intact, the results of Experiment 4c are \nthe same as if no measurement were made at the second analyzer. So even though we have used a mea-\nsuring device in the middle of Experiment 4c, we generally say that no measurement was made there. \nWe can summarize our conclusions by saying that if no measurement is made on the intermediate state, \nthen we add amplitudes and then square to ﬁnd the probability, while if an intermediate measurement is \n performed (i.e., watching), then we square the amplitudes ﬁrst and then add to ﬁnd the probability. One \nis the square of a sum and the other is the sum of squares, and only the former exhibits interference.\n 2.3 \u0002 MEASUREMENT\nLet’s discuss how the probabilistic nature of quantum mechanics affects the way experiments are \nperformed and compared with theory. In classical physics, a theoretical prediction can be reliably \ncompared to a single experimental result. For example, a prediction of the range of a projectile can be \ntested by doing an experiment. The experiment may be repeated several times in order to understand \nand possibly reduce any systematic errors (e.g., wind) and measurement errors (e.g., misreading the \ntape measure). In quantum mechanics, a single measurement is meaningless. If we measure an atom to \nhave spin up in a Stern-Gerlach analyzer, we cannot discern whether the original state was 0  +9 or 0  -9x \nor any arbitrary state 0 c9 1except 0  -92. Moreover, we cannot repeat the measurement on the same \natom, because the original measurement changed the state, per the projection postulate.\nThus, one must, by necessity, perform identical measurements on identically prepared systems. \nIn the spin-1/2 example, an initial Stern-Gerlach analyzer is used to prepare atoms in a particular state \n0 c9. Then a second Stern-Gerlach analyzer is used to perform the same experiment on each identically \n prepared atom. Consider performing a measurement of Sz on N identically prepared atoms. Let N+ be the \nnumber of times the result +U>2 is recorded and N− be the number of times the result -U>2 is recorded. \nBecause there are only two possible results for each measurement, we must have N = N+ + N-. The \nprobability postulate (postulate 4) predicts that the probability of measuring +U>2 is\n \nP+ = 08+ 0 c9 0\n2. \n(2.72)\n\n2.3 Measurement \n51\nFor a ﬁnite number N of atoms, we expect that N+ is only approximately equal to P+ N due to the statis-\ntical ﬂuctuations inherent in a random process. Only in the limit of an inﬁnite number N do we expect \nexact agreement:\n \nlim\nNS \u0005\nN+\nN = P+ = 08+ 0 c9 0\n2. \n(2.73)\nIt is useful to characterize a data set in terms of the mean and standard deviation (see Appendix \nA for further information on probability). The mean value of a data set is the average of all the mea-\nsurements. The expected or predicted mean value of a measurement is the sum of the products of each \npossible result and its probability, which for this spin-1/2 measurement is\n \n8Sz9 = a+  U\n2b P+ + a-  U\n2b P-   , \n(2.74)\nwhere the angle brackets signify average or mean value. Using the rules of quantum mechanics we \nrewrite this mean value as\n \n 8Sz9 = +  U\n2\n 08+ 0 c9 0\n2 + a-  U\n2b 08- 0 c9 0\n2\n \n \n = +  U\n2\n 8c0  +98+ 0 c9 + a-  U\n2b8c0  -98- 0 c9 \n \n = 8c0 J\n +  U\n2 0  +98+ 0 c9 + a-  U\n2b 0  -98- 0 c9\n R \n \n = 8c03Sz0  +98+ 0 c9 + Sz0  -98- 0 c94\n \n \n = 8c0 Sz30  +98+ 0 + 0  -98- 04 0 c9.\n \n \n(2.75)\nAccording to the completeness relation, the term in square brackets in the last line is unity, so \nwe obtain\n \n8Sz9 = 8c0 Sz0 c9  . \n(2.76)\nWe now have two ways to calculate the predicted mean value, Eq. (2.74) and Eq. (2.76). Which you \nuse generally depends on what quantities you have readily available. The matrix element version in \nEq. (2.76) is more common and is especially useful in systems that are more complicated than the \n2-level spin-1/2 system. This predicted mean value is commonly called the expectation value, but \nit is not the expected value of any single experiment. Rather it is the expected mean value of a large \nnumber of experiments. It is not a time average, but an average over many identical experiments. For a \ngeneral quantum mechanical observable, the expectation value is\n \n8A9 = 8c0 A0 c9 = a\nn\nanPan   , \n(2.77)\nwhere an are the eigenvalues of the operator A.\nTo see how the concept of expectation values applies to our study of spin-1/2 systems, consider \ntwo examples. First consider a system prepared in the state 0  +9. The expectation value of Sz is\n \n8Sz9 = 8+ 0 Sz0  +9, \n(2.78)\n\n52 \nOperators and Measurement\nwhich we calculate with bra-ket notation\n \n 8Sz9 = 8+ 0 Sz0  +9 \n \n = 8+ 0 U\n2 0  +9  \n \n = U\n2\n 8+ 0  +9  \n \n = U\n2.\n \n \n(2.79)\nThis result should seem obvious because +U>2 is the only possible result of a measurement of Sz for \nthe 0  +9 state, so it must be the expectation value.\nNext consider a system prepared in the state 0  +9x. In this case, the expectation value of Sz is\n \n8Sz9 = x8+ 0 Sz0 +9x. \n(2.80)\nUsing matrix notation, we obtain\n \n 8Sz9 =\n1\n12  11\n12 U\n2\n a1\n0\n0\n-1b 1\n12 a1\n1b \n \n = U\n4\n  11\n12a 1\n-1b = 0 U.\n \n \n(2.81)\nAgain this is what you expect, because the two possible measurement results {U>2 each have 50% \nprobability, so the average value is zero. Note that the value of zero is never measured, so it is not the \nvalue “expected” for any given measurement, but rather the expected mean value of an ensemble of \nmeasurements.\nIn addition to the mean value, it is common to characterize a measurement by the standard devia-\ntion, which quantiﬁes the spread of measurements about the mean or expectation value. The standard \ndeviation is deﬁned as the square root of the mean of the square of the deviations from the mean, and \nfor an observable A is given by\n \n\u0006A = 481A - 8A9229, \n(2.82)\nwhere the angle brackets signify average value as used in the deﬁnition of an expectation value. This \nresult is also often called the root-mean-square deviation, or r.m.s. deviation. We need to square the \ndeviations, because the deviations from the mean are equally distributed above and below the mean in \nsuch a way that the average of the deviations themselves is zero. This expression can be simpliﬁed by \nexpanding the square and performing the averages, resulting in\n \n \u0006A = 481A2 - 2A8A9 + 8A9229 \n \n = 48A29 - 28A98A9 + 8A92 \n \n = 48A29 - 8A92,\n \n \n(2.83)\n\n2.3 Measurement \n53\nwhere one must be clear to distinguish between the square of the mean 8A9\n2 and the mean of the \nsquare 8A29. While the mean of the square of an observable may not be a common experimental quan-\ntity, it can be calculated using the deﬁnition of the expectation value\n \n8A29 = 8c0 A20 c9. \n(2.84)\nThe square of an operator means that the operator acts twice in succession:\n \nA20 c9 = AA0 c9 = A1A0 c92. \n(2.85)\nTo gain experience with the standard deviation, return to the two examples used above. To calcu-\nlate the standard deviation, we need to ﬁnd the mean of the square of the operator S z. In the ﬁrst case \nA 0  +9 initial stateB, we get\n \n 8S\n 2\nz 9 = 8+ @S\n 2\nz @  +9 = 8+ @Sz Sz@  +9 = 8+ @Sz U\n2\n @  +9\n \n = 8+ 0 a U\n2b\n2\n0  +9\n \n = a U\n2b\n2\n.\n \n(2.86)\nWe already have the mean of the operator Sz in Eq. (2.79) so the standard deviation is\n \n \u0006Sz = 48S\n 2\nz 9 - 8Sz9\n2  \n \n = Ca U\n2b\n2\n- a U\n2b\n2\n \n \n = 0 U,\n \n \n(2.87)\nwhich is to be expected because there is only one possible result, and hence no spread in the results of \nthe measurement, as shown in the histogram in Fig. 2.8(a).\n(a)\n1\nP\nP−\nP+\nSz\n\u0003\u0002\n2\n\u0002\n2\n1\nP\nP−\nP+\nSz\n\u0003\u0002\n2\n\u0002\n2\n\u000bSz \u0006 \u0002\n2\n\u000bSz \u0006\u00050\n\u0004Sz\u0003 \u0006\u0005\n\u0004Sz\u0003 \u0006\u00050\u0005\n\u0002\n2\n\u0002Ψin\u0003\u0005\u0006\u0005\u0002\u0002\u0003\n\u0002Ψin\u0003\u0005\u0006\u0005\u0002\u0002\u0003x\nFIGURE 2.8 Idealized measurements of Sz with (a) a 0  +9 input state and (b) with a 0  +9x input state.\n\n54 \nOperators and Measurement\nIn the second case 1 0  +9x initial state2, the mean of the square of the operator Sz is\n \n 8S\n 2\nz 9 = x8+ @ S\n 2\nz @  +9x\n \n \n =\n1\n12\n 11\n12  U\n2\n a1\n0\n0\n-1b U\n2\n a1\n0\n0\n-1b 1\n12\n a1\n1b \n \n = 1\n2\n a U\n2b\n2\n11\n12a1\n0\n0\n-1ba 1\n-1b\n \n \n = 1\n2\n a U\n2b\n2\n11\n12a1\n1b\n \n \n = a U\n2b\n2\n.\n \n \n(2.88)\nThe mean of the operator Sz is in Eq. (2.81), giving a standard deviation of \n \n \u0006Sz = 48S\n 2\nz 9 - 8Sz9\n2 \n \n = Ca U\n2b\n2\n- 0 U2 \n(2.89)\n \n = U\n2.\nAgain this makes sense because each measurement deviates from the mean (0 U) by the same value of \nU>2, as shown in the histogram in Fig. 2.8(b).\nThe standard deviation \u0006A represents the uncertainty in the results of an experiment. In quan-\ntum mechanics, this uncertainty is inherent and fundamental, meaning that you cannot design the \nexperiment any better to improve the result. What we have calculated then is the minimum uncertainty \nallowed by quantum mechanics. Any actual uncertainty may be larger due to experimental error. \nThis is another ramiﬁcation of the probabilistic nature of quantum mechanics and will lead us to the \nHeisenberg uncertainty relation in Section 2.5.\n 2.4 \u0002 COMMUTING OBSERVABLES\nWe found in Experiment 3 that two incompatible observables could not be known or measured simul-\ntaneously, because measurement of one somehow erased knowledge of the other. Let us now explore \nfurther what it means for two observables to be incompatible and how incompatibility affects the results \nof measurements. First we need to deﬁne a new object called a commutator. The commutator of two \noperators is deﬁned as the difference between the products of the two operators taken in alternate orders:\n \n3A, B4 = AB - BA. \n(2.90)\nIf the commutator is equal to zero, we say that the operators or observables commute; if it is not zero, we \nsay they don’t commute. Whether or not two operators commute has important ramiﬁcations in analyzing \na quantum system and in making measurements of the two observables represented by those operators.\n\n2.4 Commuting Observables \n55\nConsider what happens when two operators A and B do commute:\n \n 3A, B4 = 0\n \n \n AB - BA = 0\n \n \n AB = BA. \n(2.91)\nThus, for commuting operators the order of operation does not matter, whereas it does for noncom-\nmuting operators. Now let 0 a9 be an eigenstate of the operator A with eigenvalue a:\n \nA0 a9 = a0 a9. \n(2.92)\nOperate on both sides of this equation with the operator B and use the fact that A and B commute:\n \n BA0 a9 = Ba0 a9\n \n \n AB0 a9 = aB0 a9\n \n \n A1B0 a92 = a1B0 a92. \n(2.93)\nThe last equation says that the state B0 a9 is also an eigenstate of the operator A with the same eigen-\nvalue a. Assuming that each eigenvalue has a unique eigenstate (which is true if there is no degen-\neracy, but we haven’t discussed degeneracy yet), the state B0 a9 must be some scalar multiple of the \nstate 0 a9. If we call this multiple b, then we can write\n \nB0 a9 = b0 a9, \n(2.94)\nwhich is just an eigenvalue equation for the operator B. Thus, we must conclude that the state 0 a9 is \nalso an eigenstate of the operator B, with the eigenvalue b. The assumption that the operators A and B \ncommute has led us to the result that A and B have common or simultaneous sets of eigenstates. This \nresult bears repeating:\nCommuting operators share common eigenstates.\nThe ramiﬁcations of this result for experiments are very important. Recall that a measurement of \nthe observable A projects the initial state 0 c9 onto an eigenstate of A: 0 a9. A subsequent measurement \nof the observable B then projects the input state 0 a9 onto an eigenstate of B. But the eigenstates of \nthe commuting operators A and B are the same, so the second measurement does not change the state \n0 a9. Thus, another measurement of A following the measurement of B yields the same result as the \n initial measurement of A, as illustrated in Fig. 2.9. Thus we say that we can know the eigenvalues of \nthese two observables simultaneously. It is common to extend this language and say that these two \nobservables can be measured simultaneously, although, as illustrated in Fig. 2.9, we do not really measure \nthem simultaneously. What we mean is that we can measure one observable without erasing our knowl-\nedge of the previous results of the other observable. Observables A and B are said to be compatible.\n100\n0\n0\nA\nB\nA\na1\na1\na1\na2\na3\na1\na2\na3\na1\na2\na3\nb1\nb2\nb3\nFIGURE 2.9 Successive measurements of commuting observables.\n\n56 \nOperators and Measurement\nConversely, if two operators do not commute, then they are incompatible observables and cannot \nbe measured or known simultaneously. This is what we saw in Experiment 3 in Chapter 1. In that case, the \ntwo observables were Sx and Sz. Let’s take a look at their commutator to show that they are not compatible:\n \n3Sz, Sx4 \u0003 U\n2\n a1\n0\n0\n-1b U\n2\n a0\n1\n1\n0b - U\n2\n a0\n1\n1\n0b U\n2\n a1\n0\n0\n-1b \n \n\u0003 a U\n2b\n2\n c a 0\n1\n-1\n0b - a0\n-1\n1\n0 bd\n \n \n\u0003 a U\n2b\n2\na 0\n2\n-2\n0b\n \n \n= i USy. \n(2.95)\nAs expected, these two operators do not commute. In fact, none of the spin component operators com-\nmute with each other. The complete commutation relations are\n \n3Sx, Sy4 = i USz\n \n \n3Sy, Sz4 = i USx \n \n3Sz, Sx4 = i USy   , \n(2.96)\nso written to make the cyclic relations clear.\nWhen we represent operators as matrices, we can often decide whether two operators commute \nby inspection of the matrices. Recall the important statement: An operator is always diagonal in its \nown basis. If you are presented with two matrices that are both diagonal, they must share a common \nbasis, and so they commute with each other. To be explicit, the product of two diagonal matrices\n \nAB \u0003 §\na1\n0\n0\ng\n0\na2\n0\ng\n0\n0\na3\ng\nf\nf\nf\nf\n¥  §\nb1\n0\n0\ng\n0\nb2\n0\ng\n0\n0\nb3\ng\nf\nf\nf\nf\n¥ \n \n\u0003 §\na1b1\n0\n0\ng\n0\na2b2\n0\ng\n0\n0\na3b3\ng\nf\nf\nf\nf\n¥ , \n(2.97)\nis clearly independent of the order of the product. Note, however, that you may not conclude that two \noperators do not commute if one is diagonal and one is not, nor if both are not diagonal.\n 2.5 \u0002 UNCERTAINTY PRINCIPLE\nThe intimate connection between the commutator of two observables and the possible precision of \nmeasurements of the two corresponding observables is reﬂected in an important relation that we sim-\nply state here (see more advanced texts for a derivation). The product of the uncertainties or standard \ndeviations of two observables is related to the commutator of the two observables:\n \n\u0006A\u0006B Ú 1\n2 083A, B49 0   . \n(2.98)\n\n2.6 S2 Operator \n57\nThis is the uncertainty principle of quantum mechanics. Consider what it says about a simple Stern-\nGerlach experiment. The uncertainty principle for the Sx and Sy spin components is\n \n \u0006Sx\u0006Sy Ú 1\n2 083Sx, Sy49 0  \n \n Ú 1\n2 08i USz9 0\n \n \n Ú U\n2 08Sz9 0 .\n \n(2.99)\nThese uncertainties are the minimal quantum mechanical uncertainties that would arise in any experi-\nment. Any experimental uncertainties due to experimenter error, apparatus errors, and statistical limi-\ntations would be additional.\nLet’s now apply the uncertainty principle to Experiment 3 where we ﬁrst learned of the impact of \nmeasurements in quantum mechanics. If the initial state is 0  +9, then a measurement of Sz results in an \nexpectation value 8Sz9 = U>2 with an uncertainty \u0006Sz = 0, as illustrated in Fig. 2.8(a). Thus the uncer-\ntainty principle dictates that the product of the other uncertainties for measurements of the 0  +9 state is\n \n\u0006Sx\u0006Sy Ú a U\n2b\n2\n, \n(2.100)\nor simply\n \n\u0006Sx\u0006Sy \u0002 0. \n(2.101)\nThis implies that\n \n  \u0006Sx \u0002 0  \n \n \u0006Sy \u0002 0. \n(2.102)\nThe conclusion to draw from this is that while we can know one spin component absolutely (\u0006Sz = 0), \nwe can never know all three, nor even two, simultaneously. This is in agreement with our results from \nExperiment 3. This lack of ability to measure all spin components simultaneously implies that the spin \ndoes not really point in a given direction, as a classical spin or angular momentum does. So when we \nsay that we have measured “spin up,” we really mean only that the spin component along that axis is up, \nas opposed to down, and not that the complete spin angular momentum vector points up along that axis.\n 2.6 \u0002 S2 OPERATOR\nAnother indication that the spin does not point along the axis along which you measure the spin com-\nponent is obtained by considering a new operator that represents the magnitude of the spin vector but \nhas no information about the direction. It is common to use the square of the spin vector for this task. \nThis new operator is\n \nS2 = S2\nx + S2\ny + S2\nz , \n(2.103)\nand it is calculated in the Sz representation as\n \nS2 \u0003 a U\n2b\n2\n c a0\n1\n1\n0b a0\n1\n1\n0b + a0\n-i\ni\n0\n ba0\n-i\ni\n0\n b + a1\n0\n0\n-1ba1\n0\n0\n-1bd  \n \n\u0003 a U\n2b\n2\n c a1\n0\n0\n1b + a1\n0\n0\n1b + a1\n0\n0\n1bd  \n(2.104)\n \n\u0003 3\n4 U2 a1\n0\n0\n1b. \n\n58 \nOperators and Measurement\nThus the S2 operator is proportional to the identity operator, which means it must commute with all \nthe other operators Sx, Sy, and Sz. It also means that all states are eigenstates of the S2 operator. Thus, \nwe can write \n \nS20 c9 = 3\n4 U20 c9 \n(2.105)\nfor any state 0 c9 in the spin-1/2 system.\nFor the case of spin 1/2, note that the expectation value of the operator S2 is\n \n8S29 = 3\n4 U2, \n(2.106)\nwhich would imply that the “length” of the spin vector is\n \n0 S0 = 48S29 = 23 U\n2. \n(2.107)\nThis is appreciably longer than the measured component of U>2, implying that the spin vector can \nnever be fully aligned along any axis. A useful mental model of the spin vector and its component is \nshown in Fig. 2.10. In this vector model, one can imagine the total spin vector S precessing around the \nz-axis at a constant angle to form a cone, with a constant spin component Sz. For a spin-1/2 system in \nthe “spin up” state 0  +9, this classical model yields the same expectation values and uncertainties as the \nquantum model (Problem 2.9)\n \n 8Sz9 = U\n2   \u0006Sz = 0  \n \n 8Sx9 = 0   \u0006Sx \u0002 0  \n \n 8Sy9 = 0   \u0006Sy \u0002 0. \n(2.108)\nS\nz\ny\nx\n(a)\nS\n(b)\nz\n\u0002\n2\n√3\u0002\n2\n\u0002\u0002\n2\n\u0002\n2\n√3\u0002\n2\n\u0002\u0002\n2\nFIGURE 2.10 (a) Vector model illustrating the classical precision of a spin vector and the allowed \nquantum mechanical components. (b) Two-dimensional version of the vector model with constant spin \nvector length and two possible components.\n\n2.7 Spin-1 System \n59\n?\n?\n?\n0\n1\n0\n1\nZ\nFIGURE 2.11 Spin-1 Stern-Gerlach experiment.\nHowever, a quantum mechanical experiment on a spin component eigenstate does not yield the time \ndependence of the precession implied by the picture in Fig. 2.10(a). Rather, the quantum mechanical \nspin vector is more accurately thought of as smeared out over the whole cone in a uniform random sense. \nThis randomness is often termed quantum fuzziness and will be evident in other systems we will study \nlater. To avoid the inaccurate precession part of the vector model, it is often illustrated as in Fig. 2.10(b).\n 2.7 \u0002 SPIN-1 SYSTEM\nThe Stern-Gerlach experiment depicted in Fig. 1.1 can be performed on a variety of atoms or par-\nticles. Such experiments always result in a ﬁnite number of discrete beams exiting the analyzer. For \nspin-1/2 particles, there are two output beams. For the case of three output beams, the deﬂections are \nconsistent with magnetic moments arising from spin angular momentum components of 1U, 0 U, and \n-1U. For an analyzer aligned along the z-axis, the three output states are labeled 0 19, 0 09, and 0  -19, \nas shown in Fig. 2.11. This is what we call a spin-1 system. (Note that the SPINS software and our \nStern-Gerlach schematics use arrows for the 0 19 and 0  -19 output beams, but these outputs are not the \nsame as the spin-1/2 states that are also denoted with arrows.)\nThe three eigenvalue equations for the spin component operator Sz of a spin-1 system are\n \n Sz0 19 = U0 19\n \n \n Sz0 09 = 0 U0 09\n \n \n Sz0  -19 = -U0  -19. \n(2.109)\nAs we did in the spin-1/2 case, we choose the Sz basis as the standard basis in which to express kets \nand operators using matrix representation. In Section 2.1, we found that eigenvectors are unit vectors \nin their own basis and an operator is always diagonal in its own basis. Using the ﬁrst rule, we can \nimmediately write down the eigenvectors of the Sz operator:\n \n0 19 \u0003 °\n1\n0\n0\n¢  0 09 \u0003 °\n0\n1\n0\n¢  0  -19 \u0003 °\n0\n0\n1\n¢ , \n(2.110)\nwhere we again use the convention that the ordering of the rows follows the eigenvalues in descending \norder. Using the second rule, we write down the Sz operator\n \nSz \u0003 °\n1U\n0\n0\n0\n0 U\n0\n0\n0\n-1U\n¢ = U °\n1\n0\n0\n0\n0\n0\n0\n0\n-1\n¢ \n(2.111)\nwith the eigenvalues 1U, 0 U, and -1U ordered along the diagonal. The value zero is a perfectly valid \neigenvalue in some systems.\n\n60 \nOperators and Measurement\nThe same four experiments performed on the spin-1/2 system can be performed on a spin-1 sys-\ntem. Conceptually the results are the same. One important difference occurs in Experiment 2, where a \nmeasurement of Sz is ﬁrst performed to prepare a particular state, and then a subsequent measurement \nof Sx (or Sy) is performed. Based upon the results of the spin-1/2 experiment, one might expect each of \nthe possible components to have one-third probability. Such is not the case. Rather, one set of results is\n \n P1x = 0 x810 19 0\n2 = 1\n4\n \n \n P0x = 0 x800 19 0\n2 = 1\n2\n \n \n P-1x = 0 x8-10 19 0\n2 = 1\n4, \n(2.112)\nas illustrated in Fig. 2.12. These experimental results can be used to determine the Sx eigenstates in \nterms of the Sz basis\n \n 0 19x = 1\n2@ 19 +\n1\n12@ 09 + 1\n2 @ -19\n \n \n 0 09x =\n1\n12@ 19 -\n1\n12@  -19\n \n \n 0  -19x = 1\n2@ 19 -\n1\n12@ 09 + 1\n2@  -19. \n(2.113)\nLikewise, we can ﬁnd the Sy eigenstates:\n \n 0 19y = 1\n2@ 19 + i 1\n12@ 09 - 1\n2@  -19\n \n \n 0 09y =\n1\n12@ 19 +\n1\n12@  -19\n \n \n 0  -19y = 1\n2@ 19 - i 1\n12@ 09 - 1\n2@  -19. \n(2.114)\nThe matrix representations of the Sx and Sy operators are\n \nSx \u0003\nU\n12\n °\n0\n1\n0\n1\n0\n1\n0\n1\n0\n¢  Sy \u0003\nU\n12\n °\n0\n-i\n0\ni\n0\n-i\n0\ni\n0\n¢ . \n(2.115)\n25\n25\nX\n0\n50\nZ\n0\n1\n1 x\n0 x\n1 x\nFIGURE 2.12 Experiment 2 in the spin-1 case.\n\n2.7 Spin-1 System \n61\n0\nP\nP−1\nP1\nP0\n1\nSz\nΨin\n(2 1\ni 0\ni\n1 )\n√6\n1\nFIGURE 2.13 Histogram of measurements of z-component of spin for spin-1 particle.\nExample 2.3 A spin-1 system is prepared in the state\n \n0 cin9 =\n2\n16 0 19 -\ni\n16 0 09 +\ni\n16 0  -19. \n(2.116)\nFind the probabilities of measuring each of the possible spin components along the z-axis.\nThe probability of measuring Sz = +1U is\n \n P1 = 0810 cin90\n2\n \n \n = @810C 2\n16 0 19 -\ni\n16 0 09 +\ni\n16 0  -19D@\n2\n \n \n = @ 2\n16 810 19 -\ni\n16810 09 +\ni\n16 810  -19@\n2\n \n \n = @ 2\n16\n @\n2\n= 2\n3.\n \n(2.117)\nThe probability of measuring Sz = 0 U is\n \n P0 = 0800 cin90\n2\n \n \n = @800C 2\n16 0 19 -\ni\n16 0 09 +\ni\n16 0  -19D@\n2\n \n \n = @ -i\n16\n @\n2\n= 1\n6.\n \n(2.118)\nThe probability of measuring Sz = -1U is\n \n P-1 = 08-10 cin90\n2\n \n \n = @8-10C 2\n16 0 19 -\ni\n16 0 09 +\ni\n16 0  -19D@\n2\n \n \n = @ i\n16\n @\n2\n= 1\n6.\n \n(2.119)\nThe three probabilities add to unity, as they must. A histogram of the predicted measurement results \nis shown in Fig. 2.13.\n\n62 \nOperators and Measurement\nTo generalize to other possible spin systems, we need to introduce new labels. We use the label \ns to denote the spin of the system, such as spin 1/2, spin 1, spin 3/2. The number of beams exiting a \nStern-Gerlach analyzer is 2s + 1. In each of these cases, a measurement of a spin component along \nany axis yields results ranging from a maximum value of s U to a minimum value of -s U, in unit steps \nof the value U. We denote the possible values of the spin component along the z-axis by the label m, \nthe integer or half-integer multiplying U. A quantum state with speciﬁc values of s and m is denoted as \n0 sm9, yielding the eigenvalue equations\n \n S20 sm9 = s(s + 1) U20 sm9 \n \n Sz0 sm9 = m U0 sm9.\n \n \n(2.120)\nThe label s is referred to as the spin angular momentum quantum number or the spin quantum \nnumber for short. The label m is referred to as the spin component quantum number or the mag-\nnetic quantum number because of its role in magnetic ﬁeld experiments like the Stern-Gerlach \nexperiment. The connection between this new 0 sm9 notation and the spin-1/2 0{9 notation is\n \n @ 1\n2 1\n29 = 0  +9  \n \n @ 1\n2, -1\n29 = 0  -9. \n(2.121)\nFor the spin-1 case, the connection to this new notation is\n \n 0 119 = 0 19\n \n \n 0 109 = 0 09\n \n \n 0 1, -19 = 0  -19. \n(2.122)\nWe will continue to use the 0{9 notation, but we will ﬁnd the new notation useful later (Chapter 7).\n 2.8 \u0002 GENERAL QUANTUM SYSTEMS\nLet’s extend the important results of this chapter to general quantum mechanical systems. For a gen-\neral observable A with quantized measurement results an, the eigenvalue equation is\n \nA0 an9 = an0 an9. \n(2.123)\nIn the basis formed by the eigenstates 0 an9, the operator A is represented by a matrix with the eigen-\nvalues along the diagonal\n \nA \u0003 §\na1\n0\n0\ng\n0\na2\n0\ng\n0\n0\na3\ng\nf\nf\nf\nf\n¥ , \n(2.124)\nwhose size depends on the dimensionality of the system. In this same basis, the eigenstates are repre-\nsented by the column vectors\n \n0 a19 \u0003 §\n1\n0\n0\nf\n¥ , 0 a29 \u0003 §\n0\n1\n0\nf\n¥ , 0 a39 \u0003 §\n0\n0\n1\nf\n¥ , ... . \n(2.125)\n\nSummary \n63\nThe projection operators corresponding to measurement of the eigenvalues an are \n \nPan = 0 an98an0 . \n(2.126)\nThe completeness of the basis states is expressed by saying that the sum of the projection operators is \nthe identity operator\n \na\nn\nPan = a\nn\n0 an98an0 = 1. \n(2.127)\nSUMMARY\nIn this chapter we have extended the mathematical description of quantum mechanics by using \noperators to represent physical observables. The only possible results of measurements are the \neigenvalues of operators. The eigenvectors of the operator are the basis states corresponding to each \npossible eigenvalue. We ﬁnd the eigenvalues and eigenvectors by diagonalizing the matrix representing \nthe operator, which allows us to predict the results of measurements. The eigenvalue equations for the \nspin-1/2 component operator Sz are\n \n Sz0  +9 = +  U\n2 0  +9 \n \n Sz0  -9 = -  U\n2 0  -9. \n(2.128)\nThe matrices representing the spin-1/2 operators are\n \n Sx \u0003 U\n2\n a0\n1\n1\n0b\n \n Sy \u0003 U\n2\n a0\n-i\ni\n0 b  \n \n Sz \u0003 U\n2\n a1\n0\n0\n-1b  \n S2 \u0003 3U2\n4\n a1\n0\n0\n1b. \n(2.129)\nWe characterized quantum mechanical measurements of an observable A by the expectation value\n \n8A9 = 8c0 A0 c9 = a\nn\nanPan \n(2.130)\nand the uncertainty\n \n\u0006A = 48A29 - 8A92. \n(2.131)\nWe made a connection between the commutator [A, B] = AB - BA of two operators and the \nability to measure the two observables. If two operators commute, then we can measure both observ-\nables simultaneously, but if they do not commute, then we cannot measure them simultaneously. \nWe quantiﬁed this disturbance that measurement inﬂicts on quantum systems through the quantum \nmechanical uncertainty principle\n \n\u0006A\u0006B Ú 1\n2@83A, B49@ . \n(2.132)\nWe also introduced the projection postulate, which states how the quantum state vector is changed \nafter a measurement.\n\n64 \nOperators and Measurement\nPROBLEMS\n 2.1 Given the following information:\n \n Sx0{9x = { U\n2 0{9x\n \n Sy0{9y = { U\n2 0{9y\n \n 0{9x =\n1\n12 30  +9 { 0  -94  \n 0{9y =\n1\n12 30  +9 { i0  -94\n \n ﬁnd the matrix representations of Sx and Sy in the Sz basis.\n 2.2 From the previous problem we know that the matrix representation of Sx in the Sz basis is\nSx \u0003 U\n2\n a0\n1\n1\n0b.\n \n Diagonalize this matrix to ﬁnd the eigenvalues and the eigenvectors of Sx.\n 2.3 Find the matrix representation of Sz in the Sx basis for spin 1/2. Diagonalize this matrix to ﬁnd \nthe eigenvalues and the eigenvectors in this basis. Show that the eigenvalue equations for Sz are \nsatisﬁed in this new representation.\n 2.4 Show by explicit matrix calculation that the matrix elements of a general operator A (within a \nspin-1/2 system) are as shown in Eq. (2.13).\n 2.5 Calculate the commutators of the spin-1/2 operators Sx, Sy, and Sz, thus verifying Eqs. (2.96).\n 2.6 Verify that the spin component operator Sn along the direction nn has the matrix representation \nshown in Eq. (2.41).\n 2.7 Diagonalize the spin component operator Sn along the direction nn to ﬁnd its eigenvalues and \nthe eigenvectors.\n 2.8 Find the probabilities of the measurements shown below in Fig. 2.14. The ﬁrst Stern-Gerlach \nanalyzer is aligned along the direction nn deﬁned by the angles u = p>4 and f = 5p>3.\n 2.9 For the state 0 +9, calculate the expectation values and uncertainties for measurements of Sx, Sy, \nand Sz in order to verify Eq. (2.108).\n 2.10 For the state 0 +9y, calculate the expectation values and uncertainties for measurements of Sx, \nSy, and Sz. Draw a diagram of the vector model applied to this state and reconcile your quan-\ntum mechanical calculations with the classical results.\n 2.11 Show that the S2 operator commutes with each of the spin component operators of Sx, Sy, and \nSz. Do this once with matrix notation for a spin-1/2 system and a second time using only the \ncomponent commutation relations in Eqs. (2.96) and the deﬁnition of S2 in Eq. (2.103).\nY\n?\n?\nP\u0002y\nP\u0003y\n^n\nFIGURE 2.14 Measurement of spin components (Prob. 2.8).\n\n 2.12 Diagonalize the Sx and Sy operators in the spin-1 case to ﬁnd the eigenvalues and the eigenvec-\ntors of both operators.\n 2.13 For a spin-1 system, show by explicit matrix calculation that the spin component operators \nobey the commutation relations in Eqs. (2.96).\n 2.14 Find the matrix representation of the S2 operator for a spin-1 system. Do this once by explicit \nmatrix calculation and a second time by inspection of the S2 eigenvalue equation (2.120).\n 2.15 A beam of spin-1 particles is prepared in the state\n0 c9 =\n2\n129 0 19 + i 3\n129 0 09 -\n4\n129 0  -19.\na) What are the possible results of a measurement of the spin component Sz, and with what \nprobabilities would they occur?\nb) What are the possible results of a measurement of the spin component Sx, and with what \nprobabilities would they occur?\nc) Plot histograms of the predicted measurement results from parts (a) and (b), and calculate \nthe expectation values for both measurements.\n 2.16 A beam of spin-1 particles is prepared in the state\n0 c9 =\n2\n129 0 19y + i 3\n129 0 09y -\n4\n129 0  -19y.\na) What are the possible results of a measurement of the spin component Sz, and with what \nprobabilities would they occur?\nb) What are the possible results of a measurement of the spin component Sy, and with what \nprobabilities would they occur?\nc) Plot histograms of the predicted measurement results from parts (a) and (b), and calculate \nthe expectation values for both measurements.\n 2.17 A spin-1 particle is in the state\n0 c9 \u0003\n1\n130\n °\n1\n2\n5i\n¢ .\na) What are the possible results of a measurement of the spin component Sz, and with what \nprobabilities would they occur? Calculate the expectation value of the spin component Sz.\nb) Calculate the expectation value of the spin component Sx. Suggestion: Use matrix mechan-\nics to evaluate the expectation value.\n 2.18 A spin-1 particle is prepared in the state\n0 c9 =\n1\n114 0 19 -\n3\n114 0 09 + i 2\n114 0  -19.\na) What are the possible results of a measurement of the spin component Sz, and with what \nprobabilities would they occur?\nb) Suppose that the Sz measurement on the particle yields the result Sz = -U. Subsequent to \nthat result a second measurement is performed to measure the spin component Sx. What are \nthe possible results of that measurement, and with what probabilities would they occur?\nc) Draw a schematic diagram depicting the successive measurements in parts (a) and (b).\nProblems \n65\n\n66 \nOperators and Measurement\n 2.19 A spin-1 particle is prepared in the state\n0 ci9 = 4\n1\n6 0 19 - 4\n2\n6 0 09 + i 4\n3\n6 0  -19.\n \n Find the probability that the system is measured to be in the ﬁnal state\n0 cf9 = 1+i\n17 0 19y +\n2\n17 0 09y - i 1\n17 0  -19y.\n 2.20 In part (2) of SPINS Lab #3, you measured the spin components of the unknown (spin 1) ini-\ntial states 0 ci9 (i \u0003 1, 2, 3, 4) along the three axes. Using your measured values, deduce the \nunknown initial states.\n 2.21 In part (3) of SPINS Lab #3, you built a spin-1 interferometer and measured the relative prob-\nabilities after the ﬁnal Stern-Gerlach analyzer for the seven possible cases where one beam, \na pair of beams, or all three beams from the second Stern-Gerlach analyzer were used. Show \nhow you used the projection postulate to calculate the theoretical probabilities.\n 2.22 A beam of spin-1/2 particles is sent through a series of three Stern-Gerlach analyzers, as shown \nin Fig. 2.15. The second Stern-Gerlach analyzer is aligned along the nn direction, which makes \nan angle \u0007 in the x-z plane with respect to the z-axis.\na) Find the probability that particles transmitted through the ﬁrst Stern-Gerlach analyzer are \nmeasured to have spin down at the third Stern-Gerlach analyzer?\nb) How must the angle \u0007 of the second Stern-Gerlach analyzer be oriented so as to maximize \nthe probability that particles are measured to have spin down at the third Stern-Gerlach \nanalyzer? What is this maximum fraction?\nc) What is the probability that particles have spin down at the third Stern-Gerlach analyzer if \nthe second Stern-Gerlach analyzer is removed from the experiment?\n 2.23 Consider a three-dimensional ket space. In the basis deﬁned by three orthogonal kets 0 19, 0 29, \nand 0 39, the operators A and B are represented by\n \nA \u0003 °\na1\n0\n0\n0\na2\n0\n0\n0\na3\n ¢  B \u0003 °\nb1\n0\n0\n0\n0\nb2\n0\nb2\n0\n¢ ,\n \n where all the quantities are real.\na) Do the operators A and B commute?\nb) Find the eigenvalues and normalized eigenvectors of both operators.\nZ\n?\n?\n^n\nZ\nFIGURE 2.15 Measurement of spin components (Prob. 2.22).\n\nResources \n67\nc) Assume the system is initially in the state 0 29. Then the observable corresponding to the oper-\nator B is measured. What are the possible results of this measurement and the probabilities of \neach result? After this measurement, the observable corresponding to the operator A is mea-\nsured. What are the possible results of this measurement and the probabilities of each result?\nd) How are questions (a) and (c) above related?\n 2.24 If a beam of spin-3/2 particles is input to a Stern-Gerlach analyzer, there are four output beams \nwhose deﬂections are consistent with magnetic moments arising from spin angular momentum \ncomponents of 3\n2 U, 1\n2 U, -  1\n2 U, and -  3\n2 U. For a spin-3/2 system:\na) Write down the eigenvalue equations for the Sz operator.\nb) Write down the matrix representation of the Sz eigenstates.\nc) Write down the matrix representation of the Sz operator.\nd) Write down the eigenvalue equations for the S2 operator.\ne) Write down the matrix representation of the S2 operator.\n 2.25 Are the projection operators P+ and P- Hermitian? Explain.\nRESOURCES\nActivities\nThis activity is available at\nwww.physics.oregonstate.edu/qmactivities\nSpins Lab 3: Stern-Gerlach measurements of a spin-1 system.\n\nC H A P T E R \n3\nSchrödinger Time Evolution\nThis chapter marks our ﬁnal step in developing the mathematical basis of a quantum theory. In \n Chapter 1, we learned how to use kets to describe quantum states and how to predict the probabili-\nties of results of measurements. In Chapter 2, we learned how to use operators to represent physical \nobservables and how to determine the possible measurement results. The key missing aspect is the \nability to predict the future. Physics theories are judged on their predictive power. Classical mechan-\nics relies on Newton’s second law F = ma to predict the future of a particle’s motion. The ability to \npredict the quantum future started with Erwin Schrödinger and bears his name.\n3.1 \u0002 SCHRÖDINGER EQUATION\nThe sixth postulate of quantum mechanics says that the time evolution of a quantum system is \n governed by the differential equation\n \niU d\ndt\n 0  c 1t29 = H 1t2 0  c 1t29, \n(3.1)\nwhere the operator H corresponds to the total energy of the system and is called the Hamiltonian \noperator of the system because it is derived from the classical Hamiltonian. This equation is known as \nthe Schrödinger equation.\nPostulate 6\nThe time evolution of a quantum system is determined by the  Hamiltonian \nor total energy operator H1t2 through the Schrödinger equation\niU d\ndt\n 0  c 1t29 = H 1t2 0  c 1t29.\nThe Hamiltonian is a new operator, but we can use the same ideas we developed in Chapter 2 to \nunderstand its basic properties. The Hamiltonian H is an observable, so it is a Hermitian operator. The \neigenvalues of the Hamiltonian are the allowed energies of the quantum system, and the eigenstates \nof H are the energy eigenstates of the system. If we label the allowed energies as En, then the energy \neigenvalue equation is\n \nH 0 En9 = En0 En9 . \n(3.2)\n\n3.1 Schrödinger Equation \n69\nIf we have the Hamiltonian H in a matrix representation, then we diagonalize the matrix to ﬁnd the \neigenvalues En and the eigenvectors 0 En9 just as we did with the spin operators in Chapter 2. For the \nmoment, let’s assume that we have already diagonalized the Hamiltonian [i.e., solved Eq. (3.2)] so that \nwe know the eigenvalues En and the eigenvectors 0 En9, and let’s see what we can learn about quantum \ntime evolution in general by solving the Schrödinger equation.\nThe eigenvectors of the Hamiltonian form a complete basis because the Hamiltonian is an observ-\nable, and therefore a Hermitian operator. Because H is the only operator appearing in the Schrödinger \nequation, it would seem reasonable (and will prove invaluable) to consider the energy eigenstates as \nthe basis of choice for expanding general state vectors:\n \n0 c 1t29 = a\nn\ncn1t2 0 En9. \n(3.3)\nThe basis of eigenvectors of the Hamiltonian is also orthonormal, so\n \n8Ek\u0004 En9 = dkn. \n(3.4)\nWe refer to this basis as the energy basis.\nFor now, we assume that the Hamiltonian is time independent (we will do the time-dependent case \nH(t) in Section 3.4). The eigenvectors of a time-independent Hamiltonian come from the diagonaliza-\ntion procedure we used in Chapter 2, so there is no reason to expect the eigenvectors themselves to \ncarry any time dependence. Thus if a general state 0 c9 is to be time dependent, as the Schrödinger equa-\ntion implies, then the time dependence must reside in the expansion coefﬁcients cn1t2, as expressed in \nEq. (3.3). Substitute this general state into the Schrödinger equation (3.1)\n \niU d\ndt a\nn\ncn1t2 0 En9 = Ha\nn\ncn1t2 0 En9 \n(3.5)\nand use the energy eigenvalue equation (3.2) to obtain\n \niUa\nn\ndcn1t2\ndt\n0 En9 = a\nn\ncn1t2En0 En9. \n(3.6)\nEach side of this equation is a sum over all the energy states of the system. To simplify this equation, \nwe isolate single terms in these two sums by taking the inner product of the ket on each side with one \nparticular ket 0 Ek9 (this ket can have any label k, but must not have the label n that is already used in \nthe summation). The orthonormality condition 8Ek\u0004En9 = dkn then collapses the sums:\n \n 8Ek0 iUa\nn\ndcn1t2\ndt\n0 En9 = 8Ek0 a\nn\ncn1t2En0 En9 \n \n iUa\nn\ndcn1t2\ndt\n 8Ek0 En9 = a\nn\ncn1t2En8Ek0 En9  \n \n iUa\nn\ndcn1t2\ndt\n  dkn = a\nn\ncn1t2Endkn\n \n \n iU \ndck1t2\ndt\n= ck1t2Ek.\n \n(3.7)\nWe are left with a single differential equation for each of the possible energy states of the systems \nk = 1, 2, 3, ... . This ﬁrst-order differential equation can be rewritten as\n \ndck1t2\ndt\n= -i Ek\nU\n ck1t2. \n(3.8)\n\n70 \nSchrödinger Time Evolution\nThe solution to Eq. (3.8) is a complex exponential\n \nck1t2 = ck102e-iEkt>U. \n(3.9)\nIn Eq. (3.9), we have denoted the initial condition as ck102, but we denote it simply as ck hereafter. \nEach coefﬁcient in the energy basis expansion of the state obeys the same form of the time dependence \nin Eq. (3.9), but with a different exponent due to the different energies. The time-dependent solution \nfor the full state vector is summarized by saying that if the initial state of the system at time t \u0003 0 is\n \n0 c1029 = a\nn\ncn0 En9, \n(3.10)\nthen the time evolution of this state under the action of the time-independent Hamiltonian H is\n \n0 c1t29 = a\nn\ncne-iEnt>U0 En9  . \n(3.11)\nSo the time dependence of the original state vector is found by multiplying each energy eigenstate \ncoefﬁcient by its own phase factor e-iEnt>U that depends on the energy of that eigenstate. Note that the \nfactor E>U is an angular frequency, so that the time dependence is of the form e-ivt, a form commonly \nfound in many areas of physics. It is important to remember that one must use the energy eigenstates for \nthe expansion in Eq. (3.10) in order to use the simple phase factor multiplication in Eq. (3.11) to account \nfor the Schrödinger time evolution of the state. This key role of the energy basis accounts for the impor-\ntance of the Hamiltonian operator and for the common practice of ﬁnding the energy eigenstates to use \nas the preferred basis.\nA few examples help to illustrate some of the important consequences of this time evolution of \nthe quantum mechanical state vector. First, consider the simplest possible situation where the system \nis initially in one particular energy eigenstate:\n \n0 c1029 = 0 E19, \n(3.12)\nfor example. The prescription for time evolution tells us that after some time t the system is in the state\n \n0 c1t29 = e-iE1t>U0 E19. \n(3.13)\nBut this state differs from the original state only by an overall phase factor, which we have said before \ndoes not affect any measurements (Problem 1.3). For example, if we measure an observable A, then \nthe probability of measuring an eigenvalue aj is given by\n \n Paj = 08aj0 c1t290\n2\n \n \n = 08aj0 e-iE1t>U0 E190\n2 \n \n = 08aj0 E190\n2.\n \n(3.14)\nThis probability is time independent and is equal to the probability at the initial time. Thus, we \n conclude that there is no measureable time evolution for this state. Hence, the energy eigenstates are \ncalled stationary states. If a system begins in an energy eigenstate, then it remains in that state.\nNow consider an initial state that is a superposition of two energy eigenstates:\n \n0 c1029 = c10 E19 + c20 E29. \n(3.15)\nIn this case, time evolution takes the initial state to the later state\n \n0 c1t29 = c1e-iE1t>U0 E19 + c2e-iE2t>U0 E29. \n(3.16)\n\n3.1 Schrödinger Equation \n71\nA measurement of the system energy at the time t would yield the value E1 with a probability\n \n PE1 = 08E10 c1t290\n2\n \n \n = 08E103c1e-iE1t>U0 E19 + c2e-iE2t>U0 E294 0\n2 \n \n = 0 c10\n2,\n \n(3.17)\nwhich is independent of time. The same is true for the probability of measuring the energy E2. Thus, \nthe probabilities of measuring the energies are stationary, as they were in the ﬁrst example.\nHowever, now consider what happens if another observable is measured on this system in this \nsuperposition state. There are two distinct situations: (1) If the other observable A commutes with the \nHamiltonian H, then A and H have common eigenstates. In this case, measuring A is equivalent to mea-\nsuring H because the inner products used to calculate the probabilities use the same eigenstates. Hence, \nthe probability of measuring any particular eigenvalue of A is time independent, as in Eq. (3.17). (2) If \nA and H do not commute, then they do not share common eigenstates. In this case, the eigenstates of A \nin general consist of superpositions of energy eigenstates. For example, suppose that the eigenstate of \nA corresponding to the eigenvalue a1 were\n \n0 a19 = a10 E19 + a20 E29. \n(3.18)\nThen the probability of measuring the eigenvalue a1 would be\n \n Pa1 = 08a10 c1t290\n2\n \n \n = 03a*\n18E10 + a*\n28E2043c1e-iE1t>U0 E19 + c2e-iE2t>U0 E294 0\n2 \n \n = @ a*\n1c1e-iE1t>U + a*\n2c2e-iE2t>U@\n2.\n \n(3.19)\nFactoring out the common phase gives\n \n Pa1 = @ e-iE1t>U@\n2\n @ a*\n1c1 + a*\n2c2e-i1E2-E12t>U@\n2\n \n \n = 0 a10\n20 c10\n2 + 0 a20\n20 c20\n2 + 2Re1a1c*\n1a*\n2c2e-i1E2-E12t>U2. \n(3.20)\nThe different time-evolution phases of the two components of 0 c1t29 lead to a time dependence in the \nprobability. The overall phase in Eq. (3.20) drops out, and only the relative phase remains in the prob-\nability calculation. Hence, the time dependence is determined by the difference of the energies of the \ntwo states involved in the superposition. The corresponding angular frequency of the time evolution\n \nv 21 = E2 - E1\nU\n \n(3.21)\nis called the Bohr frequency.\nTo summarize, we list below a recipe for solving a standard time-dependent quantum mechanics \nproblem with a time-independent Hamiltonian.\nGiven a Hamiltonian H and an initial state 0 c1029, what is the probability that \nthe eigenvalue aj of the observable A is measured at time t?\n 1. Diagonalize H (ﬁnd the eigenvalues En and eigenvectors 0 En92.\n 2. Write 0 c1029 in terms of the energy eigenstates 0 En9.\n 3. Multiply each eigenstate coefﬁcient by e-iEnt>U to get 0 c1t29.\n 4. Calculate the probability Paj = 08aj0 c1t290\n2.\n\n72 \nSchrödinger Time Evolution\n3.2 \u0002 SPIN PRECESSION\nNow apply this new concept of Schrödinger time evolution to the case of a spin-1/2 system. The Ham-\niltonian operator represents the total energy of the system, but because only energy differences are \nimportant in time-dependent solutions (and because we can deﬁne the zero of potential energy as \nwe wish), we need consider only energy terms that differentiate between the two possible spin states \nin the system. Our experience with the Stern-Gerlach apparatus tells us that the magnetic potential \nenergy of the magnetic dipole differs for the two possible spin-component states. So to begin, we \nconsider the potential energy of a single magnetic dipole (e.g., in a silver atom) in a uniform magnetic \nﬁeld as the sole term in the Hamiltonian. Recalling that the magnetic dipole is given by\n \nM = g q\n2me\n S, \n(3.22)\nthe Hamiltonian is\n \n H = -M~B\n \n \n = -g q\n2me\n S~B \n \n = e\nme\n S~B,\n \n(3.23)\nwhere q = -e and g = 2 have been used in the last line. The gyromagnetic ratio, g, is slightly differ-\nent from 2, but we ignore that detail.\n3.2.1 \u0002 Magnetic Field in the z-Direction\nFor our ﬁrst example, we assume that the magnetic ﬁeld is uniform and directed along the z-axis. \n Writing the magnetic ﬁeld as\n \nB = B0\n zn \n(3.24)\nallows the Hamiltonian to be simpliﬁed to\n \n H = eB0\nme\n Sz \n \n = v0 Sz,  \n(3.25)\nwhere we have introduced the deﬁnition\n \nv0 K eB0\nme\n. \n(3.26)\nThis deﬁnition of an angular frequency simpliﬁes the notation now and will have an obvious \n interpretation at the end of the problem.\nThe Hamiltonian in Eq. (3.25) is proportional to the Sz operator, so H and Sz commute and \n therefore share common eigenstates. This is clear if we write the Hamiltonian as a matrix in the \nSz  representation:\n \nH \u0003 U v0\n2\n a1\n0\n0\n-1b. \n(3.27)\n\n3.2 Spin Precession \n73\nBecause H is diagonal, we have already completed step 1 of the Schrödinger time-evolution recipe. \nThe eigenstates of H are the basis states of the representation, while the eigenvalues are the diagonal \nelements of the matrix in Eq. (3.27). The eigenvalue equations for the Hamiltonian are thus\n \n H 0  +9 = v0Sz0  +9 = U v0\n2\n 0  +9 = E+ 0  +9\n \n \n H 0  -9 = v0Sz0  -9 = -  U v0\n2\n 0  +9 = E - 0  -9, \n(3.28)\nwith eigenvalues and eigenvectors given by\n \n E + = U v 0\n2\n \n E - = -  U v 0\n2  \n \n 0\n E +9 = 0  +9\n \n 0\n E -9 = 0  -9.  \n(3.29)\nThe information regarding the energy eigenvalues and eigenvectors is commonly presented in a \ngraphical diagram, which is shown in Fig. 3.1 for this case. The two energy states are separated \nby the energy E+ - E- = U v0, so the angular frequency v0 characterizes the energy scale of this \nsystem. The spin-up state 0  +9 has a higher energy because the magnetic moment is aligned against \nthe ﬁeld in that state; the negative charge in Eq. (3.22) causes the spin and magnetic moment to be \nantiparallel.\nNow we look at a few examples to illustrate the key features of the behavior of a spin-1/2 system \nin a uniform magnetic ﬁeld. First, consider the case where the initial state is spin up along the z-axis:\n \n0\n c1029 = 0  +9. \n(3.30)\nThis initial state is already expressed in the energy basis (step 2 of the Schrödinger recipe), so the \nSchrödinger equation time evolution takes this initial state to the state\n \n 0\n c1t29 = e-iE +\n t>U0  +9 \n \n = e-iv 0\n t>20  +9 \n(3.31)\n\u00030.5\n\u00030.25\n0.0\n0.25\n0.5\nE/\u0002Ω0\n\u0002\u0002\u0003\n\u0002Ω0\nE\u0002\u0005 \u0002Ω0\n2\n\u0003\nE\u0003\u0005\n\u0002Ω0\n2\n\u0002\u0003\u0003\nFIGURE 3.1 Energy level diagram of a spin-1/2 particle in a uniform magnetic ﬁeld.\n\n74 \nSchrödinger Time Evolution\naccording to step 3 of the Schrödinger recipe. As we saw before [(Eq. (3.13)], because the initial state is \nan energy eigenstate, the time-evolved state acquires an overall phase factor, which does not represent \na physical change of the state. The probability for measuring the spin to be up along the z-axis is (step 4 \nof the Schrödinger recipe)\n \n P+ = 08+ 0 c1t290\n2\n \n \n = @8+ 0 e-iv0t>20  +9@\n2 \n \n(3.32)\n \n = 1.\n \n \nAs expected, this probability is not time dependent, and we therefore refer to 0  +9 as a stationary state \nfor this system. A schematic diagram of this experiment is shown in Fig. 3.2, where we have intro-\nduced a new element to represent the applied ﬁeld. This new depiction is the same as the depictions in \nthe SPINS software, where the number in the applied magnetic ﬁeld box (42 in Fig. 3.2) is a measure \nof the magnetic ﬁeld strength. In this experiment, the results shown are independent of the applied \nﬁeld strength, as indicated by Eq. (3.32), and as you can verify with the software.\nNext, consider the most general initial state, which we saw in Chapter 2 corresponds to spin \nup along an arbitrary direction deﬁned by the polar angle u and the azimuthal angle f. The initial \nstate is\n \n0 c1029 = 0  +9n = cos u\n2 0  +9 + sin u\n2\n eif0  -9, \n(3.33)\nor using matrix notation:\n \n0 c1029 \u0003 a cos1u>22\neif sin1u>22b. \n(3.34)\nSchrödinger time evolution introduces a time-dependent phase term for each component, giving\n \n 0 c1t29 \u0003 a e-iE+t>U cos1u>22\ne-iE-t>Ueif sin1u>22b\n \n \n \u0003 a e-iv0t>2 cos1u>22\neiv0t>2eif sin1u>22b\n \n \n(3.35)\n \n \u0003 e-iv0t>2 a\ncos1u>22\nei1f+v0t2 sin1u>22b. \nZ\nZ\n100\n0\n42\nZ\nFIGURE 3.2 Schematic diagram of a Stern-Gerlach measurement with an applied uniform magnetic ﬁeld \nrepresented by the box in the middle, with the number 42 representing the strength of the magnetic ﬁeld.\n\n3.2 Spin Precession \n75\nNote again that an overall phase does not have a measurable effect, so the evolved state is a spin up \neigenstate along a direction that has the same polar angle u as the initial state and a new azimuthal \nangle f + v0t. The state appears to have simply rotated around the z-axis, the axis of the magnetic \nﬁeld, by the angle v0t. Of course, we have to limit our discussion to results of measurements, so let’s \nﬁrst calculate the probability for measuring the spin component along the z-axis:\n \n P+ = 08+  0 c1t290\n2\n \n \n = 2 11 02e-iv0t>2 a\ncos1u>22\nei1f+v0t2 sin1u>22b 2\n2\n \n \n = 0 e-iv0t>2 cos1u>22 0\n2\n \n \n = cos21u>22.\n \n \n(3.36)\nThis probability is time independent because the Sz eigenstates are also energy eigenstates for this \nproblem (i.e., H and Sz commute). The probability in Eq. (3.36) is consistent with the interpretation \nthat the angle u that the spin vector makes with the z-axis does not change.\nThe probability for measuring spin up along the x-axis is\n \n P+x = 0 x8+  0 c1t290\n2\n \n \n = 2 1\n12 11  12e-iv0t>2a\ncos1u>22\nei(f +v0t) sin1u>22b 2\n2\n \n \n = 1\n2 @ cos1u>22 + ei1f+v0t2 sin1u>22 @\n2\n \n(3.37)\n \n = 1\n2 3cos21u>22 + cos1u>22sin1u>221ei1f+v0t2 + e-i1f +v0t22 + sin21u>224 \n \n = 1\n2 31 + sin u cos1f + v0t24.\n \nThis probability is time dependent because the Sx eigenstates are not stationary states (i.e., H and Sx \ndo not commute). The time dependence in Eq. (3.37) is consistent with the spin precessing around \nthe z-axis.\nTo illustrate this spin precession further, it is useful to calculate the expectation values for each of \nthe spin components. For Sz, we have\n \n 8Sz9 = 8c1t2 0 Sz0 c1t29\n \n \n = eiv0t>2 a\n cos a u\n2b  e-i1f +v0t2 sin a u\n2b b  U\n2\n a1\n0\n0\n-1b e-iv0t>2 a\ncos1u>22\nei1f+v0t2 sin1u>22b \n \n = U\n2\n 3cos21u>22 - sin21u>224\n \n \n = U\n2\n cos u,\n \n(3.38)\n\n76 \nSchrödinger Time Evolution\nwhile the other components are\n \n 8Sy9 = 8c1t2 0 Sy0 c1t29\n \n \n = eiv0t>2 a\n cos a u\n2b  e-i1f+v0t2 sin a u\n2b b U\n2\n a0\n-i\ni\n0 be-iv0t>2 a\ncos1u>22\nei1f +v0t2 sin1u>22b (3.39)\n \n = U\n2\n sin u sin1f + v0t2 \nand\n \n 8Sx9 = 8c1t2 0 Sx0 c1t29\n \n \n = U\n2\n sin u cos1f + v0t2. \n \n(3.40)\nThe expectation value of the total spin vector 8S9 is shown in Fig. 3.3, where it is seen to precess \naround the magnetic ﬁeld direction with an angular frequency v0. The precession of the spin vector is \nknown as Larmor precession and the frequency of precession is known as the Larmor frequency.\nThe quantum mechanical Larmor precession is analogous to the classical behavior of a magnetic \nmoment in a uniform magnetic ﬁeld. A classical magnetic moment M experiences a torque M * B \nwhen placed in a magnetic ﬁeld. If the magnetic moment is associated with an angular momentum L, \nthen we can write\n \nM =\nq\n2m\n L, \n(3.41)\nwhere q and m are the charge and mass, respectively, of the system. The equation of motion for the \nangular momentum\n \nd L\ndt = M * B \n(3.42)\nthen results in\n \ndM\ndt =\nq\n2m\n M * B. \n(3.43)\ny\nx\nz\n\u0004S(t)\u0003\n\u0004S(0)\u0003\nB\nΩ0t\nFIGURE 3.3 The expectation value of the spin vector precesses in a uniform magnetic ﬁeld.\n\n3.2 Spin Precession \n77\nBecause the torque M * B is perpendicular to the angular momentum L = 2mM>q, it causes the \nmagnetic moment to precess about the ﬁeld with the classical Larmor frequency vcl = qB>2m.\nIn the quantum mechanical example we are considering, the charge q is negative (meaning the \nspin and magnetic moment are antiparallel), so the precession is counterclockwise around the ﬁeld. A \npositive charge would result in clockwise precession. This precession of the spin vector makes it clear \nthat the system has angular momentum, as opposed to simply having a magnetic dipole moment. The \nequivalence of the classical Larmor precession and the expectation value of the quantum mechanical \nspin vector is one example of Ehrenfest’s theorem, which states that quantum mechanical expecta-\ntion values obey classical laws.\nPrecession experiments like the one discussed here are of great practical value. For example, if \nwe measure the magnetic ﬁeld strength and the precession frequency, then the gyromagnetic ratio can \nbe determined. This spin precession problem is also of considerable theoretical utility because it is \nmathematically equivalent to many other quantum systems that can be modeled as two-state systems. \nThis utility is broader than you might guess at ﬁrst glance, because many multistate quantum systems \ncan be reduced to two-state systems if the experiment is designed to interact only with two of the many \nlevels of the system.\nExample 3.1 A spin-1/2 particle with a magnetic moment is prepared in the state 0  -9x and is \nsubject to a uniform applied magnetic ﬁeld B = B0\n zn. Find the probability of measuring spin up in \nthe x-direction after a time t. This experiment is depicted in Fig. 3.4.\nWe solve this problem using the four steps of the Schrödinger time-evolution recipe from \nSection 3.1. The initial state is\n \n0 c1029 = 0  -9x. \n(3.44)\nThe applied magnetic ﬁeld is in the z-direction, so the Hamiltonian is H = v0Sz and the energy \neigenstates are 0{9 with energies E{ = {U v0>2 (step 1). The Larmor precession frequency is \nv0 = eB0>me. We must express the initial state in the energy basis (step 2):\n \n0 c1029 = 0  -9x =\n1\n12 0  +9 -\n1\n12 0  -9. \n(3.45)\nThe time-evolved state is obtained by multiplying each energy eigenstate coefﬁcient by the appro-\npriate phase factor (step 3):\n \n 0 c1t29 =\n1\n12 e-iE +t>U0  +9 -\n1\n12 e-iE -t>U0  -9 \n(3.46)\n \n =\n1\n12 e-iv0t>20  +9 -\n1\n12 e+iv0t>2 0 -9.\nX\n?\n?\n42\nZ\nX\nFIGURE 3.4 Spin precession experiment.\n\n78 \nSchrödinger Time Evolution\nThe measurement probability is found by projecting 0 c1t29 onto the measured state and complex \nsquaring (step 4):\n \n P+x = 0 x8+ 0\n c1t29 0\n2\n \n \n = @x8+ 0A 1\n12 e-iv0t>20  +9 -\n1\n12 e+iv0t>20  -9B @\n2\n \n \n = @ A 1\n128+ 0 +\n1\n128- 0 BA 1\n12 e-iv0t>20  +9 -\n1\n12 e+iv0t>20  -9B @\n2\n \n(3.47)\n \n = 1\n4 @e-iv0t>2 - e+iv0t>2@\n2\n \n \n = sin2 1v0t>22.\n \nThe probability that the system has spin up in the x-direction oscillates between zero and unity \nas time evolves, as shown in Fig. 3.5(a), which is consistent with the model of the spin vector \nprecessing around the applied ﬁeld, as shown in Fig. 3.5(b).\n3.2.2 \u0002 Magnetic Field in a General Direction\nFor our second example, consider a more general direction for the magnetic ﬁeld by adding a magnetic \nﬁeld component along the x-axis to the already existing ﬁeld along the z-axis. The simplest approach \nto solving this new problem would be to redeﬁne the coordinate system so the z-axis pointed along the \ndirection of the new total magnetic ﬁeld. Then the solution would be the same as was obtained above, \nwith a new value for the magnitude of the magnetic ﬁeld being the only change. This approach would \nbe considered astute in many circumstances, but we will not take it because we want to get practice \nsolving this new type of problem and because we want to address some issues that are best posed in the \noriginal coordinate system. Thus, we deﬁne a new magnetic ﬁeld as\n \nB = B0zn + B1xn . \n(3.48)\nt\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP\u0002x\ny\nx\nz\n(a)\n(b)\nΩ0t\nB\n\u0004S(0)\u0003\n\u0004S(t)\u0003\n2Π\nΩ0\n4Π\nΩ0\n6Π\nΩ0\nFIGURE 3.5 (a) Probability of a spin component measurement and (b) the corresponding \nprecession of the expectation value of the spin.\n\n3.2 Spin Precession \n79\nThis ﬁeld is oriented in the xz-plane at an angle u with respect to the z-axis, as shown in Fig. 3.6. In \nlight of the solution above, it is useful to deﬁne Larmor frequencies associated with each of the ﬁeld \ncomponents:\n \nv0 K eB0\nme\n, \nv1 K eB1\nme\n. \n(3.49)\nUsing these deﬁnitions, the Hamiltonian becomes\n \n H = - M~B\n \n \n = v0 Sz + v1Sx, \n(3.50)\nor in matrix representation\n \nH \u0003 U\n2\n av0\nv1\nv1\n-v0\nb. \n(3.51)\nThis Hamiltonian is not diagonal, so its eigenstates are not the same as the eigenstates of Sz. Rather we \nmust use the diagonalization procedure to ﬁnd the new eigenvalues and eigenvectors. The characteristic \nequation determining the energy eigenvalues is\n \n ∞ \nU\n2\n  v0 - l\nU\n2\n  v1\nU\n2\n  v1\n-  U\n2\n  v0 - l\n∞= 0  \n \n - a U\n2\n v0b\n2\n+ l2 - a U\n2\n v1b\n2\n= 0, \n \n(3.52)\nwith solutions\n \nl = { U\n2\n 4v2\n0 + v2\n1. \n(3.53)\nNote that the energy eigenvalues are {1U v0>22 when v1 = 0, which they must be given our previ-\nous solution. Rather than solve directly for the eigenvectors, let’s make them obvious by rewriting the \nHamiltonian. From Fig. 3.6 it is clear that the angle is determined by the equation\n \ntan u = B1\nB0\n= v1\nv0\n. \n(3.54)\nB0\nB1\nB\nθ\nFIGURE 3.6 A uniform magnetic ﬁeld in a general direction.\n\n80 \nSchrödinger Time Evolution\nUsing this, the Hamiltonian can be written as\n \nH \u0003 U\n2\n 4v2\n0 + v2\n1 acos u\nsin u\nsin u\n-cos ub. \n(3.55)\nIf we let nn be the unit vector in the direction of the total magnetic ﬁeld, then the Hamiltonian is propor-\ntional to the spin component Sn along the direction nn:\n \nH = 4v2\n0 + v2\n1 Sn. \n(3.56)\nThis is what we expected at the beginning: that the problem could be solved by using the ﬁeld direc-\ntion to deﬁne a coordinate system. Thus, the eigenvalues are as we found in Section 2.2.1 and the \neigenstates are the spin up and down states along the direction nn, which are\n \n0  +9n = cos u\n2\n 0  +9 + sin u\n2\n 0  -9 \n \n0  -9n = sin u\n2\n 0  +9 - cos u\n2\n 0  -9 \n \n(3.57)\nfor this case, because the azimuthal angle f is zero. These are the same states you would ﬁnd by \ndirectly solving for the eigenstates of the Hamiltonian. Because we have already done that for the Sn \ncase, we do not repeat it here.\nNow consider performing the following experiment: begin with the system in the spin-up state \nalong the z-axis, and measure the spin component along the z-axis after the system has evolved in \nthis magnetic ﬁeld for some time, as depicted in Fig. 3.7. Let’s speciﬁcally calculate the probabil-\nity that the initial 0  +9 is later found to have evolved to the 0  -9 state. This is commonly known as a \nspin ﬂip. According to our time-evolution prescription, we must ﬁrst write the initial state in terms \nof the energy eigenstates of the system. In the previous examples, this was trivial because the energy \neigenstates were the 0{9 states that we used to express all general states. But now this new problem is \nmore involved, so we proceed more slowly. The initial state\n \n0\n c1029 = 0  +9 \n(3.58)\nmust be written in the 0{9n basis. Because the 0{9n basis is complete, we can use the completeness \nrelation [Eq. (2.55)] to decompose the initial state\n \n 0\n c1029 = 10  +9n n8+ 0 + 0  -9n n8- 02 0  +9 \n \n = 0  +9n n8+ 0  +9 + 0  -9n n8- 0  +9 \n \n = n8+ 0  +9 0  +9n + n8- 0  +9 0  -9n  \n \n = cos u\n2\n 0  +9n + sin u\n2\n 0  -9n.\n \n \n(3.59)\nZ\n?\n?\n42\n^n\nZ\nFIGURE 3.7 A spin precession experiment with a uniform magnetic ﬁeld aligned in a general direction nn.\n\n3.2 Spin Precession \n81\nNow that the initial state is expressed in the energy basis, the time-evolved state is obtained by multi-\nplying each coefﬁcient by a phase factor dependent on the energy of that eigenstate:\n \n0\n c1t29 = e-iE+t>U cos u\n2\n 0  +9n + e-iE-t>U sin u\n2\n 0  -9n. \n(3.60)\nWe leave it in this form and substitute the energy eigenvalues\n \nE{ = { U\n2 4v2\n0 + v2\n1 \n(3.61)\nat the end of the example.\nThe probability of a spin ﬂip is\n \nP+ S  - = 08-  0\n c1t290\n2 \n \n= 2 8- 0 Je-iE+t>U cos u\n2 0  +9n + e-iE-t>U sin u\n2 0  -9n R 2\n2\n \n \n= 2 e-iE+t>U cos u\n2\n 8- 0  +9n + e-iE-t>U sin u\n2\n 8- 0  -9n 2\n2\n \n \n= 2 e-iE+t>U cos u\n2 sin u\n2\n + e-iE-t>U sin u\n2\n a-cos u\n2b 2\n2\n \n(3.62)\n \n= cos2 u\n2 sin2\n  u\n2\n @ 1 - ei1E+-E-2t>U@\n2 \n \n= sin2 u\n  sin2 a1E+ - E-2t\n2U\nb. \nThe probability oscillates at the frequency determined by the difference in energies of the eigen-\nstates. This time dependence results because the initial state was a superposition state, as we saw in \nEq. (3.20). In terms of the Larmor frequencies used to deﬁne the Hamiltonian in Eq. (3.51), the prob-\nability of a spin ﬂip is\n \nP+ S - =\nv2\n1\nv2\n0 + v2\n1\n sin2\n a 2v2\n0 + v2\n1\n2\n tb  . \n(3.63)\nEq. (3.63) is often called Rabi’s formula, and it has important applications in many problems as we \nshall see.\nTo gain insight into Rabi’s formula, consider two simple cases. First, if there is no added ﬁeld in \nthe x-direction, then v1 \u0003 0 and P+ S - = 0 because the initial state is a stationary state. Second, if \nthere is no ﬁeld component in the z-direction, then v0 \u0003 0 and P+ S - oscillates between 0 and 1 at the \nfrequency v1, as shown in Fig. 3.8(a). The second situation corresponds to spin precession around the \napplied magnetic ﬁeld in the x-direction, as shown in Fig. 3.8(b), with a complete spin ﬂip from 0  +9 to \n0  -9 and back again occurring at the precession frequency v1. In the general case where both magnetic \nﬁeld components are present, the probability does not reach unity and so there is no time at which the \nspin is certain to ﬂip over. If the x-component of the ﬁeld is small compared to the z-component, then \nv1 << v0 and P+ S - oscillates between 0 and a value much less than 1 at a frequency approximately \nequal to v0, as shown in Fig. 3.9.\n\n82 \nSchrödinger Time Evolution\nExample 3.2 A spin-1/2 particle with a magnetic moment is prepared in the state 0  -9 and is sub-\nject to a uniform applied magnetic ﬁeld B = B0yn. Find the probability of measuring spin up in the \nz-direction after a time t.\nThe initial state is\n \n0\n c1029 = 0  -9. \n(3.64)\nThe applied magnetic ﬁeld is in the y-direction, so the Hamiltonian is H = v0Sy and the energy \neigenstates are 0{9y with energies E{ = {U v0>2 (step 1). The Larmor precession frequency is \nt\n0\n0.2\n0.4\n0.6\n0.8\n1.0\n)\nb\n(\n)\na\n(\ny\nx\nz\nB\n〈S(0)〉\n〈S(t)〉\n2\u0002\n\u00030\n4\u0002\n\u00030\n6\u0002\n\u00030\nP+→−\nFIGURE 3.9 (a) Spin-ﬂip probability for a uniform magnetic ﬁeld with x- and z-components and \n(b) the corresponding precession of the expectation value of the spin.\nt\n)\nb\n(\n)\na\n(\ny\nx\nz\nB\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP+→−\nS(0)\nS(t)\n2Π\nΩ1\n4Π\nΩ1\n6Π\nΩ1\nFIGURE 3.8 (a) Spin-flip probability for a uniform magnetic ﬁeld in the x-direction and (b) the \ncorresponding precession of the expectation value of the spin.\nP+→−\n\n3.2 Spin Precession \n83\nv0 = eB0>me. We must express the initial state in the energy basis (step 2), which in this case is \nthe Sy basis:\n \n 0\n c1029 = 0  -9 = 10  +9y y0  + 0 + 0  -9y y8- 02 0  -9 \n \n = 0  +9y y8+ 0  -9 + 0  -9y y8- 0  -9\n \n \n = y8+ 0  -9 0  +9y + y8- 0  -9 0  -9y\n \n(3.65)\n \n =\n-i\n12 0  +9y +\ni\n12 0  -9y.\n \nThe time evolved state is obtained by multiplying each energy eigenstate coefﬁcient by a phase \nfactor (step 3):\n \n 0\n c1t29 =\n-i\n12\n e-iE +t>U0  +9y +\ni\n12\n e-iE -t>U0  -9y \n \n =\n-i\n12\n e-iv0t>20  +9y +\ni\n12\n e+iv0t>20  -9y. \n(3.66)\nThe measurement probability is found by projecting onto the measured state and squaring (step 4):\n \n P+ = 08+ 0\n c1t290\n2\n \n \n = @8+ 0A -i\n12 e-iv0t>20  +9y +\ni\n12 e+iv0t>20  -9y2@\n2\n \n \n = @ A -i\n12 e-iv0t>28+ 0  +9y +\ni\n12 e+iv0t>28+ 0  -9yB @\n2\n \n \n(3.67)\n \n = @ A -i\n12 e-iv0t>2 A 1\n12B +\ni\n12 e+iv0t>2 A 1\n12B B @\n2\n \n \n = 1\n4 @-ie-iv0t>2 + ie+iv0t>2@\n2\n= 1\n4 @-2sin1v0t>22 @\n2\n \n \n = sin2\n 1v0t>22.\n \n \nThe probability oscillates between zero and unity as time evolves, as shown in Fig. 3.10(a), which \nis consistent with the model of the spin vector precessing around the applied ﬁeld, as shown in \nFig. 3.10(b).\nt\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ny\nx\nz\nΩ0t\nB\n(a)\n(b)\nP+→−\n\u0004S(t)\u0003\n\u0004S(0)\u0003\n2Π\nΩ0\n4Π\nΩ0\n6Π\nΩ0\nFIGURE 3.10 (a) Spin measurement probability and (b) the corresponding precession \nof the expectation value of the spin.\n\n84 \nSchrödinger Time Evolution\nThough we have derived Rabi’s formula [Eq. (3.63)] in the context of a spin-1/2 particle in a \nuniform magnetic ﬁeld, its applicability is much more general. If we can express the Hamiltonian \nof any two-state system in the matrix form of Eq. (3.51) with the parameters v0 and v1, then we can \nuse Rabi’s formula to ﬁnd the probability that the system starts in the “spin-up” state 0  +9 and is then \nmeasured to be in the “spin-down” state 0  -9 after some time t. In the general case, the 0  +9 and 0  -9 \nstates are whatever states of the system are used to represent the Hamiltonian operator in the form of \nEq. (3.51). In the next section, we’ll look at the example of neutrino oscillations to see how this exam-\nple can be applied more generally.\n3.3 \u0002 NEUTRINO OSCILLATIONS\nNeutrinos have enjoyed an almost mystical history in particle physics because they are very hard to \ndetect and yet play an important role in many fundamental processes. In 1930, the neutrino was pos-\ntulated by Wolfgang Pauli as a solution to the beta decay problem. A free neutron decays to a proton \nand an electron with a lifetime of about 10 minutes in the most basic beta decay process. However, the \ndecay scheme n S p + e- violates conservation of angular momentum, and experimental data sug-\ngest that conservation of energy is also violated. That’s not good. Rather than reject these two basic \nconservation laws, as some suggested, Pauli proposed that a third particle is involved in the decay \nprocess. Enrico Fermi named this new particle the “neutrino.” Fermi developed a theory that used the \nneutrino to properly explain beta decay, but it was 25 more years before a neutrino was detected.\nNeutrinos are uncharged, relativistic particles. In nuclear beta decay, neutrinos are produced in \nprocesses such as\n \nn S p + e- + ne  \n \np S n + e+ + ne, \n \n(3.68)\nwhere the subscript labels the neutrino ne as an electron neutrino and the bar labels ne as an antineu-\ntrino. In the standard model of particle physics, neutrinos are massless, like photons. Neutrinos are so \nelusive because they interact via the weak force or weak interaction, which is the weakest of the four \nfundamental forces—the strong nuclear force, electromagnetism, and gravity being the other three.\nThe reaction p S n + e+ + ne is part of the thermonuclear reaction chain in the sun and other \nstars, so we earthlings are constantly bombarded with neutrinos along with the essential photons we \nreceive from the sun. In the 1960s and 70s, landmark experiments indicated that there are only about \nhalf as many solar neutrinos arriving on earth as we would expect, given reliable models of stellar ther-\nmonuclear reactions. This solar neutrino problem has recently been solved by experiments detecting \nneutrinos from the sun and from nuclear reactors that demonstrate that neutrinos have nonzero mass. \nThese results are counter to the standard model and so have profound implications for particle physics \nand cosmology. Understanding how these experiments provide information on the neutrino mass is a \npowerful illustration of the applicability of Rabi’s formula to other two-state systems.\nIn addition to the electron neutrinos in Eq. (3.68), there are other types of neutrinos associated \nwith other reactions, such as\n \n p+ S m+ + nm\n \n \n m- S e- + nm + ne, \n \n(3.69)\nwhich represent the decay of a pion (p) to a muon (m) and the decay of a muon to an electron, respectively. \nA muon behaves exactly like an electron but has a larger mass. Electrons, muons, and a third particle \n(tau) and their associated neutrinos are collectively called leptons. In reactions involving these particles \n\n3.3 Neutrino Oscillations \n85\nit is convenient to define a lepton “ﬂavor” quantum number L, with the assigned values Le = 1 for the \nelectron e− and its associated neutrino ne, Le = -1 for the positron e+ and the antineutrino ne, Lm = 1\nfor the muon μ− and its associated neutrino nm, and Lm = -1 for the μ+ and nm. With these assignments, \nthe individual electron and muon ﬂavor numbers are conserved in the processes shown above. However, \nthere is no theoretical basis for this conservation, and so we allow for the possibility that these quantum \nnumbers are only approximately conserved. This possibility then allows for reactions of the type\n \nne 4 nm, \n(3.70)\nwhere an electron neutrino changes its ﬂavor and becomes a muon neutrino, or the reverse. Such \nchanges are called neutrino mixing or neutrino oscillations.\nThe labeling of neutrinos according to their association with electrons or muons arises from their behavior \nin the weak interaction processes described above. In other words, the quantum states 0 ne9 and 0 nm9 are \neigenstates of the Hamiltonian describing the weak interaction. However, when neutrinos propagate in \nfree space, the weak interaction is not relevant and the only Hamiltonian of relevance is that due to the \nrelativistic energy of the particles, which includes their rest masses and momenta. The eigenstates of this \nHamiltonian are generally referred to as the mass eigenstates. If the masses of the two types of neutrinos \n(electron and muon) are different, then, in general, the mass eigenstates do not coincide with the weak \ninteraction eigenstates. This distinction between sets of eigenstates allows for ﬂavor-changing processes.\nTo see why this is so, let the mass eigenstates be labeled 0 n19 and 0 n29. Either one of the two bases \n(mass or weak eigenstates) can be used as a complete basis upon which to expand any general state in \nthis system. Let’s assume that the relation between the bases is\n \n 0 ne9 = cos u\n2\n 0 n19 + sin u\n2\n 0 n29  \n \n 0 nm9 = sin u\n2\n 0 n19 - cos u\n2\n 0 n29. \n \n(3.71)\nThe angle u>2 is generally referred to as the mixing angle (some treatments drop the factor 1/2, but \nwe retain it to be consistent with the previous spin-1/2 discussion). If the mixing angle is small, then \nthe relations become\n \n 0 ne9 \u0003 0 n19  \n \n 0 nm9 \u0003 0 n29. \n \n(3.72)\nAssume that an electron neutrino is created in some weak interaction process and then propagates \nthrough free space to a detector. We wish to know the probability that a muon neutrino is detected, \nwhich is the signature of neutrino ﬂavor mixing. The initial state vector is\n \n 0 c1029 = 0 ne9\n \n \n = cos u\n2\n 0 n19 + sin u\n2\n 0 n29. \n \n(3.73)\nDuring the free-space propagation, the energy eigenstates of the system are the mass eigenstates \nbecause there is no weak interaction present. Thus the Schrödinger time evolution for this state is\n \n0 c1t29 = cos u\n2\n e-iE1t>U0 n19 + sin u\n2\n e-iE2t>U0 n29. \n(3.74)\nThe energy eigenvalues are simply the relativistic energies, which are determined by the rest masses \nand the momenta:\n \nEi = 41pc22 + 1mi c222,  i = 1, 2. \n(3.75)\n\n86 \nSchrödinger Time Evolution\nAssuming that the neutrinos are highly relativistic 1mc2 V pc2, we ﬁnd\n \n Ei = pc c1 + ami c2\npc b\n2\nd\n1>2\n \n \n \u0002 pc c1 + 1\n2\n amic2\npc b\n2\nd  \n \n(3.76)\n \n \u0002 pc + 1mi c222\n2pc\n.\n \nThe beauty of studying two-level systems such as spin-1/2 particles and neutrino oscillations is \nthat they are formally identical. In the spin-1/2 case, we phrased the problem in terms of ﬁnding the \nprobability of a spin ﬂip, whereas here we are looking for a change in the ﬂavor of the neutrino. In \nboth cases, the initial and ﬁnal states are not energy eigenstates, but rather orthogonal states in a dif-\nferent basis. The problems are mathematically identical, so the probability of a transition between the \northogonal states takes the same form. The probability of a neutrino oscillation is thus given by the \nsame equation as the spin-ﬂip probability, Eq. (3.62),\n \n PneSnm = 08nm0 c1t290\n2\n \n \n = sin2 u\n  sin2 a1E1 - E22t\n2U\nb, \n \n(3.77)\nwhere the parameter u has been deﬁned the same in both problems and the energy difference E+ - E- \nhas been changed to the energy difference E1 - E2. This energy difference is\n \n E1 - E2 = 1m1c22\n2\n2pc\n- 1m2c22\n2\n2pc\n \n \n = c3\n2p\n 1m2\n1 - m2\n22.\n \n \n(3.78)\nNeutrinos move at nearly the speed of light c, so we approximate the time from the creation of the \nelectron neutrino to the detection of the muon neutrino as t \u0002 L>c, where L is the distance from the \nsource to the detector. We also approximate the relativistic momentum as p = E>c. This gives a prob-\nability for neutrino ﬂavor change of\n \n PneSnm = sin2 u sin2 a1m2\n1 - m2\n22Lc3\n4E U\nb . \n(3.79)\nAs a function of the distance L, the probability oscillates from 0 to a maximum value of sin2 u—hence \nthe term neutrino oscillation. By measuring the fractions of different neutrino ﬂavors at a distance \nfrom a neutrino source (e.g., the sun or a reactor) and comparing to a model for the expected fractions, \nexperimenters have been able to infer the masses of the different neutrinos, or at least the differences \nof the squares of the masses. Recent results from solar neutrino and reactor neutrino experiments \nindicate a squared mass difference of approximately\n \nm2\n1 - m2\n2 \u0005 8 * 10-5 eV 2>c4. \n(3.80)\nThese experiments also provide information on the mixing angle u, with recent results indicating\n \nu \u0005 69\b. \n(3.81)\nNeutrino experiments such as these continue to provide information about the fundamental physics of \nthe universe.\n\n3.4 Time-Dependent Hamiltonians \n87\n3.4 \u0002 TIME-DEPENDENT HAMILTONIANS\nUp to now, we have studied the time evolution of quantum mechanical systems where the Hamiltonian \nis time independent. We solved the Schrödinger equation once for the general case and developed \na recipe for the time evolution of the system that we can apply to all cases with time-independent \nHamiltonians. However, if the Hamiltonian is time dependent, then we cannot use that simple recipe. \nWe must know the form of the Hamiltonian time dependence in order to solve the Schrödinger equa-\ntion. Fortunately, there are common forms of time dependence that we can solve in general and then \napply in many cases. The most common form of time dependence is sinusoidal time dependence at one \nfrequency. We will solve this problem in the context of a spin-1/2 particle in a magnetic ﬁeld and then \nalso apply it to atom-light interactions.\n3.4.1 \u0002 Magnetic Resonance\nIn the spin precession example in Section 3.2.2, we concluded that a complete spin ﬂip required a large \nmagnetic ﬁeld in the x-direction, which represents a large change or perturbation compared to the \ninitial situation of a magnetic ﬁeld in the z-direction. Now consider whether we can induce a complete \nspin ﬂip without such a large perturbation. That is, what small magnetic ﬁeld can we add to the system \nthat will cause a 0  +9 state to ﬂip to a 0  -9 state? The answer is that we must apply a time-dependent \nmagnetic ﬁeld that oscillates at a frequency close to the Larmor precession frequency v0 that charac-\nterizes the energy difference between the spin-up and spin-down states, as shown in Fig. 3.1. By mak-\ning the oscillating magnetic ﬁeld resonant with the Larmor frequency, we induce transitions between \nthe energy states shown in Fig. 3.1. This effect is known as magnetic resonance. I. I. Rabi won the \nNobel Prize in physics in 1944 for his work in developing the magnetic resonance technique and using \nit to measure the magnetic moments of nuclei. Following Rabi’s work, nuclear magnetic resonance \n(NMR) became a widely used tool for studying the properties of materials. The Larmor frequency \ndepends on the magnetic ﬁeld magnitude at the location of the particular nucleus being studied. This \nmagnetic ﬁeld includes the applied external ﬁeld and any internal ﬁelds created by the local environ-\nment, such that measuring the resonance frequency provides valuable information about the environ-\nment of the nucleus. In biology and chemistry, NMR has been used extensively to distinguish different \ntypes of bonds and identify structures. More recently, magnetic resonance imaging (MRI) has been \ndeveloped for medical diagnosis.\nTo understand how magnetic resonance works, it is instructive to consider the classical problem \nﬁrst. A classical magnetic moment aligned with an angular momentum precesses around the direc-\ntion of an applied magnetic ﬁeld. Now imagine going to a reference frame that rotates about the ﬁeld \n(assumed to be in the z-direction) with the same frequency as the precession. An observer in the rotat-\ning frame would see the magnetic moment stationary and so would conclude that there is no magnetic \nﬁeld in that frame. If that rotating observer were asked to ﬂip the magnetic moment from up to down \nalong the z-axis, she would answer, “Simple, just impose a small magnetic ﬁeld perpendicular to the \nz-axis, which will cause the spin to precess around that direction.” Because that ﬁeld is the only ﬁeld \nacting in the rotating frame, it can be as small as one likes. The magnitude simply determines the time \nfor the spin to ﬂip.\nIn this situation, the transverse applied ﬁeld is stationary in the rotating frame, so it will appear to \nbe rotating at the precessional frequency in the original frame. Thus, we could write it as\n \nB = B1 cos1vt2xn + B1 sin1vt2yn, \n(3.82)\nwhere we allow the frequency v to differ from the precessional frequency v0 in order to solve the \nproblem more generally. In that case, there would be some residual precession in the rotating frame, \nand so the rotating observer would conclude that there is some residual ﬁeld in the z-direction. Hence, \n\n88 \nSchrödinger Time Evolution\nwe expect that the added transverse ﬁeld would not cause a complete ﬂipping of the magnetic moment \nfrom up to down in this general case.\nLet’s now apply this reasoning to the quantum mechanical case. Assume a magnetic ﬁeld of the form\n \nB = B0zn + B13cos1vt2xn + sin1vt2yn4, \n(3.83)\nwhere the role of B0 is to split the energies of the spin-up and spin-down states and the role of B1 is to \nﬂip the spin between the the up and down states. The Hamiltonian is\n \n H = -M~B\n \n \n = v0 Sz + v13cos1vt2Sx + sin1vt2Sy4, \n \n(3.84)\nwhere we again deﬁne the Larmor frequencies corresponding to the two magnetic ﬁeld components,\n \nv0 K eB0\nme\n,   v1 K eB1\nme\n. \n(3.85)\nThe matrix representation of the Hamiltonian is\n \nH \u0003 U\n2\n ¢ v0\nv1e-ivt\nv1eivt\n-v0\n≤. \n(3.86)\nThis Hamiltonian is time dependent, so we can no longer use our simple recipe for Schrödinger \ntime evolution. Rather, we must return to the Schrödinger equation and solve it with these new time-\ndependent terms. Because we are not using our recipe for Schrödinger time evolution, we are not \nbound to use the energy basis as the preferred basis. The obvious choice would be to use the basis we \nhave used for representing the Hamiltonian as a matrix, which becomes the basis of energy states if the \ntransverse part B1 of the magnetic ﬁeld vanishes. Using this basis, we write the state vector as\n \n0 c1t29 = c+1t2 0  +9 + c-1t2 0  -9 \u0003  ¢c+1t2\nc-1t2≤. \n(3.87)\nSchrödinger’s equation\n \niU d\ndt\n 0\n c1t29 = H1t2 0\n c1t29 \n(3.88)\nin matrix form is\n \niU d\ndt\n ¢c+1t2\nc-1t2≤= U\n2\n ¢ v0\nv1e-ivt\nv1eivt\n-v0\n≤¢c+1t2\nc-1t2≤ \n(3.89)\nand leads to the differential equations\n \niUc#\n+ 1t2 = U v0\n2  c+1t2 + U v1\n2  e-ivtc-1t2 \n \niUc#\n- 1t2 = U v1\n2  eivt c+1t2 - U v0\n2  c-1t2, \n \n(3.90)\nwhere c#\n+1t2 denotes a time derivative. To solve these time-dependent coupled differential equations, \nit is useful to follow the lead of the classical discussion and consider the problem from the rotating \n\n3.4 Time-Dependent Hamiltonians \n89\nframe. Though we don’t yet have the complete tools to know how to effect this transformation, we take \nit on faith that after a frame transformation the state vector is\n \n0 c\u00031t29 = c+1t2eivt>2 0  +9 + c-1t2e-ivt>2 0  -9 \u0003 ¢ c+1t2eivt>2\nc-1t2e-ivt>2≤, \n(3.91)\nwhere 0 c\u00031t29 is the state vector as viewed from the rotating frame. If we call the coefﬁcients of this \nvector a{1t2, then we can write\n \n0 c\u00031t29 = a+1t2 0  +9 + a-1t2 0  -9 \u0003 ¢a+1t2\na-1t2≤, \n(3.92)\nwhere the relations between the sets of coefﬁcients are\n \n c+1t2 = e-ivt>2a+1t2 \n \n c-1t2 = eivt>2a-1t2. \n \n(3.93)\nThe state vector in the nonrotating frame can thus be written as\n \n0 c1t29 = a+1t2e-ivt>2 0  +9 + a-1t2eivt>2 0  -9 \u0003 ¢a+1t2e-ivt>2\na-1t2eivt>2 ≤. \n(3.94)\nAnother way of viewing this transformation is to say that based upon earlier solutions of similar \nproblems [Eq. (3.35)], we expect the coefﬁcients c{1t2 to have time dependence of the form e|ivt>2, \nand so we have extracted that part of the solution and now need to solve for the remaining time depen-\ndence in the coefﬁcients a{1t2. In this view, we have simply performed a mathematical trick to make \nthe solution easier.\nIf we now substitute the expressions for c{1t2 in terms of a{1t2 into the differential \nequations (3.90), then we obtain\n \n iUa#\n+1t2 = -  U\u0006v\n2\n a+1t2 + U v1\n2\n a-1t2 \n \n iUa#\n-1t2 = U v1\n2\n a+1t2 + U\u0006v\n2\n a-1t2,  \n \n(3.95)\nwhere we have deﬁned a new term\n \n\u0006v K v - v0, \n(3.96)\nwhich is the difference between the angular frequencies of the rotating ﬁeld and the Larmor preces-\nsion due to the z-component of the magnetic ﬁeld. Because a{1t2 are the coefﬁcients of the trans-\nformed state vector 0 c\u00031t29, these differential equations can be considered as comprising a transformed \nSchrödinger equation\n \niU d\ndt\n 0\n c\u00031t29 = H\u0003\n 0\n c\u00031t29, \n(3.97)\nwhere the new Hamiltonian H\u0003 has the matrix representation\n \nH\u0003 \u0003 U\n2\n a-\u0006v\nv1\nv1\n\u0006vb. \n(3.98)\n\n90 \nSchrödinger Time Evolution\nThus, we have transformed (by rotation or mathematical sleight of hand) the original problem \ninto a new problem that has a time-independent Hamiltonian. Once we solve the new problem, we can \nuse the transformation equations to ﬁnd the solution to the original problem. However, because the \nnew Hamiltonian H\u0003 is time independent, we already know the solution. That is, this new problem has \nthe same form of the Hamiltonian as the spin precession problem in Section 3.2.2. Comparing the spin \nprecession Hamiltonian in Eq. (3.51) with the transformed Hamiltonian in Eq. (3.98), we note that the \nterm v0 is replaced by the new term -\u0006v. We are interested in ﬁnding the same probability P+ S - that \nan initial 0  +9 state is later found to have evolved to the 0  -9 state. The rotational transformation does \nnot alter the 0{9 basis states so if\n \n0 c1029 = 0  +9, \n(3.99)\nthen\n \n0 c\u00031029 = 0  +9. \n(3.100)\nThe probability for a spin flip is given by\n \n P+ S - = 08- 0\n c1t290\n2 \n \n = 0 c-1t20\n2.\n \n \n(3.101)\nFrom Eq. (3.93) relating the coefﬁcients, we have\n \n 0 c-1t20\n2 = 0 e-ivt>2a-1t20\n2 \n \n = 0 a-1t20\n2\n \n \n(3.102)\n \n = 08- 0 c\u00031t290\n2,  \nwhich means that the probability we desire is\n \nP+ S - = 08- 0 c\u00031t290\n2. \n(3.103)\nWe obtain this spin-ﬂip probability using Rabi’s formula in Eq. (3.63), with the change v0 S -\u0006v, \nresulting in\n \n P+ S - =\nv2\n1\n\u0006v2 + v2\n1\n  sin2 ¢ 4\u0006v2 + v2\n1\n2\n t≤ \n \n =\nv2\n1\n1v - v022 + v2\n1\n  sin2 ¢41v - v022 + v2\n1\n2\n t≤. \n \n(3.104)\nThis spin-ﬂip probability is a generalization of Rabi’s formula. Note that Eq. (3.104) reduces to \nEq. (3.63) for the case v = 0, which is expected because the applied ﬁeld in Eq. (3.83) is static and \naligned the same as the static ﬁeld in Eq. (3.48) for the case v = 0. The static magnetic ﬁeld case is \ngenerally referred to as spin precession, while the rotating ﬁeld case is referred to as Rabi ﬂopping. \nThough we have used their similarities to help us derive Eq. (3.104), it is important to clarify their dif-\nferences. In the static applied magnetic ﬁeld case, the resulting spin precession is a manifestation of \nthe natural Bohr oscillation of a quantum system that starts in a superposition of energy eigenstates. \nThe initial superposition remains intact and there is no exchange of energy between the system and \nthe applied ﬁeld. In the rotating applied magnetic ﬁeld case, the Rabi ﬂopping represents transitions \nbetween energy eigenstates, and there is exchange of energy between the system and the applied ﬁeld. \nThe energy exchange occurs because the Hamiltonian is time dependent.\n\n3.4 Time-Dependent Hamiltonians \n91\nThe probability of a Rabi spin ﬂip oscillates with an angular frequency given by\n \n\t = 41v - v022 + v2\n1, \n(3.105)\nthat is typically referred to as the generalized Rabi frequency. The term Rabi frequency generally \nrefers to the frequency v1, which is the value of the generalized Rabi frequency when the frequency v \nof the rotating ﬁeld is on resonance (i.e., v is set equal to the Larmor precession frequency v0 of the \nsystem in the presence of the magnetic ﬁeld B0 alone). For this choice of v = v0, the probability of a \nspin ﬂip becomes\n \nP+ S - = sin2 av1\n2\n tb, \n(3.106)\nwhich implies that the spin is ﬂipped with 100% probability at an angular frequency v1. For other off-\nresonance choices of the frequency v, the probability of a spin ﬂip oscillates with an amplitude smaller \nthan one. The amplitude of the spin-ﬂip oscillation, as a function of the frequency v of the rotating \nﬁeld, is plotted in Fig. 3.11. This curve has the form of a Lorentzian curve and clearly exhibits the \nimportant resonant behavior of the spin-ﬂip probability. The full width at half maximum (FWHM) of \nthe resonance curve is 2v1.\nFor the resonance condition v = v0, the probability of a spin ﬂip as a function of time is plotted \nin Fig. 3.12. Because the frequency v1 is proportional to the applied ﬁeld B1, the rate of spin ﬂipping \nincreases with increasing rotating magnetic ﬁeld strength. However, it is important to note that there \nis still 100% probability of a spin ﬂip for very small ﬁelds. This is the property we were looking for at \nthe beginning of the problem—a way to ﬂip the spin without perturbing the system appreciably. After \na time t given by v1t = p, the probability for a spin ﬂip is 100%. We have assumed that the applied \nﬁeld is on continuously, but this spin ﬂip can also be produced by a pulsed ﬁeld with a magnitude and \nduration that satisfy v1t = p. Such a pulse is often called a P-pulse and is used to ﬂip a spin, or more \ngenerally to make a transition from one energy state to another with 100% certainty. The diagram on \nthe right of Fig. 3.12 illustrates the energy levels of the spin in the magnetic ﬁeld and how the spin-ﬂip \noscillations are associated with transitions between the two energy levels. A transition from the upper \nlevel to the lower level takes energy from the atom and gives it to the magnetic ﬁeld and is known as \nemission, while the opposite process takes energy from the ﬁeld and is known as absorption.\n0\n0.5\n1.0\n2Ω1\nΩ0\nΩ\nP+→−,max\nFIGURE 3.11 Magnetic resonance curve showing the probability \nof a spin ﬂip as a function of the applied  frequency.\n\n92 \nSchrödinger Time Evolution\n3.4.2 \u0002 Light-Matter Interactions\nThis same model of the interaction between a two-level system and an applied time-dependent ﬁeld is \nused to explain how atoms absorb and emit light. In the magnetic resonance example above, the oscil-\nlating magnetic ﬁeld interacts with the magnetic dipole and energy is exchanged between the ﬁeld and \nthe dipole. In the interaction of atoms with light, the oscillating electric ﬁeld of the light wave interacts \nwith the electric dipole of the atom, and energy exchange between the ﬁeld and the atom corresponds to \nabsorption and emission of photons. We can use the Rabi ﬂopping formula of Eq. (3.104) to model the \natom-light interaction as long as we express the Hamiltonian of the system in the form of Eq. (3.86). \nThough atoms have more than two energy levels, we can reduce the problem to a two-level system if \nthe frequency v of the applied light ﬁeld is close to just one of the Bohr frequencies of the atom.\nConsider two levels of an atom, as shown in Fig. 3.13. Following the convention used in this com-\nmon problem, we label the lower state 0 g9 (for ground state) and the upper state 0 e9 (for excited state). \nThe energy difference between the two levels is deﬁned to be\n \nEe - Eg = U v0 \n(3.107)\nto connect to the spin notation. The applied light ﬁeld (e.g., laser beam) has a frequency v that is close \nto, but not necessarily equal to, the atomic Bohr frequency v0. Using the same notation as the spin \nproblem [Eq. (3.86)], we express the Hamiltonian for this atom-light system in two parts\n \nH \u0003 U\n2\n a v0\nv1e-ivt\nv1eivt\n-v0\nb = U\n2\n av0\n0\n0\n-v0\nb + U\n2\n a\n0\nv1e-ivt\nv1eivt\n0\nb \n(3.108)\nEe\nEg\n\u0002Ω0\n\u0002Ω\n\u0002e\u0003\n\u0002g\u0003\nFIGURE 3.13 Energy level diagram of a two-level atom interacting with \nan applied light ﬁeld of frequency v.\nt\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nemission\nabsorption\nemission\nabsorption\nemission\nabsorption\nP+→−\nΠ\nΩ1\n2Π\nΩ1\n3Π\nΩ1\n4Π\nΩ1\n\u0002\u0004\u0003\n\u0002\u0005\u0003\nFIGURE 3.12 Rabi oscillations of the spin-ﬂip probability for the resonance condition.\n\nSummary \n93\nand identify the ﬁrst term as the atomic Hamiltonian and the second term as the interaction Hamilto-\nnian. In this way, we see that the parameter v1 is really an off-diagonal matrix element of the interac-\ntion Hamiltonian that connects the two states:\n \nv1 = 2\nU\n 8e0 Hint0 g9. \n(3.109)\nThe Rabi formula in Eq. (3.104) then gives the probability for the light ﬁeld to cause transitions \nbetween the two atomic energy states. Transitions between the atomic states correspond to absorption \n10 g9 S 0 e92 and emission 10 e9 S 0 g92 of photons in the light ﬁeld. Total energy is conserved as it is \nexchanged between the atom and the light ﬁeld.\nStudying these induced transitions is the most powerful tool we have for discovering what the \nenergy levels of a system are and ultimately for determining the Hamiltonian of the system. This \ntool is known as spectroscopy and has played a pivotal role in relating experiments and theory in \nquantum mechanics. As we encounter new quantum mechanical systems in this text, we will point \nout the spectroscopic aspects of these systems. For now, we can make a few general comments. If the \nmatrix element of the interaction Hamiltonian in Eq. (3.109) happens to be zero, then the transition \nprobability between the two levels is zero and we say that this is a forbidden transition. By studying \nthe general properties of the matrix elements 8e0 Hint0 g9 for a system and an interaction, we can dis-\ncover a set of basic rules governing whether transitions are allowed or forbidden. These are known as \nselection rules and are often representative of some underlying symmetry in the system. We will discuss \nselection rules brieﬂy as we encounter new systems and then will study them more fully in Chapter 14.\nSUMMARY\nIn this chapter we have learned the key aspect of quantum mechanics—how to predict the future. \nSchrödinger’s equation\n \niU d\ndt\n 0\n c1t29 = H1t20\n c1t29 \n(3.110)\ntells us how quantum state vectors evolve with time. In the common case where the Hamiltonian \nis time independent, the solution to Schrödinger’s equation has the same form no matter the problem. The \ntime-evolved state includes energy-dependent phase factors for each component of the superposition \nthat the system starts in:\n \n0 c1t29 = a\nn\ncne-iEnt>U0 En9. \n(3.111)\nThe general recipe for solving time-dependent problems is\nGiven a Hamiltonian H and an initial state 0 c1029, what is the probability that \nthe eigenvalue aj of the observable A is measured at time t?\n 1. Diagonalize H (ﬁnd the eigenvalues En and eigenvectors 0 En92.\n 2. Write 0 c1029 in terms of the energy eigenstates 0 En9.\n 3. Multiply each eigenstate coefﬁcient by e-iEnt>U to get 0 c1t29.\n 4. Calculate the probability Paj = 08aj0 c1t290\n2.\nWe will use this recipe throughout the rest of the book to study the time evolution of quantum mechan-\nical systems where the Hamiltonian is time independent.\n\n94 \nSchrödinger Time Evolution\nPROBLEMS\n 3.1 Write out the Schrödinger equation as expressed in Eq. (3.5) in matrix form for the two-state \nsystem and verify the result in Eq. (3.8).\n 3.2 Show that the probability of a measurement of the energy is time independent for a general state \n \n 0 c1t29 = a\nn\ncn1t2 0 En9 that evolves due to a time-independent Hamiltonian. Show that the \n \n probability of measurements of other observables are also time independent if those observables \ncommute with the Hamiltonian.\n 3.3 Show that the Hamiltonian in Eq. (3.51) can be written in the simple form of Eq. (3.56). \nDiagonalize the Hamiltonian in Eq. (3.55) and conﬁrm the results in Eq. (3.57).\n 3.4 Consider a spin-1/2 particle with a magnetic moment placed in a uniform magnetic ﬁeld \naligned with the z-axis. Verify by explicit matrix calculations that the Hamiltonian commutes \nwith the spin component operator in the z-direction but not with spin component operators in \nthe x- and y-directions. Comment on the relevance of these results to spin precession.\n 3.5 Consider a spin-1/2 particle with a magnetic moment. At time t = 0, the state of the particle is \n0 c1t = 029 = 0  +9.\na) If the observable Sx is measured at time t = 0, what are the possible results and the \nprobabilities of those results?\nb) Instead of performing the above measurement, the system is allowed to evolve in a uniform \nmagnetic ﬁeld B = B0yn. Calculate the state of the system (in the Sz basis) after a time t.\nc) At time t, the observable Sx is measured. What is the probability that a value U>2 will be \nfound?\nd) Draw a schematic diagram of the experiment in parts (b) and (c), similar to Fig. 3.2.\n 3.6 Consider a spin-1/2 particle with a magnetic moment.\na) At time t = 0, the observable Sx is measured, with the result U>2. What is the state vector \n0 c1t = 029 immediately after the measurement?\nb) Immediately after the measurement, a magnetic ﬁeld B = B0zn is applied and the particle is \nallowed to evolve for a time T. What is the state of the system at time t \u0003 T?\nc) At t = T, the magnetic ﬁeld is very rapidly changed to B = B0yn. After another time inter-\nval T, a measurement of Sx is carried out once more. What is the probability that a value U>2 \nis found?\n 3.7 A beam of identical neutral particles with spin 1/2 travels along the y-axis. The beam passes \nthrough a series of two Stern-Gerlach spin-analyzing magnets, each of which is designed to \nanalyze the spin component along the z-axis. The ﬁrst Stern-Gerlach analyzer allows only \nparticles with spin up (along the z-axis) to pass through. The second Stern-Gerlach analyzer \nallows only particles with spin down (along the z-axis) to pass through. The particles travel at \nspeed v between the two analyzers, which are separated by a region of length d in which there \nis a uniform magnetic ﬁeld B0 pointing in the x-direction. Determine the smallest value of d \nsuch that 25% of the particles transmitted by the ﬁrst analyzer are transmitted by the second \nanalyzer.\n 3.8 A beam of identical neutral particles with spin 1/2 is prepared in the 0  +9 state. The beam enters \na uniform magnetic ﬁeld B0, which is in the xz-plane and makes an angle u with the z-axis. \nAfter a time T in the ﬁeld, the beam enters a Stern-Gerlach analyzer oriented along the y-axis. \nWhat is the probability that particles will be measured to have spin up in the y-direction? Check \nyour result by evaluating the special cases u = 0 and u = p>2.\n\n 3.9 Consider a spin-1/2 particle with a magnetic moment. At time t = 0, the state of the particle is \n0 c1t = 029 = 0  +9n with the direction n = 1xn + yn2> 12. The system is allowed to evolve in \na uniform magnetic ﬁeld B = B0zn. What is the probability that the particle will be measured to \nhave spin up in the y-direction after a time t?\n 3.10 Consider a spin-1/2 particle with a magnetic moment. At time t = 0, the state of the \nparticle is 0 c1t = 029 = 0  +9. The system is allowed to evolve in a uniform magnetic ﬁeld \nB = B01xn + zn2> 12. What is the probability that the particle will be measured to have spin \ndown in the z-direction after a time t?\n 3.11 Consider a spin-1/2 particle with a magnetic moment. At time t = 0, the state of the particle is \n0 c1t = 029 = 0  +9n with the direction n = 1xn + yn2> 12. The system is allowed to evolve in \na uniform magnetic ﬁeld B = B01xn + zn2> 12. What is the probability that the particle will be \nmeasured to have spin up in the y-direction after a time t?\n 3.12 Consider a two-state quantum system with a Hamiltonian\nH \u0003 aE1\n0\n0\nE2\nb .\n \n Another physical observable A is described by the operator\nA \u0003 a0\na\na\n0b,\n \n where a is real and positive. Let the initial state of the system be 0 c1029 = 0 a19, where 0 a19 is \nthe eigenstate corresponding to the larger of the two possible eigenvalues of A. What is the \nfrequency of oscillation (i.e., the Bohr frequency) of the expectation value of A?\n 3.13 Let the matrix representation of the Hamiltonian of a three-state system be\nH \u0003 °\nE0\n0\nA\n0\nE1\n0\nA\n0\nE0\n ¢\n \n using the basis states 0 19, 0 29, and 0 39.\na) If the state of the system at time t = 0 is 0 c1029 = 0 29, what is the probability that the \nsystem is in state 0 29 at time t?\nb) If, instead, the state of the system at time t = 0 is 0 c1029 = 0 39, what is the probability that \nthe system is in state 0 39 at time t?\n 3.14 A quantum mechanical system starts out in the state\n0 c1029 = C130 a19 + 40 a292,\n \n where 0 ai9 are the normalized eigenstates of the operator A corresponding to the eigenvalues ai. \nIn this 0 ai9 basis, the Hamiltonian of this system is represented by the matrix\nH \u0003 E0 a2\n1\n1\n2b.\na) If you measure the energy of this system, what values are possible, and what are the \nprobabilities of measuring those values?\nb) Calculate the expectation value 8A9 of the observable A as a function of time.\nProblems \n95\n\n96 \nSchrödinger Time Evolution\n 3.15 Show that the general energy state superposition 0 c1t29 = a\nn\ncne-iEnt>U0 En9 satisﬁes the \n \n Schrödinger equation, but not the energy eigenvalue equation.\n 3.16 For a spin-1/2 system undergoing Rabi oscillations, assume that the resonance condition \nv = v0 holds.\na) Solve the differential equations for the coefﬁcients a{1t2. Use your results to ﬁnd the \ntransformed state vector 0 c\u00031t29 and the state vector 0 c1t29, assuming the most general \ninitial state of the system.\nb) Verify that a p-pulse (v1t = p) produces a complete spin ﬂip. Calculate both the \ntransformed state vector 0 c\u00031t29 and the state vector 0 c1t29.\nc) Assume that the interaction time is such that v1t = p>2. Find the effect on the system \nif the initial state is 0  +9.\nd) Discuss the differences between the original reference frame and the rotating reference \nframe in light of your results.\n 3.17 Consider an electron neutrino with an energy of 8 MeV. How far must this neutrino travel \nbefore it oscillates to a muon neutrino? Assume the neutrino mixing parameters given in the \ntext. How many complete oscillations (ne S nm S ne) will take place if this neutrino travels \nfrom the sun to the earth? Through the earth?\n 3.18 Many weak decay processes produce neutrinos with a spectrum of energies. Assume electron \nneutrinos are produced with a uniform distribution from 4 MeV to 8 MeV. By averaging the \nprobability over the energy spectrum, calculate and plot, as a function of the travel distance L, \nthe probability that electron neutrinos are measured at the detector. Compare the result with the \nprobability for monoenergetic neutrinos at 8 MeV. The integral required for the averaging does \nnot yield an elementary expression, so a computer is advisable. Assume the neutrino mixing \nparameters given in the text.\nRESOURCES\nActivities\nThis activity is available at\nwww.physics.oregonstate.edu/qmactivities\nSpins Lab 4: Students design experiments to study spin precession in a magnetic ﬁeld.\nFurther Reading\nPedagogical articles on neutrino oscillations:\nW. C. Haxton and B. R. Holstein, “Neutrino physics,” Am. J. Phys. 68, 15–32 (2000).\nW. C. Haxton and B. R. Holstein, “Neutrino physics: An update,” Am. J. Phys. 72, 18–24 (2004).\nE. Sassaroli, “Neutrino oscillations: A relativistic example of a two-level system,” Am. J. Phys. \n67, 869–875 (1999).\nC. Waltham, “Teaching neutrino oscillations,” Am. J. Phys. 72, 742–752 (2004).\nThe application of Rabi oscillations to atomic physics is the main focus of this book:\nL. Allen and J. H. Eberly, Optical Resonance and Two-Level Atoms, New York: Dover \nPublications, Inc., 1987.\n\n \n97\nC H A P T E R \n4\nQuantum Spookiness\nAs we have seen in the previous chapters, many aspects of quantum mechanics run counter to our \nphysical intuition, which is formed from our experience living in the classical world. The probabilistic \nnature of quantum mechanics does not agree with the certainty of the classical world—we have no \ndoubt that the sun will rise tomorrow. Moreover, the disturbance of a quantum mechanical system \nthrough the action of measurement makes us part of the system, rather than an independent observer. \nThese issues and others make us wonder what is really going on in the quantum world. As quantum \nmechanics was being developed in the early twentieth century, many of the world’s greatest physicists \ndebated the “true meaning” of quantum mechanics. They often developed gedanken experiments or \nthought experiments to illustrate their ideas. Some of these gedanken experiments have now actually \nbeen performed and some are still being pursued.\nIn this chapter, we present a few of the gedanken and real experiments that demonstrate the \nspookiness of quantum mechanics. We present enough details to give a ﬂavor of the spookiness and \nprovide references for further readings on these topics at the end of the chapter.\n4.1 \u0002  EINSTEIN-PODOLSKY-ROSEN PARADOX\nAlbert Einstein was never comfortable with quantum mechanics. He is famously quoted as saying \n“Gott würfelt nicht” or “God does not play dice,” to express his displeasure with the probabilistic \nnature of quantum mechanics. But his opposition to quantum mechanics ran deeper than that. He felt \nthat properties of physical objects have an objective reality independent of their measurement, much \nas Erwin felt that his socks were black or white, or long or short, independent of his pulling them out \nof the drawer. In quantum mechanics, we cannot say that a particle whose spin is measured to be up \nhad that property before the measurement. It may well have been in a superposition state. Moreover, \nwe can only know one spin component of a particle, because measurement of one component disturbs \nour knowledge of the other components. Because of these apparent deﬁciencies, Einstein believed that \nquantum mechanics was an incomplete description of reality.\nIn 1935, Einstein, Boris Podolsky, and Nathan Rosen published a paper presenting a gedan-\nken experiment designed to expose the shortcomings of quantum mechanics. The EPR Paradox \n(Einstein-Podolsky-Rosen) tries to paint quantum mechanics into a corner and expose the “absurd” \nbehavior of the theory. The essence of the argument is that if you believe that measurements on two \nwidely separated particles cannot inﬂuence each other, then the quantum mechanics of an ingeniously \nprepared two-particle system leads you to conclude that the physical properties of each particle are \nreally there—they are elements of reality in the authors’ words.\n\n98 \nQuantum Spookiness\nThe experimental situation is depicted in Fig. 4.1 (this version of the EPR experiment is due to \nDavid Bohm and has been updated by N. David Mermin). An unstable particle with spin 0 decays into \ntwo spin-1/2 particles, which by conservation of angular momentum must have opposite spin compo-\nnents and by conservation of linear momentum must travel in opposite directions. For example, a neu-\ntral pi meson decays into an electron and a positron: p0 S e- + e+. Observers A and B are on opposite \nsides of the decaying particle and each has a Stern-Gerlach apparatus to measure the spin component \nof the particle headed in its direction. Whenever one observer measures spin up along a given direc-\ntion, then the other observer measures spin down along that same direction. The quantum state of this \ntwo-particle system is\n \n0\n c9 =\n1\n12 10  +91 0  -92 - 0  -91 0  +922 , \n(4.1)\nwhere the subscripts label the particles and the relative minus sign ensures that this is a spin-0 state \n(as we’ll discover in Chapter 11). The use of a product of kets 1e.g., 0  +91 0  -922 is required here to \ndescribe the two-particle system (Problem 4.1). The kets and operators for the two particles are inde-\npendent, so, for example, operators act only on their own kets\n \nS1z0  +91 0  -92 = 1S1z0  +912 0  -92 = + U\n2\n 0  +91 0  -92, \n(4.2)\nand inner products behave as\n \n118+ 0 2 8-  0210  +91 0  -922 = 118+\n 0  +912128-  0  -922 = 1. \n(4.3)\nAs shown in Fig. 4.1, observer A measures the spin component of particle 1 and observer B mea-\nsures the spin component of particle 2. The probability that observer A measures particle 1 to be spin \nup is 50% and the probability for spin down is 50%. The 50-50 split is the same for observer B. For a \nlarge ensemble of decays, each observer records a random sequence of spin up and spin down results, \nwith a 50>50 ratio. But, because of the correlation between the spin components of the two particles, \nif observer A measures spin up (i.e., S1z = +U>2), then we can predict with 100% certainty that the \nresult of observer B’s measurement will be spin down (S2z = -U>2). The result is that even though \neach observer records a random sequence of ups and downs, the two sets of results are perfectly anticor-\nrelated. The state 0\n c9 in Eq. (4.1) that produces this strange mixture of random and correlated measure-\nment results is known as an entangled state. The spins of the two particles are entangled with each \nother and produce this perfect correlation between the measurements of observer A and observer B.\nImagine that the two observers are separated by a large distance, with observer B slightly farther \nfrom the decay source than observer A. Once observer A has made the measurement S1z = +U>2, we \nknow that the measurement by observer B in the next instant will be spin down 1S2 z = -U>22. We con-\nclude that the state 0\n c9 in Eq. (4.1) instantaneously collapses onto the state 0  +91 0  -92 , and the measure-\nment by observer A has somehow determined the measurement result of observer B. Einstein referred \nto this as “spooky action at a distance” (spukhafte Fernwirkungen). The result that observer B records is \nstill random, it is just that its randomness is perfectly anticorrelated with observer A’s random result. \nA\nB\nParticle 1 \nParticle 2 \nSpin 0\nSource \nS2z\nS1z\nFIGURE 4.1 Einstein-Podolsky-Rosen gedanken experiment.\n\n4.1 Einstein-Podolsky-Rosen Paradox \n99\nHence, there is no problem with faster-than-light communication here because there is no information \ntransmitted between the two observers.\nThe EPR argument contends that because we can predict a measurement result with 100% cer-\ntainty 1e.g., S2z = -U>22, then that result must be a “real” property of the particle—it must be an ele-\nment of reality. Because the particles are widely separated, this element of reality must be independent \nof what observer A does, and hence, must have existed all along. The independence of the elements of \nreality of the two  particles is called Einstein’s locality principle, and is a fundamental assumption of \nthe EPR argument.\nThe correlation of spin measurements of the two observers is independent of the choice of mea-\nsurement direction, assuming the same direction for both observers. That is, if observer A measures \nthe x-component of spin and records S1x = +U>2, then we know with 100% certainty that observer B \nwill measure S2x = -U>2. Observer A is free to choose to measure S1x, S1y, or S1z, so EPR argue that \nS2x, S2y, and S2z must all be elements of reality for particle 2. However, quantum mechanics maintains \nthat we can know only one spin component at a time for a single particle. EPR conclude that quantum \nmechanics is an incomplete description of physical reality because it does not describe all the elements \nof reality of the particle.\nIf the EPR argument is correct, then the elements of reality, which are also called hidden vari-\nables or instruction sets, are really there, but for some reason we cannot know all of them at once. \nThus, one can imagine constructing a local hidden variable theory wherein there are different types of \nparticles with different instruction sets that determine the results of measurements. The theory is local \nbecause the instruction sets are local to each particle so that measurements by the two observers are \nindependent. The populations or probabilities of the different instruction sets can be properly adjusted \nin a local hidden variable theory to produce results consistent with quantum mechanics. Because quan-\ntum mechanics and a local hidden variable theory cannot be distinguished by experiment, the question \nof which is correct is then left to the realm of metaphysics. For many years, this was what many physi-\ncists believed. After all, it doesn’t seem unreasonable to believe that there are things we cannot know!\nHowever, in 1964, John Bell showed that the hidden variables that we cannot know cannot even \nbe there! Bell showed that there are speciﬁc measurements that can be made to distinguish between a \nlocal hidden variable theory and quantum mechanics. The results of these quantum mechanics experi-\nments are not compatible with any local hidden variable theory. Bell derived a very general relation, \nbut we present a speciﬁc one here for simplicity.\nBell’s argument relies on observers A and B making measurements along a set of different direc-\ntions. Consider three directions an, bn, cn in a plane as shown in Fig. 4.2, each 120° from any of the other \ntwo. Each observer makes measurements of the spin projection along one of these three directions, \nchosen randomly. Any single observer’s result can be only spin up or spin down along that direction, \nbut we record the results independent of the direction of the Stern-Gerlach analyzers, so we denote \none observer’s result simply as  + or  -, without noting the axis of measurement. The results of the pair \nParticle 1 \nParticle 2 \na∧\nb\n∧\nc∧\nA\nB\nSpin 0\nSource \na∧\nb\n∧\nc∧\nFIGURE 4.2 Measurement of spin components along three directions as proposed by Bell.\n\n100 \nQuantum Spookiness\nof measurements from one correlated pair of particles (i.e., one decay from the source) are denoted \n+ -, for example, which means observer A recorded a + and observer B recorded a -. There are only \nfour possible system results: + +,  + -,  - +, or  - -. Even more simply, we classify the results as \neither the same, + + or  - -, or opposite, + - or  - +.\nA local hidden variable theory needs a set of instructions for each particle that speciﬁes ahead \nof time what the results of measurements along the three directions an, bn, cn will be. For example, the \ninstruction set 1an  +, bn  +, cn +2 means that a measurement along any one of the three directions will \nproduce a spin up result. For the entangled state of the system given by Eq. (4.1), measurements by the \ntwo observers along the same direction can yield only the results + - or  - +. To reproduce this aspect \nof the data, a local hidden variable theory would need the eight instruction sets shown in Table 4.1. For \nexample, the instruction set 1an  +, bn  -, cn +2 for particle 1 must be paired with the set 1an  -, bn  +, cn -2 for \nparticle 2 in order to produce the proper correlations of the entangled state. Beyond that requirement, \nwe allow the proponent of the local hidden variable theory freedom to adjust the populations Ni (or \nprobabilities) of the different instruction sets as needed to make sure that the hidden variable theory \nagrees with the quantum mechanical results.\nNow use the instruction sets (i.e., the local hidden variable theory) to calculate the prob-\nability that the results of the spin component measurements are the same 1Psame = P+ + + P- -2 \nand the probability that the results are opposite 1Popp = P+ - + P+ -2, considering all possible \norientations of the spin measurement devices. There are nine different combinations of measure-\nment directions for the pair of observers: anan, anbn, ancn, bnan, bnbn, bncn, cnan, cnbn, cncn. If we consider particles \nof type 1 (i.e., instruction set 1), then for each of these nine possibilities, the results are opposite \n(+ -). The results are never the same for particles of type 1. The same argument holds for type \n8 particles. For type 2 particles, the instruction sets 1an  +, bn  +, cn -2 and 1an  -, bn  -, cn +2 yield the\nnine possible results + -,  + -,  + +,  + -,  + -,  + +,  - -,  - -,  - + with four possibilities of \nrecording the same results and ﬁve possibilities for recording opposite results. Thus, we arrive at the \nfollowing probabilities for the different particle types:\n \nPopp = 1\nPsame = 0 r types 1 & 8 \n \nPopp = 5\n9\nPsame = 4\n9\nt types 2 S 7 . \n \n(4.4)\nTable 4.1 Instruction Sets (Hidden Variables)\nPopulation\nParticle 1\nParticle 2\nN1\nN2\nN3\nN4\nN5\nN6\nN7\nN8\n1an\n  +, bn\n +, cn\n +2\n1an\n  +, bn\n +, cn\n -2\n1an\n  +, bn\n -, cn\n +2\n1an\n  +, bn\n -, cn\n -2\n1an\n  -, bn\n +, cn\n +2\n1an\n  -, bn\n +, cn\n -2\n1an\n  -, bn\n -, cn\n +2\n1an\n  -, bn\n -, cn\n -2\n1an\n  -, bn\n -, cn\n -2\n1an\n  -, bn\n -, cn\n +2\n1an\n  -, bn\n +, cn\n -2\n1an\n  -, bn\n +, cn\n +2\n1an\n  +, bn\n -, cn\n -2\n1an\n  +, bn\n -, cn\n +2\n1an\n  +, bn\n +, cn\n -2\n1an\n  +, bn\n +, cn\n +2\n\n4.1 Einstein-Podolsky-Rosen Paradox \n101\nTo ﬁnd the probabilities of recording the same or opposite results in all the measurements, we \nperform a weighted average over all the possible particle types. The weight of any particular particle \ntype, for example type 1, is simply N1  \u0006aNi (recall we will adjust the actual values later as needed). \nThus, the averaged probabilities are:\n \n Psame =\n1\na\ni\nNi\n 4\n9\n 1N2 + N3 + N4 + N5 + N6 + N72 … 4\n9\n \n \n Popp =\n1\na\ni\nNi\n aN1 + N8 + 5\n9\n 1N2 + N3 + N4 + N5 + N6 + N72b Ú 5\n9\n ,  \n(4.5)\nwhere the inequalities follow because the sum of all the weights for the different particle types must \nbe unity. In summary, we can adjust the populations all we want, but that will always produce prob-\nabilities of the same or opposite measurements that are bound by the above inequalities. That is what \nis meant by a Bell inequality.\nWhat does quantum mechanics predict for these probabilities? For this system of two spin-1/2 \nparticles, we can calculate the probabilities using the concepts from the previous chapters. Assume \nthat observer A records a “+” along some direction (of the three). Deﬁne that direction as the z-axis \n(no law against that). Observer B measures along a direction nn at some angle u with respect to the \nz-axis. The probability that observer A records a “+” along the z-axis and observer B records a “+” \nalong the nn direction is\n \nP+ + = 0118+ 0   n   \n2n 8+ 02 0\n c9 0\n2 . \n(4.6)\nSubstituting the entangled state 0\n c9 and the direction eigenstate 0  +9nn  gives\n \nP+ + = 2 18+ 0 ¢cos u\n2  28+ 0 + e-if sin u\n2  28- 0 ≤ 1\n12 10  +91 0  -92 - 0  -91 0  +922 2\n2\n \n \n= 2 1\n12 ¢cos u\n2  28+ 0 + e-if sin u\n2  28- 0 ≤10  -922 2\n2\n \n \n= 1\n2\n sin2  u\n2\n . \n \n(4.7)\nThe same result is obtained for the probability that observer A records a “-” along the z-axis and \nobserver B records a “-” along the nn direction. Hence, the result for the same measurements is\n \nPsame = P+ + + P- - = sin2 u\n2\n . \n(4.8)\nThe probability that observer B records a “-” along the direction nn, when A records a “+” is\n \nP+ - = 0118+ 0     n \n2n 8- 02 0\n c9 0\n2 \n \n= 2 18+ 0 ¢sin u\n2  28+ 0 - e-if cos u\n2  28- 0 ≤ 1\n12 10  +91 0  -92 - 0  -91 0  +922 2\n2\n \n \n= 2 1\n12 ¢sin u\n2  28+ 0 - e-if cos u\n2  28- 0 ≤ 1 0  -922 2\n2\n \n \n= 1\n2\n  cos2  u\n2\n , \n \n(4.9)\n\n102 \nQuantum Spookiness\nand the probability for opposite results is\n \nPopp = P+  - + P-  + = cos2 u\n2\n . \n(4.10)\nThe angle u between the measurement directions of observers A and B is 0° in 1>3 of the mea-\nsurements and 120° in 2>3 of the measurements, so the average probabilities are\n \nPsame = 1\n3 ~ sin2 0\b\n2 + 2\n3 ~ sin2 120\b\n2\n= 1\n3 ~ 0 + 2\n3 ~ 3\n4 = 1\n2  \n \nPopp = 1\n3 ~ cos2 0\b\n2 + 2\n3 ~ cos2 120\b\n2\n= 1\n3 ~ 1 + 2\n3 ~ 1\n4 = 1\n2\n . \n \n(4.11)\nThese predictions of quantum mechanics are inconsistent with the range of possibilities that we \nderived for local hidden variable theories in Eq. (4.5). Because these probabilities can be measured, \nwe can do experiments to test whether local hidden variable theories are possible. The results of exper-\niments performed on systems that produce entangled quantum states have consistently agreed with \nquantum mechanics and hence, exclude the possibility of local hidden variable theories. We are forced \nto conclude that quantum mechanics is an inherently nonlocal theory.\nThe EPR paradox also raises issues regarding the collapse of the quantum state and how a mea-\nsurement by A can instantaneously alter the quantum state at B. However, there is no information \ntransmitted instantaneously and so there is no violation of relativity. What observer B measures is not \naffected by any measurements that A makes. The two observers notice only when they get together \nand compare results that some of the measurements (along the same axes) are correlated.\nThe entangled states of the EPR paradox have truly nonclassical behavior and so appear spooky \nto our classically trained minds. But when you are given lemons, make lemonade. Modern quantum \nresearchers are now using the spookiness of the entangled states to enable new technologies that take \nadvantage of the way that quantum mechanics stores information in these correlated systems. Quan-\ntum computers, quantum communication, and quantum information processing in general are active \nareas of research and promise to enable a new revolution in information technology.\n 4.2 \u0002 SCHRÖDINGER CAT PARADOX\nThe Schrödinger cat paradox is a gedanken experiment designed by Schrödinger to illustrate some of \nthe problems of quantum measurement, particularly in the extension of quantum mechanics to classi-\ncal systems. The apparatus of Schrödinger’s gedanken experiment consists of a radioactive nucleus, a \nGeiger counter, a hammer, a bottle of cyanide gas, a cat, and a box, as shown in Fig. 4.3. The nucleus \nhas a 50% probability of decaying in one hour. The components are assembled such that when the \nnucleus decays, it triggers the Geiger counter, which causes the hammer to break the bottle and release \nthe poisonous gas, killing the cat. Thus, after one hour there is a 50% probability that the cat is dead.\nAfter the one hour, the nucleus is in an equal superposition of undecayed and decayed states:\n \n0 cnucleus9 =\n1\n12 10 cundecayed9 + 0\n cdecayed92. \n(4.12)\nThe apparatus is designed such that there is a one-to-one correspondence between the undecayed \nnuclear state and the live-cat state and a one-to-one correspondence between the decayed nuclear state \nand the dead-cat state. Though the cat is macroscopic, it is made up of microscopic particles and so \nshould be describable by a quantum state, albeit a complicated one. Thus, we expect that the quantum \nstate of the cat after one hour is\n \n0 ccat9 =\n1\n12 10 calive9 + 0 cdead92. \n(4.13)\n\n4.2 Schrödinger Cat Paradox \n103\nBoth quantum calculations and classical reasoning would predict 50>50 probabilities of observ-\ning an alive or a dead cat when we open the box. However, quantum mechanics would lead us to \nbelieve that the cat was neither dead nor alive before we opened the box, but rather was in a super-\nposition of states, and the quantum state collapses to the alive state 0 calive9 or dead state 0 cdead9 only \nwhen we open the box and make the measurement by observing the cat. But our classical experiences \nclearly run counter to this. We would say that the cat really was dead or alive, we just did not know \nit yet. (Imagine that the cat is wearing a cyanide sensitive watch—the time will tell us when the cat \nwas killed, if it is dead!)\nWhy are we so troubled by a cat in a superposition state? After all, we have just ﬁnished three \nchapters of electrons in superposition states! What is so inherently different about cats and electrons? \nExperiment 4 that we studied in Chapters 1 and 2 provides a clue. The superposition state in that \nexperiment exhibits a clear interference effect that relies on the coherent phase relationship between \nthe two parts of the superposition state vector for the spin-1/2 particle. No one has ever observed such \nan interference effect with cats, so our gut feeling that cats and electrons are different appears justiﬁed.\nThe main issues raised by the Schrödinger cat gedanken experiment are (1) Can we describe mac-\nroscopic states quantum mechanically? and (2) What causes the collapse of the wave function?\nThe Copenhagen interpretation of quantum mechanics championed by Bohr and Heisenberg \nmaintains that there is a boundary between the classical and quantum worlds. We describe micro-\nscopic systems (the nucleus) with quantum states and macroscopic systems (the cat, or even the Gei-\nger counter) with classical rules. The measurement apparatus causes the quantum state to collapse and \nto produce the single classical or meter result. The actual mechanism for the collapse of the wave func-\ntion is not speciﬁed in the Copenhagen interpretation, and where to draw the line between the classical \nand the quantum world is not clear. Others have argued that the human consciousness is responsible \nfor collapsing the wave function, while some have argued that there is no collapse, just bifurcation into \nalternate, independent universes. Many of these different points of view are untestable experimentally \nand thus raise more metaphysical than physical questions.\nThese debates about the interpretation of quantum mechanics arise when we use words, which \nare based on our classical experiences, to describe the quantum world. The mathematics of quantum \nNucleus\nCyanide\nGeiger Counter\nCat\nFIGURE 4.3 Schrödinger cat gedanken experiment.\n\n104 \nQuantum Spookiness\nmechanics is clear and allows us to calculate precisely. No one is disagreeing about the probability \nthat the cat will live or die. The disagreement is all about “what it really means!” To steer us toward \nthe clear mathematics, Richard Feynman admonished us to “Shut up and calculate!” Two physicists \nwho disagree on the words they use to describe a quantum mechanical experiment generally agree on \nthe mathematical description of the results.\nRecent advances in experimental techniques have allowed experiments to probe the boundary \nbetween the classical and quantum worlds and address the quantum measurement issues raised by \nthe Schrödinger cat paradox. The coupling between the microscopic nucleus and the macroscopic \ncat is representative of a quantum measurement whereby a classical meter (the cat) provides a clear \nand unambiguous measurement of the state of the quantum system (the nucleus). In this case, the two \npossible states of the nucleus (undecayed or decayed) are measured by the two possible positions on \nthe meter (cat alive or cat dead). The quantum mechanical description of this complete system is the \nentangled state\n \n0 csystem9 =\n1\n12 10 cundecayed9 0 calive9 + 0 cdecayed9 0 cdead92. \n(4.14)\nThe main issue to be addressed by experiment is whether Eq. (4.14) is the proper quantum mechanical \ndescription of the system. That is, is the system in a coherent quantum mechanical superposition, as \ndescribed by Eq. (4.14), or is the system in a 50>50 statistical mixed state of the two possibilities? As \ndiscussed above, we can distinguish these two cases by looking for interference between the two states \nof the system.\nTo build a Schrödinger cat experiment, researchers use a two-state atom as the quantum system \nand an electromagnetic ﬁeld in a cavity as the classical meter (or cat). The atom can either be in the \nground 0\n g9 or excited 0\n e9 state. The cavity is engineered to be in a coherent state 0\n a9 described \nby the complex number a, whose magnitude is equal to the square root of the average number of \nphotons in the cavity. For large a, the coherent state is equivalent to a classical electromagnetic \nﬁeld, but for small a, the ﬁeld appears more quantum mechanical. The beauty of this experiment is \nthat the experimenters can tune the value of a between these limits to study the region between the \nmicroscopic and macroscopic descriptions of the meter (cat). In this intermediate range, the meter is \na mesoscopic system.\nAtoms travel through the cavity and disturb the electromagnetic ﬁeld in the cavity. Each atom is \nmodeled as having an index of refraction that alters the phase of the electromagnetic ﬁeld. The sys-\ntem is engineered such that the ground and excited atomic states produce opposite phase shifts {f. \nBefore the atom enters the cavity, it undergoes a p-pulse that places it in an equal superposition of \nground and excited states\n \n0 catom9 =\n1\n12 10\n e9 + 0\n g92, \n(4.15)\nas shown in Fig. 4.4. Each component of this superposition produces a different phase shift in the \ncavity ﬁeld such that after the atom passes through the cavity, the atom-cavity system is in the entan-\ngled state\n \n0 catom +cavity9 =\n1\n12 10\n e9 0 ae if9 + 0\n g9 0 ae -if92 \n(4.16)\nthat mirrors the Schrödinger cat state in Eq. (4.14). The state of the cavity ﬁeld is probed by sending \na second atom into the cavity and looking for interference effects in the atom that are produced by the \ntwo components of the ﬁeld. In this experiment, the two ﬁeld states are classically distinguishable, \nakin to the alive and dead cat states. For small values of the phase difference 2f between the two ﬁeld \ncomponents, the interference effect is evident. However, for large values of the phase difference 2f \n\nProblems \n105\nbetween the two ﬁeld components, the interference effect vanishes, indicating that the superposition \nstate in Eq. (4.16) has lost the ﬁxed phase relationship between the two parts of the entangled state and \ncan no longer produce interference effects. The system has undergone decoherence due to its interac-\ntion with the random aspects of the environment. The decoherence effect also increases as the number \nof photons in the cavity ﬁeld increases, which makes the cavity ﬁeld more like a classical state. Hence, \nthe experiment demonstrates that the quantum coherence of a superposition state is rapidly lost when \nthe state becomes complex enough to be considered classical. Further details on this recent experiment \nare available in the references below (Brune et al.).\nPROBLEMS\n 4.1 Show that the quantum state vector of a two-particle system must be a product 0\n c910\n f92 of \ntwo single-particle state vectors rather than a sum 0\n c91 + 0\n f92. Hint: consider the action of a \nsingle-particle state operator on the two-particle state vector.\n 4.2 Consider the two-particle entangled state\n0 c9 =\n1\n12 10  +91 0  -92 - 0  -91 0  +922.\na) Show that 0\n c9 is not an eigenstate of the spin component operator S1z for particle 1.\nb) Show that 0\n c9 is properly normalized.\n 4.3 Consider the two-particle entangled state\n0 c9 =\n1\n12 10  +91 0  -92 - 0  -91 0  +922.\n \n Show that the probability of observer A measuring particle 1 to have spin up is 50% for any \norientation of the Stern-Gerlach detector used by observer A. To ﬁnd this probability, sum over \nall the joint probabilities for observer A to measure spin up and observer B to measure anything.\n 4.4 Show that the state\n0 ca9 =\n1\n12 10  +91 0  -92 - 0  -91 0  +922\n \n is equivalent to the state\n0 cb9 =\n1\n12 10  +91x 0  -92  x - 0  -91x 0  +92  x2.\n \n That is, the two observers record perfect anticorrelations independent of the orientation of their \ndetectors, as long as both are aligned along the same direction.\nAtom Source\nΠ\u00062 Pulse\nCavity\n1\n2 \u0002g\u0003+\u0002e\u0003\n\u0002g\u0003\nFIGURE 4.4 Schrödinger cat experiment with atoms in a cavity.\n\n106 \nQuantum Spookiness\n 4.5 Calculate the quantum mechanical probabilities in Eqs. (4.7) and (4.9) without assuming that \nobserver A’s Stern-Gerlach device is aligned with the z-axis. Let the direction of observer A’s \nmeasurements be described by the angle u1 and the direction of observer B’s measurements be \ndescribed by the angle u2. Show that the averaged results in Eq. (4.11) are still obtained.\nRESOURCES\nFurther Reading\nThe EPR Paradox and Bell’s theorem are discussed in these articles:\nF. Laloe, “Do we really understand quantum mechanics? Strange correlations, paradoxes, \nand  theorems,” Am. J. Phys. 69, 655–701 (2001); “Erratum: Do we really understand \nquantum mechanics? Strange correlations, paradoxes, and theorems,” Am. J. Phys. 70, \n556 (2002).\nN. D. Mermin, “Bringing home the atomic world: Quantum mysteries for anybody,” Am. J. \nPhys. 49, 940–943 (1981).\nN. D. Mermin, “Is the moon there when nobody looks? Reality and the quantum theory,” \nPhys. Today 38(5), 38–47 (1985).\nN. D. Mermin, “Quantum mysteries revisited,” Am. J. Phys. 58, 731–734 (1990).\nN. D. Mermin, “Not quite so simply no hidden variables,” Am. J. Phys. 60, 25–27 (1992).\nN. D. Mermin, “Quantum mysteries reﬁned,” Am. J. Phys. 62, 880–887 (1994).\nN. D. Mermin, “Nonlocal character of quantum theory?” Am. J. Phys. 66, 920–924 (1998).\nN. D. Mermin, “What is quantum mechanics trying to tell us?” Am. J. Phys. 66, 753–767 \n(1998).\nSchrödinger’s cat is discussed in these references:\nT. J. Axon, “Introducing Schrodinger’s cat in the laboratory,” Am. J. Phys. 57, 317–321 (1989).\nM. Brune, E. Hagley, J. Dreyer, X. MaÓtre, A. Maali, C. Wunderlich, J. M. Raimond, and  \nS. Haroche, “Observing the progressive decoherence of the ‘meter’ in a quantum \nmeasurement,” Phys. Rev. Lett. 77, 4887–4890 (1996).\nB. S. DeWitt, “Quantum mechanics and reality,” Phys. Today 23(9), 30–35 (1970).\nA. J. Legett, “Schrodinger’s cat and her laboratory cousins,” Contemp. Phys. 25, 583–598 \n(1984). \nJ. G. Loeser, “Three perspectives on Schrodinger’s cat,” Am. J. Phys. 52, 1089–1093 (1984).\nW. H. Zurek, “Decoherence and the transition from quantum to classical,” Phys. Today \n44(10), 36–44 (1991).\nRichard Feynman’s directive to “Shut up and calculate!” is discussed in: \nN. D. Mermin, “What’s wrong with this pillow?” Phys. Today 42(4), 9–11 (1989).\nN. D. Mermin, “Could Feynman have said this?” Phys. Today 57(5), 10–11 (2004).\n\n \n107\nC H A P T E R \n5\nQuantized Energies:  \nParticle in a Box\nIn the ﬁrst part of this book we used the spin system to illustrate the basic concepts and tools of quan-\ntum mechanics. With a ﬁrm foundation in how quantum mechanics works, we are ready to address the \ncentral question that quantum mechanics was designed to answer: How do we explain the structure of \nthe microscopic world? All around us are nuclei, atoms, molecules, and solids with unique properties \nthat cannot be explained with classical physics but require quantum mechanics. For example, quantum \nmechanics can tell us why sodium lamps are yellow, why laser diodes have a unique color, and why \nuranium is radioactive.\nThe key to understanding the structure of microscopic systems lies in the energy states that the \nsystems are allowed to have. Each microscopic system has a unique set of energy levels that gives that \nsystem a “ﬁngerprint” that sets it apart from other systems. With the tools of quantum mechanics, we \ncan build a theoretical model for the system, predict that ﬁngerprint, and compare it to the experimen-\ntal measurement. Our goal in this chapter and the ones that follow is to learn how to predict this energy \nﬁngerprint. In this chapter we will study a particularly simple model system that exhibits most of the \nimportant features that are shared by all microscopic systems.\n5.1 \u0002 SPECTROSCOPY\nThe energy ﬁngerprint of a system not only identiﬁes that system uniquely, but the allowed energies \ndetermine the time evolution of the system through the Schrödinger equation, as we learned in Chapter 3. \nOne of the primary experimental techniques for measuring the energy ﬁngerprint of a system is spectros-\ncopy. We saw a hint of this in the magnetic resonance example of Section 3.4: absorption and emission of \nphotons causes transitions between quantized energy levels of the system only when the photon energy \nmatches the spacing between the energy eigenstates. Historically, the spectrum of hydrogen was a key \ningredient in the development of quantum mechanics, and spectroscopy continues to play an important \nrole in characterizing new quantum systems and in verifying the rules of quantum mechanics.\nIn the magnetic resonance example of Section 3.4, the two quantized energy levels arose from the \ntwo possible spin components (up or down) and their different interactions with an applied magnetic \nﬁeld. The more common situation that gives rise to quantized energy levels is where two or more \nparticles interact in a way that limits their spatial motion and binds them together into a compos-\nite system. Bound systems such as nuclei, atoms, molecules, and solids are everyday examples that \nare characterized by distinct spectral lines associated with quantized energy states, (i.e., eigenstates \nof the Hamiltonian with discrete energy eigenvalues). For example, the hydrogen atom energy levels \n\n108 \nQuantized Energies: Particle in a Box\nand the corresponding optical spectrum are shown in Fig. 5.1. The spectral lines appear when elec-\ntrons make transitions between energy levels. Downward transitions emit photons and give rise to an \nemission spectrum, while upward transitions absorb photons and yield an absorption spectrum. For \nevery pair of energy eigenvalues Ei and Ej  , there is a possible spectral line with photon energy Ei - Ej\n , \nand photon frequency fij and wavelength \nij given by\n \nfij =\nvij\nU =\nEi - Ej\nh\n \n \nlij = c\nfij\n=\nhc\nEi - Ej\n , \n(5.1)\nassuming that Ei 7 Ej\n . The set of spectral lines of atomic hydrogen that share a common lower level \nforms a series that is named after its discoverer. The ﬁrst three series in hydrogen are shown in Fig. 5.1 \nand listed in Table 5.1. The lowest energy state (n \u0003 1 for hydrogen) is called the ground state, and \nthe levels above that are called excited states. Though the word spectrum often refers to the observed \noptical lines, the set of quantized energy states is also commonly referred to as the energy spectrum \nof the system.\n1\n2\n3\n4\n5\nn\nLyman\nBalmer\nPaschen\n0\n\u00051\n\u00052\n\u00053\n\u00054\n\u00055\n\u00056\n\u00057\n\u00058\n\u00059\n\u000510\n\u000511\n\u000512\n\u000513\n\u000514\nE\nf\nΛ\n\b\nEnergy (eV)\nFIGURE 5.1 Hydrogen energy levels and the corresponding optical spectrum as a function \nof energy,  frequency, and wavelength (the wavelength scale is not a linear scale).\n\n5.1 Spectroscopy \n109\nA spectroscopy experiment can be considered to be a measurement of the energy of a quantum \nstate. A spectroscopic energy measurement is depicted in Fig. 5.2(a) in a simpliﬁed schematic that is \nanalogous to the Stern-Gerlach spin measurement we discussed earlier. A system is prepared in an \n initial state 0 c9, and we measure the probability that the state is measured to have a particular energy \nEi. If we write the energy eigenstates as 0\n Ei9, then the probability of a particular energy measurement is\n \nPEi = 08Ei0 c90\n2. \n(5.2)\nAs we did in the spins problem, we represent the collection of measurements on an ensemble of iden-\ntical states as a histogram, as shown in Fig. 5.2(b). In a real spectroscopy experiment, the measured \nenergies are really energy differences between levels, so it can be a bit of a puzzle to decode the energy \nlevels from the observed spectrum. We assume that this decoding process can be done and we assume \nthat the histogram in Fig. 5.2(b) faithfully represents the energy levels of the system. The energy levels \nEi and the eigenstates 0\n Ei9 are solutions to the energy eigenvalue equation\n \nHn  0 Ei9 = Ei0\n Ei9, \n(5.3)\nso the spectroscopic measurement is how the theoretical Hamiltonian is compared with experiment. \nOur task in this chapter is to learn how to predict the allowed energy eigenstates of a particular system \ngiven the Hamiltonian of the system.\n)\nb\n(\n)\na\n(\nPE1\nE1\n2\nPEi\nPE2\nPE3\nPE4\nPE5\nE5\n2\nE\nH\nE1\nE2\nE3\nE4\nE5\nE1\n2\nE4\n2\nE5\n2\nE1 E2\nE3\nE4\nE5\nE3\n2\nE2\n2\nE4\n2\nE2\n2\nE3\n2\nFIGURE 5.2 (a) Energy measurement and (b) histogram of results.\nTable 5.1 Hydrogen Transition Wavelengths\nFinal state\nInitial state\nSeries\n2\n3\n4\n5\n1\n122 nm\n103 nm\n97 nm\n95 nm\nLyman\n2\n656 nm\n486 nm\n434 nm\nBalmer\n3\n1875 nm\n1282 nm\nPaschen\n\n110 \nQuantized Energies: Particle in a Box\n5.2 \u0002 ENERGY EIGENVALUE EQUATION\nIn classical mechanics, we often solve problems by using Newton’s second law F = ma to predict the \nposition r1t2 of a particle subject to some known forces. Another common method is the energy method, \nwhereby we use conservation of energy and the relation E = T + V between the total energy (E ) \nand the kinetic (T ) and potential (V ) energies to predict the motion. Of course, the two methods are \nrelated because the force is related to the potential energy by\n \nFx = -  dV\ndx  \n(5.4)\nin one dimension. Hence the potential energy function V(x) is what determines the classical motion of \na particle.\nThe potential energy is also the key element in quantum mechanics, because of the important role \nit plays in the Hamiltonian of the system in question. The Hamiltonian determines the energy states \nthrough the energy eigenvalue equation\n \nHn 0\n Ei9 = Ei0\n Ei9. \n(5.5)\nNote that many other textbooks refer to Eq. (5.5) as the time-independent Schrödinger equation \nbecause it can be derived from the Schrödinger equation by separating the time and space parts; how-\never, we refer to it always as the energy eigenvalue equation. The prescription for ﬁnding a quantum \nmechanical Hamiltonian operator is to ﬁnd the classical form of the energy and replace the physical \nobservables with their quantum mechanical operators. For a moving particle, the classical mechanical \nenergy is the sum of the kinetic energy and the potential energy, which in one dimension is\n \nE =\np2\nx\n2m + V1x2. \n(5.6)\nWe use the position x and momentum p as the primary physical observables in quantum mechanics, \nfollowing the Hamiltonian approach to classical mechanics. Hence the quantum mechanical Hamilto-\nnian operator for a particle moving in one dimension is\n \nHn =\npn 2\nx\n2m + V1xn2. \n(5.7)\nWe use carets or hats on operators on occasion to distinguish them from the same symbol used as a \nvariable. If the distinction is clear from the context, then that notation may be dropped.\nSo now what? What are these new operators xn and pn for position and momentum? And how do \nwe use them to solve the energy eigenvalue equation? In the spins chapters, we learned much of the \nmachinery of quantum mechanics and would rightly expect to be able to use it in this new problem \non particle motion. However, position and momentum are different enough from spin that we need to \nredevelop some of the mathematical machinery we have already learned.\nWhen we discussed spin quantum states, we either used abstract kets, such as 0  +9 or 0  -9x, or we \nused column vectors to represent the abstract kets in a particular basis of eigenstates. For example, we \noften used the eigenstates of the Sz operator as the preferred basis, in which case the abstract kets 0  +9 \nand 0  -9x are expressed as\n \n0  +9 \u0003 a1\n0b \n(5.8)\n\n5.2 Energy Eigenvalue Equation \n111\nand\n \n0  -9x \u0003\n1\n12\n a 1\n-1b. \n(5.9)\nIn fact, there are very few quantum mechanical problems that can be solved using abstract kets. It is \ngenerally necessary to use a representation of the kets that is convenient for solving the problem. In \nthe problems that we wish to address now, it is most convenient to represent abstract quantum states as \nspatial functions, so we need to explain what that means.\nThe spatial functions we use to represent quantum states are called wave functions and are gener-\nally written using the Greek letter c as\n \nc 1x2. \n(5.10)\nThe wave function is a representation of the abstract quantum state, so we can use our representation \nnotation to write\n \n0\n c9 \u0003 c 1x2. \n(5.11)\nWe call this representation the position representation, which means that we are using the position \neigenstates as the preferred basis (more on these eigenstates later). For clarity, we will use the Greek \nletter c when referring to generic quantum states and other Greek letters to denote speciﬁc eigenstates. \nFor example, in the case of the energy eigenstates, we write the wave functions representing them as\n \n0\n Ei9 \u0003 wEi1x2 \n(5.12)\nto distinguish them as speciﬁc eigenstates.\nUsing this new wave function notation, the energy eigenvalue equation Eq. (5.5) becomes\n \nHnwEi1x2 = Ei\n wEi1x2. \n(5.13)\nTo solve this equation, we must know how to represent the operators in the Hamiltonian of Eq. (5.7) \nusing the position representation. It turns out that in the position representation, the action of the posi-\ntion operator xn is represented by multiplication by the position variable x, while the action of the \nmomentum operator pn is represented by application of a derivative with respect to position (see an \nadvanced text for justiﬁcation or take these as postulates). Using our representation notation, these two \nstatements are\n \n xn \u0003 x\n \n \n pn \u0003 -iU d\ndx  . \n \n(5.14)\nThe momentum operator has a factor of -iU to get the dimensions correct and to ensure that the mea-\nsurable results are real (not imaginary).\nWith these representations of the position and momentum operators, we now begin to solve the \nenergy eigenvalue equation. Inserting Eq. (5.14) into the energy eigenvalue equation gives\n \n HnwEi1x2 = Ei\n wEi1x2  \n \n a pn 2\n2m + V 1xn2\n b wEi1x2 = Ei\n wEi1x2  \n(5.15)\n \n a 1\n2m a-iU d\ndxb\n2\n+ V 1x2\n b wEi1x2 = Ei\n wEi1x2.\n\n112 \nQuantized Energies: Particle in a Box\nThe result is that the energy eigenvalue equation becomes a differential equation\n \na-  U2\n2m\n d 2\ndx2 + V1x2\n b wE1x2 = EwE1x2  . \n(5.16)\nThis differential equation is a big change from the matrix eigenvalue equations we encountered in the \nspin problems. This result is a common occurrence when using the wave function approach: operator \nequations turn into differential equations. Hence, when we use the wave function approach to ﬁnd \nthe allowed energy eigenstates of a system, we typically solve differential equations. We will solve \nthis differential equation for several different potential energy functions V1x2 in the remainder of this \nbook, but ﬁrst we pause to examine the wave function idea more carefully.\n5.3 \u0002 THE WAVE FUNCTION\nTo better understand the new concept of a wave function c1x2, let’s see how it relates to the quantum \nstate vector 0\n c9 we used in spins. In the spin case, we found that a useful way to represent a state vec-\ntor was as a column vector of numbers, with each number being the probability amplitude for the state \n0\n c9 to be measured in a particular spin eigenstate. For example, we could write the state 0\n c9 using the \nSz representation as\n \n0\n c9 \u0003 ¢8+ 0\n c9\n8- 0\n c9≤ d Sz = +U>2\n d Sz = -U>2. \n(5.17)\nThe numbers 8{0\n c9 in the column vector are the projections of the state vector 0\n c9 onto the Sz \neigenstates 0{9, corresponding to the two possible eigenvalues. If we measure the spin projection, as \ndepicted in Fig. 5.3(a), then the amplitudes 8{0\n c9 are used to calculate the probabilities\n \nP{ = 08{ 0\n c90\n2 \n(5.18)\nshown in the histogram in Fig. 5.3(b).\nIf we now consider an energy measurement, such as depicted in Fig. 5.2(a), then the basis of \nenergy eigenstates is the appropriate basis for representing the state vector:\n \n0\n c9 \u0003 •\n8E1@ c9\n8E2@ c9\n8E3@c9\nf\nμ \nd E = E1\nd E = E2\nd E = E3.\nf\n  \n(5.19)\nP\n2\n2\n(a)\n(b)\n1\nP\nZ\n2\n2\nSz\nP\n2\n2\nFIGURE 5.3 (a) Spin measurement and (b) probability histogram.\n\nIn such an energy measurement, the probabilities shown in Fig. 5.2(b) are calculated using the pro-\njections 8Ei0\n c9 of the state 0\n c9 onto the energy eigenstates 0\n Ei9. The probabilities of measuring the \nquantized energies are\n \nPEi = 08Ei 0\n c90\n2\n . \n(5.20)\nIn analogy to these two examples, the wave function is a representation of a quantum state using \nthe eigenstates of the position operator xn as the basis states. If we call the position eigenstates 0\n xi9, \nthen the analog to Eqs. (5.17) and (5.19) would be\n \n0\n c9 \u0003 •\n8x1@ c9\n8x2@ c9\n8x3@ c9\nf\nμ \nd x1\nd x2\nd x3 ,\nf\n  \n(5.21)\nwhere the projection 8xi0 c9 is  the probability amplitude for the state 0 c9 to be measured in the posi-\ntion eigenstate 0\n xi9. However, experiment tells us that the physical observable x is not quantized. \nRather, all values of position x are allowed. This is in stark contrast to the case of the spin component \nSz, where only two results were possible. We say that the spectrum of eigenvalues of position is con-\ntinuous and the spectrum of eigenvalues of spin is discrete. Future experiments may shed new light on \nthis, but to date, space appears to be continuous. “Discrete vs. continuous” is an important distinction \nthat affects how we use and interpret the quantum state vector, the probability amplitudes, and the \nprobabilities when position is the relevant quantum mechanical observable.\nFor a continuous variable like position, the column vector representation of Eq. (5.21) is not con-\nvenient because we cannot write down the inﬁnite number of components. Even if the number were \nﬁnite but large, say 100, then we would ﬁnd a column vector cumbersome. Instead, we might choose to \nrepresent the 100 discrete numbers 8xi0 c9 as points in a graph, such as shown in Fig. 5.4(a). However, \nbecause the position spectrum is continuous, there is an inﬁnite continuum of the probability ampli-\ntudes 8x0 c9, and the natural way to represent such a continuous set of numbers is as a continuous func-\ntion, as shown in Fig. 5.4(b). This function is what we call the quantum mechanical wave  function c1x2. \nThe wave function is the collection of numbers that represents the quantum state vector in terms of the \nposition eigenstates, in the same way that the column vector used to represent a general spin state is a \ncollection of numbers that represents the quantum state vector in terms of the spin eigenstates. Whether \nyou write the wave function as c1x2 or as 8x0 c9 is ultimately a matter of taste. It is more common to \n(a)\n(b)\nx\nx\n\u0004x1\u0002Ψ\u0003\n\u0004x2\u0002Ψ\u0003\n\u0004x3\u0002Ψ\u0003\n\u0004x\u0002Ψ\u0003\nΨ(x)\nΨ(x)\n\u0004x4\u0002Ψ\u0003\u0004x5\u0002Ψ\u0003\n\u0004x6\u0002Ψ\u0003\n\u0004x7\u0002Ψ\u0003\n\u0004x8\u0002Ψ\u0003\n\u0004x9\u0002Ψ\u0003\n\u0004x10\u0002Ψ\u0003\nx1 x2 x3 x4 x5 x6 x7 x8 x9 x10\nFIGURE 5.4 (a) Discrete basis representation and (b) continuous basis representation.\n5.3 The Wave Function \n113\n\n114 \nQuantized Energies: Particle in a Box\nsee the form c1x2 used as the wave function, and we will follow that convention mostly, using the \nDirac notation when convenient. But it is important to remember both forms, so we repeat them here:\n \nc1x2 = 8x0 c9  . \n(5.22)\nIn words, we say that the wave function c1x2 is the probability amplitude for the quantum state 0 c9 to be \nmeasured in the position eigenstate 0 x9. We will say more about the position eigenstates in Chapter 6 and \nthen also make more connections between the wave function language and the Dirac bra-ket notation.\nContinuing with the analogy to the spin and energy examples above, we expect that the prob-\nability of measuring a particular value of position is obtained by taking the absolute square of the \nprojection 8x0 c9, as was done in Eqs. (5.18) and (5.20) for spin and energy representations. However, \nbecause the projection 8x0 c9 is the continuous wave function c1x2, the absolute square yields a con-\ntinuous probability function (actually a probability density, as we’ll ﬁnd in a moment), which we write \nas P1x2 so as to distinguish it from the discrete case Ae.g. PSz=+U>2B by making x an argument rather than \na subscript. In wave function notation, this new probability function is\n \nP1x2 = 0 c1x20\n2  . \n(5.23)\nThus, given a wave function c1x2, such as shown in Fig. 5.5(a), we use Eq. (5.23) to calculate the prob-\nability function P1x2, which is shown in Fig. 5.5(b). The probability function in Fig. 5.5(b) is analogous \nto the histograms of discrete probabilities in Figs. 5.2(b) and 5.3(b). We must stress that measuring the \nprobability function P1x2 does not allow us to infer the wave function c1x2. We saw in the spin measure-\nments of Chapters 1 and 2 that measurements of three different observables, Sx, Sy, and Sz, were required \nto deduce the state vector 0 c9 because the probability amplitudes are complex numbers. The relative \nphases between the probability amplitudes are not accessible from measurement of a single observable.\nHaving a continuous function for the probability rather than a set of discrete values raises some \nimportant issues. In quantum mechanics we require that the sum of all possible probabilities be equal \nto unity (i.e., the state vector must be normalized). In the discrete spins case this meant that:\n \na\n{\nP{ = a\n{\n08{0 c90\n2 = 1. \n(5.24)\nIf position were discrete instead of continuous, then the normalization condition would be:\n \na\nn\nPxn = a\nn\n08xn0 c90\n2 = 1. \n(5.25)\n(a)\n(b)\nx\nP(x)\nΨ(x)\nx\nFIGURE 5.5 (a) Wave function and (b) corresponding probability density.\n\n5.3 The Wave Function \n115\nHowever, because the spectrum of position eigenvalues is continuous rather than discrete, the sum \nover discrete probabilities must be changed to an integral over the continuous probability function \nP1x2, with the requisite differential term dx added. For now, we restrict the discussion to one spatial \ndimension. Thus the normalization condition is\n \nL\n\u0005\n- \u0005\nP1x2dx =\n \nL\n\u0005\n- \u0005\n0 c1x20\n2\n dx = 1. \n(5.26)\nThe differential dx has dimensions of length and the total integrated probability must be dimension-\nless, so the probability function P1x2 must have dimensions of inverse length. This means that P1x2 is \na probability density (in one dimension a probability per unit length) rather than a probability. Hence \nwe interpret the quantity\n \nP1x2dx \n(5.27)\nas the inﬁnitesimal probability of detecting a particle at position x within an inﬁnitesimal region of \nwidth dx [i.e., between x and x + dx, as shown in Fig. 5.6(a)]. To calculate the probability that a par-\nticle is measured to be in a ﬁnite interval a 6 x 6 b, we add all the inﬁnitesimal probabilities in that \ninterval, which is the integral\n \nPa6x6b =\n \nL\nb\na\n0 c1x20\n2\n dx \n(5.28)\nas depicted in Fig. 5.6(b). Equation (5.28) is an incredibly important formula. We use it, for example, \nto ﬁnd the probability that an electron is in a certain region of an atom (extended to three dimensions, \nof course).\nTo calculate other experimental quantities, such as expectation values, we must learn how to trans-\nlate bra-ket rules for discrete basis systems to wave function rules for continuous basis systems. We can \nlearn some rules for this translation by comparing the new wave function form of the normalization \ncondition in Eq. (5.26) to the bra-ket normalization condition. In Dirac notation, the requirement of \nprobability normalization is expressed in terms of the inner product of the state vector with itself:\n \n8c0 c9 = 1. \n(5.29)\nRewrite the wave function normalization condition Eq. (5.26) to make it look more like the bra-ket form:\n \nL\n\u0005\n- \u0005\nc*1x2c1x2dx = 1. \n(5.30)\n(a)\n(b)\nx x + dx\na\nb\nP(x)\nP(x)\nx\nx\nFIGURE 5.6 Probability for measuring a particle to be in the position range (a) x to x + dx, and (b) a to b.\n\n116 \nQuantized Energies: Particle in a Box\nComparing Eq. (5.29) and Eq. (5.30), we postulate the following rules for translating bra-ket formulae \nto wave function formulae:\n 1) Replace ket with wave function \n0 c9 S c1x2\n 2) Replace bra with wave function conjugate \n8c 0 S c*1x2\n 3) Replace bracket with integral over all space \n8 0 9 S\nL\n\u0005\n- \u0005\n dx\n 4) Replace operator with position representation An S A1x2\nwhere we have added a rule about operators that will become obvious in a moment.\nExample 5.1 Normalize the wave function\n \nc1x2 = Ce-a0 x -  20. \n(5.31)\nUse Eq. (5.26) for the normalization condition and integrate over all space\n \n 1 =\n \nL\n\u0005\n- \u0005\n0 c1x20\n2\n dx\n \n \n =\n \nL\n\u0005\n- \u0005\n@ Ce-a0 x -  2@ 0\n2\n dx\n \n \n =\n \nL\n\u0005\n- \u0005\n0 C0\n2 e-2a0 x -  20  dx. \n(5.32)\nBreak the integral into two pieces to remove the absolute value:\n \n 1 =\n \nL\n2\n- \u0005\n0 C0\n2 e2a1x -  22 dx +\n \nL\n\u0005\n2\n0 C0\n2 e-2a1x -  22 dx \n \n = J 0 C0 2\n2a\n e2a 1x -  22 R\n2\n- \u0005\n+ J 0 C0 2\n-2a\n e-2a 1x  -22 R\n\u0005\n2\n \n \n = 0 C0\n2\na\n .\n \n(5.33)\nOnce again, we have freedom to choose the overall phase, so we let C be real and positive:\n \nC = 1a \n(5.34)\ngiving the normalized wave function\n \nc1x2 = 1a e\n -  a0 x -  2 0 . \n(5.35)\nUsing the rules for translating bra-ket notation to wave function notation, a general state vector \nprojection or probability amplitude expressed in wave function language is\n \n8f0 c9 =\n \nL\n\u0005\n- \u0005\nf*1x2c1x2dx . \n(5.36)\n\n5.3 The Wave Function \n117\nThe square of this probability amplitude is the probability that the state c1x2 is measured to be in the \nstate f1x2\n \nPcSw = 08w0 c90\n2 = 2\nL\n\u0005\n- \u0005\nw*1x2cx1x2dx 2\n2   \n . \n(5.37)\nTechnically, we should say that this is the probability that the system prepared in state c1x2 is measured \nto have the physical observable for which f1x2 is the eigenstate, because we measure observables, not \nstates. But the looser language is common and does not create any ambiguity in the calculation. If we \nmeasure the energy, for example, then the probability of obtaining the result En is\n \nPEn = 08En0 c90\n2 = 2\nL\n\u0005\n- \u0005\nw*\nn1x2c1x2dx 2\n2\n, \n(5.38)\nwhere wn1x2 is the energy eigenstate with energy En. Note that Eq. (5.28) and Eq. (5.37) look simi-\nlar but have important differences. In Eq. (5.28) we integrate the probability density (wave function \ncomplex squared) over a ﬁnite range of position in order to sum the probabilities of measuring many \ndifferent positions. In Eq. (5.37) we integrate the product of two wave functions over all space to deter-\nmine their mutual overlap, and then we complex square that result to get the probability of measuring \na single result.\nTo transform an expectation value to wave function language, we must consider the operator. The \nexpectation value of an observable A is the matrix element of the operator\n \n8An9 = 8c0 An 0 c9. \n(5.39)\nIf we rewrite the expectation value as\n \n8An9 = 8c 05An0 c96, \n(5.40)\nwe see that it is an inner product where one ket has been transformed by the operator An. To write this \nin terms of wave functions, we must make sure to use the position representation form of the operator. \nFor example, the position operator xn in the position representation is simply multiplication by the sca-\nlar position x. Using the translation rules to write the expectation value of the position in wave function \nnotation yields\n \n 8xn9 = 8c0 xn 0 c9\n \n \n =\n \nL\n\u0005\n- \u0005\nc*1x2x c1x2dx \n \n =\n \nL\n\u0005\n- \u0005\nx0 c1x20\n2 dx,\n \n \n(5.41)\nwhere we have used the fact that scalar multiplication is commutative. For the expectation value of the \nmomentum, we ﬁnd\n \n 8pn9 = 8c0 pn 0 c9\n \n \n =\n \nL\n\u0005\n- \u0005\nc*1x2a-iU d\ndxb c1x2dx, \n \n(5.42)\n\n118 \nQuantized Energies: Particle in a Box\nwhich cannot be simpliﬁed more without knowing the wave function. In the next section, we will solve \nthe energy eigenvalue equation for a speciﬁc potential energy to allow us to calculate these expectation \nvalues explicitly.\nExample 5.2 Consider the wave function from Example 5.1:\n \nc1x2 = 1a e\n -  a0 x -  2 0 . \n(5.43)\nCalculate the expectation value of the position and the probability that the particle is measured to \nbe in the interval 4 6 x 6 6.\nThe expectation value of position is given by Eq. (5.41)\n \n 8xn9 =\n \nL\n\u0005\n- \u0005\nx0 c1x20\n2\n dx\n \n \n =\n \nL\n\u0005\n- \u0005\nx 11ae -a 0 x -  202\n2\n dx\n \n \n = a\nL\n\u0005\n- \u0005\nxe -2a 0 x-\n 20  dx\n \n \n = a\nL\n2\n- \u0005\nxe2a 1x-22 dx + a\nL\n\u0005\n2\nxe-2a 1x-22 dx\n \n(5.44)\n \n = a Je2a 1x-22 1-1 + 2ax2\n4a2\nR\n2\n- \u0005\n+ a Je-2a 1x-22 1-1 - 2ax2\n4a2\nR\n\u0005\n2\n \n \n = a J1-1 + 4a2\n4a2\n- 0 + 0 - 1-1 - 4a2\n4a2\nR\n \n \n = 2.\n \nThis is what you expect based upon the plot of wave function shown in Fig. 5.7(a) and the probabil-\nity density in Fig. 5.7(b), which are symmetric about the point x = 2.\n(a)\n(b)\n\u00022\n0\n2\n4\n6\n\u00022\n0\n2\n4\n6\nP(x)\n(x)\nx\nx\nFIGURE 5.7 (a) Wave function and (b) corresponding probability density. The hatched region \nin (b) represents the probability for the particle to be measured in the region 4 6 x 6 6.\n\n5.4 Inﬁnite Square Well \n119\nTo calculate the probability of ﬁnding the particle in the interval, use Eq. (5.28)\n \n P46x66 =\n \nL\n6\n4\n2 1a e -a 0 x -\n 20 2\n2\n dx \n \n =\n \nL\n6\n4\nae -2a1x-22 dx\n \n(5.45)\n \n = c a\n-2a\n e -2a 1x-22 d\n6\n4\n \n \n = e -4a\n2\n 31 - e -4a4 .\n \nThis probability is shown as the hatched region in Fig. 5.7(b). The actual value of the probability \ndepends on the value of the parameter a.\n5.4 \u0002 INFINITE SQUARE WELL\nOur task now is to solve the energy eigenvalue equation, which we found to be a differential equation\n \na-  U2\n2m d 2\ndx2 + V 1x2\n b wE 1x2 = E wE 1x2. \n(5.46)\nAs you might expect, the solutions to this differential equation depend critically on the functional \ndependence of the potential energy V1x2. A generic potential energy function is depicted in Fig. 5.8 \nin a potential energy diagram that illustrates some important aspects of the motion of the particle. \nMost of the interesting systems to which we will apply Eq. (5.46) resemble the potential energy func-\ntion depicted in Fig. 5.8 in that V1x2 has a minimum, so we refer to the potential energy function as a \nx\nE1\nX1\nX2\nEnergy\nT \u0002\u0003E \u0004\u0003V(x)\nV(x)\nClassical\nturning points\nClassically allowed region\nClassically\nforbidden region\nClassically\nforbidden region\nFIGURE 5.8 A generic potential energy well.\n\n120 \nQuantized Energies: Particle in a Box\npotential well. The particle energy is conserved, so the kinetic energy T1x2 = E - V1x2 is illustrated \nin the potential energy diagram by the vertical arrow between the ﬁxed energy E1 and the potential \nenergy V1x2. For a classical particle, the kinetic energy cannot be negative, so a classical particle with \nthe energy E1 chosen in Fig. 5.8 has its motion constrained to the region between x1 and x2. These \nextreme points of the classical motion are called classical turning points and the region within the \nturning points is called the classically allowed region, while the regions beyond are called classically \nforbidden regions. Particles that have their motion constrained by the potential well are said to be \nin bound states. Particles with energies above the top of the potential well do not have their motion \nconstrained and so are in unbound states. Note that the extent of the classically forbidden and allowed \nregions depends on the speciﬁc value of the energy, E1, for a particular bound state.\nSolving Eq. (5.46) for various important potential energy functions is the subject of this and later \nchapters. In this chapter, our goal is to study a simple potential energy system and learn the mathemat-\nics required for this new wave function approach.\nWe begin our journey to energy quantization with the simplest example of a particle that is con-\nﬁned to a region of space. The classical picture is a super ball bouncing between two perfectly elastic \nwalls. We call this system a particle in a box. We observe three important characteristics of this \nclassical system: (1) the ball ﬂies freely between the walls, (2) the ball is reﬂected perfectly at each \nbounce, and (3) the ball remains in the box no matter how large its energy. These three observations \nare consistent with (1) zero force on the ball when it is between the walls, (2) inﬁnite force on the ball \nat the walls, and (3) inﬁnite potential energy outside the box.\nThe mathematical model that is consistent with these three observations of the motion of a par-\nticle in a box is given by the potential energy function shown in Fig. 5.9. The potential energy is zero \nwithin the well (any constant would sufﬁce, but we choose zero for simplicity), and it is inﬁnite out-\nside the well. The discontinuity at the sides of the well requires us to write the potential energy func-\ntion in a piecewise fashion\n \nV1x2 = •\n\u0005,\n0 ,\n\u0005,   \nx 6 0\n0 6 x 6 L\nx 7 L.\n \n(5.47)\nBecause of the shape of the potential energy in Fig. 5.9, this system is also referred to as an inﬁnite \nsquare well. Though this model is too simple to accurately represent any real quantum mechanical \nsystem, it does illustrate most of the important features of a particle bound to a limited region of space.\n0\nL/2\nL\nx\nV(x)\n\u0005\nFIGURE 5.9 Inﬁnite square potential energy well.\n\n5.4 Inﬁnite Square Well \n121\nOur goal is to ﬁnd the energy eigenstates and eigenvalues of the system by solving the energy \neigenvalue equation using the potential energy in Eq. (5.47). The potential energy is piecewise, so we \nmust solve the differential equation (5.46) separately inside and outside the box. Outside the box, the \npotential energy is inﬁnite and the energy eigenvalue equation is\n \na-  U2\n2m d 2\ndx2 + \u0005b wE 1x2 = E wE 1x2,   outside box. \n(5.48)\nWe are looking for solutions with ﬁnite energy E, so Eq. (5.48) is satisﬁed only if the energy eigenstate \nwave function wE1x2 is zero everywhere outside the box. This means that the quantum mechanical \nparticle is excluded from the classically forbidden regions in this example. This correspondence with \nthe classical situation holds only for the case of inﬁnite potential energy walls on the potential well.\nInside the box, the potential energy is zero and the energy eigenvalue equation is\n \na-  U2\n2m\n d 2\ndx2 + 0b wE1x2 = EwE1x2,   inside box. \n(5.49)\nThus our task reduces to solving the differential equation inside the box:\n \n-  U2\n2m\n d 2\ndx2 wE1x2 = EwE1x2. \n(5.50)\nIt is worth reminding ourselves at this point what is known and what is not. The particle has a mass \nm and is conﬁned to a box of size L. These quantities are known, as is U, a fundamental constant. The \nunknowns that we need to ﬁnd are the energy E and the wave function wE1x2, which is what it means to \nsolve an eigenvalue problem (now posing as a differential equation).\nIt is convenient to rewrite the differential equation (5.50) as\n \n d 2\ndx2 wE1x2 = -  2mE\nU2  wE1x2 \n \n = -k2wE1x2,\n \n(5.51)\nwhere we have deﬁned a new parameter\n \nk2 = 2mE\nU2 , \n(5.52)\nwhich is positive because the energy E is positive in this problem. The parameter k is called the wave \nvector, and its physical interpretation will be evident in Eq. (5.67). Equation (5.51) says that the \nenergy eigenstate wE1x2 is a function whose second derivative is equal to that function itself times a \nnegative constant. We can write the solution either in terms of complex exponential functions\n \nwE1x2 = A\u0004e ikx + B\u0004e -ikx \n(5.53)\nor in terms of sine and cosine functions\n \nwE1x2 = A sin kx + B cos kx. \n(5.54)\nEither solution includes two as yet unknown constants, as you would expect for a second-order differ-\nential equation. It turns out that bound state energy eigenstates can always be written as real functions, \nso we choose to work with the sine and cosine form of the general solution (if you choose the complex \n\n122 \nQuantized Energies: Particle in a Box\nexponential form, you will arrive at the sine and cosine solutions at the end of the problem anyway: \nProblem 5.3). Hence the energy eigenstate wave function throughout space is\n \nwE 1x2 = •\n0 ,\nA sin kx + B cos kx ,\n0 ,\n   \nx 6 0\n0 6 x 6 L\nx 7 L.\n \n(5.55)\nWe now need some more information to reach the ﬁnal solution. There are three unknowns in \nthe problem: A, B, and k [which contains the energy E through Eq. (5.52)], so we expect to need three \npieces of information to solve for the three unknowns. We get two of these pieces of information from \nimposing boundary conditions on the wave function. To make sure that the mathematical solutions \nproperly represent real physical systems, we require that the wave function be continuous across each \nboundary between different regions of space where different solutions exist. Applying this require-\nment on the continuity of the wave function at the sides of the box x = 0 and L yields two boundary \ncondition equations:\n \n wE102: A sin102 + B cos102 = 0 \n \n wE1L2: A sin kL + B cos kL = 0.  \n(5.56)\nThe boundary condition at the left side of the box yields\n \nB = 0. \n(5.57)\nThis tells us that the cosine part of the general solution is not allowed because the cosine solution is not \nzero at the edge of the box and so does not match the wave function outside the box. The exclusion of \nthe cosine part of the solution arises because we chose to locate our box with one side at x \u0003 0; if the \nbox is located differently, then both sine and cosine solutions may be allowed. Given that the allowed \nwave functions must be sine functions, the boundary condition at the right side of the box yields\n \nA sin kL = 0. \n(5.58)\nThis equation is satisﬁed if A \u0003 0, but that yields a wave function that is zero everywhere, so it is unin-\nteresting. The more interesting possibility is that\n \nsin kL = 0. \n(5.59)\nThis is a transcendental equation that places limitations on the allowed values of the wave vector k. We \nwill ﬁnd other transcendental equations when we study other potentials. This transcendental equation \nhas solutions when the sinusoid function is zero. Hence the wave vectors that satisfy this equation are\n \n kL = np\n \n \n kn = n p\nL\n ,   n = 1, 2, 3, ... . \n(5.60)\nOnly discrete wave vectors are allowed, so this is termed the quantization condition. The index n is \nthe quantum number, which we use to label the quantized states and energies. The value n = 0 is \nexcluded because that would yield a wave function equal to zero, which is uninteresting. The nega-\ntive values of n are excluded because they yield the same states as the corresponding positive n values, \n\n5.4 Inﬁnite Square Well \n123\nrecalling that an overall phase A-1 = eip in this caseB does not change the physical state. Using the deﬁ-\nnition of the wave vector in Eq. (5.52), we relate the quantized wave vectors to the quantized energies\n \nEn = U2k 2\nn\n2m . \n(5.61)\nHence, the wave vector quantization condition in Eq. (5.60) results directly in the energy quantization \nfor this system:\n \nEn = n2p2U2\n2mL2 ,   n = 1, 2, 3, ...  . \n(5.62)\nThese allowed energies scale with the square of the quantum number n and produce the set of energy \nlevels shown in Fig. 5.10. The ground state is the n \u0003 1 level.\nThe allowed energy eigenstate wave functions are:\n \nwn1x2 = A sin npx\nL ,   n = 1, 2, 3, ...  . \n(5.63)\nThe constant A was not determined by the boundary conditions. To determine A, we need the third \npiece of information, which is that the wave function is normalized to unity:\n \n1 = 8En 0\n En9 =\n \nL\n\u0005\n- \u0005\nw*\nn1x2wn1x2dx =\n \nL\n\u0005\n- \u0005\n0 wn1x20\n2\n dx. \n(5.64)\n0\n5\n10\n15\n20\n25\nE /E1\nn \u0002\u00031\nn \u0002\u00032\nn \u0002\u00033\nn \u0002\u00034\nn \u0002\u00035\nE1 \u0002 1E1\nE2 \u0002 4E1\nE3 \u0002 9E1\nE4 \u0002 16E1\nE5 \u0002 25 E1\nFIGURE 5.10 Energy spectrum of the inﬁnite square potential energy well.\n\n124 \nQuantized Energies: Particle in a Box\nSubstitute the wave function from Eq. (5.63) and note that the wave function is zero for x 6 0 and \nx 6 L to limit the range of integration, resulting in\n \n1 =\n \nL\nL\n0\n0 A0\n2 sin2 kn\n x dx = 0 A0\n2 L\n2 . \n(5.65)\nWe are free to choose the normalization constant to be real and positive, because an overall phase is \nnot measurable. Thus the normalization constant is A = 12>L and the properly normalized energy \neigenstates are\n \nwn1x2 = A\n2\nL sin npx\nL ,   n = 1, 2, 3, ...  . \n(5.66)\nThe ﬁrst few allowed energy states are shown in Fig. 5.11. From these plots, it is now clear why \nwe call c1x2 the wave function. These energy eigenstates have a “wavy” spatial dependence, much \nlike the modes on a guitar string. For the inﬁnite square well, the waves “ﬁt” into the potential well \nsuch that there are an integer number of half wavelengths within the well. If we relate the wave vector \nk to a wavelength l through the relation\n \nk = 2p\nl , \n(5.67)\nthen we can rewrite the quantization condition in terms of the wavelength\n \n kn = n p\nL  \n \n 2p\nln\n= n p\nL  \n \n ln = 2L\nn\n \n(5.68)\n \n L = n ln\n2 . \nIn words, the well must contain an integer number of half wavelengths. This is the sense in which the \nwaves must “ﬁt” into the well. This is the same as the classical result for the allowed standing waves \non a vibrating string, such as a guitar string. The distinction between the classical wave and the quan-\ntum wave is that the classical wave does not have a quantized energy. The energy of a vibrating guitar \nn \u0002\u00031\nL/2\nL\nx\nΨ(x)\n(a)\nn \u0002\u00032\nL/2\nL\nx\nΨ(x)\n(b)\nn \u0002\u00033\nL/2\nL\nx\nΨ(x)\n(c)\nFIGURE 5.11 Wave functions of the ﬁrst three energy eigenstates of the inﬁnite \nsquare potential energy well.\n\n5.4 Inﬁnite Square Well \n125\nstring depends on the amplitude of oscillation, not on the wavelength or wave vector, and so it can have \nany energy value. The amplitude of the quantum wave function is determined by the normalization \ncondition and is independent of the energy for the inﬁnite square well.\nThe wave properties of this quantum system are a new aspect that is not evident in the classical \ndescription of a particle. In classical mechanics, waves and particles are clearly distinct, whereas in \nquantum mechanics a system exhibits properties that remind us of classical particles but also exhibits \nproperties of classical waves. This is often referred to as wave-particle duality. We will see more of \nthis in the next chapter when we discuss free particles.\nExample 5.3 It is useful to put some numbers into these expressions to get a sense of scale. For \nexample, if we conﬁne an electron 1me = 511 k eV>c22 in a box of size 0.2 nm (about the size of an \natom), the ground state (n \u0003 1) energy is\n \n E1 =\np2U2\n2me L2\n \n \n =\np216.58 * 10-16 eV s2\n2\n210.511 * 106 eV>c2210.2 * 10-9 m2\n2 \n(5.69)\n \n = 9.4 eV. \nThis is comparable to typical atomic binding energies.\nThe spectrum of this system will include the transition between the ground state and the ﬁrst excited \nstate. The ﬁrst excited state has energy E2 = 22E1 = 4E1, so the wavelength of light for this transition is\n \n l21 =\nhc\nE2 - E1\n= hc\n3E1\n \n \n = 1240 eV nm\n319.4 eV2\n= 44 nm . \n(5.70)\nNote that l21 is the wavelength of the photon emitted or absorbed in the transition, not the wave-\nlength of the bound particle that is associated with the wave vector of the wave function, which is \n0.4 nm for the ground state and 0.2 nm for the excited state, in agreement with Eq. (5.68).\nNow that we have found the energy eigenstates, we have what we need to calculate probabilities \nand expectation values to compare with experiments. The square of the wave function gives us the \nprobability density\n \n Pn1x2 = 0 wn1x2 0\n2\n \n \n = 2\nL sin2 npx\nL , \n(5.71)\nwhich is shown in Fig. 5.12 for the ﬁrst three states. Note that the probability density is zero outside \nthe well, so the probability of ﬁnding the particle anywhere outside the well is zero, just as in the clas-\nsical case. However, in the quantum system there are positions within the well where the probability \nof ﬁnding the particle is zero, which does not happen in the classical case. These positions are at the \nnodes of the wave function and hence are characteristic of the wave nature of the particle.\n\n126 \nQuantized Energies: Particle in a Box\nExample 5.4 Find the expectation value of the position for a particle in the ground state of an \ninﬁnite square potential energy well.\nThe expectation value of position is given by Eq. (5.41)\n \n 8xn9 = 8E10\n xn  0 E19 =\n \nL\n\u0005\n- \u0005\nw*\n11x2xw11x2dx =\n \nL\n\u0005\n- \u0005\nx  0\n w11x20\n2\n dx \n \n = 2\nL L\nL\n0\nx sin2a px\nL b dx = 2\nL\n a L\npb\n2\nL\np\n0\ny sin21y2dy\n \n \n = 2\nL\n a L\npb\n2\n c y2\n4 - y sin 2y\n4\n-\n cos 2y\n8\nd\np\n0\n \n(5.72)\n \n = 2\nL\n a L\npb\n2\n c p2\n4 -\np sin12p2\n4\n-\n cos12p2\n8\n+ 1\n8 d\n \n \n = 2\nL\n a L\npb\n2\n c p2\n4 d\n \n \n = L\n2 .\n \nThis is what we would expect to get given the symmetry of the problem. There is no preference for \nthe left or right side of the well, so the average value of a set of position measurements must be the \nmidpoint of the well. We get the same result for any energy eigenstate of the system.\nTo summarize, we have solved the problem of a particle bound in an inﬁnitely deep square poten-\ntial energy well, which means we have found the energy eigenvalues and eigenstates. The well is \ndepicted in Fig. 5.13(a), the spectrum of allowed energies is depicted in Fig. 5.13(b), and the wave \nfunctions of the energy eigenstates are depicted in Fig. 5.13(c). It is common practice to unify the three \ndiagrams of Fig. 5.13 in a single diagram, shown in Fig. 5.14, that represents the quantum mechani-\ncal potential energy well problem and its solution. The well, the energies, and the wave functions are \nsuperimposed on each other, such that different aspects of the diagram have different vertical axes. \nThe wave function for each energy eigenstate has its vertical coordinate origin located at the energy of \nthat state.\nn \u0002\u00033\n0\n(c)\nL/2\nL\nx\n\u0002Ψ\u00022\nn \u0002\u00031\n0\n(a)\nL/2\nL\nx\n\u0002Ψ\u00022\nn \u0002\u00032\n0\n(b)\nL/2\nL\nx\n\u0002Ψ\u00022\nFIGURE 5.12 Probability densities of the ﬁrst three energy eigenstates of the \ninﬁnite square potential energy well.\n\n5.4 Inﬁnite Square Well \n127\n0\nL\nx\nV(x)\n(a)\n\u0005\n0\n5\n10\n15\n20\n25\n(b)\nE /E1\nn \u0002\u00031\nn \u0002\u00032\nn \u0002\u00033\nn \u0002\u00034\nn \u0002\u00035\nL\nx\nn \u0002\u00035\nL\nx\nn \u0002\u00034\nL\nx\nn \u0002\u00033\nL\nx\nn \u0002\u00032\nL\n(c)\nx\nn \u0002\u00031\nΨ(x)\nFIGURE 5.13 (a) Inﬁnite square potential energy well, (b) spectrum of allowed energies, and \n(c) energy eigenstate wave functions.\n0\nL\nx\nn \u0002\u00031\nn \u0002\u00032\nn \u0002\u00033\nn \u0002\u00034\nn \u0002\u00035\nFIGURE 5.14 Uniﬁed schematic diagram of inﬁnite square well problem and solution. \nNote that two vertical scales are implied. For the potential energy well and the energy  \nspectrum, the vertical scale is energy with the origin at the bottom of the well. For the  \nwave functions, the vertical scale is probability amplitude 11>length1>22 with the \nc = 0 origin for each state centered on the energy of that state.\n\n128 \nQuantized Energies: Particle in a Box\nThe take home message of this problem is that the imposition of boundary conditions on the \nwave function limits the possible states that can “ﬁt” into the well and directly leads to the quantiza-\ntion of energy. This is a general result that we will return to time and again as we study other potential \nwell landscapes.\n5.5 \u0002 FINITE SQUARE WELL\nNow let’s make the problem a little more realistic by having the potential energy outside the well be ﬁnite \ninstead of inﬁnite. We still assume that the well is square, which still results in an inﬁnite force at the \nwalls. However, this new problem illustrates several important features of bound energy states that were \nnot evident in the inﬁnite well. A ﬁnite well can be used to model many real systems, such as an electron \nin a thin semiconductor. In Section 5.8, we use this model to discuss quantum well semiconductor lasers.\nThe ﬁnite square well potential energy is shown in Fig. 5.15 and is written as\n \nV1x2 = •\nV0,\nx 6 -a\n  0,\n-a 6 x 6 a\nV0,  \nx 7 a,\n \n(5.73)\nwhere we have deliberately chosen a different position origin from the inﬁnite well case in order to \ngive you practice and also for convenience. For now, we look for bound state solutions, that is, for \nenergies below the potential V0. Energies above V0 correspond to unbound states that we will discuss \nin the next chapter.\nWith this new potential energy function, the energy eigenvalue equation is\n \n a-  U2\n2m d 2\ndx 2 + 0b wE1x2 = EwE1x2,    inside box  \n \n a-  U2\n2m d 2\ndx 2 + V0b wE1x2 = EwE1x2,          outside box . \n(5.74)\n0\n\u0004a\na\nx\nV(x)\nV0\nFIGURE 5.15 Finite square potential energy well.\n\n5.5 Finite Square Well \n129\nIn the inﬁnite well problem, we found it useful to use the wave vector k\n \nk = A\n2mE\nU2 . \n(5.75)\nIn this case, it is also useful to deﬁne a similar constant outside the well\n \nq = A\n2m\nU2 1V0 - E2. \n(5.76)\nFor bound states, 0 6 E 6 V0, and therefore both k and q are real. We use these two constants to \nrewrite the energy eigenvalue equation:\n \n \nd 2wE1x2\ndx2\n= -k2wE1x2,    inside box  \n \n \nd 2wE1x2\ndx 2\n= q 2wE1x2,       outside box . \n(5.77)\nThe energy eigenvalue equation inside the box is identical to the one we solved for the inﬁnite well poten-\ntial. The differential equation outside the box is similar except the constant is positive instead of negative, \ngiving real exponential solutions rather than complex exponentials. Thus the solution outside the box is\n \nwE1x2 = Aeqx + Be-qx. \n(5.78)\nThis solution in the classically forbidden region is exponentially decaying, or growing, with a decay \nlength, or growth length, of 1Nq.\nThe energy eigenstate must be constructed by connecting solutions in the three regions shown in \nFig. 5.15. We write the general solution as\n \nwE1x2 = •\nAeqx + Be-qx,\nC sin kx + D cos kx,\nFeqx + Ge-qx,    x 6 -a\n -a 6 x 6 a\n x 7 a.\n \n(5.79)\nAs we discussed in the inﬁnite well problem, the solutions in the three regions must satisfy bound-\nary conditions where the regions connect. In constructing the inﬁnite well solutions, we used the \ncondition that the wave function must be continuous across a boundary. We now introduce a second \nrequirement that the slope of the wave function be continuous across a boundary. If the slope were \ndiscontinuous, that would imply an inﬁnite kinetic energy. However, this requirement has one excep-\ntion: it does not apply if the potential is inﬁnite (Problem 5.24), which is why we did not use it in the \ninﬁnite well problem. You can see in Fig. 5.14 that the inﬁnite well solutions have a change in slope \nat the edges of the box where the potential energy becomes inﬁnite. We now summarize these two \nboundary conditions:\n 1) wE1x2 is continuous\n 2) dwE1x2\ndx\n is continuous unless V = \u0005.\nBefore we impose the boundary conditions, we make two immediate simpliﬁcations to the gen-\neral solutions in Eq. (5.79). In the regions outside the well, the wave function must be a decaying \n\n130 \nQuantized Energies: Particle in a Box\nexponential because a growing exponential term all the way out to inﬁnity would not permit the wave \nfunction to be normalized. This normalization condition, which can also be termed a boundary condi-\ntion at inﬁnity, requires that B = F = 0 in Eq. (5.79). The second simpliﬁcation comes from recog-\nnizing that the potential energy is symmetric with respect to the origin 3V1x2 = V1-x24. This means \nthat the energy eigenstates will either be symmetric or antisymmetric (even or odd). This symmetry \nis evident in the inﬁnite well solutions shown in Fig. 5.14. (This can also be discussed in terms of the \ncommutation of the Hamiltonian and the parity operator, which we discuss in Section 7.6.4) We can \nthus solve for the two sets of solutions independently. If you don’t impose this symmetry condition \nnow, it will come out naturally after some algebra on the general solutions anyway (Problem 5.14). \nWith these two simpliﬁcations, the even solutions reduce to\n \nweven1x2 = •\nAeqx,\nD cos1kx2,\nAe-qx,\n   \nx 6 -a\n-a … x … a\nx 7 a.\n \n(5.80)\nThe odd solutions are\n \nwodd1x2 = •\nAeqx,\nC sin1kx2,\n-Ae-qx,\n   \nx 6 -a\n-a … x … a\nx 7 a.\n \n(5.81)\nLet’s ﬁrst do the even solutions. The boundary conditions at the right side of the well 1x = a2 give\n \n weven1a2: D cos1ka2 = Ae-qa\n \n \n \ndweven1x2\ndx\n`\nx=a \n: -kD sin1ka2 = -qAe-qa. \n(5.82)\nThe boundary conditions at the left side of the well 1x = -a2 yield the same equations, which must be \ntrue because of the symmetry. The two equations above have three unknowns: the amplitudes A and D \nand the energy E, which is contained in the parameters k and q. The normalization condition provides \nthe third equation required to solve for all three unknowns. We ﬁnd the energy condition rather simply \nby dividing the two equations, which eliminates the amplitudes and yields\n \nk tan1ka2 = q. \n(5.83)\nBecause both k and q are functions of the energy, this equation gives us a formula to ﬁnd the allowed \nenergies. It is independent of the constants A and D, which are found by applying the normalization \ncondition and using Eq. (5.82) again. As usual with these types of problems, the eigenvalue condi-\ntion is obtained ﬁrst, and then the eigenfunctions are obtained later. To make the energy dependence \nexplicit, we use Eqs. (5.75) and (5.76) to write Eq. (5.83) as\n \nA\n2m\nU2  E tana A\n2m\nU2  E ab = A\n2m\nU2  1V0 - E2. \n(5.84)\nThe next step is to solve this transcendental equation for the energy E.\nFor the odd solutions, a similar argument leads to the transcendental equation (Problem 5.15)\n \n-k cot1ka2 = q. \n(5.85)\n\n5.5 Finite Square Well \n131\nA graphical solution for the allowed energies using these two transcendental equations is most useful \nhere. There are many ways of doing this. One way involves deﬁning some new dimensionless parameters:\n \n z = ka = B\n2mEa2\nU2\n \n \n z0 = B\n2mV0a2\nU2\n \n \n qa = B\n2m1V0 - E2a2\nU2\n, \n \n(5.86)\nwhere the variable z parameterizes the energy of the state and the constant z0 characterizes the strength \nof the potential energy well. These deﬁnitions lead to the convenient expressions\n \n 1ka2\n2 + 1qa2\n2 = z 2\n0\n \n \n 1qa2\n2 = z 2\n0 - 1ka2\n2 = z 2\n0 - z 2. \n \n(5.87)\nThis allows us to write the transcendental equations in this form:\n \n ka tan1ka2 = qa  S  z tan1z2 = 4z 2\n0 - z 2\n \n \n -ka cot1ka2 = qa  S  -z cot1z2 = 4z 2\n0 - z 2. \n \n(5.88)\nIn each of these new transcendental equations, the left side is a modiﬁed trig function, while the right \nside is a circle with radius z 0. These functions are plotted in Fig. 5.16 as a function of the parameter \nz. The intersection points of these curves determine the allowed values of z and hence the allowed \nenergies En through Eq. (5.86). Because the constant z 0 is the radius of the circle, there are a limited \nnumber of allowed energies, and that number grows as z 0 gets larger. Wells that are deeper and wider \nhave more allowed bound energy states. \nThat’s it for the energies. There is no simple formula—the transcendental equations must be \nsolved graphically or numerically for each different well. For example, the curves in Fig. 5.16 corre-\nspond to a well with z 0 = 6, which results in four intersection points and hence four bound states. The \nintersection points and four allowed energies are\n \n z1 = 1.34 S E1 = 1.81 U2\n2ma2\n \n \n z2 = 2.68 S E2 = 7.18 U2\n2ma2\n \n \n z3 = 3.99 S E3 = 15.89 U2\n2ma2  \n \n z4 = 5.23 S E4 = 27.31 U2\n2ma2 . \n \n(5.89)\nThe energy eigenstate wave functions are characterized by the allowed values of the parameters \nk and q from Eq. (5.86). All that remains to do is normalize the wave function, which is straightfor-\nward but tedious (Problem 5.16). Once again, we use a uniﬁed diagram to show the potential energy \nwell, the allowed energies, and the allowed eigenstate wave functions superimposed in Fig. 5.17. \n\n132 \nQuantized Energies: Particle in a Box\n0\n\u0004a\na\nx\nn \u0002\u00031\nn \u0002\u00032\nn \u0002\u00034\nn \u0002\u00033\nFIGURE 5.17 Uniﬁed schematic diagram of the ﬁnite potential energy well and the bound state \nsolutions, showing the well, the allowed energies, and the energy eigenstate wave functions.\n0\n\u000b\n2\n\u000b\n3\u000b\n2\nz\n\u000b\n2\n\u000b\n3\u000b\n2\n2\u000b\nf(z)\nz tanz\n_z cotz\nz02\u000b\nz0\nz2\n0 \n_ z 2\nFIGURE 5.16 Graphical solution of the transcendental equations for the allowed energies of a ﬁnite \nsquare well 1z0 = 62.\n\n5.6 Compare and Contrast \n133\nNote that the ﬁnite well eigenstates share many features with the inﬁnite well states, with one major \nexception—they extend into the classically forbidden region. Quantum mechanical particles have a \nﬁnite probability of being found where classical particles may not exist! This is a purely quantum \nmechanical effect and is commonly referred to as barrier penetration. The ability of the particle \nto penetrate the potential energy barrier leads to the phenomenon of tunneling, an example of which \nis radioactive decay. We’ll say more about these wave functions in a bit, but let’s ﬁrst check that our \nsolution is consistent with the solution we derived earlier for the inﬁnite energy well case.\nThe limit of an inﬁnitely deep well corresponds to the radius z0 in Fig. 5.16 going to inﬁnity, in \nwhich case the allowed values of z become the asymptotes of the modiﬁed trig functions, shown by the \ndashed lines in Fig. 5.16. These limits are the same as for the simple trig functions and yield\n \n zn = n p\n2 1 kna = n p\n2 \n \n kn = np\n2a ,\n \n \n(5.90)\nfrom which we recover the inﬁnite well energy eigenvalues:\n \nEn =\nn2p2 U2\n2m12a2\n2 . \n(5.91)\nNote that the width of the well is 2a here, whereas we called the width L in the inﬁnite well case. The \ninﬁnite well eigenstate wave functions for this symmetric well position are\n \n wn1x2 = A\n2\n2a\n cos npx\n2a ,    n = 1, 3, 5, ...  \n \n wn1x2 = A\n2\n2a\n sin npx\n2a ,    n = 2, 4, 6, ... . \n \n(5.92)\nThere are two sets of solutions because we chose a different coordinate system to solve the problem. \nIn the limit z0 S \u0005, the decay length q becomes zero and the energy eigenstates are zero outside the \nwell, as expected. The inﬁnite well eigenstates are shown in Fig. 5.18(a) for this new choice of coor-\ndinates. Comparing the wave functions in Fig. 5.18(a) with those from Fig. 5.14, though, we see that \nthese are the same eigenstate wave functions that we found before. In Fig. 5.18(b) we show the ﬁnite \nwell states for comparison.\n5.6 \u0002 COMPARE AND CONTRAST\nNow that we have solved two similar problems, the inﬁnite and ﬁnite square wells, let’s discuss some \nof the important features of these solutions and see which features are common to both problems and \nothers, and which are distinct.\n 5.6.1 \u0002 Wave Function Curvature\nThe ﬁrst common feature is that the wave function is oscillatory 1sin kx or cos kx2 inside the well and \nexponentially decaying (e-qx or eqx) outside the well. This aspect is explained by examining the curvature \n(i.e., second derivative) of the wave function. To see this, we rewrite the energy eigenvalue equation\n \nd 2wE 1x2\ndx2\n= -  2m\nU2  3E - V 1x24wE 1x2, \n(5.93)\n\n134 \nQuantized Energies: Particle in a Box\nwhich then directly relates the wave function curvature to the difference between the energy E and \nthe potential energy V(x). Thus, inside the well, in the classically allowed region, we have E 7 V1x2 \nand the differential equation admits only sinusoidal solutions characterized by the wave vector k or \nwavelength l = 2p>k. Outside the well, in the classically forbidden region, we have E 6 V1x2 and \nthe differential equation admits only real exponential solutions with a decay length of 1>q, which is \nzero for the inﬁnite square well. The growing exponential terms in these problems are excluded by the \nnormalization requirement (i.e., the boundary condition at inﬁnity).\nThese comments can be generalized as shown in Fig. 5.19. Equation (5.93) tells us that in a clas-\nsically allowed region where E 7 V, the curvature has the opposite sign to the wavefunction, and in \nthe classically forbidden region where E 6 V, the curvature has the same sign as the wavefunction. \nThis means that in the classically allowed region the wave function is concave toward the axis, while \nin the classically forbidden region the wave function is convex toward the axis, as shown in Fig. 5.19.\nWe can also make some general observations regarding the length scales of the wave functions. In \na general potential well, the wave vector is given by\n \nk =\n22m 1E - V2\nU\n. \n(5.94)\nHence, the oscillatory part of the wave function (inside the well) has a characteristic wavelength\n \nl = 2p\nk\n=\nh\n22m 1E - V2\n\f\n1\n2T\n. \n(5.95)\nSo the larger the energy difference between the eigenvalue and the potential energy (i.e., the larger \nthe kinetic energy), the smaller the wavelength. That relationship is evident in the eigenstates shown \nin Fig. 5.18; the higher the energy, the more “wiggly” the wave function. In the forbidden region, the \ndecay constant \n \nq =\n22m 1V - E2\nU\n \n(5.96)\n\u0004a\n0\na\nx\n0\n\u0004a\na\nx\n(a)\n(b)\nFIGURE 5.18 (a) Inﬁnite and (b) ﬁnite well energy eigenstates.\n\n5.6 Compare and Contrast \n135\ndecreases as the energy increases toward V, which means that the decay length becomes larger. Hence, \nfor higher energy states the wave function penetrates further into the classically forbidden region \n(Problem 5.17). This increasing penetration with increasing energy is evident in the ﬁnite well states \nof Fig. 5.17.\nIn comparing the ﬁnite and inﬁnite well energies in Fig. 5.18, we also note that a given ﬁnite well \nenergy eigenvalue En lies below the corresponding inﬁnite well energy eigenvalue. This is consistent \nwith the longer wavelength of the ﬁnite well eigenstate compared to the corresponding inﬁnite well \nstate. For the ﬁnite well eigenstate to “ﬁt” in the well, the wavelength can be longer because part of the \nwave function is outside the well. The increasing penetration of the wave function into the classically \nforbidden region with increasing energy implies that the difference in energies between the ﬁnite and \ninﬁnite wells is larger for higher energies, as is also evident in Fig. 5.18 (Problem 5.19).\n 5.6.2 \u0002 Nodes\nThe ground state has a single antinode in the wave function, with each subsequent higher state acquiring \nan extra antinode. Thus the nth energy level has n antinodes and 1n - 12 nodes. This is a general char-\nacteristic of the energy eigenstates of any potential energy well. In the inﬁnite well we found an inﬁnite \nnumber of states. In the ﬁnite well we found a ﬁnite number of states, but we looked only for bound \nstates. We will see later that there are an inﬁnite number of unbound states with E 7 V0, which means \nthat there are an inﬁnite number of total allowed energy states. The inﬁnite number of states is a common \nfeature of potential energy wells. In the ﬁnite well, if the well is small enough (small V0 and/or small a), \nthen there might be only one bound state, but there is always at least one bound state. This is generally \ntrue for any well shape. The delta-function potential is an extreme case (Problem 5.25).\n 5.6.3 \u0002 Barrier Penetration\nIn the ﬁnite potential well, the wave function is nonzero in the classically forbidden region. This \nimplies a ﬁnite probability that the quantum mechanical particle can be found where the classical \nparticle cannot. As mentioned above, this penetration of the wave function into the potential energy \nx\nE0\nE, Ψ\nExponential\nOscillatory\nAllowed\nForbidden\nV(x)\nΨ > 0, Ψ\" < 0\nΨ < 0, Ψ\" > 0\nΨ > 0, Ψ\" > 0\nΨ < 0, Ψ\" < 0\nFIGURE 5.19 Curvature of the energy eigenstate wave functions in the allowed and forbidden regions.\n\n136 \nQuantized Energies: Particle in a Box\nbarrier leads to the phenomenon of tunneling, which we explore in the next chapter. The wave function \nplots in Fig. 5.18 indicate that the barrier penetration is more pronounced for higher energy levels and \ncan become quite large for energies close to the top of the well. This aspect is clear quantitatively if we \nnote that the decay constant q in the forbidden region decreases as the energy increases, which means \nthat the decay length becomes larger, so more of the wave function is outside the well.\n 5.6.4 \u0002 Inversion Symmetry and Parity\nIn both square well problems, the allowed wave functions are either symmetric (even) or antisym-\nmetric (odd) with respect to the center of the well. In both cases, the potential energy well, and hence \nthe Hamiltonian, is symmetric with respect to the well center. We say that the Hamiltonian is invariant \nunder the parity operation x S -x. Because the Hamiltonian is invariant under the parity operation, \nit must commute with the parity operator, and hence the energy eigenstates are also eigenstates of \nthe parity operator. The symmetric states satisfy wn1x2 = +wn1-x2, have a parity eigenvalue +1, \nand are called even parity states. The antisymmetric states satisfy wn1x2 = -wn1-x2, have a parity \neigenvalue -1, and are called odd parity states. Identifying the parity of an energy eigenstate is useful \nbecause the parity of the state often indicates whether a particular matrix element involving that state \nis zero or not. For example, the probability of a transition between two energy eigenstates caused by \nincident laser light is proportional to the matrix element of the electric dipole operator (-ex in one \ndimension) between the two states:\n \n8wm 0\n -ex 0\n wn9 = -\nL\n\u0005\n- \u0005\nwm1x2ex wm1x2d 3r. \n(5.97)\nThis integral is zero if the integrand has odd parity. The electric dipole operator has odd parity, so the \nenergy eigenstates must have different parity for the transition to be allowed. If the integral is zero, then \nthe transition is a forbidden transition. Many of the selection rules that determine which transitions \nare allowed and which are forbidden come from these types of parity arguments. More complete dis-\ncussion of electric dipole transitions must wait until we discuss time-dependent perturbation theory in \nChapter 14.\n 5.6.5 \u0002 Orthonormality\nThe energy eigenstates form an orthonormal set, as we have found for other sets of eigenstates, such \nas spin states. The normalization is not an intrinsic property of the solutions but rather something that \nwe impose so that the total probability of ﬁnding the particle somewhere is unity. The orthogonality is \na fundamental trait of eigenstates of Hermitian operators. The orthonormality condition is expressed \nin Dirac notation as\n \n8En0 Em9 = dnm \n(5.98)\nand in wave function language as\n \nL\n\u0005\n- \u0005\nw*\nn 1x2wm1x2dx = dnm. \n(5.99)\nThis condition is straightforward to show for the inﬁnite well states (Problem 5.12) but is a little tedious \nfor the ﬁnite well states because of the lack of a general expression for the allowed wave vectors.\n\n5.7 Superposition States and Time Dependence \n137\n 5.6.6 \u0002 Completeness\nThe energy eigenstates form a complete basis, as we have found for other sets of basis states. Com-\npleteness is also a fundamental trait of eigenstates of Hermitian operators. Completeness means \nthat we can use these basis functions to construct all possible solutions to the Schrödinger equation \nH0 c9 = iU d 0 c9>dt for this problem. The wave function of a general superposition state is\n \nc1x2 = a\nn\ncn\n wn1x2. \n(5.100)\nNote that the energy eigenvalue equation Hwn1x2 = En\n wn1x2 is satisﬁed by each particular energy \neigenstate in turn but is not satisﬁed by general superposition states. For the inﬁnite well, Eq. (5.100) \nis exact, while for the ﬁnite well we must also include unbound energy states above the well in the sum \nover basis states. Obviously, for a well that is so small that there is only one bound state, we would \nexpect to need more states to form a complete basis. The completeness relation is also called the clo-\nsure relation and, as we saw in the spins problem, is expressed as a sum of all the projection operators\n \na\nn\n0 En98En0 = 1, \n(5.101)\nwhere the right-hand side is understood to be the identity operator\n 5.7 \u0002 SUPERPOSITION STATES AND TIME DEPENDENCE\nSolving for the energy eigenvalues and eigenstates is an important aspect of any problem, but it is not \nthe only goal. As physicists, our aim is to predict the future of a physical system. In quantum mechan-\nics, we do this through the Schrödinger equation\n \nH0 c9 = iU d\ndt\n 0 c9 \n(5.102)\nthat governs the time evolution of any quantum system. Though different systems clearly have differ-\nent Hamiltonians, we need not solve the Schrödinger equation for the time evolution separately for \neach system. We have already solved it in Chapter 3 for a time-independent Hamiltonian, where we \nfound that the most general time-dependent solution to the Schrödinger equation is\n \n0 c1t29 = a\nn\ncn\n e-iEnt>U0 En9. \n(5.103)\nThat is, the energy eigenstates form the preferred basis in which to expand a general quantum state \nvector, with the time evolution determined by phase factors dependent on the energy of each compo-\nnent state. In a general superposition, each energy eigenstate acquires a different phase. It is critical \nto remember that one must use the energy basis in order to use this simple recipe for time evolu-\ntion. This is why we spend much of our time ﬁnding energy eigenstates.\nTo use Eq. (5.103) we need to know the expansion coefﬁcients cn for the particular state in ques-\ntion. The quantum state at time t = 0 is\n \n0 c 1029 = a\nn\ncn0 En9, \n(5.104)\n\n138 \nQuantized Energies: Particle in a Box\nso the expansion coefﬁcients cn are determined by the initial state of the system. The coefﬁcients cn are \nthe probability amplitudes for the state 0 c1029 to be in the energy eigenstates 0 En9\n \ncn = 8En0 c1029. \n(5.105)\nTo show this again, we perform a manipulation with the closure relation in Eq. (5.101). The identity \noperator does not change the state vector, so we act on the state vector to obtain\n \n 0 c1029 = 10 c1029\n \n \n = e a\nn\n0 En98En0f 0 c1029 \n \n = a\nn\n0 En98En0 c1029\n \n \n = a\nn\n8En0 c1029 0 En9\n \n \n(5.106)\nand hence identify the coefﬁcients cn as given in Eq. (5.105).\nOf course, once we know the probability amplitudes, we can calculate the probabilities for mea-\nsuring the system to have one of the energy eigenvalues:\n \nPEn = 08En0 c1029 0\n2 = 0 cn0\n2. \n(5.107)\nWe showed in Chapter 3 that the probabilities of energy measurements are time independent, but let’s \ndo it again here, using the time-dependent state vector in Eq. (5.103)\n \nP En = 08En0 c1t29 0\n2 \n \n = `8En0 a\nm\ncm0 Em9e-iEmt>U `\n2\n= ` a\nm\ncm8En0 Em9e-iEmt>U `\n2\n \n \n = ` a\nm\ncmdmne-iEmt>U `\n2\n= 0 cne-iEnt>U0\n2 \n \n = 0 cn0\n2.\n \n \n(5.108)\nThe Kronecker delta from the energy eigenstate orthonormality condition collapses the sum to a single \nterm. Time independence of the energy probabilities implies that the expectation value of the energy is \nalso time independent:\n \n8H9 = a\nn\nPEnEn = a\nn\n0 cn0\n2En. \n(5.109)\n\n5.7 Superposition States and Time Dependence \n139\nWe can also show this by explicit calculation with the time-dependent states:\n \n 8H9 = 8c1t2 0 H0 c1t29\n \n \n = a\nm\nc*\nm8Em0 eiEmt>UHa\nn\ncn0 En9e-iEnt>U \n \n = a\nm,n\nc*\nmcneiEmt>Ue-iEnt>U8Em0 H0 En9\n \n \n = a\nm,n\nc*\nmcnei 1Em-En2 t>U En8Em0 En9\n \n \n = a\nm,n\nc*\nmcnei 1Em-En2 t>U Endmn\n \n \n = a\nn\nc*\nncnEn\n \n \n = a\nn\n0 cn0\n2\n En.\n \n \n(5.110)\nNote that we had no need to use wave function notation in these calculations. Wave function calcula-\ntions of Eqs. (5.108) and (5.110) would require spatial integrals that would also yield the Kronecker \ndelta from the energy eigenstate orthonormality condition that collapses the sums. The results would \nclearly be the same, so the message is: if you can avoid integrals by using Dirac notation instead of \nwave function notation, do so.\nWe need to use wave function language to answer questions about the spatial distribution of the \nparticle, so let’s use the rules we developed in Section 5.3 to translate the Dirac notation equations \nto wave function notation. The time evolution of the state vector [Eq. (5.103)], in wave function \nlanguage, is\n \nc1x, t2 = a\nn\ncn\n wn1x2e-iEnt>U. \n(5.111)\nTo ﬁnd the expansion coefﬁcients cn (i.e., the probability amplitudes), we translate Eq. (5.105) to wave \nfunction language:\n \ncn =\n \nL\n\u0005\n- \u0005\nw*\nn1x2c1x, 02dx. \n(5.112)\nSo, given the initial wave function of the system c1x, 02, the expansion coefﬁcients are overlap inte-\ngrals between each energy eigenstate and the initial wave function. These overlap integrals are analo-\ngous to the integrals used to ﬁnd Fourier expansion coefﬁcients. Let’s brieﬂy illustrate the Fourier \napproach for calculating the coefﬁcients cn. Set the time equal to zero in Eq. (5.111) to ﬁnd the initial \nwave function superposition:\n \nc1x, 02 = a\nn\ncn\n wn1x2. \n(5.113)\n\n140 \nQuantized Energies: Particle in a Box\nProject both sides of Eq. (5.113) onto the energy eigenstates by multiplying each side by w*\nm1x2 and \nintegrating over all space:\n \n \nL\n\u0005\n- \u0005\nw*\nm1x2c1x, 02dx =\n \nL\n\u0005\n- \u0005\nw*\nm1x2 a\nn\ncn\n wn1x2dx  \n \n = a\nn\ncn L\n\u0005\n- \u0005\nw*\nm1x2wn1x2dx \n(5.114)\n \n = a\nn\ncndnm\n \n \n = cm,\n \n \nyielding\n \ncm =\n \nL\n\u0005\n- \u0005\nw*\nm1x2c1x, 02dx \n(5.115)\nas we expected from Eq. (5.112). Once we have the wave function expansion coefﬁcients in the energy \nbasis, we can predict the future time evolution of the system. Then we can calculate any physical quan-\ntities we need to, such as probabilities and expectation values.\nExample 5.4 Consider a particle in an inﬁnite square well with the initial wave function\n \nc 1x, 02 = A J a x\nLb\n3\n- 11\n7\n a x\nLb\n2\n+ 4\n7\n a x\nLb R  \n(5.116)\nin the interval 0 6 x 6 L and zero elsewhere, as shown in Fig. 5.20. Find (i) the wave function at a \nlater time, (ii) the probabilities of energy measurements, and (iii) the expectation value of the energy.\n(i) First we must normalize the state to ﬁnd the constant A:\n \n 8c0 c9 = 1 =\n \nL\nL\n0\n0 c 1x, 020\n2\n dx\n \n \n = 0 A0\n2\nL\nL\n0\n J a x\nLb\n3\n- 11\n7\n a x\nLb\n2\n+ 4\n7\n a x\nLb R\n2\n dx = 0 A0\n2\n L\n735. \n(5.117)\nWe choose the constant to be real and positive and the normalized wave function is\n \nc1x, 02 = B\n735\nL  J a x\nLb\n3\n- 11\n7\n a x\nLb\n2\n+ 4\n7\n a x\nLb R. \n(5.118)\nNow perform the overlap integral to ﬁnd the expansion coefﬁcients:\n \n cn = 8En0 c9 =\n \nL\n\u0005\n- \u0005\nw*\nn 1x2  c 1x, 02dx\n \n \n =\n \nL\nL\n0 B\n2\nL\n sin anpx\nL b B\n735\nL  J a x\nLb\n3\n- 11\n7\n a x\nLb\n2\n+ 4\n7\n a x\nLb Rdx . \n(5.119)\n\n5.7 Superposition States and Time Dependence \n141\nDo the integral\n \n cn = 7230\nL\n b c3L \n1npx>L2\n2 - 2\n1np24\n sin anpx\nL b - L \n1npx>L2\n3 - 61npx>L2\n1np24\n cos anpx\nL bd\nL\n0\n \n \n -   11\n7\n c 2L \n1npx>L2\n1np23\n sin anpx\nL b - L \n1npx>L2\n2 - 2\n1np23\n cos anpx\nL b d\nL\n0\n \n(5.120)\n \n + 4\n7\n c L \n1\n1np22 sin anpx\nL b - L \n1npx>L2\n1np22\n cos anpx\nL b d\nL\n0\nr .\nEvaluate the limits and simplify:\n \n cn = 322 + 201-12\nn4230\n1np23\n \n \n = e\n2230\n1np2\n3  , if n is odd\n42230\n1np2\n3\n , if n is even .\n \n(5.121)\nThe ﬁrst few coefﬁcients are\n \nc1 = 0.3533  \n \nc2 = 0.9274  \n \nc3 = 0.0131  \n \nc4 = 0.1159 , \n(5.122)\nso the state is composed mostly of the ﬁrst excited state, which is evident from the shape of the \nwave function in Fig. 5.20.\nL/2\nL\nx\nΨ(x)\nFIGURE 5.20 An initial state wave function [Eq. (5.116)] in the inﬁnite square well.\n\n142 \nQuantized Energies: Particle in a Box\nThe wave function at later times is the superposition with each energy state evolved at its pre-\nscribed frequency\n \n c 1x, t2 = a\n\u0005\nn=1\ncn\n wn 1x2  e-iEnt = a\n\u0005\nn=1\ncnA\n2\nL\n sin npx\nL\n e-i n2p2Ut>2mL2 \n \n = A\n60\nL a\n\u0005\nn=1\n322 + 201-12\nn4\n1np2\n3\n sin npx\nL\n e-i n2p2Ut>2mL2.\n \n(5.123)\n(ii)  The probabilities of measuring the energy eigenvalues are the squares of the expansion \ncoefﬁcients:\n \n PEn = 08En0 c\n 1t290\n2 = 0 cn0\n2\n \n \n =\n30\n1np26 322 + 201-12\nn4\n2\n \n \n =\n120\n1np26 3221 + 2201-12\nn4. \n(5.124)\nThe energy probabilities are shown in the histogram in Fig. 5.21, reﬂecting the predominance of the \nsecond state.\n(iii) The expectation value of the energy is\n \n 8H9 = a\nn\nPEnEn = a\nn\n0 cn0\n2\n En\n \n \n = a\n\u0005\nn=1\n120\n1np26 1221 + 2201-12\nn2an2p2  U2\n2mL2 b\n \n \n =\na\n\u0005\nn=1,3,5...\n120\n1np26 an2p2U2\n2mL2 b +\na\n\u0005\nn=2,4,6...\n12014412\n 1np26\n an2p2U2\n2mL2 b \n \n =\n60U2\np4mL2 c\na\n\u0005\nn=1,3,5...\n 1\nn4 + 441 \na\n\u0005\nn=2,4,6...\n 1\nn4 d\n \n \n =\n60U2\np4mL2 c p4\n96 + 441 p4\n1440 d\n \n \n = 19 U2\nmL2 = 38\np2 E1 \u0002 3.85E1 ,\n \n(5.125)\nwhich is slightly smaller than the energy (E2 = 4E1) of the ﬁrst excited state, as expected from the \nhistogram in Fig. 5.21.\n\n5.7 Superposition States and Time Dependence \n143\nNotice that the energy expectation value, such as we calculated in Eq. (5.125), is time inde-\npendent regardless of whether the system is in an energy eigenstate or a general superposition of \nenergy eigenstates. On the other hand, the expectation values of position or momentum are time \nindependent when the system is in an energy eigenstate, but they are time dependent for a general \nsuperposition state. Let’s demonstrate this in the inﬁnite square well where the time dependence of \na general state is\n \nc 1x, t2 = a\nn\ncnA\n2\nL\n sin npx\nL\n e-i n2p2Ut>2mL2 . \n(5.126)\nConsider a simple superposition of two states in an inﬁnite well. If the initial state is\n \n0 c 1029 =\n1\n12 0 E19 +\n1\n12 0 E29, \n(5.127)\nthen the time-evolved state is\n \n0 c 1t29 =\n1\n12 0 E19e-iE1t>U +\n1\n12 0 E29e-iE2t>U. \n(5.128)\nThe wave function representation is\n \n c 1x, t2 =\n1\n12 w11x2e-iE1t>U +\n1\n12 w21x2e-iE2t>U\n \n \n = A\n1\nL\n c sin px\nL\n e-iE1t>U + sin 2px\nL\n e-iE2t>U d . \n \n(5.129)\nNow ﬁnd the expectation value of the position:\n \n 8x9 = 8c 1t2 0 x0 c 1t29\n \n \n = E 1\n12 8E10 eiE1t>U +\n1\n12 8E20 eiE2t>UF\n \nx E 1\n12 0 E19e-iE1t>U +\n1\n12 0 E29e-iE2t>UF\n (5.130)\n \n = 1\n2 38E10 x0 E19 + 8E20 x0 E29 + 8E10 x0 E29ei1E1-E22t>U + 8E20 x0 E19e-i1E1-E22t>U4. \nE1\nE2\nE3\nE4\nE\n0.5\n1.0\nP\nPE1\nPE2\nPE3\nPE4\nFIGURE 5.21 Histogram of the probabilities of energy measurements.\n\n144 \nQuantized Energies: Particle in a Box\nAgain notice that we are using Dirac notation to simplify the calculation. However, at this point we \nneed to use integrals to calculate the matrix elements. Let’s deﬁne them in general:\n \n 8x9n = 8En0 x0 En9 =\n \nL\nL\n0\nw*\nn1x2  x wn1x2dx =\n \nL\nL\n0\nx0 wn1x2 0 2 dx \n \n 8x9nk = 8En0 x0 Ek9 =\n \nL\nL\n0\nw*\nn1x2  x wk1x2dx . \n \n(5.131)\nWe calculated the ﬁrst matrix element, which is the expectation value of position in an energy eigen-\nstate, in Example 5.3. We saw that the answer is the midpoint of the well L>2. The second integral \ncomes from the cross term in the superposition:\n \n 8x9nk =\n \nL\nL\n0\nw*\nn1x2  x wk1x2dx\n \n \n = 2\nL L\nL\n0\n sin anpx\nL b x sin akpx\nL b dx \n \n = 2\nL\n a L\npb\n2\nL\np\n0\ny sin1ny2 sin1k y2 dy . \n \n(5.132)\nSimplify with a trig identity and integrate\n \n 8x9nk = 2\nL\n a L\npb\n2\nL\np\n0\ny 1\n2 3 cos1n - k2y -  cos1n + k2y4dy\n \n \n = 1\nL\n a L\npb\n2\n c\n cos1n - k2y\n1n - k22\n+\ny sin1n - k2y\n1n - k2\n-\n cos1n + k2y\n1n + k22\n-\ny sin1n + k2y\n1n + k2\nd\np\n0\n \n \n = 1\nL\n a L\npb\n2\n c\n cos1n - k2p\n1n - k22\n-\n cos1n + k2p\n1n + k22\n-\n1\n1n - k22 +\n1\n1n + k22d ,\n \n(5.133)\nyielding\n \n8x9nk =\n-4Lnk\np21n2 - k222 C1 - 1-12\nn+kD . \n(5.134)\nThis result is zero for states where n + k is even (i.e., if the states have the same parity). The results for \nthe two-state example are\n \n 8x91 = 8x92 = L\n2\n \n \n 8x912 = 8x921 = -  16L\n9p2 , \n \n(5.135)\n\n5.7 Superposition States and Time Dependence \n145\ngiving the ﬁnal result\n \n 8x9 = 8c1t2 0 x0 c1t29\n \n \n = 1\n2\n c L\n2 + L\n2 - 16L\n9p2 ei 1E1-E22 t>U - 16L\n9p2 e-i 1E1-E22 t>U d  \n \n = L\n2\n c 1 - 32\n9p2 cos a 3p2U\n2mL2 tb d .\n \n(5.136)\nThe position of this two-state superposition oscillates at the Bohr frequency 1E2 - E12>U.\nThe time-dependent position is also evident in the spatial probability density:\n \n P1x, t2 = 08x0 c1t290\n2 = 0 c1x, t20\n2\n \n \n = ` A\n1\nL\n c  sin px\nL\n e-iE1t>U +  sin 2px\nL\n e-iE2t>U d `\n2\n \n \n = 1\nL\n csin2 px\nL +  sin2 2px\nL\n+ 2 sin px\nL\n sin 2px\nL\n cos 1E2 - E12t\nU\nd . \n(5.137)\nThe oscillation of the probability density is depicted in the animation frames shown in Fig. 5.22, \nwhere the constant t is the oscillation period t = 2p>vBohr (see activity on time evolution of inﬁnite \nwell solutions). The superposition probability distribution “sloshes back and forth” in the well at the \nBohr frequency. This motion of the superposition state provides a model for how atoms and other \nbound systems radiate light. An electron undergoing this oscillatory motion accelerates and hence \nradiates electromagnetic energy. So far, our model does not account for the energy loss from this \nradiation, but we will address that in Chapter 14.\n0\nL/2\nL\nt/\u0004 = 0.0\nt/\u0004 = 0.1\nt/\u0004 = 0.2\nt/\u0004 = 0.3\nt/\u0004 = 0.4\nt/\u0004 = 0.5\nFIGURE 5.22 Time dependence of the probability distribution of a superposition state.\n\n146 \nQuantized Energies: Particle in a Box\nA calculation of the momentum expectation value (Problem 5.27) also yields a time-dependent result:\n \n 8  p9 = 8c1t20 p0 c1t29\n \n \n =\n \nL\nL\n0\nc*1x, t2aU\ni\n  d\ndxb c 1x, t2dx \n \n = 8\n3 U\nL\n sin a 3p2U\n2mL2 tb.\n \n(5.138)\nIf we compare Eqs. (5.136) and (5.138), we notice that the quantum mechanical position and momen-\ntum obey the classical relation p = mv, provided we restrict the relation to expectation values:\n \n8p1t29 = m \nd 8x1t29\ndt\n. \n(5.139)\nThis is another example of Ehrenfest’s theorem, which says that quantum mechanical expectation \nvalues obey classical laws.\n 5.8 \u0002 MODERN APPLICATION: QUANTUM WELLS AND DOTS\nThe square well potential problem has been a staple of quantum mechanics textbooks since the early \ndays. However, for many years it was only a textbook problem because no systems in nature could be \nmodeled accurately as a square well. The progress of semiconductor fabrication technology has changed \nthat, as we are now able to make artiﬁcial systems of square potential energy wells. Semiconductor \nquantum wells are now routinely used to fabricate diode lasers and other semiconductor devices.\nThe key advance that allowed fabrication of quantum well devices was the ability to grow pure \ncrystals of semiconductors using techniques such as molecular beam epitaxy (MBE) and metal-\norganic chemical vapor deposition (MOCVD). With these techniques, layers of semiconductors can be \ngrown with atomic scale precision, yielding structures with layers thin enough (several nm or less) for \nquantum effects to be important.\nA typical quantum well structure is shown in Fig. 5.23(a). Alternate layers of GaAs and AlGaAs \nare grown epitaxially on a GaAs substrate. GaAs and AlGaAs have similar crystal unit cell sizes that \npermit dislocation-free crystals to be grown. This lattice-matched growth is crucial to obtaining reli-\nGaAs substrate\nAlGaAs\nGaAs\nAlGaAs\nGaAs\nsubstrate\nAlGaAs\nGaAs\nAlGaAs\nconduction band\nvalence band\nEg\nGaAs\nEg\nAlGaAs\nVc(x)\nVv(x)\n(b)\n(a)\nFIGURE 5.23 (a) Structure and (b) potential energy diagram of a GaAs quantum well.\n\n5.9 Asymmetric Square Well: Sneak Peek at Perturbations \n147\nable devices. The band gap of GaAs (1.42 eV) is smaller than the band gap of AlGaAs (2.67 eV), so the \nelectrons in the conduction band and the holes in the valence band experience the different potentials \nshown in Fig. 5.23(b). Because the layers change on the atomic scale, this is as close to a square well \nas nature allows.\nWe can calculate the energy levels in the well using the same analysis we used for the ﬁnite square \nwell. Figure 5.24 shows the energy levels and how they vary with changes in the GaAs layer thickness. \nNote that there are only two or three bound states in the well for the range of thickness shown.\nFor making practical devices with quantum wells, there are two important features. First, the \nenergy levels can be adjusted, or “tuned,” by changing the thickness of the quantum well layer, as \nshown in Fig. 5.24, or by changing the stoichiometry of the surrounding AlxGa1 -  xAs layers to adjust \nthe band gap and hence the potential energy depth of the well. Second, the quantization of the electron \nenergy in the conﬁned well increases the number of electrons with speciﬁc energies (compared to the \ncontinuum of energies of unconﬁned electrons), which in turn increases the probability of creating \nphotons with the corresponding wavelengths. Hence, a semiconductor diode laser made with quantum \nwells is more efﬁcient than one made with bulk material, so quantum well diode lasers are now the \nmost common type of diode lasers in use.\nThe quantum well structure shown in Fig. 5.23 conﬁnes the electron in one dimension, but the \nelectrons are not conﬁned in the plane of the thin well. Further conﬁnement leads to quantum wires \n(2D conﬁnement) and quantum dots (3D conﬁnement). Quantum dots are semiconductor nanocrys-\ntals with a typical size range of 2–20 nm. The size of the dot determines the conﬁnement size and \nhence the wavelength of light emitted by the dot. A simple Web search reveals beautiful pictures of \nquantum dots glowing in a rainbow of colors.\n 5.9 \u0002 ASYMMETRIC SQUARE WELL: SNEAK PEEK AT PERTURBATIONS\nWhile the square potential wells we have studied in this chapter illustrate many of the ideas of bound \nstate wave functions, there is one important aspect that we have not encountered. All the square well \nsolutions have a constant wave vector and a constant wave function amplitude throughout the well, \nbecause the potential is constant throughout the well. To see how the wave vector and amplitude of \n5\n10\n15\n20 Well width (nm)\n20\n40\n60\n80\n100\nEnergy (meV)\n\u00021\u0003\n\u00022\u0003\n\u00023\u0003\nFIGURE 5.24 Energy levels in a GaAs quantum well as the thickness of the GaAs layer is changed.\n\n148 \nQuantized Energies: Particle in a Box\nan eigenstate can vary within the well, let’s make a slight modiﬁcation to the inﬁnite square well. \nConsider the well shown in Fig. 5.25, which is commonly referred to as the asymmetric square \nwell. By adding a “shelf” within the well, we now have two regions of constant but different poten-\ntial energy.\nThe potential energy for this asymmetric square well is\n \nV1x2 = μ\n\u0005,\n0,\nV0,\n\u0005,\n      \nx 6 0\n0 6 x 6 L>2\nL>2 6 x 6 L\nx 7 L.\n  \n(5.140)\nWe know that the inﬁnite potential outside the well demands that the energy eigenstates are zero outside \nthe well. Inside the well, we now have different energy eigenvalue equations in the left and right halves:\n \n a-  U2\n2m d 2\ndx2 + 0b wE 1x2 = EwE 1x2,    left half\n \n \n a-  U2\n2m d 2\ndx2 + V0b wE 1x2 = EwE 1x2, \n right half. \n \n(5.141)\nFor this discussion, let’s assume that the energy E is greater than the potential V0 so that the \nsolutions in each half of the well are sinusoidal. We then have different wave vectors in each half, \ndeﬁned by\n \n k1 = B\n2mE\nU2 , \n left half\n \n \n k2 = B\n2m 1E - V02\nU2\n,    right half, \n \n(5.142)\nwhich yields a smaller wave vector 1k2 6 k12 and hence larger wavelength of the wave in the right \nhalf. We know that the left-half solution must be a sine function in order to match the zero wave func-\ntion outside the well, so the general solution is\n \nwE 1x2 = e\nA sin k1x ,\nB sin k2x + C cos k2x ,   0 6 x 6 L>2\nL>2 6 x 6 L. \n(5.143)\n0\nL/2\nL\nV0\nx\nV(x)\n\u0002\nFIGURE 5.25 Asymmetric square well.\n\n5.9 Asymmetric Square Well: Sneak Peek at Perturbations \n149\nNow we apply the boundary condition on the wave function continuity at the middle and right side of \nthe well and the boundary condition on the continuity of the ﬁrst derivative of the wave function at the \nmiddle of the well (recall that the inﬁnite potential on the right means that the derivative condition is \nnot applicable). The three boundary conditions are\n \n wE 1L>22: A sin1k1L>22 = B sin1k2L>22 + C cos1k2L>22\n \n \n dwE1x2\ndx\n2\nx=L>2\n: k1A cos1k1L>22 = k2B cos1k2L>22 - k2C sin1k2L>22 \n \n wE 1L2: B sin k2L + C cos k2L = 0.\n \n \n(5.144)\nThese three equations contain four unknowns: the amplitudes A, B, and C, and the energy E through \nthe wave vectors k1 and k2. The normalization condition supplies the fourth equation required to solve \nfor all unknowns. By eliminating the amplitude coefﬁcients from the three boundary condition equa-\ntions, we arrive at a transcendental equation for the energy eigenvalues (Problem 5.28):\n \nk1 cos1k1L>22sin1k2L>22 + k2 cos1k2L>22sin1k1L>22 = 0. \n(5.145)\nThis looks a bit intimidating, so how do we know it’s correct? Well, we know what the solutions are \nfor the inﬁnite (symmetric) square well, which is the case where V0 = 0; so we can check to see if our \nsolution agrees with the inﬁnite square well solutions. This won’t tell us whether our solution is cor-\nrect, but we can at least make sure that it is not obviously wrong. If V0 = 0, then the two wave vectors \nare equal and the transcendental equation becomes:\n \n k1cos1k1L>22sin1k1L>22 + k1 cos1k1L>22sin1k1L>22 = 0 \n \n k1 sin31k1L>22 + 1k1L>224 = 0\n \n \n k1 sin k1L = 0.\n \n \n(5.146)\nIf we divide this result by k1, then we have the same equation  sin k1L = 0 that we had for the inﬁnite \nsquare well. So our intimidating result may well be correct.\nIn order to compare the asymmetric square well with the inﬁnite square well, it is useful to divide \neach transcendental equation by the factor k1 and plot the energy eigenvalue equations for the asym-\nmetric square well\n \ncos1k1L>22sin1k2L>22 + k2\nk1\n cos1k2L>22sin1k1L>22 = 0 \n(5.147)\nand for the inﬁnite square well:\n \nsin1k1L2 = 0. \n(5.148)\nA plot of the two equations as a function of k1L is shown in Fig. 5.26 for the case where the potential \nstep height is 0.75 times the energy of the ground state in the inﬁnite well case. The inﬁnite square well \neigenstates occur at the values k1L = np marked on the axis. The eigenstates for the asymmetric well \nare each slightly larger, with the difference decreasing as the energy increases. This is a sneak preview \nof perturbation theory that we will study in Chapter 10.\nLet’s now use these solutions to draw the energy eigenstates. A plot of a typical energy eigen-\nstate is shown in Fig. 5.27. The wavelength and the amplitude of the wave in the right half are larger, \nmeaning that the probability to ﬁnd the particle in the right half is larger than in the left half. This is \nconsistent with our classical expectation, because a classical particle moves more slowly in the right \nhalf where its kinetic energy is lower, and so it spends more time in the right half with an increased \nprobability to ﬁnd it there.\n\n150 \nQuantized Energies: Particle in a Box\n 5.10 \u0002 FITTING ENERGY EIGENSTATES BY EYE OR BY COMPUTER\n 5.10.1 \u0002 Qualitative (Eyeball) Solutions\nThe problems we have solved in this chapter illustrate most of the important features of bound states in \npotential wells. Using these common traits allows us to make qualititative estimates of energy eigen-\nstate solutions to other potential well problems. The important features are\n 1(a). Oscillatory wave solution inside well\n 1(b). Wavelength proportional to 1> 2E - V1x2\n 2(a). Exponentially decaying solution outside well\n 2(b). Decay length proportional to 1> 2V1x2 - E\n3.  Amplitude inside well related to wavelength\n4.  Match wE1x2 and dwE1x2>dx at boundaries.\nUsing these rules of thumb, we can get a very good idea of the wave function before we tackle the dif-\nferential equation that gives us the exact solution.\nConsider the potential shown in Fig. 5.28. It has an inﬁnite wall, a ﬂat potential region, a sloped \npotential region, and a ﬁnite wall. Given our rules, we draw the approximate wave function. From left \nx\nE7\n0\nL/2\nL\nV0\nFIGURE 5.27 An energy eigenstate of the asymmetric square well.\nΠ\n2Π\n3Π\n4Π\nk1L\nF(k1L)\nFIGURE 5.26 Transcendental equations for the energy eigenvalues of \nthe asymmetric square well (solid) and the inﬁnite square well (dashed).\n\n5.10 Fitting Energy Eigenstates by Eye or by Computer \n151\nto right, starting at zero at the inﬁnite wall, the wave function oscillates with a constant wavelength and \nhas a constant amplitude over the ﬂat potential region; it oscillates with an increasing wavelength and \nhas an increasing amplitude over the sloped potential region; and then it exponentially decays in the \nclassically forbidden region. The wave function is drawn qualitatively and the main features are indi-\ncated. This wave function represents the 17th energy state because there are 17 antinodes in the wave \nfunction. Remember that the wave function oscillates about the value zero in the well and decays to \nzero outside the well. The ﬁgure shows the wave function c1x2 drawn superimposed on the potential \nwell, so you have to imagine a “c axis” with its zero as indicated by the dashed line.\n 5.10.2 \u0002 Numerical Solutions\nWe can be more quantitative by using a computer to help us “draw” the wave functions. Rather than \nfollow the rules listed above, we directly solve the energy eigenvalue equation by numerical integra-\ntion, which is a common technique for solving differential equations and is easily accomplished in \ncommon mathematical packages like Matlab, Mathematica, and Maple, and even in a spreadsheet. The \nenergy eigenvalue equation is\n \nd 2wE1x2\ndx2\n= -  2m\nU2  3E - V1x24wE1x2. \n(5.149)\nYou may not yet know how to solve such a differential equation, but you do know how to solve a very \nsimilar one—Newton’s second law, F = ma, which yields the differential equation\n \nd 2x\ndt 2 = F\nm. \n(5.150)\nIn the case where the acceleration a = F>m is constant, one integral of Eq. (5.150) gives\n \nv = dx\ndt = v0 + at, \n(5.151)\nE,Ψ\nMatch Ψ\nMatch Ψ,dΨ\u0005dx\nOscillating wave\nExponential decay\nConstant Λ,\namplitude\nIncreasing Λ,\namplitude\nΨ\u0007\b\u0007\t\n0\nL/2\nL\nV0\nV0/2\nx\nFIGURE 5.28 Drawing approximate energy eigenstate solutions.\n\n152 \nQuantized Energies: Particle in a Box\nand a second integration gives\n \nx = x0 + v0\n t + 1\n2\n at 2, \n(5.152)\nwhich are the equations of motion you learned in introductory physics. With these equations, one can \npredict the future if one knows the initial position x0, the initial velocity v0, and the acceleration a.\nIn the Newtonian case, the motion function x(t) is determined by its curvature d 2x>dt 2, which is \nthe acceleration a. In the quantum case, the wave function is determined by its curvature d 2c>dx 2, which \ndepends on the energy, the potential, and the wave function itself. The potential and the wave function \nboth depend on position, so the wave function curvature is not constant and the simple integrations in \nEqs. (5.151) and (5.152) cannot be used. However, if the acceleration in the Newtonian example is not \nconstant, then we can modify Eqs. (5.151) and (5.152) for use on a computer by using them to predict \nmotion only in the very near future, say from t to t + \u0006t:\n \n x1t + \u0006t2 = x1t2 + v1t2\u0006t + 1\n2\n a1t21\u0006t2\n2 \n \n v1t + \u0006t2 = v1t2 + a1t2\u0006t.\n \n(5.153)\nAs long as we choose the time steps \u0006t small enough that the acceleration does not vary appreciably from \none time step to the next, then these equations can be used to reliably update the position and velocity at \neach time step. These update equations produce estimates of the full motion by iterating from step to step.\nThis method works well but suffers from one failing: the update equations use “old” information \nabout the velocity and the acceleration. We can improve this slightly by using the new acceleration in \nthe velocity update equation:\n \n x1t + \u0006t2 = x1t2 + v1t2\u0006t + 1\n2\n a1t21\u0006t2\n2\n \n \n v1t + \u0006t2 = v1t2 + 1\n2 3a1t2 + a1t + \u0006t24\u0006t. \n(5.154)\nWe can’t use the new acceleration in the position update equation because the acceleration typically \ndepends on position (through the potential), so we do the position update ﬁrst and then the modiﬁed \nvelocity update. This method is known as the velocity Verlet algorithm and yields more reliable \nresults than Eq. (5.153).\nTo solve the energy eigenvalue equation, we use the wave function and its spatial derivatives \nrather than the position and its time derivatives used in the Newtonian case. Thus, we generalize the \nposition and velocity update equations (5.154) to\n \n wE1x + \u0006x2 = wE1x2 + adwE\ndx b\nx  \n\u0006x + 1\n2\n ad 2wE\ndx2 b\nx\n1\u0006x2\n2\n \n \n adwE\ndx b\nx+\u0006x\n= adwE\ndx b\nx\n+ 1\n2\n c a d 2wE\ndx2 b\nx\n+ ad 2wE\ndx2 b\nx+\u0006x\nd \u0006x. \n(5.155)\nSo, given the wave function (analogous to “position”), the slope of the wave function (“velocity”), and \nthe curvature of the wave function (“acceleration”) at any position x (“time”), we can predict the wave \nfunction and its slope at the next position x + \u0006x. At each step we calculate the wave function curva-\nture using the energy eigenvalue equation\n \nd 2wE1x2\ndx2\n= -  2m\nU2  3E - V1x24wE1x2. \n(5.156)\n\n5.10 Fitting Energy Eigenstates by Eye or by Computer \n153\nWe don’t have to impose the continuity conditions on wE1x2 and dwE1x2>dx at boundaries; the \nupdate equations guarantee that they are met. What we do need are initial values of the wave function \nand the ﬁrst derivative to get the update equations started. In principle, we should start at x = - \u0005 \nand integrate (i.e., update) all the way to x = + \u0005. In practice, it sufﬁces to start a reasonable way \ninto the left-hand forbidden region, integrate into and through the potential well, and then integrate \na reasonable way into the right-hand forbidden region. The wave function in the forbidden region \nshould be decaying toward zero as it approaches x = { \u0005, which indicates how we should choose \nthe initial values of the wave function and the ﬁrst derivative. Recall, however, that the energy eigen-\nvalue equation is linear in the wave function wE1x2, so we can scale the wave function by any factor \nand it will still solve the differential equation. This means that we can choose the initial wave function \narbitrarily, but the resultant wave function will not be normalized. In principle, the initial wave func-\ntion slope should be chosen to have the appropriate decay length. In practice, the method is insensitive \nto this choice.\nNotice that the calculation of the wave function curvature from the energy eigenvalue equation \n(5.156) requires us to know the energy. But we don’t know the energy—we are trying to ﬁnd it! So we \nguess a value of the energy and then we solve for the resultant wave function and see if it “ﬁts” into the \npotential well. From the problems above we have plenty of practice recognizing wave functions that \nﬁt, so it should be clear. And it is, as you will see.\nAs an example of how this numerical technique works, let’s try it out on the ﬁnite square well \nand compare to the results in Eq. (5.89). We choose an energy and start integrating with Eq. (5.155). \nThis is well suited to a spreadsheet, and the results shown in Fig. 5.29 are from an Excel worksheet. \nThe trademark results of this technique are illustrated in Fig. 5.29(a). If the chosen energy does not \nmatch an energy eigenstate solution, then as we integrate toward x = + \u0005 the wave function solution \nthat should decay starts to grow exponentially, because as the integration crossed the boundary into \nthe classically forbidden region (at x = a) there was a small component of the growing exponential \nsolution contained in the numerical wave function. Only by choosing the energy exactly equal to one \nof the allowed energies can this “bad” component be eliminated from the integration. Because of the \nseverity of exponential growth, combined with the discreteness of computer calculations, it is impos-\nsible to ﬁnd the energy solution exactly. However, as Fig. 5.29 illustrates, you can ﬁnd nearby energies \nthat cause the wave function to grow either negatively [Fig. 5.29(a)] or positively [Fig. 5.29(c)]. These \nsolutions then bracket the approximate solution [Fig. 5.29(b)]. The ﬁnite square well used for the cal-\nculation in Fig. 5.29 is the same as the well used for Fig. 5.16, and the resultant energy eigenvalue of \nthis fourth energy level matches well with the result in Eq. (5.89). To obtain a more accurate value, one \nhas to be more careful about the initial conditions.\nΨ(x)\nΨ(x)\nΨ(x)\n(a)\n(b)\n(c)\n\na\na\nx\n\na\na\nx\n\na\na\nx\nE = 27.30454\nE = 27.30455\nE = 27.30456\nFIGURE 5.29 Numerical integration for solution of the ﬁnite square well eigenvalue equation.\n\n154 \nQuantized Energies: Particle in a Box\n 5.10.3 \u0002 General Potential Wells\nGiven our approximate and numerical techniques, we can solve for the bound states in any potential \nwell, in principle. A typical bound state solution is shown in Fig. 5.30. It exhibits the key features that \nwe have mentioned above for bound state solutions:\n• Oscillatory in allowed region\n• Exponential decay in forbidden region\n• Oscillatory wave becomes less wiggly near classical turning point as kinetic energy \ndecreases\n• Amplitude becomes larger near classical turning points\nThus, though potential energy wells may appear quite different at ﬁrst glance, they all can be \ncalled “particle-in-a-box” systems, albeit with differently shaped boxes. Some common boxes are \nshown in Fig. 5.31: (a) inﬁnite square well, (b) ﬁnite square well, (c) harmonic oscillator (mass on a \nspring), and (d) linear potential (bouncing ball potential).\nSUMMARY\nIn this chapter we learned the language of the wave function, which is the representation of the quan-\ntum state vector in position space. We express this as\n \n 0 c9 \u0003 c1x2  \n \n c1x2 = 8x0 c9. \n(5.157)\nThe complex square of the wave function yields the spatial probability density\n \nP1x2 = 0 c1x20\n2. \n(5.158)\nThe normalization condition is\n \n1 = 8c@ c9 =\n \nL\n\u0005\n- \u0005\n@ c1x2@\n2 dx = 1. \n(5.159)\nThe rules for translating bra-ket formulae to wave function formulae are:\n1) Replace ket with wave function \n0 c9 S c1x2\n2) Replace bra with wave function conjugate \n8c0 S c*1x2\n3) Replace bracket with integral over all space \n8@ 9 S\n \nL\n\u0005\n- \u0005\n dx\n4) Replace operator with position representation \nAn S A1x2.\nThe probability of measuring the position of a particle to be in a ﬁnite spatial region is\n \nPa6x6b =\n \nL\nb\na\n0 c1x20\n2 dx. \n(5.160)\n\nSummary \n155\nx\nE,Ψ\nFIGURE 5.30 Bound state in a generic potential energy well.\n0\n(a)\nL/2\nL\nx\n0\n(b)\n\na\na\nx\nx\n(c)\nx\n(d)\nFIGURE 5.31 Different versions of the particle-in-a-box: (a) inﬁnite square well, \n(b) ﬁnite square well, (c) harmonic oscillator (quadratic potential), and (d) linear potential.\n\n156 \nQuantized Energies: Particle in a Box\nThe probability of measuring the energy to be En is\n \nPEn = @8En@ c9@\n2 = 2\nL\n\u0005\n- \u0005\nw*\nn1x2c1x2dx 2\n2\n, \n(5.161)\nwhere wn1x2 = 8x0 En9 is the wave function representation of the energy eigenstate.\nPosition and momentum operators in the position representation are\n \n xn \u0003 x\n \n \n pn \u0003 -iU d\ndx \n(5.162)\nand lead to the energy eigenvalue equation becoming a differential equation:\n \na-  U2\n2m d 2\ndx2 + V1x2\n b wE1x2 = EwE1x2. \n(5.163)\nIn solving the energy eigenvalue equation, two boundary conditions are imposed upon the wave function:\n1) wE1x2 is continuous\n2) \ndwE1x2\ndx\n is continuous unless V = \u0005.\nIn an inﬁnite square potential energy well, the allowed energies are\n \nEn = n2p2\n U2\n2mL2\n ,   n = 1, 2, 3, ..., \n(5.164)\nand the allowed energy eigenstates are\n \nwn1x2 = A\n2\nL\n sin  npx\nL ,   n = 1, 2, 3, ... . \n(5.165)\nThe energy eigenstates obey the following properties:\nProperty\nDirac notation\nWave function notation\nNormalization\n8En0En9 = 1\nL\n\u0005\n- \u0005\n0wn1x20\n2dx = 1\nOrthogonality\n8En0Em9 = dnm\nL\n\u0005\n- \u0005\nw*\nn1x2wm1x2dx = dnm\nCompleteness\n0c9 = a\nn\ncn0En9\nc1x2 = a\nn\ncnwn1x2\nPROBLEMS\n 5.1 Show that the operators xn and pn do not commute.\n 5.2 A particle in an inﬁnite square well potential has an initial state vector \n0 c1t = 029 = A10 w19- 0 w29 + i0 w392 where 0 wn9 are the energy eigenstates.\n\nProblems \n157\na) Normalize the state vector.\nb)  What are the possible outcomes of a measurement of the energy, and with what probabilities \nwould they occur?\nc) What is the average value of the energy?\nd) Find the state vector at some later time, t.\ne)  At time t = U>E1, what are the possible outcomes of a measurement of the energy, and with \nwhat probabilities would they occur?\n 5.3 Solve the inﬁnite square well problem using the complex exponential form of the general solu-\ntion in Eq. (5.53) as the assumed form of the wave function inside the well. Assume that the \npotential well boundaries are at x = 0 and x = L.\n 5.4 Solve the inﬁnite square well problem with the well boundaries at x = {a. Comment on the \ndifferences and similarities with the solution in the text.\n 5.5 Calculate the expectation values and the uncertainties of position and momentum for the inﬁ-\nnite square well energy eigenstates.\n 5.6 For a particle in an inﬁnite square well, calculate the probability of ﬁnding the particle in the \nrange 3L>4 6 x 6 L for each of the ﬁrst three energy eigenstates.\n 5.7 A particle in an inﬁnite square well potential has an initial state vector \n0 c1t = 029 = 10 w19 - 2i0 w292> 15 where the 0 wn9 are the eigenfunctions of the Hamiltonian \noperator. Find the time evolution of the state vector.\n 5.8 A particle in an inﬁnite square well potential has an initial wave function \nc1x, t =\n 02 = Ax1L - x2. Find the time evolution of the state vector. Find the expectation \nvalue of the position as a function of time.\n 5.9 A particle in an inﬁnite square well has the initial wave function\n \nc1x, 02 = A c a x\nLb\n3\n-\n 3\n2\n a x\nLb\n2\n+ 1\n2\n a x\nLbd \n \n in the interval 0 6 x 6 L and zero elsewhere. Find (a) the wave function at a later time, (b) the \nprobabilities of energy measurements, and (c) the expectation value of the energy.\n 5.10 A particle at t = 0 is known to be in the right half of an inﬁnite square well with a probability \ndensity that is uniform in the right half of the well. What is the initial wave function of the par-\nticle? Calculate the expectation value of the energy. Find the probabilities that the particle is \nmeasured to have energy E1, E2, or E3.\n 5.11 A particle is in the ground state of an inﬁnite square well. The potential wall at x = L suddenly \nmoves to x = 3L such that the well is now three times its original size. Find the probabilities \nthat the particle is measured to have the ground state energy or the ﬁrst excited state energy of \nthe new well.\n 5.12 Show that the energy eigenstates of the inﬁnite square well are orthogonal.\n 5.13 Use the closure relation in Eq. (5.101) to show that the normalization condition is\n \n1 = 8c@ c9 = a\nn\n@8En@ c9@\n2. \n 5.14 Solve the energy eigenvalue problem for the ﬁnite square well without using the symmetry \nassumption and show that the energy eigenstates must be either even or odd.\n\n158 \nQuantized Energies: Particle in a Box\n 5.15 Derive the transcendental equation (5.85) for the energy eigenvalues of the odd states in the \nﬁnite square well.\n 5.16 Normalize the energy eigenstates of the ﬁnite square well.\n 5.17 Find the probability that a particle in the ground state of a ﬁnite square well is measured to \nhave a position outside of the well. Derive a general relation involving only the parameters z \nand z0 deﬁned in Eqs. (5.86). Show that the probability increases as the energy increases.\n 5.18 An electron is bound in a ﬁnite square well of depth V0 = 5 eV and width 2a = 1.5 nm. Find \nthe allowed energies of the bound states in the well using the transcendental equations (5.88).\n 5.19 Give a qualitative, graphical argument that the difference in energy eigenvalues between the \nﬁnite and inﬁnite square wells is larger for higher energy states.\n 5.20 Find the bound energy eigenstates and eigenvalues of a “half-inﬁnite” square well (i.e., a \nsquare well with inﬁnite potential for x 6 0 and ﬁnite potential with value V0 for x 7 L).\n 5.21 Consider a quantum system with a set of energy eigenstates 0 Ei9. The system is in the state\n \n0 c9 =\n1\n130 0 E19 +\n2\n130 0 E29 +\n3\n130 0 E39 +\n4\n130 0 E49, \n \n where the energies are given by En = nE1. Find the probabilities for measuring the energy \neigenvalues and make a histogram similar to Fig. 5.2(b). Find the expectation value of the \nenergy. Find the uncertainty of the energy.\n 5.22 Consider a quantum system with a set of energy eigenstates 0 En9 where the energies are given \nby En = 1n + 1\n22U v for n = 0, 1, 2, ... . The system is in the state\n \n0 a9 = a\n\u0005\nn=0\nane-a2>2\n2n!\n0 En9, \n \n where a is a positive real number. Find the probabilities for measuring the energy eigenvalues \nand make a histogram similar to Fig. 5.2(b). Find the expectation value of the energy. Find the \nuncertainty of the energy.\n 5.23 Consider the following wave functions\n \n c1x2 = Ae-x2>3\n \n c1x2 = B \n1\nx2 + 2\n \n c1x2 = C secha x\n5b.\n \n In each case, normalize the wave function, plot the wave function, and ﬁnd the probability that \nthe particle is measured to be in the range 0 6 x 6 1.\n 5.24 Demonstrate the requirement that the ﬁrst derivative of the wave function be continuous, \nunless the potential is inﬁnite. To do this, integrate the energy eigenvalue equation from -e \nto +e and take the limit as e S 0 to derive a condition on the difference of the wave function \nderivatives between two adjacent points.\n 5.25 Find the energy eigenstates and eigenvalues of a particle conﬁned to a delta function potential \nV1x2 = -b d1x2, where b is a positive real constant. Note that you will need to follow the \napproach in the previous problem to properly address how the inﬁnite potential at the origin affects \nthe wave function derivative. How many bound energy states exist in this potential energy well?\n\nResources \n159\n 5.26 Find the energy eigenstates and eigenvalues of a particle conﬁned to a double delta function \npotential V1x2 = -b 1d1x - a2 + d1x + a22, where b is a positive real constant. How many \nbound energy states exist in this potential energy well?\n 5.27 Calculate the expectation value of the momentum for the two-state superposition in Eq. (5.128) \nand verify Eq. (5.138).\n 5.28 Solve the boundary condition equations (5.144) for the asymmetric square well and verify \nEq. (5.145).\n 5.29 Find the transcendental equation that determines the energy eigenvalues in an asymmetric \nsquare well for the case E 6 V0. Compare with Eq. (5.145) for the E 7 V0 case and comment.\n 5.30 Implement the update equations (5.155) using a spreadsheet or other computer program and \nﬁnd the numerical solutions for the energy eigenvalues of a ﬁnite square well with a well \nparameter z0 = 6. Compare your results with Eq. (5.89).\n 5.31 Use a spreadsheet or other computer program to ﬁnd the numerical solution of the ground \nstate and ﬁrst excited state energy eigenvalues and wave functions for a ﬁnite square well with \nparameters V0 = 5 eV, 2a = 1.5 nm, and m = me. Compare your results with the transcen-\ndental equations (5.88).\n 5.32 Reproduce the results for the GaAs quantum well states shown in Fig. 5.24 using the transcen-\ndental equations (5.88). The relevant GaAs parameters are V0 = 0.1 eV and m = 0.067 me.\n 5.33 For each of the potential wells shown in Fig. 5.32, make a qualitative sketch of the two energy \neigenstate wave functions whose energies are indicated. For each energy state, identify the clas-\nsically allowed and forbidden regions. Discuss the important qualitative features of each state.\n 5.34 Sketch a copy of Fig. 5.30 and identify the classically allowed and forbidden regions. Which \nenergy eigenstate is drawn in Fig. 5.30? Make a similar plot for the next lower energy eigenstate.\nRESOURCES\nActivities\nThe bulleted activities are available at\nwww.physics.oregonstate.edu/qmactivities\n• Operators and Functions: Students investigate the differential forms of quantum mechanical \noperators and identify eigenfunctions and eigenvalues of quantum mechanical operators.\n• Solving the Energy Eigenvalue Equation for the Finite Well: Students solve the energy eigenvalue \nequation for different regions of the ﬁnite well and make their solutions match at the boundaries.\nE,Ψ\nx\nE11\nE4\nE,Ψ\nx\nE10\nE5\nFIGURE 5.32 Potential wells for Problem 5.33.\n\n160 \nQuantized Energies: Particle in a Box\n• Time Evolution of Inﬁnite Well Solutions: Students animate wave functions consisting of linear \ncombinations of eigenstates.\nQuantum Bound States: This simulation experiment from the PHET group at the \nUniversity of Colorado animates wave function superpositions in bound states: \nhttp://phet.colorado.edu/en/simulation/bound-states\nShooting Method Model: This program from the Open Source Physics group implements \nthe shooting method to numerically solve the energy eigenvalue equation: \nhttp://www.compadre.org/osp/items/detail.cfm?ID=6987\nFurther Reading\nQuantum wells are discussed in these Physics Today articles:\nD. Chemla, “Quantum wells for photonics,” Phys. Today 38(5), 57–64 (1985): \nhttp://dx.doi.org/10.1063/1.880974\nD. Gammon, D. Steel, “Optical studies of single quantum dots,” Phys. Today 55(10), 36–41 (2002): \nhttp://dx.doi.org/10.1063/1.1522165\nFurther details on numerical solutions of the energy eigenvalue equation are available in these \nreferences:\nR. H. Landau, M. J. Páez and C. C. Bordeianu, A Survey of Computational Physics: Introductory \nComputational Science, Princeton, NJ: Princeton University Press, 2008.\nH. Gould, J. Tobochnik, and W. Christian, An Introduction to Computer Simulation Methods: \n Applications to Physical Systems (3rd edition), San Francisco, CA: Addison-Wesley, 2007.\n\n \n161\nC H A P T E R \n6\nUnbound States\nIn the last chapter we learned how to use the new concept of wave functions to describe the motion of \na particle in a potential well. We found that states corresponding to particles conﬁned within the poten-\ntial well had quantized energies. We now turn our attention to unbound states, and we will ﬁnd that the \nenergies are no longer quantized. The simplest case is that of the free particle with no potential affect-\ning the particle motion at all. The free particle states help us better understand the wave-particle dual-\nity of quantum mechanics. We then consider the case of particles that are affected by potentials but are \nnot bound. This includes potential wells where the energy is larger than the well depth and cases where \nthe potential has no localized minimum. Studying these unbound states is important in understanding \nscanning tunneling microscopy, nuclear alpha decay, and the scattering of particles.\nIn all cases, we are still charged with solving the energy eigenvalue equation\n \nHn 0 E9 = E0 E9 \n(6.1)\nwith the Hamiltonian operator\n \nHn = pn2\n2m\n + V 1xn2. \n(6.2)\nAs we did in the last chapter, we work in wave function language (i.e., in the position representation), \nand so the energy eigenvalue equation becomes a differential equation:\n \n HnwE 1x2 = EwE 1x2  \n \n a-  U2\n2m\n d 2\ndx 2 + V  1x2b  wE 1x2 = EwE 1x2  \n \n -  U2\n2m\n d 2\ndx2 wE 1x2 + V 1x2wE 1x2 = EwE 1x2. \n \n(6.3)\n6.1 \u0002 FREE PARTICLE EIGENSTATES\n6.1.1 \u0002 Energy Eigenstates\nFor a free particle, the potential energy function V1x2 is zero everywhere and the energy eigenvalue \ndifferential equation is\n \nd 2\ndx2 wE 1x2 =  -  2mE\nU2  wE 1x2. \n(6.4)\n\n162 \nUnbound States\nThis is the same differential equation we solved in Chapter 5 inside the square potential energy well. \nAgain, it is convenient to deﬁne a wave vector\n \nk2 = 2mE\nU2  \n(6.5)\nand write the differential equation as\n \nd 2\ndx2 wE 1x2 =  -k2wE 1x2. \n(6.6)\nThe solutions to this differential equation are the familiar sinusoidal functions, which we can \nexpress either as the trigonometric functions sin kx and cos kx or the complex exponential functions \ne+ikx and e-ikx. Note that the energy E must be positive, so the wave vector is real for this problem. It \nis more convenient in this problem to use the complex exponential functions, so we write the general \nsolution to the energy eigenvalue equation as\n \nwE 1x2 = Ae+ikx + Be-ikx, \n(6.7)\nwhere we need to account for both possible signs of the wave vector and A and B are normalization \nconstants.\nThe critical physical difference between a free particle 1with V1x2 = 02 and a bound particle is \nthe lack of a conﬁning potential. Because the wave function of the free particle is not required to “ﬁt” \ninto the potential energy well, there are no limitations on the wave functions and hence no quantization \nof the energy. Mathematically, there are not enough constraints on the two normalization constants A \nand B and the energy E (through the wave vector k). There are three unknowns in Eq. (6.7), but the \nnormalization condition is the only constraining equation. The result is that the energy is a continuous \nvariable, not quantized, in contrast to the bound-state solutions in Chapter 5. The continuous nature of \nthe energy has important ramiﬁcations, which we will explore. But ﬁrst, let’s look more closely at the \nphysics of quantum wave motion.\nTo understand free particle wave motion, let’s look at the time evolution of the energy eigenstates \nof Eq. (6.7). The time dependence of this state is obtained by applying the recipe for Schrödinger time \nevolution that we learned in Chapter 3. Because the state is already written in the energy basis, the \nSchrödinger time-evolution recipe says to multiply by a phase factor dependent on the energy of the \nstate, giving\n \n cE1x, t2 = wE1x2e-i Et>U\n \n \n = 1Aeikx + Be-ikx2e-i Et>U. \n(6.8)\nIf we use the Einstein energy relation E = U v, we can rewrite Eq. (6.8) in a suggestive way:\n \n cE1x, t2 = 1Aeikx + Be-ikx2e-ivt\n \n \n = Aei1kx-vt2 + Be-i1kx+vt2\n \n \n = Aeik1x-vt>k2 + Be-ik1x+vt>k2. \n \n(6.9)\nThis quantum wave function has the same form we know from classical waves—a function f  1x { vt2 \nwith the argument 1x { vt2. This functional form represents a wave that retains its shape as it moves, \nand any given point on that shape moves with a speed determined by the parameter \r, which in this \ncase yields 0 v0 = v>k. For the sinusoidal waves of this free particle state, such points of constant \nphase move at the phase velocity. The energy eigenstate has two parts—the ei1kx-vt2 part moving in \nthe positive x-direction and the e-i1kx+vt2 part moving in the negative x-direction. So now we know \n\n6.1 Free Particle Eigenstates \n163\nthat whenever we see a wave function with spatial dependence e{ikx, the sign of the wave vector in the \nexponent indicates the direction of motion. It is convenient to work with the wave  vector eigenstates\n \nwk1x2 = Aeikx \n(6.10)\nas long as we remember that we must use both positive and negative k values to make a general energy \neigenstate.\n6.1.2 \u0002 Momentum Eigenstates\nTo learn more about the phase velocity of the wave vector eigenstates, it is useful to study the momen-\ntum of these wave functions. Let’s operate on one of the states with the momentum operator, which is \na differential operator in the position representation:\n \n pnwk1x2 = a-iU d\ndxb Aeikx \n \n = -iU1ik2Aeikx\n \n \n = Ukwk1x2.\n \n \n(6.11)\nThus the action of the momentum operator on a wave vector eigenstate yields the same state with \na constant multiplier. Well, that is an eigenvalue equation! So the wave vector eigenstates are also \nmomentum eigenstates. The momentum eigenvalue equation is\n \npnwp1x2 =  pwp1x2 \n(6.12)\n 1 pn 0  p9 = p0  p9 in bra@ket notation2, so we have identiﬁed\n \np = Uk \n(6.13)\nas the momentum eigenvalue and\n \nwp1x2 = Aei px>U \n(6.14)\nas the momentum eigenstate. The momentum eigenstate wave function wp1x2 is a function of position \nand not of momentum—x is a variable and p is the particular momentum eigenvalue. The wave \nvector is related to the wavelength through k = 2p>l, so we can rewrite Eq. (6.13) as\n \np = h\nl  . \n(6.15)\nThis equation was introduced in the early days of quantum mechanics by Louis de Broglie and pro-\nvides the connection between the particle properties (momentum) and the wave properties (wave-\nlength) of a system. The de Broglie relation between momentum and wavelength is at the heart of \nthe wave-particle duality of quantum mechanics. We can turn Eq. (6.15) around to write an equation \ndeﬁning the de Broglie wavelength of a particle with momentum p:\n \nlde Broglie = h\np  . \n(6.16)\nThe momentum eigenstates are also energy eigenstates for the free particle, with energy [Eq. (6.5)]\n \nE = p2\n2m. \n(6.17)\n\n164 \nUnbound States\nThe fact that the momentum and energy operators share eigenstates is an important aspect of the free \nparticle problem and is a consequence of the general rule we discussed in Section 2.4 that commuting \noperators have common eigenstates 1like Sz and S2 sharing 0{9 states2 (Problem 6.5). A given momen-\ntum eigenstate has a deﬁnite energy given by Eq. (6.17), but a given energy state does not necessarily \nhave a deﬁnite momentum, because a general energy eigenstate is a superposition of the two momentum \nstates 0  p9 \u0003 wp1x2 and 0  -p9 \u0003 w-p1x2 with opposite momenta, as in Eq. (6.7). Because a given \nenergy state corresponds to multiple momentum states, we say that the energy state is degenerate with \nrespect to momentum. In the free particle case, the energy states are two-fold degenerate. This is our \nﬁrst example of degeneracy, but it will be more common once we address two- and three-dimensional \nsystems in Chapter 7.\nThe wave nature of the quantum mechanical description of the free particle is evident in Fig. 6.1, \nwhich shows the wave function of a momentum eigenstate. It is evident that a single wavelength char-\nacterizes the wave function, consistent with the single momentum of the eigenstate and the de Broglie \nrelation between wavelength and momentum. The wave function is complex, so we must plot both the \nreal and imaginary parts to completely describe the state.\nLet’s now return to the question of the phase velocity of the free particle eigenstates. A momentum \neigenstate has time dependence\n \n cp1x, t2 = wp1x2e-i Ept>U\n \n \n = Aei px>U e-i p2t>2m U \n \n = Aei p>U1x-pt>2m2.  \n \n(6.18)\nThis wave is moving at a speed of v = p>2m, which is half the speed of a classical particle \nvclassical = p>m. This apparent contradiction exists because we are using the phase velocity of the \nwave. As we will see in Section 6.2, the proper way to use a wave to describe a particle leads us to the \nconcept of “group velocity of a wave packet” as the more appropriate velocity.\nA more serious problem with the momentum eigenstates becomes evident if we examine the prob-\nability density of the state. Taking the complex square of the wave function yields the probability density\n \nP1x2 = 0 wp1x20\n2\n \n \n = w*\np1x2wp1x2\n \n \n = A*e-i px>U Aei px>U \n \n = 0 A0\n2.\n \n \n(6.19)\nx\nRe \u000bp\u0007x\b\nx\nIm \u000bp\u0007x\b\n(a)\n(b)\nFIGURE 6.1 Momentum eigenstate. Both the (a) real and (b) imaginary parts of the wave \nfunction extend to {\u0005. A single wavelength characterizes the momentum eigenstate.\n\n6.1 Free Particle Eigenstates \n165\nAs shown in Fig. 6.2, the probability density of a momentum eigenstate is a constant independent of \nposition, extending to inﬁnity. This presents us with two problems. Conceptually, we expect a particle \nto be localized to a small region of space, not spread out over an inﬁnite region. Mathematically, we \ncannot normalize the momentum eigenstates because the integral of the probability density over all \nspace is inﬁnite. This is a new and quite serious problem. All previous basis states we have encoun-\ntered have been normalizable. This lack of normalizability is a pathology of all continuous bases—\nthis one being our ﬁrst example. Fortunately, there is a solution to this mathematical problem that \nalso solves our conceptual problem. By constructing superpositions of momentum eigenstates to make \nwave packets, we get wave functions that are normalizable and are localized to ﬁnite regions of space. \nBefore we construct wave packets, it is useful to discuss some of the mathematical properties of the \nmomentum eigenstates.\nWe expect a set of basis states to exhibit three important properties. The states should be: (1) nor-\nmalized, (2) orthogonal, and (3) complete. All the discrete basis sets we have encountered have satisﬁed \nthese conditions, which we express in Dirac notation as\n \n 8ai0 aj\u0002i9 = 0   orthogonality \n \n 8ai0 ai9 = 1   normalization \n \n a\ni\n0 ai98ai0 = 1   completeness, \n \n(6.20)\nassuming a set of discrete eigenstates 0 ai9. The orthogonality and normalization conditions are com-\nbined into one orthonormality equation by using the Kronecker delta:\n \n8ai0 aj9 = dij. \n(6.21)\nTo adapt this orthonormality equation to a continuous basis, we need to use the continuous analog \nof the discrete Kronecker delta, which is the Dirac delta function. The Dirac delta function, writ-\nten d1x - x02, is a function that is zero at every value of x, except at x = x0, where it is inﬁnite (not \nunity). This inﬁnity means that the Dirac delta function does not strictly represent the normalization \ncondition, but it is consistent with the inﬁnite norm we found for the momentum eigenstates above. \nThus, we expect that the “orthonormality” condition for a continuous basis set of momentum states is\n \n8p\u000e 0\n p\u00049 = d1 p\u000e - p\u00042 \n(6.22)\nx\n\u0002\u000bp(x)\u00022\nFIGURE 6.2 Position probability distribution for a momentum eigenstate.\n\n166 \nUnbound States\nin Dirac notation. Using the rules developed in Chapter 5 for translating bra-ket notation to wave \n function notation, we express the inner product in Eq. (6.22) as an overlap integral\n \nL\n\u0005\n- \u0005\nw*\np\u000e1x2wp\u00041x2dx = d1 p\u000e - p\u00042. \n(6.23)\nThe momentum eigenstates deﬁned in Eq. (6.14) satisfy this new form of the  orthonormality \n equation, as long as we deﬁne the normalization constant A for the momentum eigenstates as \n (Problem 6.7)\n \nA =  \n1\n22pU\n. \n(6.24)\nAlthough continuous basis sets, such as the momentum basis, do not strictly satisfy the normalization \ncondition required by quantum mechanics, it is still practical to use Eqs. (6.22) and (6.23) to “normalize” \na basis, and we refer to this process as Dirac normalization. We thus write the “normalized” momen-\ntum eigenstates as\n \nwp1x2 =  \n1\n22pU\n ei px>U    . \n(6.25)\nIt is worth thinking about dimensions at this point. With the normalization of the momentum eigen-\nstates in Eq. (6.25), we see that the dimensions of the left hand side of Eq. (6.23) are 3length4>3U4, \nwhich from Eq. (6.16) are equivalent to 1>3p4 or inverse momentum. Thus, the Dirac delta function \nhas dimensions of the inverse of its argument. This is another difference from the Kronecker delta that \nwe have to live with.\nThe completeness of a basis implies that any function (relevant to the problem at hand) can \nbe written as a superposition of the basis states. Completeness is difﬁcult to prove mathematically, so we \ngenerally just assume that it is satisﬁed. In the discrete basis case, the completeness condition (closure \nrelation) in Eq. (6.20) is a sum of the projection operators over the discrete basis set. To change to a con-\ntinuous basis, we change the sum over the discrete label to an integral over the continuous label. For the \nmomentum eigenstates, the completeness condition is\n \nL\n\u0005\n- \u0005\n0  p98p0 dp = 1, \n(6.26)\nwhere we understand that the right hand side is the identity operator. To demonstrate how complete-\nness allows us to express any general state as a superposition of the basis states, insert Eq. (6.26) into \nthe Dirac expression for a wave function\n \n c1x2 = 8x0 c9\n \n \n = 8x0 b\nL\n\u0005\n- \u0005\n0  p98p0 dpr 0 c9 \n \n =\n \nL\n\u0005\n- \u0005\n8x0  p98p0 c9dp.\n \n \n(6.27)\nThe ﬁrst term 8x0 p9 in the integrand is the projection of the momentum eigenstate 0  p9 onto the posi-\ntion basis, which is the wave function representation wp1x2 of the momentum eigenstate. The second \nterm 8p0 c9 in the integrand is the projection of the general state 0 c9 onto the momentum basis 0  p9 \n(i.e., the probability amplitude for the general state 0 c9 to have momentum p). Given the rules of Dirac \n\n6.1 Free Particle Eigenstates \n167\nnotation, you might expect the probability amplitude 8p0 c9 to be written as 8p0 c9 = c 1 p2. However, \nthere is risk of confusion here with the wave function c1x2 because c1 p2 and c1x2 are not the same \nmathematical function with different arguments, but rather are different mathematical functions. To \navoid this possible confusion, it is common to use a different symbol for the momentum probability \namplitude, such as\n \nf1 p2 = 8p0 c9, \n(6.28)\nalthough such notation brings its own confusion between the different Greek symbols. The function \nf1 p2 is known as the momentum space wave function. As in the position case, the probability ampli-\ntude f1 p2 = 8p0 c9 is a continuous function that is the collection of numbers that represents the quan-\ntum state vector in terms of the momentum eigenstates. The wave function c1x2 and the momentum \nspace wave function f1 p2 are both representations of the state 0 c9, but they are representing that \nstate in different bases. Which basis we should use is up to us and is generally a matter of convenience \ndecided by what we wish to calculate. Using this deﬁnition of the momentum space wave function, we \nwrite Eq. (6.27) as\n \nc1x2 =\n \nL\n\u0005\n- \u0005\nwp1x2f1 p2dp, \n(6.29)\nwhich, in words, says that a general state 0 c9 \u0003 c1x2 can be decomposed into an integral (i.e., super-\nposition) over all momentum eigenstates 0  p9 \u0003 wp1x2 with a proportionality coefﬁcient given by the \nprobability amplitude f1p2 = 8p0 c9 for the general state to be measured in that particular momentum \nbasis state.\nIf we put the explicit form of the momentum eigenstates wp1x2 into Eq. (6.29), then the superposi-\ntion becomes\n \nc1x2 =  \n1\n22pU L\n\u0005\n- \u0005\nf1 p2ei px>U dp   . \n(6.30)\nThis should look familiar! It is the Fourier transform of the function f1 p2. Thus, quantum mechani-\ncal superpositions behave much like classical wave superpositions. In both cases, the Fourier trans-\nform represents a superposition of sinusoidal waves that combine to make a wave packet. We thus \nexpect that the connection in the opposite direction (i.e., writing the momentum space wave function \nin terms of the position space wave function) would be an inverse Fourier transform. We can show that \nthis is so by using our prescription for writing a probability amplitude in wave function language as an \noverlap integral. The momentum space wave function f1 p2 is a probability amplitude f1 p2 = 8p0 c9, \nand the rule for converting a Dirac bra-ket projection to wave function overlap integral is to convert the \nket 0 c9 to a wave function c1x2, the bra 8p0  to a wave function conjugate w*\np1x2 = e-i px>U> 12pU, and \nthen integrate over all space. Thus, we get\n \nf1 p2 =  \n1\n22pU L\n\u0005\n- \u0005\nc1x2e-i px>U dx   , \n(6.31)\nwhich we recognize as an inverse Fourier transform. Thus, we see that the connection between the \nmomentum space wave function f1 p2 and the (position space) wave function c1x2 is the Fourier \ntransform. As we saw in the spins case, we are free to use whichever representation of a quantum state \nvector that we ﬁnd most convenient. The position and momentum representations are similarly equally \nvalid representations. We focus on the position representation because it is generally the most useful.\n\n168 \nUnbound States\n6.2 \u0002 WAVE PACKETS\nThe key result from the previous section is that Fourier superpositions of momentum eigenstates are \nrequired for proper representation of free particle states. Let’s ﬁrst consider a discrete Fourier series \nexample that illustrates many of the important features of wave packets, and then we’ll make a real \nwave packet using continuous Fourier transforms.\n6.2.1 \u0002 Discrete Superposition\nIn this example, we add just three momentum eigenstates together. We choose one “central” state \nwith momentum p0 to have twice the amplitude of two “side mode” states that are equally spaced at \np = p0 { dp about the central state, as shown in the momentum state distribution in Fig. 6.3. As the \ndashed line hints, we are using this three-mode superposition as a model of a continuous momentum \ndistribution characterized by a center momentum p0 and a momentum distribution width dp that we \nwill discuss in Section 6.2.2.\nA graphical representation of this three-state superposition of sinusoidal waves and the resultant \nwave is shown in Fig. 6.4. The different wavelengths of the three components lead to constructive and \ndestructive interference, as indicated in the plots. The resultant wave is localized to a region of space \nand hence is referred to as a wave packet. The wave packet shown in Fig. 6.4 has a characteristic \nwavelength determined by the central momentum, so it resembles a wave, but it also has a limited spa-\ntial extent, and so it also resembles a particle. In this case, we are using a discrete Fourier sum, so this \nlocalization is repeated periodically. For the more realistic continuum distribution, only one localized \nregion exists and a true wave packet is realized. The coexisting particle and wave characteristics of a \nwave packet are the essence of the wave-particle duality of quantum mechanics.\nTo understand the motion of the wave packet, we must study the time evolution. The wave func-\ntion at time t = 0 is given by the weighted superposition of the three momentum eigenstates\n \n c1x, 02 = a\nj\ncj wpj1x2\n \n \n c1x, 02 = a\nj\ncj \n1\n22pU\n ei pj\n x>U\n \n \n c1x, 02 =\n1\n22pU\n 31\n2 ei1 p0-dp2x>U + ei p0\n x>U + 1\n2 ei1 p0+dp2x>U4. \n \n(6.32)\np0 \n Δp\np0 \r Δp\np0\np\nΦ\u0007p\b\nFIGURE 6.3 Discrete momentum distribution used to model continuous distributions \nand to build a discrete wave packet.\n\n6.2 Wave Packets \n169\nThe time-dependent wave function representing this wave packet is obtained by following the Schrödinger \ntime-evolution recipe. Momentum eigenstates are also energy eigenstates of free particles, so the \nsuperposition is already written in the energy basis and we multiply each energy eigenstate by its own \nenergy-dependent phase factor:\n \nc1x, t2 = a\nj\ncj wpj1x2e-i Ej\n t>U. \n(6.33)\nThe energy of each momentum eigenstate is given by the free particle energy\n \nEj =\np2\nj\n2m, \n(6.34)\nwhich for the three states yields\n \n Ep0 =\np2\n0\n2m\n \n \n Ep0{dp = 1p0 { dp2\n2\n2m\n=\np2\n0 { 2p0dp + 1dp2\n2\n2m\n. \n \n(6.35)\nWe assume that the width of the momentum distribution is narrow enough that dp V p0 and so we \nneglect the small 1dp22 term in the energies. Hence, the time-evolved wave packet state is\n \nc1x, t2 =\n1\n22pU\n 31\n2\n ei1 p0-dp2x>U e-i1p2\n0 -2p0dp2t>2m  U + ei p0\n x>U e-i p2\n0\n t>2m  U + 1\n2\n ei1 p0+dp2x>U e-i1p2\n0 +2p0dp2t>2m  U4\n \nc1x, t2 =\n1\n22pU\n  ei p0\n x>U e-i p2\n0\n \n t>2m  U 31\n2\n e-idpx>U ei p0dpt>m  U + 1 + 1\n2\n eidpx>U e-i p0dpt>m  U4\n \nc1x, t2 =\n1\n22pU\n ei p0\n x>U e-i p2\n0\n \n t>2m U c 1 + cosadp\nU\n x - p0dp\nm U\n tb d , \n \n(6.36)\nDestructive Interference\nDestructive Interference\nConstructive Interference\n\r\n\r\n\b\nx\n0\nΔx\n\nΔx\nFIGURE 6.4 Discrete wave packet with three components.\n\n170 \nUnbound States\nwhich yields\n \nc1x, t2 =\n1\n22pU\n eip01x-p0t>2m2>UJ1 + cos adp\nU\n c x - p0\nm\n td b R. \n(6.37)\nThis wave packet contains the expected form f  1x { vt2 of a wave, but it has two such parts with \ndifferent arguments. The ﬁrst part of Eq. (6.37) (in curly brackets) is characterized by the momentum \np0 and hence wavelength l0 = h>p0 of the single harmonic wave. This part is called the carrier wave, \nand from its argument we ﬁnd that it moves at the phase velocity vph = p0>2m, as we discussed above. \nThe second part of the wave packet (in square brackets) is characterized by the momentum width dp \nand hence a wavelength lenv = h>dp that is much longer than l0 1because dp V p02. This second \npart is known as the envelope of the wave packet because it modulates the carrier wave, as shown in \nFig. 6.5. Because of the different arguments of the two parts, the envelope moves at a different  velocity \nvgp = p0>m from the carrier. This velocity is called the group velocity because it characterizes the \nvelocity of the group of waves together.\nThe different velocities are evident if the plot of the wave packet in Fig. 6.5 is animated \n( Problem 6.8). Several frames from such an animation are shown in Fig. 6.6, where you can see that \nthe velocity of the envelope—the group velocity—is twice the velocity of the wiggles within the \n envelope—the phase velocity. Notice that the group velocity is equal to the classical velocity of a par-\nticle with momentum p0. This is the sense in which this wave packet can properly represent the motion \nof a particle. This discrete superposition is a good starting point, but it still suffers from the pathologies \nof harmonic waves—it is not normalizable and it therefore cannot predict expectation values—so we \nmust use a continuous momentum distribution to model real experiments. Moreover, the “localiza-\ntion” of the discrete Fourier series superposition is repeated periodically, and so cannot represent a \nsingle particle.\nx\nΨ(x)\nEnvelope\nCarrier\nFIGURE 6.5 Wave packet showing the carrier wave and the modulation envelope.\n\n6.2 Wave Packets \n171\n6.2.2 \u0002 Continuous Superposition\nTo go from the discrete case to the continuous case, we change the superposition sum in Eq. (6.32) to \na superposition integral (i.e., we change the Fourier series to a Fourier integral or Fourier transform). \nWhile this may seem like a trivial extension, there are important differences. As we did in the dis-\ncrete case, we perform the expansion using the momentum eigenstate basis wp1x2 because these states \nare also energy eigenstates in the free particle example, which then sets us up to use the Schrödinger \ntime-evolution recipe. In the integral superposition, we specify the amplitudes of the momentum \neigenstate as a continuous distribution f1 p2 rather than specifying discrete amplitudes. Thus, we \nwrite the initial superposition state as\n \n c1x, 02 =\n \nL\n\u0005\n- \u0005\nf1 p2wp1x2dp\n \n \n =\n \nL\n\u0005\n- \u0005\nf1 p2 \n1\n22pU\n ei px>U dp, \n \n(6.38)\nwhere f1p2 is also called the momentum space wave function. The time-evolved state is found by fol-\nlowing the recipe for Schrödinger time evolution and including the energy dependent phase factors:\n \nc1x, t2 =\n \nL\n\u0005\n- \u0005\nf1 p2wp1x2e-iEp\n t>U dp. \n(6.39)\nFIGURE 6.6 Discrete wave packet animation with time increasing from top to bottom. \nOpen circles identify a point of constant phase, which moves at the phase velocity. Filled \ncircles identify the peak of the envelope, which moves at the group velocity.\n\n172 \nUnbound States\nPutting in the explicit momentum eigenstate wave functions and the expression for the free particle \nenergy results in\n \nc1x, t2 =  \n1\n22pU L\n\u0005\n- \u0005\nf1 p2ei px>U e-i p2t>2m U dp, \n(6.40)\nwhich simpliﬁes to\n \nc1 x , t2 =  \n1\n22pU L\n\u0005\n- \u0005\nf1 p2ei p1x  - pt>2m2>U dp. \n(6.41)\nThis is the time-dependent generalization of the Fourier transform in Eq. (6.30) for the case of a free \nparticle. The time-dependent generalization of the inverse Fourier transform in Eq. (6.31) is\n \nf1 p, t2 =  \n1\n22pU L\n\u0005\n- \u0005\nc1x, t2e-i px>U dx. \n(6.42)\nTo evaluate the Fourier integral in Eq. (6.41) and determine the wave function for any particular case, \nwe need to know the particular momentum distribution f1p2, which may be speciﬁed as an initial \ncondition, or can be determined from the initial wave function c1x, 02 via the Fourier transform in \nEq. (6.31) that relates the spatial and momentum space wave functions.\nAs an example, consider the case of a Gaussian momentum distribution. This is a very common \nexample because Gaussian functions are easy to integrate—you get another Gaussian in the Fourier \nspace. In addition, the Gaussian distribution is a very good representation of many real experimental \nsituations. The Gaussian function is one of the standard classical probability distributions and is com-\nmonly written as\n \nf 1z2 =  e-1z-m2\n2>2s2\ns22p\n, \n(6.43)\nwhere m is the mean value or average of the distribution and s is the standard deviation of the distribu-\ntion. Relating these deﬁnitions to the quantum mechanical quantities, the mean value is the expectation \nvalue 8z9 and the standard deviation is the uncertainty \u0006z. The probability distribution in Eq. (6.43) is \nnormalized to unity:\n \nL\n\u0005\n- \u0005\nf 1z2dz = 1. \n(6.44)\nNotice that the function f 1z2 is not squared in the normalization integral in Eq. (6.44), contrary to \nthe normalization of quantum mechanical wave functions to which you have become accustomed. In \nquantum mechanics, we have to square the wave function to get the probability density, which is then \nnormalized, analogous to Eq. (6.44). So, technically speaking, the phrase “normalize the quantum \nmechanical wave function” is not correct, because we actually normalize the probability distribution, \nnot the wave function. But that phrase is ingrained into all practicing physicists, so we are stuck with it.\nJust as we did in the discrete case, let’s assume that the momentum distribution is peaked at p0 and \nhas a width characterized by a parameter b. The Gaussian momentum space wave function is\n \nf1 p2 = ¢\n1\n2pb2 ≤\n1>4\n e-1p-p02\n2>4b2, \n(6.45)\n\n6.2 Wave Packets \n173\nwhere the scale factor ensures proper normalization. This momentum space wave function is shown in \nFig. 6.7, with the previous discrete case for comparison. The momentum probability distribution (per \nunit momentum) is the absolute square of the momentum space wave function:\n \nP1 p2 = 0 f1p20\n2 = e-1p-p02\n2>2b2\nb22p\n. \n(6.46)\nComparison of this quantum mechanical momentum probability distribution with the standard \n Gaussian probability function in Eq. (6.43) allows us to determine the momentum expectation value \n8p9 and momentum uncertainty \u0006p by inspection as\n \n 8p9 = p0 \n \n \u0006p = b . \n \n(6.47)\nThe time-evolved spatial wave function for this Gaussian wave packet is obtained by substituting \nEq. (6.45) into the Fourier transform in Eq. (6.41):\n \nc1x, t2 =\n1\n22pU L\n\u0005\n- \u0005\n¢\n1\n2pb2 ≤\n1>4\n e-1 p-p022>4b2 ei px>U e-i p2t>2m U dp. \n(6.48)\nThis integral can be performed using the standard Gaussian integral shown in Appendix F, Eq. (F.23): \n(Problem 6.9). The result is\n \nc1x, t2 =\n22b\n3Ug22p\n ei p01x-p0t>2m2>U e-1x-p0t>m2\n2b2>U2g, \n(6.49)\nwhere the new parameters are\n \n g = 1 +  it\nt\n \n t =  m U\n2b2 .  \n(6.50)\np\nΦ(p)\n2Β\np0 \u0004 Δp \np0 \u0006 Δp\np0\nFIGURE 6.7 Gaussian momentum space wave function.\n\n174 \nUnbound States\nIf we deﬁne\n \na =\nU\n2b , \n(6.51)\nthen we can express the wave function as\n \nc1x, t2 = a\n1\n2pa2b\n1>4 1\n1g\n  ei p01x-p0t>2m2>U e-1x-p0t>m2\n2>4a2g, \n(6.52)\nwhere a is useful later as a measure of the width in position space.\nJust as in Eq. (6.37) for the discrete momentum distribution, this wave packet has a carrier wave \npart (in curly brackets) that is characterized by p0 and propagates at the phase velocity p0>2m, and \nan envelope part (in square brackets) that is characterized by the momentum width b (through the a \nparameter) and propagates at the group velocity p0>m. As we expected, the envelope is a Gaussian \nfunction. To isolate the envelope propagation, calculate the spatial probability density by taking the \nsquare modulus of the wave function:\n \nP1x, t2 = 0 c1x, t20\n2 =  \n1\n22pa\u000f\n e-1x -p0t>m2\n2>2a2\u000f2, \n(6.53)\nwhere we have deﬁned a new parameter\n \n\u000f = 30 g0\n2 = B1 + t2\nt2 . \n(6.54)\nThe only velocity that appears in the probability density is the group velocity p0>m, which agrees \nwith our classical expectation that the particle propagates at this velocity. This Gaussian wave packet \nis shown in Fig. 6.8(a) and the probability density is shown in Fig. 6.8(b). This wave packet is truly \nlocalized; the probability density decays to zero away from the central peak in Fig. 6.8(b) with none \nof the secondary peaks that were evident in the discrete superposition in Fig. 6.4. The continuum of \nmomentum states used in this superposition ensures that the destructive interference of the constituent \nwaves away from the central peak is effective in truly localizing the wave/particle. This localization \nthrough interference means that this wave packet superposition is normalizable even though the indi-\nvidual waves used are not themselves normalizable.\nThe experimental parameters that one would like to measure in order to fully characterize a wave \npacket are the position and momentum. The expectation value of the position is, formally,\n \n8x9 =\n \nL\n\u0005\n- \u0005\nx  P1x, t2dx =\n \nL\n\u0005\n- \u0005\nx 0 c1x, t20\n2 dx, \n(6.55)\nx\nRe\tΨ\u0007x\b\n2\u0003x\nx\nP\u0007x\b\n2\u0003x\n(a)\n(b)\nFIGURE 6.8 Gaussian wave packet (a) wave function and (b) probability density.\n\n6.2 Wave Packets \n175\nbut it can also be obtained by inspection of the Gaussian probability density [compare Eq. (6.53) with \nEq. (6.43)]:\n \n8x9 =  p0\nm\n t. \n(6.56)\nThis result again shows that the wave packet moves with the group velocity p0>m.\nThe expectation value of the momentum can be calculated either with a spatial integral\n \n8p9 =\n \nL\n\u0005\n- \u0005\nc*1x, t2 pn  \n c1x, t2dx \n(6.57)\nor a momentum integral\n \n8p9 =\n \nL\n\u0005\n- \u0005\np P1p, t2dp =\n \nL\n\u0005\n- \u0005\np 0 f1p, t20\n2 dp. \n(6.58)\nEither way, we get the result found by inspection previously in Eq. (6.47):\n \n8p9 = p0. \n(6.59)\nThe uncertainties of position and momentum are (again by inspection)\n \n \u0006x = a\u000f =\nU\n2b\n B\n1 + a2b2t\nm U b\n2\n \n \n \u0006p = b.\n \n \n(6.60)\nThe wave packet momentum width remains constant, which is consistent with the conservation of \nmomentum. The position width grows in time because the different momentum components used to \nconstruct the wave packet all move with different phase velocities. The spatial spreading of the quan-\ntum mechanical wave packet agrees with our classical ideas about waves. It could be considered analo-\ngous to a short laser pulse propagating through glass with dispersion in the index of refraction such \nthat different colors in the pulse travel at different speeds. However, the wave packet spreading is not \nwhat we expect for a classical particle, and we have uncovered one of the counterintuitive realities of \nthe quantum world—quantum particles do not stay intact.\nAs we did for the discrete wave packet, we visualize the motion of the continuous Gaussian wave \npacket with frames of an animation in Fig. 6.9. Again, we note that the carrier wave moves at the phase \nvelocity, which in this case is half of the group velocity of the envelope motion. From previous study \nof optics or waves, you may recall that the formal deﬁnitions of the phase and group velocities that \nwork for any wave packet are\n \n vphase = v\nk\n \n \n vgroup = dv\ndk `\nk0\n, \n \n(6.61)\nwhere the derivative in the group velocity is evaluated at the peak of the distribution of wave vector \nstates comprising the group. Applying these wave relations to the quantum mechanical free particle, \nwe ﬁnd that the phase velocity of the wave is\n \nvphase = v\nk = U v\nUk = E\np =\np2>2m\np\n=\np\n2m = vclassical\n2\n, \n(6.62)\n\n176 \nUnbound States\nwhich is half the classical particle velocity. The group velocity is\n \nvgroup = dv\ndk `\nk0\n=\nd1U v2\nd1Uk2 `\nk0\n= dE\ndp `\np0\n=\nd1 p2>2m2\ndp\n`\np0\n= p0\nm = vclassical, \n(6.63)\nwhich is equal to the classical particle velocity. Both results agree with the results we obtained by \ninspection of the Gaussian wave packet for a free particle.\n6.3 \u0002 UNCERTAINTY PRINCIPLE\nThe Fourier connection between position space and momentum space is also important for under-\nstanding the Heisenberg uncertainty principle as it applies to position and momentum. We learned \nin Chapter 2 that spin projection measurements along different axes are incompatible, meaning that \nwe cannot simultaneously measure both observables. We saw that, in general, two observables cannot \nbe measured simultaneously if they do not commute. We expressed this incompatibility in terms of the \nproduct of the measurement uncertainties of the two observables\n \n\u0006A\u0006B Ú 1\n2 083A, B49 0 , \n(6.64)\nwhere the uncertainty is deﬁned as the standard deviation\n \n\u0006A = 481A - 8A9229 = 48A29 - 8A92. \n(6.65)\nWe can now ask whether position and momentum measurements are compatible. Because we \nknow how to represent the position and momentum operators, we can calculate their commutator to \nanswer this question. The answer is that position and momentum do not commute (Problem 6.6). Their \ncommutator is\n \n3xn,  pn4 = i U. \n(6.66)\nFIGURE 6.9 Gaussian wave packet animation with time increasing from top to bottom. \nOpen circles identify a point of constant phase, which moves at the phase velocity. Filled \ncircles identify the peak of the envelope, which moves at the group velocity.\n\n6.3 Uncertainty Principle \n177\nThus, the Heisenberg uncertainty principle as applied to position and momentum is\n \n\u0006x\u0006p Ú U\n2  . \n(6.67)\nThis condition limits the product of the uncertainties of position and momentum to a minimum value. \nThe Heisenberg uncertainty principle represents a tradeoff between our knowledge of position and \nour knowledge of momentum. The Fourier connection between position and momentum helps us to \nunderstand this limitation.\nConsider the Fourier wave packet constructed from discrete momentum components. The uncer-\ntainty in momentum \u0006p is approximately the spacing \u0010p of the side modes from the central mode, as \nshown in the momentum distribution of Fig. 6.3. We estimate the uncertainty in position \u0006x as the \nseparation \u0010x of the two destructive interference minima from the central maximum of the correspond-\ning spatial wave function in Fig. 6.4. The minima are located where the phases of the side mode waves \nare p out of phase with the central sinusoid. These phases are determined by the arguments of the \nei pj x>U terms in Eq. (6.32). If we assume that the wave packet maximum, where the three waves are in \nphase, is at x = 0, then the destructive interference minimum on the right is at x = dx, as indicated in \nFig. 6.4. To calculate \u0010x, set the phase difference between the upper side mode 1 p = p0 + dp2 and \nthe central mode 1p = p02 equal to p and solve:\n \n 1p0 + dp2dx\nU\n- p0dx\nU\n= p  \n \n dpdx\nU\n= p. \n \n(6.68)\nThe uncertainty product for this discrete wave packet is approximately\n \n\u0006x\u0006p \u0003 pU. \n(6.69)\nHence, there is an inverse relationship between the width \u0006x of the position distribution and the \nwidth \u0006p of the momentum distribution. A wave packet that is well localized in space 1small \u0006x2 \nrequires a broad distribution \u0006p of momentum states, while a broad spatial distribution requires a \nnarrow momentum distribution. While this wave packet of discrete momentum components (i.e., a \nFourier series) does not strictly obey Eq. (6.69) because the “localization” is repeated out to inﬁnity, \nthe inverse relation between the position and momentum widths is a hallmark of Fourier transforms of \ncontinuous distributions.\nWe learned in the last section that a Gaussian momentum distribution leads to a Gaussian posi-\ntion distribution because the Fourier transform of a Gaussian function is itself a Gaussian function. In \nFig. 6.10 we plot these Fourier transform pairs for a range of widths; the inverse relation between the \nposition and momentum spaces is graphically evident. Using the position and momentum uncertain-\nties in Eq. (6.60), we calculate the uncertainty product of a Gaussian wave packet:\n \n\u0006x\u0006p = U\n2\n D1 + a2b2t\nm U b\n2\n. \n(6.70)\nAt time t = 0 the Gaussian wave packet obeys the equality of the Heisenberg uncertainty relation \n\u0006x\u0006p = U>2. For this reason, a Gaussian wave function 1at t = 02 is a minimum uncertainty state. \nAs the wave packet evolves in time, it broadens in position space and the uncertainty product increases \n(Problem 6.12).\n\n178 \nUnbound States\n(a)\n\"Wave\"\nx\nRe\tΨ\u0007x\b\n2\u0003x\n2\u0003x\n2\u0003x\np0\np0\np0\np\nΦ\u0007p\b\n\"Wave/Particle\"\nx\nRe\tΨ\u0007x\b\np\nΦ\u0007p\b\n\"Particle\"\nx\nRe\tΨ\u0007x\b\np\nΦ\u0007p\b\n2\u0003p\n2\u0003p\n2\u0003p\n(b)\n(c)\nFIGURE 6.10 Gaussian wave packets with decreasing spatial widths and the \ncorresponding momentum space wave functions obtained by Fourier transform.\nThe wave packet in Fig. 6.10(a) extends spatially over many wavelengths, so the “wave” nature \nof the packet is evident. In contrast, the wave packet in Fig. 6.10(c) extends only over one wavelength \nand so is more representative of a well-localized “particle.” If we take this wave-particle duality to its \nlogical extremes, we get the states shown in Fig. 6.11. A pure “wave” has an inﬁnite spatial extent, \nwhich corresponds to an inﬁnitesimal momentum width, as shown in Fig. 6.11(a). The pure wave state \nis the momentum eigenstate wave function 0  p09 \u0003 wp01x2 = ei p0\n x>U> 12pU, and the corresponding \nmomentum space wave function must be a Dirac delta function because there is only one momentum \nvalue. This is consistent with the Fourier connection between position and momentum because the \nFourier transform of a pure sinusoid is a delta function:\n \n fp01p2 =\n1\n12pU L\n\u0005\n- \u0005\nwp01x2e-i px>U dx\n \n \n =\n1\n12pU L\n\u0005\n- \u0005\n \n1\n12pU ei p0\n x>U e-i p x>U dx \n \n =\n1\n2pU L\n\u0005\n- \u0005\nei 1p0\n -\n p2x>U dx\n \n \n = d1p - p02.\n \n \n(6.71)\n\n6.3 Uncertainty Principle \n179\nA pure “particle” state has an inﬁnitesimally narrow spatial extent, which corresponds to an inﬁ-\nnite momentum width, as shown in Fig. 6.11(b). This state represents a particle that is measured to be \nat a unique position, x0 for example. A state with a unique value of the position observable is a position \neigenstate 0\n x09. In analogy with the momentum space representation of the momentum eigenstate above, \nthe position representation (i.e., spatial wave function) of a position eigenstate is the Dirac delta function\n \n0\n x09 \u0003 wx01x2 = d1x - x02. \n(6.72)\nThis state satisﬁes the position eigenvalue equation\n \n xn 0 x09 = x00 x09\n \n \n xn d1x - x02 = x0 d1x - x02. \n \n(6.73)\nSo we have ﬁnally found the wave function for the position eigenstate we introduced in the last chap-\nter. The inﬁnite extent of the momentum space representation of this state is now clear, because the \nFourier transform of a delta function is a pure sinusoid:\n \n fx01 p2 =\n1\n12pU L\n\u0005\n- \u0005\nwx01x2e-i px>U dx\n \n \n =\n1\n12pU L\n\u0005\n- \u0005\nd1x - x02e-i px>U dx \n \n =\n1\n12pU e-i px0>U.\n \n \n(6.74)\nThe position eigenstates have the same pathologies as the momentum eigenstates—they cannot be \nnormalized and so they cannot truly represent physical states.\nx\nRe\t\u0005p0(x)\nRe\tΦx0(p)\nx\n\u0005x0(x)\np\nΦp0(p) \np\n(a)\n(b)\np0\nx0\nFIGURE 6.11 (a) Momentum eigenstate wave function and its corresponding delta-function \nmomentum distribution, and (b) position eigenstate wave function and its corresponding inﬁnite \nextent momentum distribution.\n\n180 \nUnbound States\nIn summary, the eigenstates of position and momentum in the two representations\n \n \n \nPosition space \nMomentum space \n \nPosition eigenstate \n0  x09 \u0003 d1x - x02 \n0  x09 \u0003\n1\n12pU e-i p0 x>U \n \nMomentum eigenstate 0  p09 \u0003\n1\n12pU ei p0 x>U 0  p09 \u0003 d1 p - p02  \n(6.75)\ndemonstrate an appealing parallel between position and momentum. This parallel is also evident in the \nposition and momentum operators. In the position representation, the position operator is simple mul-\ntiplication, while the momentum operator is a derivative with respect to position. Similar to the cor-\nrespondence of the wave functions in Eq. (6.75), it turns out that in the momentum representation, the \nmomentum operator is simple multiplication, while the position operator is a derivative with respect to \nmomentum:\n \nPosition space Momentum space \n \n xn \u0003 x\n \n xn \u0003 iU d\ndp\n  \n \n pn \u0003 -iU d\ndx\n \n pn \u0003 p     . \n(6.76)\nThe incompatibility of position and momentum measurements inherent in the Heisenberg uncer-\ntainty principle is in stark contrast to the classical notion that position and momentum are independent \nquantities that can each be measured with precision limited only by experimental technique. In quan-\ntum mechanics, position and momentum are complementary rather than independent quantities. The \nresult is that we cannot know the trajectory of a particle in quantum mechanics. We can make predic-\ntions of the probability that the particle is in a region of space, but we cannot know the trajectory as we \ndo in classical physics.\n6.3.1 \u0002 Energy Estimation\nWe can also use the uncertainty principle to estimate the minimum energy of a particle. If we know \nthat a particle is localized to a ﬁnite region \u0006x of space, then the uncertainty principle tells us that the \nmomentum distribution required to produce that localization must satisfy\n \n\u0006p Ú\n U\n2\u0006x . \n(6.77)\nIf the momentum distribution has this minimum width, then we can use this width as a rough estimate \nof the minimum momentum\n \npmin \u0005\nU\n2\u0006x . \n(6.78)\nIgnoring the potential energy for the moment, we can then estimate the minimum energy of the particle\n \n Emin =  p2\nmin\n2m\n \n \n Emin \u0005  \nU2\n8m1\u0006x2\n2 . \n \n(6.79)\nThis approach is a common “back-of-the-envelope” calculation used to get a rough estimate of bound-\nstate energies.\n\n6.4 Unbound States And Scattering \n181\nConsider a particle bound in a square well potential. The potential energy well by its nature con-\nﬁnes the particle to a spatial region \u0006x approximately the size L of the box. We then use the uncertainty \nprinciple to ﬁnd the corresponding uncertainty in the particle momentum:\n \n \u0006p\u0006x Ú  U\n2 \n \n \u0006p Ú  U\n2\u0006x \n \n \u0006p Ú  U\n2L.  \n \n(6.80)\nIf the particle momentum is uncertain to this degree, then the value of the particle momentum must be \nat least this big, and possibly much larger:\n \npmin =  U\n2L . \n(6.81)\nNow use this estimate of the minimum momentum to estimate the minimum energy that the bound \nparticle can have:\n \n Emin =  p2\nmin\n2m\n \n \n =  U2\n8mL2 . \n \n(6.82)\nCompare this with the ground-state energy in the inﬁnite well:\n \nE\u0005, n=1 =  p2U2\n2mL2 \u0002 5 U2\nmL2 . \n(6.83)\nWhile not a great match, the energy estimate from the Heisenberg uncertainty principle does predict \nthe correct dependence of the energy on the well size. As the well gets smaller the energy levels go up, \nwhich is a general feature of bound energy states. The proportionality depends on the well width and \nis 1>L2 for the square well.\nThe actual ground-state energy in the inﬁnite square well [Eq. (6.83)] is about 40 times larger than \nthe uncertainty principle estimate in Eq. (6.82). There are two reasons for this poor agreement. (1) We \noverestimated the position spread of the particle; a particle conﬁned to a well of size L has a position \nuncertainty less than L (Problem 6.20). (2) The minimum energy estimate comes from assuming that \nthe uncertainty product is a minimum \u0006x\u0006p = U>2, which is true only for Gaussian wave functions. \nBoth of these factors lead to an underestimate of the minimum momentum, which leads to an even big-\nger underestimate of the energy because it depends on the square of the momentum. This method of \nestimating energies with the Heisenberg uncertainty principle must be taken with a grain of salt, as this \nexample shows.\n6.4 \u0002 UNBOUND STATES AND SCATTERING\nWe have discussed bound states in potential wells and free particle states in ﬂat potentials. To com-\nplete our introduction to the quantum mechanics of particle motion, we now discuss unbound states \nin potential energy wells. Unbound states have an energy that is greater than the potential energy at \n\n182 \nUnbound States\nx\nE1\nE2\nE3\nE4\nE5\nE6\nEnergy\nV(x)\nBound states\nUnbound States\nFIGURE 6.12 Bound 1E 6 E1\u000522 and unbound 1E 7 E1\u000522 states in a generic potential energy well.\ninﬁnity, in contrast to bound states, which have an energy that is less than the potential energy at inﬁn-\nity, as illustrated in Fig. 6.12. Bound states must “ﬁt” into the potential well, which leads to energy \nquantization, while unbound states “lie” above the well with sinusoidal wave functions that extend to \ninﬁnity, “and beyond!” Unbound states are similar to free particle states in that there are not enough \nconstraints to fully determine the wave function, with the result that there is no energy quantization \nfor unbound states. However, the unbound states are not simply free particle states with a well-deﬁned \nmomentum. Unbound states are affected by the potential energy proﬁle, which causes the states to \n“scatter.” We often use the term scattering states in this context.\nTo begin our study of unbound states, we return to the ﬁnite square well potential. For the study of \nscattering states, it is more convenient to choose the zero of potential energy to be the energy at inﬁn-\nity, rather than the energy at the bottom of the well as we did for bound states. Hence, we deﬁne the \npotential energy shown in Fig. 6.13 as\n \nV1x2 = •\n  0,\n-V0,\n  0,    \n   x 6 -a\n-a 6 x 6 a\n   x 7 a.\n \n(6.84)\nWith this choice of potential energy origin, bound states have E 6 0 and scattering states have E 7 0. \nIt turns out that we are also able to use the solutions to this problem to study an inverted well (a barrier) \nby changing the sign of V0.\nWe follow the same approach we have used in all previous wave function problems—we ﬁrst \nsolve the energy eigenvalue equation. As in the previous well problems, we get separate equations in \nthe different regions:\n \n a-  U2\n2m\n d 2\ndx 2 - V0bwE1x2 = EwE1x2,   0 x 0 6 a  \n \n a-  U2\n2m\n d 2\ndx 2 + 0b wE1x2 = EwE1x2,   0 x 0 7 a. \n \n(6.85)\n\n6.4 Unbound States And Scattering \n183\nScattering states have E 7 0 and so we expect sinusoidal solutions in both regions. Hence, it is useful \nto deﬁne two wave vectors\n \n k1 = A\n2mE\nU2\n \n \n k2 = B\n2m1E + V02\nU2\n. \n \n(6.86)\nThese two parameters are used to rewrite the energy eigenvalue equations as\n \n \nd 2wE1x2\ndx2\n= -k2\n2wE1x2,   0 x0 6 a \n \n \nd 2wE1x2\ndx2\n= -k2\n1wE1x2,   0 x0 7 a. \n \n(6.87)\nThe solutions to these differential equations are sinusoids or complex exponentials. Which form \nwe choose to start with is a matter of convenience; the solution dictates the ﬁnal form. It turns out \nthat bound-state wave functions are real, as we found in Chapter 5, and unbound state wave func-\ntions are complex, so the complex exponentials are more convenient here. We write the general \nsolutions as\n \nwE1x2 = •\nAeik1x + Be-ik1x,\nCeik2x + De-ik2x,\nFeik1x + Ge-ik1x,   \n    x 6 -a\n -a 6 x 6 a\n    x 7 a .\n \n(6.88)\nIn principle, we should now proceed as we did in the bound-state problems earlier. That is, \nwe should impose the boundary conditions and solve for the allowed energies and wave function \nx\nV(x)\n\u0006a\n\u0006V0\n0\na\nFIGURE 6.13 Finite square potential energy well.\n\n184 \nUnbound States\namplitudes. However, that road quickly becomes a heavy slog. So it is instructive to focus on speciﬁc \nphysical problems of interest and consider what we can actually measure.\nFirst, observe that there are seven unknowns (coefﬁcients A, B, C, D, F, G, and energy E) in this \nproblem. To solve for all seven unknowns, we need seven equations, or seven pieces of information. \nWhen we impose the boundary conditions of wave function amplitude and derivative continuity at \nthe two sides of the well, we get four pieces of information. For bound-state systems, the remaining \nthree pieces of information come from the normalization condition, resulting in energy quantization. \nWe saw this explicitly in the discussion of numerical solutions of energy eigenvalue equations; only by \nchoosing the energy perfectly could we achieve a wave function that decayed to zero as it approached \ninﬁnity. Unbound or scattering states need not decay to zero at inﬁnity, so we cannot and do not need \nto impose the normalization condition. However, the absence of the normalization condition implies \nthat the energy is not quantized and any energy is allowed for a scattering state. So our ﬁrst conclusion \nis that scattering states have a continuous energy spectrum; therefore, we treat the energy E as an ini-\ntial condition rather than as an unknown.\nIn a typical scattering experiment, we shoot particles at each other and ask how their motion is \naffected by their interactions. We usually consider one particle as ﬁxed—the target—and the other \nas moving—the projectile. The potential energy well represents the interaction between them. The \nwave function we solve for then represents the motion of the projectile. In an experiment, projectile \nparticles originate from a source, which we assume is at negative inﬁnity. In the general solution then, \nthe Aeik1x term represents the incoming projectile particles, as illustrated in Fig. 6.14. These incoming \nprojectile particles can interact with the well (target) in two possible ways: they might reﬂect and head \nback to the left, which would be the Be-ik1x term, or they might continue to the right, which would be \nthe Feik1x term after passing the well region. In this scenario, there are no particles on the right side of \nthe barrier that are moving to the left—the Ge-ik1x term. That term could come about only if there were \na source of particles at positive inﬁnity headed back toward the origin, or if another potential energy \nchange occurred to the right of the well that could reﬂect the original particles back to the left. Hence, \nthe typical scattering experiment is consistent with setting G = 0. Using this viewpoint and treating \nthe energy E as an initial condition rather than as an unknown, we have now reduced the number of \nunknowns in the problem from seven to ﬁve.\nx\nE\na\n\u0002a\n\u0002V0\nE\nBe\u0002ik1 x\nDe\u0002ik2 x\nAeik1 x\nCeik2 x\nFeik1 x\nFIGURE 6.14 Waves incident upon, reﬂected from, and transmitted through \na square potential energy well.\n\n6.4 Unbound States And Scattering \n185\nUnfortunately, we still have one more unknown than we can solve for because we have only four \nequations or pieces of information from the boundary conditions. We get that one extra piece of infor-\nmation by using a new way to normalize the wave function. The coefﬁcient A represents the amplitude \nof the incoming wave, B the amplitude of the reﬂected wave, and F the amplitude of the transmitted \nwave, all of which are things we can measure. But we only expect our theory to predict the amplitudes \nof the reﬂected and transmitted waves. The amplitude of the incident wave is something we control \nin the experiment. Moreover, we expect that more incoming wave amplitude (input particle ﬂux) will \nlead to more reﬂected and transmitted wave amplitude (output particle ﬂux), so we really want to \npredict the ratios B>A and F>A of the reﬂected and transmitted waves, respectively, to the incoming \nwave. In this sense, we are normalizing our solutions to the amplitude of the incoming wave. In prac-\ntice, we divide the boundary condition equations by A, which effectively gives us four equations with \nfour unknowns. C and D represent the amplitudes of the wave function inside the potential well and \nare typically not amenable to measurement, so we try to eliminate those in favor of the measurables.\nIn light of this new way of approaching the problem, the general solution is\n \nwE1x2 = •\nAeik1x + Be-ik1x,\nCeik2x + De-ik2x,\nFeik1x,\n   \n    x 6 -a\n -a 6 x 6 a\n    x 7 a.\n \n(6.89)\nNow apply the boundary conditions of wave function amplitude and derivative continuity at the two \nsides of the well:\n \n wE1-a2:   Ae-ik1a + Beik1a = Ce-ik2a + Deik2a \n \n dwE1x2\ndx\n`\nx=-a\n:   ik1Ae-ik1a - ik1Beik1a = ik2Ce-ik2a - ik2Deik2a \n \n wE1a2:   Ceik2a + De-ik2a = Feik1a\n \n \n \ndwE1x2\ndx\n`\nx =a\n:   ik2Ceik2a - ik2De-ik2a = ik1Feik1a.\n \n(6.90)\nSolve the last two equations for C and D in terms of F and then substitute into the ﬁrst two equations \nto eliminate C and D, which are not so interesting. Then solve the ﬁrst two equations for the ratios B>A \nand F>A (Problem 6.24):\n \n F\nA =\ne-2ik1a\ncos12k2a2 - i \nk 2\n1 + k 2\n2\n2k1k 2\n sin12k2a2\n \n \n B\nA = i F\nA \nk 2\n2 - k 2\n1\n2k1k2\n sin12k2a2.\n \n(6.91)\nThe ratio F>A is the ratio of the amplitude of the transmitted wave to the amplitude of the incom-\ning wave. The absolute square of this ratio gives the relative probability T that an incident particle is \ntransmitted through the potential well, which we call the transmission coefﬁcient. The transmission \ncoefﬁcient for a ﬁnite square well is\n \nT = 0 F0 2\n0 A0 2 =\n1\n1 + 1k 2\n1 - k 2\n22\n2\n4k 2\n1k 2\n2\n sin212k2a2\n. \n(6.92)\n\n186 \nUnbound States\nExpressed in terms of the energy E and the potential well depth V0, the transmission coefﬁcient is\n \nT =\n1\n1 +\nV 2\n0\n4E1E + V02\n sin2 a2a\nU 42m1E + V02b\n. \n(6.93)\nThis is the probability that a particle with an incoming energy E is transmitted through the potential \nregion.\nThe reﬂection coefﬁcient R is the probability that an incident particle is reﬂected from the poten-\ntial well and is given by the absolute square of the ratio B>A of the amplitude of the reﬂected wave to \nthe amplitude of the incoming wave:\n \nR = 0 B0 2\n0 A0 2 =\n1\n1 +\n4k 2\n1k 2\n2\n1k 2\n1 - k 2\n22\n2 sin212k 2a2\n. \n(6.94)\nIn this ﬁnite square well problem, there is no absorption of particles by the well, so the reﬂection and \ntransmission coefﬁcients add up to unity:\n \nT + R = 1 \n(6.95)\nand the reﬂection coefﬁcient is simply R = 1 - T. In contrast to quantum mechanical particles, \n classical particles do not reﬂect from potential wells. They merely speed up and then slow down as \nthey traverse the well. The reﬂection of quantum mechanical particles is thus further evidence of the \nwave nature of particle motion. It is analogous to classical wave motion through different media. For \nexample, a light wave incident on a slab of glass is also partially reﬂected and partially transmitted.\nThe transmission and reﬂection coefﬁcients for a ﬁnite square well are plotted in Fig. 6.15 as \na function of the incident energy E. For large energy, the transmission goes to unity, which is to be \nexpected because the potential well becomes insigniﬁcant. The transmission is also unity for particular \nenergies, commonly called resonances. These resonances occur whenever the sine term in the trans-\nmission coefﬁcient is zero, which occurs if\n \n2k2a = np. \n(6.96)\n1\n2\nE/V0\n0.2\n0.4\n0.6\n0.8\n1.0\nT,R\nR\nT\nFIGURE 6.15 Reﬂection and transmission coefﬁcients for scattering from a ﬁnite square well. \nThe vertical lines indicate resonances where the transmission is unity.\n\n6.4 Unbound States And Scattering \n187\nThe reason for these resonances is evident if we rewrite this expression in terms of the wavelength \nl2 = 2p>k2 inside the potential well:\n \n 2a2p\nl2\nb  a = np \n \n 2a = n l2\n2 .\n \n(6.97)\nWhen the width of the potential well (2a) contains an integer number of half wavelengths, the trans-\nmission is unity and the reﬂection is zero. This effect is well known in physical optics, where light \nundergoes multiple reﬂections from the front and back surfaces of a glass slab, as shown in Fig. 6.16. \nForward-going waves all interfere constructively and backward-going waves all interfere destructively \nwhen the thickness of the glass slab contains an integer number of half wavelengths. In the optics case, \nthe changes in transmission and reﬂectivity that come from changing the wavelength (or the slab thick-\nness) are known as interference fringes. One of the most common manifestations of this effect is the \nappearance of colored bands in a thin ﬁlm of oil on water, as in the street after a rainstorm. In the optics \ncase, the transmission and reﬂection are found by explicitly adding up all the interfering waves shown \nin Fig. 6.16. In the quantum case, we solved the energy eigenvalue equation and imposed the boundary \nconditions to achieve the same result. In both cases, the waves look like those shown in Fig. 6.17.\n\u0002a\na\nx\nE,Ψ\nIncident\nReflected\nTotal\nFIGURE 6.17 Waves incident upon, reﬂected from, and transmitted through a ﬁnite square well. \nNote that there are two vertical axes, energy and wave function, with different zeroes.\nΛ1\nΛ1\nΛ2\nFIGURE 6.16 Optics interference analogy.\n\n188 \nUnbound States\n\u0002a\na\nx\nE\nE \u0003 V0\nV0\nFIGURE 6.18 A ﬁnite square barrier with the incident particle energy above \nthe barrier height.\nIf we write the resonance condition in terms of the energy, we get\n \na2a\nU b\n2\n2m1E + V02 = n2p2 \n \nE =  -V0 +\nn2p2\n U2\n2m12a2\n2 .\n \n(6.98)\nThus, the energies of the transmission resonances (with respect to the bottom of the well) correspond \nto the bound-state eigenenergies of the inﬁnite well. A similar effect is seen in atomic physics, where it \nis called the Ramsauer-Townsend effect.\nWe can use these same solutions to solve the problem of a barrier potential, as shown in Fig. 6.18, \nas long as the energy is above the barrier height. We simply change the well depth from V0 to \u0011V0 in \nall the formulae above. The results are the same; there are still resonances at the same energy levels. \nThe only difference is that now the wavelength in the potential region is longer rather than shorter than \nthe wavelength outside. This corresponds to the classical optics case where light from glass is incident \non a slab of air.\n6.5 \u0002 TUNNELING THROUGH BARRIERS\nIf the energy of the particle is below the barrier height, then the barrier region is classically forbid-\nden and a classical particle reﬂects perfectly from the barrier. In the quantum mechanical treatment \nthere is a possibility that the particle can penetrate the barrier and come out on the other side! This is \nbecause the quantum mechanical wave function penetrates into the classically forbidden region. This \nphenomenon is called quantum mechanical tunneling, and it is responsible for radioactive decay and \nthe current in high frequency semiconductor diodes, for example. Quantum tunneling has an optical \nanalogue where a light wave penetrates into air while being totally internally reﬂected from inside a \nglass prism. This penetrating wave is called an evanescent wave.\n\n6.5 Tunneling Through Barriers \n189\nA square potential energy barrier is shown in Fig. 6.19. The potential energy is described as\n \nV1x2 = •\n0,\nV0,\n0,   \n   x 6 -a\n-a 6 x 6 a\n   x 7 a.\n \n(6.99)\nIf the energy E of the incident particle beam is less than the well height V0, then the region \n-a 6 x 6 a is classically forbidden. As in the previous well problems, there are separate eigenvalue \nequations in the different regions:\n \n a-  U2\n2m d 2\ndx 2 + V0bwE1x2 = EwE1x2,   0 x 0 6 a  \n \n a-  U2\n2m d 2\ndx 2 + 0b wE1x2 = EwE1x2,   0 x 0 7 a. \n(6.100)\nThe energy E is less than the potential barrier height V0, so the interior solutions must be real expo-\nnentials and the exterior solutions must be complex exponentials. It is useful to deﬁne a wave vector k \noutside the well and a decay constant q inside the well:\n \n k = A\n2mE\nU2\n \n \n q = C\n2m1V0 - E2\nU2\n . \n(6.101)\nUse these two constants to rewrite the energy eigenvalue equations as\n \n \nd 2wE1x2\ndx 2\n= q2wE1x2,       0 x 0 6 a  \n \n \nd 2wE1x2\ndx 2\n= -k 2wE1x2,   0 x 0 7 a. \n(6.102)\n\u0002a\na\nx\nE\nE \u0004 V0\nV0\nFIGURE 6.19 A ﬁnite square barrier with the incident particle energy below the barrier height.\n\n190 \nUnbound States\nThe general solutions to these equations are\n \nwE1x2 = •\nAeikx + Be-ikx,\nCeqx + De-qx,\nFeikx,\n   \n   x 6 -a\n-a 6 x 6 a\n   x 7 a,\n \n(6.103)\nwhere we have again assumed that there are particles incident from the left, but not from the right. \nIt is important that the wave function in the classically forbidden region contains both the exponen-\ntially decreasing and the exponentially growing terms. The growing term cannot vanish as it did in the \ncase where the classically forbidden region extended to inﬁnity (Section 5.5). The boundary condition \nequations for continuity of the wave function and of the derivative of the wave function are\n \n w1-a2:   Ae-ika + Beika = Ce-qa + Deqa \n \n \ndw1x2\ndx\n`\nx=-a\n:   ikAe-ika - ikBeika = qCe-qa - qDeqa \n \n w1a2:   Ceqa + De-qa = Feika\n \n \n \ndw1x2\ndx\n`\nx =a\n:   qCeqa - qDe-qa = ikFeika.\n \n(6.104)\nAs before, we solve for the ratios of the amplitudes to get the transmission probability:\n \n T = 0 F0\n2\n0 A0 2 =\n1\n1 + 1k 2 + q 22\n2\n4k 2q 2\n sin h212qa2\n \n \n =\n1\n1 +\nV 2\n0\n4E1V0 - E2\n sin h2a2a\nU\n 42m1V0 - E2b\n. \n(6.105)\nThis transmission probability for quantum mechanical tunneling quantiﬁes the probability for a par-\nticle incident upon the barrier to penetrate the barrier and come out the other side. Remember that the \nclassical result would be zero—a classical particle only reﬂects from such a barrier.\nThe reﬂection coefﬁcient for the incident beam is\n \n R = 0 B0\n2\n0 A0 2 = 1 - T =\n1\n1 +\n4k 2q 2\n1k 2 + q 22\n2sin h212qa2\n \n \n =\n1\n1 +\n4E1V0 - E2\nV 2\n0 sin h2a2a\nU\n 42m1V0 - E2b\n.\n \n(6.106)\n\n6.5 Tunneling Through Barriers \n191\nThe reﬂection and transmission coefﬁcients are plotted in Fig. 6.20 for the tunneling situation \n1E>V0 6 12, along with the coefﬁcients for the “over the barrier” situation 1E>V0 7 12, using \nEqs. (6.93) and (6.94) with V0 replaced by \u0011V0. In the tunneling case, the transmission is nearly \nzero except near the top of the barrier, where the tunneling probability increases exponentially. As the \nenergy of the incident particle exceeds the barrier height, the transmission becomes large and exhibits \nthe same resonances seen in the ﬁnite well problem. For large energy, the transmission goes to unity, \nwhich is to be expected because the potential barrier becomes insigniﬁcant.\nThe wave function of a particle that tunnels through a barrier is shown in Fig. 6.21. On the left \nside of the potential barrier are the incident and transmitted oscillatory waves. On the right side is \nthe transmitted oscillatory wave. Inside the barrier there is an exponentially damped wave function \n(the evanescent wave of optics). The growing exponential term is part of the interior wave function \n[see Eq. (6.103)], but the decaying term dominates (Problem 6.32).\n\u0002a\na\nx\nE,Ψ\nIncident\nReflected\nTotal\nFIGURE 6.21 Wave function (real part) of a particle tunneling through a square barrier. \nNote that there are two vertical axes, energy and wave function, with different zeroes.\n1\n2\n3\nE /V0\n0.2\n0.4\n0.6\n0.8\n1.0\nT,R\nR\nR\nT\nT\nFIGURE 6.20 Reﬂection and transmission coefﬁcients for scattering from a square barrier.\n\n192 \nUnbound States\ntip\nair\nsample\nV0\nd\nV\u0007x\b\nx\nFIGURE 6.22 Schematic diagram of the scanning tunneling microscope, and the \nrepresentation in terms of a potential energy diagram.\nA beautiful example of quantum mechanical tunneling is the scanning tunneling microscope, \nwhich was invented by Gerd Binnig and Heinrich Rohrer in 1981 and earned them the Nobel Prize in \nphysics in 1986. This imaging device employs a small sharp conducting tip that is brought up close to \na sample, as shown in Fig. 6.22. The air (or vacuum) region between the tip and sample is a potential \nenergy barrier because the electrons inside the two materials have lower potential energy than they \nwould in the free space between them due to the work functions of the materials. The probability that \nan electron can tunnel from the tip to the sample (or vice versa) is given by Eq. (6.105) and can be \napproximated as (Problem 6.33)\n \nT \f e-2qd, \n(6.107)\nwhere d is the separation of the tip and sample. In the microscope, a small bias voltage is applied \nbetween the tip and sample to create a preferential direction for current ﬂow. The tip and sample do not \n“touch” so the current is due only to tunneling and is proportional to the tunneling probability:\n \nI = I0 e-2qd. \n(6.108)\nThe exponential dependence makes the current extremely sensitive to the tip-sample separation, which \nis typically in the nanometer range to produce measurable currents. As the tip is moved laterally above \nand parallel to the sample surface, the current provides a measure of the surface topology. A scanning \ntunneling microscope produces images with typical lateral resolution of 0.1 nm and depth resolution \nof 0.01 nm, sufﬁcient to image individual atoms on the surface. A Web image search of “scanning \ntunneling microscope” reveals many beautiful pictures of natural and man-made atomic scale objects.\n6.6 \u0002 ATOM INTERFEROMETRY\nMany of the examples we have discussed in the last two chapters have clearly demonstrated the inher-\nent wave nature of particle motion in quantum mechanics. So can some of the classical light experi-\nments like diffraction and interference be translated to electrons, or even to bigger particles like atoms \nand molecules? Yes! Electron diffraction experiments have been used for a long time and have played \nan important role in studying the atomic level structure of solid state crystals and DNA molecules. In \nrecent years, the advent of laser cooling and trapping of atoms (see Chapter 16) has made it possible to \n\n6.6 Atom Interferometry \n193\nperform interference experiments with atoms and molecules. This new ﬁeld of atom interferometry \nis leading to new ways to measure a variety of phenomena with unprecedented precision and to probe \nthe mysteries of quantum measurement theory.\nLet’s discuss how an atom interferometer works by starting with the canonical double-slit inter-\nference experiment, as depicted in Fig. 6.23. You may have already seen this experiment when you \nstudied optics, where it is commonly referred to as Young’s double-slit experiment. The beauty is that \nthe experiment can be performed with light or with particles such as electrons, neutrons, or atoms. \nMoreover, we can use it to discuss the wave-particle duality of quantum mechanics.\nLet’s ﬁrst explain how the double-slit experiment works with light and then extend that to other \nparticles. A source of light illuminates two narrow slits and the light passing through the slits lands on a \ndistant screen. Each slit by itself produces on the screen a diffraction pattern whose spatial extent depends \ninversely on the width of the slit. We assume that the slits are narrow enough that these two diffraction pat-\nterns overlap substantially. If both slits are open, the overlapping diffraction patterns exhibit an additional \ninterference pattern on the screen, within the overall single-slit diffraction pattern, as shown in Fig. 6.23. \nThese interference fringes are comfortably explained by using our notions about waves. The important \nwave idea is that the measured pattern of light cannot be explained by adding intensities, but rather we \nmust add amplitudes and then square the result to ﬁnd the total intensity, as discussed in Section 1.1.4. The \ntotal ﬁeld at the screen is thus the sum of the ﬁelds from each of the two slits:\n \n E1x2 = E11x2 + E21x2  \n \n = E0eikr1 + E0eikr2, \n(6.109)\nwhere the distances r1 and r2 depend on the transverse position x of the observation point, the wave \nvector k = 2p>l, and l is the wavelength of light. The intensity at the screen is proportional to the \ncomplex square of the electric ﬁeld\n \n I1x2 \f 0 E1x20\n2\n \n \n \f 0 E0eikr1 + E0eikr20\n2 \n \n = I00 eikr1 + eikr20\n2.\n \n(6.110)\nSource\nr1\nr2\nx\nFIGURE 6.23 Double-slit interference experiment and resulting interference intensity pattern \non the screen.\n\n194 \nUnbound States\nThe interference comes from the cross term in the complex square in Eq. (6.110):\n \n I1x2 = 2I011 + cos k1r2 - r122\n \n \n = 2I0 a1 + cos 2p 1r2 - r12\nl\nb. \n(6.111)\nAs you move the observation point up and down on the screen, the path length difference r2 - r1 \n varies, resulting in the sinusoidal intensity pattern characteristic of two interfering waves. The maxima \nin the interference pattern occur when the path length difference r2 - r1 is an integer multiple of the \n wavelength l.\nThis same wave-optics analysis applies to the wave function analysis of a quantum mechanics \nparticle, using the de Broglie wavelength to characterize the wave nature of the particle. A beam of \nparticles directed toward the double slits of Young’s experiment results in interference fringes at the \ndistant screen. The wave function at the screen resulting from equal contributions from the two slits is \nanalogous to the electric ﬁeld of the light above\n \nc = A1ei pr1>U + ei pr2>U2. \n(6.112)\nThe probability density for detecting a particle on the screen is\n \n P1x2 = 0 c1x20\n2 = 0 A0\n2\n 0 ei pr1>U + ei pr2>U0\n2 \n \n = 20 A0\n2\n a1 + cos  p\nU\n 1r2 - r12b\n \n,\n \n(6.113)\nwhich we rewrite in terms of the de Broglie wavelength using p = h>ldB:\n \nP1x2 = 20 A0\n2\n a1 + cos2p 1r2 - r12\nldB\nb. \n(6.114)\nThis has the same form as Eq. (6.111) and gives rise to the same interference pattern.\nYoung performed the original double-slit experiment with sunlight in 1801. Soon after de \n Broglie’s hypothesis in 1923 that matter can be described as a wave, diffraction experiments were \n performed with particles such as electrons, atoms, molecules, and neutrons to demonstrate matter \nwaves. Since then, Young’s double-slit interference experiment has been performed with electrons \n(1961), neutrons (1988), helium atoms (1991), and even with C60 buckyballs (1999). How about \n baseballs? Could we see interference fringes from something so large? Probably not. As we discussed \nin Section 4.2, a macroscopic object interacts strongly with the environment and its wave function \n suffers decoherence, which washes out the interference fringes.\nThe double-slit experiment is entirely consistent with the wave picture of light or matter, and so \nwould not appear to include any particle-like behavior. However, if we can control the source well \nenough to turn down the incident intensity so low that only one particle per second leaves the source, \nthen we can observe particle behavior with our own eyes. In the case of the light beam, the particles of \nlight are photons. Given that the screen is sensitive enough, the low intensity source produces individual \n\n6.6 Atom Interferometry \n195\nblips on the screen corresponding to the arrivals of the individual particles. At ﬁrst, these blips appear at \nseemingly random places on the screen, as shown in Fig. 6.24(a). However, as more blips are recorded \n[Figs. 6.24(b) and (c)] we begin to see that the density of blips coincides with the interference pattern \n[Fig. 6.24(d)] from the wave model, as described by Eq. (6.114). The individual blips are consistent with \nour notion of a particle and its spatial localization, but they are inconsistent with our notion of a wave \nbecause they do not individually exhibit the interference pattern predicted above. On the other hand, the \ninterference pattern that builds up after many particles is consistent with our wave interference model, \nbut is inconsistent with our idea that particles travel in straight lines such that each particle from the \nsource should go through one slit and arrive at the corresponding upper or lower spot on the screen.\nThus, we appear to arrive at a paradox. Some aspects of the experiment are consistent with a \nparticle model, while others are consistent with a wave model. The quantum mechanical resolution is \nto say that we use the wave model to predict the probabilities of detecting individual particles. This \nis consistent with the interpretation we used in the spins sections where the quantum state vector was \nused to predict the probability that a spin projection was measured to be up or down. So what we called \nthe light intensity in the classical wave description is now transformed into a probability of detecting \nphotons at particular places on the screen. Any given photon arrival occurs randomly on the screen and \nthe pattern builds up only after many arrivals. This is what we mean by wave-particle duality. (More \ncomplete discussions of this example can be found in Feynman and Cohen-Tannoudji et al.)\nIf you are not a little confused at this point, try this: What if you could measure which slit the par-\nticle went through? That is, which path did the particle take to arrive at the screen? Well, if you knew \nwhich slit the particle went though, then the wave description wouldn’t be right, because it requires \nthat the wave goes through both slits in order to deﬁne the path length difference in Eqs. (6.111) and \n(6.114). If the wave picture isn’t right, then the interference pattern shouldn’t be present. As it turns \nout, the interference pattern does indeed disappear if you know which slit the particle went through. \n(a)\n(b)\n(c)\n(d)\nFIGURE 6.24 A computer simulation of the arrival of particles at the detection screen in a double-slit \nexperiment, showing (a) random early arrivals, (b) and (c) the buildup of an interference pattern, and  \n(d) a plot of the predicted interference intensity distribution.\n\n196 \nUnbound States\nSource\nr2\nr1\nV2\nV1\nFIGURE 6.25 Double-slit atom interferometer for measuring potential energy differences.\nThe answer to this conundrum lies at the heart of quantum mechanical measurement theory. As hard as \nyou might try, you cannot measure, and therefore cannot know, which slit the “particle” goes through \nwithout disturbing it just a little bit. The simplest way to measure which slit the particle goes through \nis to watch, but you need some light to watch. If you see the particle, then at least one photon must \nhave scattered from the particle toward your eye, and the change in momentum of that photon in the \nscattering process will (through conservation of momentum) impart an equal and opposite change to \nthe particle’s momentum. This change is enough to alter the phase of the particle’s wave function and \ndestroy the interference fringes. In the early days of quantum mechanics, such “which path” experi-\nments were merely “thought” experiments or gedanken experiments because they were too hard to \nperform. However, in recent years careful experiments have demonstrated these effects beyond doubt.\nOne of the important features of an atom interferometer is its ability to measure extremely small \nchanges in potential energy. This ability arises from the dependence of the de Broglie wavelength of \nthe particle on the potential energy. If the potential energy varies, then the kinetic energy and hence the \nmomentum varies because the energy is conserved. The de Broglie wavelength depends on the particle \nmomentum, so a varying potential gives rises to a varying wavelength\n \n ldB = h\np\n \n \n =\nh\n22m1E - V2\n . \n(6.115)\nA measurement of the potential energy with an atom interferometer proceeds as shown in Fig. 6.25. \nDifferent regions of potential energy are placed behind slit 1 and behind slit 2. A difference in the two \npotential energies produces a phase shift between the two wave functions that interfere at the distant \nscreen. Hence, a measurement of the fringe shift in the interference pattern is a measurement of the \npotential energy difference. The different regions might, for example, have different electric ﬁelds, \nwhich produce different energies in atomic states (see Section 10.7.2). Or, if the atom interferometer \nis oriented vertically (or at an angle) instead of horizontally, then the two paths experience different \ngravitational potential energies. Recent experiments have been precise enough to test features of Ein-\nstein’s general theory of relativity. Atom interferometers can also measure rotation and acceleration, \nsimilar to ﬁber optic gyroscopes that are commonly used for navigation.\n\nProblems \n197\nSUMMARY\nIn this chapter, we learned about the unbound states of quantum particles. The momentum eigenstate \nwave functions are\n \n0  p9 \u0003 wp1x2 =\n1\n12pU ei px>U. \n(6.116)\nFor a free particle 3V1x2 = 04, the momentum eigenstates are also energy eigenstates with energy\n \nE = p2\n2m. \n(6.117)\nA free particle has a characteristic wavelength given by the de Broglie relation\n \nlde Broglie = h\np. \n(6.118)\nA more realistic representation of particle motion is obtained by superposing momentum \n eigenstates in a wave packet. The amplitude of each momentum component is f1 p2 and the resultant \nsuperposition is\n \nc1x2 =\n1\n12pU L\n\u0005\n- \u0005\nf1 p2ei px>U dp, \n(6.119)\nwhich has the form of a Fourier transform. The momentum amplitudes are related to the position space \nwave function through the inverse Fourier transform\n \nf1p2 =\n1\n12pU L\n\u0005\n- \u0005\nc1x2e-i px>U dx. \n(6.120)\nThe Heisenberg uncertainty relation between position and momentum is\n \n\u0006x\u0006p Ú U\n2 \n(6.121)\nand tells us that tight spatial localization requires a broad range of momenta, and a particle with a \nwell-deﬁned momentum is spread over a large spatial region. The Gaussian wave packet is the only \nwave packet that satisﬁes the equality of the uncertainty relation and so is referred to as a minimum \nuncertainty state.\nIf a potential energy is present, the unbound states are scattering states. A particle incident on \na potential well is partially transmitted and partially reﬂected, except at certain resonance energies \nwhere there is no reﬂection. A particle with energy below the height of a potential barrier can tunnel \nthrough the barrier, a phenomenon that is not observed classically.\nPROBLEMS\n 6.1 Calculate the de Broglie wavelengths of the following items:\na) an electron with a kinetic energy of 3 eV\nb) a proton with a kinetic energy of 7 MeV\nc) a buckyball 1C602 with a speed of 200 m>s\nd) an oxygen molecule at room temperature\n\n198 \nUnbound States\ne) a raindrop\nf) yourself walking to class\nIn which of the above cases might you expect quantum mechanics to play an important role  \nand why?\n 6.2 The wave function for a particle in one dimension is\n(i)  \nc1x2 = Ae-x 2>a2.\na) Normalize the wave function.\nb) Calculate the expectation value 8x9 of the position.\nc) Calculate the uncertainty \u0006x of the position.\nd) Calculate the probability that the particle is found in the region 0 6 x 6 a.\ne)  Plot the wave function and the probability density and indicate the results to (b), (c), and \n(d) on the plot.\nf) Calculate the expectation value 8p9 of the momentum.\ng) Calculate the uncertainty \u0006p of the momentum.\nh) Does this state satisfy the uncertainty principle?\nRepeat for other wave functions:\n(ii)  \nc1x2 = Axe-x 2>a2\n(iii)  \nc1x2 = A \n1\nx 2 + a2\n 6.3 A beam of particles is prepared in a momentum eigenstate 0  p09. The beam is directed to a \n shutter that is open for a ﬁnite time t.\na) Find the wave function of the system immediately after passing through the shutter.\nb) Find the momentum probability distribution of the beam after the shutter.\n 6.4 Calculate the momentum space wave function for a particle in an energy eigenstate of the \ninﬁnite square well. Plot the momentum probability densities for the n \u0003 1, 2, and 10 energy \neigenstates. Discuss your results.\n 6.5 Show that the momentum and Hamiltonian operators commute for a free particle. Do this two \nways, using both the differential form (position representation) of the operators and the abstract \nform.\n 6.6 Calculate the commutator of the position and momentum operators. Do this two ways, using \nboth the position representation of the operators and the momentum representation.\n 6.7 Show that the momentum eigenstates wp1x2 = Aei px>U satisfy the Dirac orthogonality condition \nin Eq. (6.23) and that the normalization constant is A = 1> 12pU. Use the Dirac orthogonality \ncondition to normalize the wave vector eigenstates wk1x2 = Aeikx\n  and explain why the result \ndiffers from that for the momentum eigenstates.\n 6.8 Use your favorite computational plotting tool to create and plot a wave packet comprising \nthree sinusoidal waves, as done in Section 6.2.1. Vary the separation dp of the side modes \nfrom the  central mode and notice the effect upon the spatial extent dx of the “localized” wave \npacket. Quantify the relationship between the momentum spread dp and the position spread dx. \n Animate your plots and distinguish the motion of the wave packet envelope and the motion of \nthe sinusoidal waves inside the envelope.\n 6.9 Perform the Gaussian integral in Eq. (6.48) and verify the result in Eq. (6.49).\n\nProblems \n199\n 6.10 Calculate the expectation values of position and momentum for a Gaussian wave packet by \ndirect integration and verify Eqs. (6.56) and (6.59).\n 6.11 Use your favorite computational plotting tool to create and plot a Gaussian wave packet. Vary \nthe width b of the momentum distribution and notice the effect upon the spatial extent \u0006x of \nthe wave packet. Quantify the relationship between the momentum spread and the position \nspread. Animate your plots and distinguish the motion of the wave packet envelope and the \nmotion of the sinusoidal waves inside the envelope.\n 6.12 Show that a propagating Gaussian wave packet broadens in position space but not in \n momentum space. Plot the position-momentum uncertainty product as a function of  \ntime and show that the Gaussian wave packet is a minimum uncertainty state. Discuss  \nyour results.\n 6.13 Discuss each step in the calculation of the phase and group velocities in Eqs. (6.62) and (6.63).\n 6.14 Consider a particle whose wave function is c1x2 = Asin1 p0  x>U2. Is this wave function an \neigenstate of momentum? Find the expectation value 8p9 of the momentum and the momentum \nprobability distribution. Calculate the uncertainty \u0006p of the momentum. What are the possible \nresults of a measurement of the momentum?\n 6.15 Use the uncertainty principle to estimate the ground state energy of a particle of mass m \n conﬁned to a box with a size of a. Calculate the energy in electron volts for an electron \n conﬁned in a box with a = 0.1 nm, which is roughly the size of an atom.\n 6.16 Use the uncertainty principle to estimate the ground-state energy of a particle of mass m bound \nin the harmonic oscillator potential V1x2 = 1\n2\n kx 2.\n 6.17 Use the uncertainty principle to estimate the ground-state energy of a particle of mass m bound \nin the potential V1x2 = a0 x0 .\n 6.18 Use the uncertainty principle to estimate the ground-state energy of a particle of mass m bound \nin the potential V1x2 = bx4.\n 6.19 Use the uncertainty principle to estimate the ground-state energy of the hydrogen atom.\n 6.20 Calculate the position uncertainty for a particle bound to an inﬁnite square well of width L \nif (a) the particle is in the ground state, and (b) if the probability density is uniform across \nthe well.\n 6.21 A beam of particles is described by the wave function\nc1x2 = Aei p0>Ux e-x2>4a2.\na) Calculate the expectation value 8p9 of the momentum by working in the position \nrepresentation.\nb) Calculate the expectation value 8p9 of the momentum by working in the momentum \nrepresentation.\n 6.22 A beam of particles is described by the wave function\nc1x2 = eAei p0 x>U1b - 0 x02,\n0,\n   0 x0 6 b\n0 x0 7 b.\na) Normalize the wave function. \nb) Plot the wave function.\nc) Calculate and plot the momentum probability distribution.\n\n200 \nUnbound States\n0\nV(x)\nV0\nx\nFIGURE 6.26 Step potential.\n 6.23 Some radioactive nuclei emit electrons (beta radiation), so you might speculate that electrons \ncan exist within a nucleus. Use the uncertainty principle to estimate the minimum kinetic \nenergy (beware of relativity) of an electron conﬁned within a nucleus of size 2 fm. Compare \nthat with the Coulomb potential energy of the electron and comment on the possibility of \n electron conﬁnement within the nucleus.\n 6.24 Solve the boundary condition equations (6.90) to ﬁnd the amplitudes for transmission and \nreﬂection in Eq. (6.91).\n 6.25 Electrons incident upon a ﬁnite square well of depth 12 eV are transmitted with unit probabil-\nity when their kinetic energy is 20 eV. What is the minimum width of the well? Assuming this \nminimum width, for what other kinetic energies are the electrons also transmitted completely? \nDoes this well have any bound states?\n 6.26 A ﬁnite square well of depth 8 eV has 5 bound states. Electrons incident upon the well are \ntransmitted with unit probability when their kinetic energy is 11 eV. What is the width of the \nwell? For what other kinetic energies are the electrons also transmitted completely?\n 6.27 A ﬁnite square well has depth 5 eV and width 0.5 nm. What are the bound-state energies of this \nwell? Find the kinetic energies of electrons incident upon the well that are transmitted with unit \nprobability.\n 6.28 A ﬁnite square barrier has height 5 eV and width 1 nm. Find the kinetic energies of electrons \nincident upon the well that are transmitted with unit probability.\n 6.29 Consider a potential energy step as shown in Fig. 6.26 with a beam of particles incident from \nthe left.\na) Calculate the reﬂection coefﬁcient for the case where the energy of the incident particles is \nless than the height of the potential energy step.\nb) Calculate the reﬂection coefﬁcient for the case where the energy of the incident particles is \ngreater than the height of the step.\nc) Plot your results as a function of the incident energy and comment.\n 6.30 Show that a double step potential can be designed such that particles of particular energies are \ntransmitted with unit probability. The optical analogue is an antireﬂection coating.\n 6.31 Calculate the probability of transmission of an electron with kinetic energy 5 eV through a \n barrier of height 10 eV and width 1 nm.\n\nResources \n201\n 6.32 Consider a particle incident upon a potential energy barrier with a barrier height larger than the \nkinetic energy. Show that the growing exponential wave inside the barrier is always less than or \nequal to the decaying exponential term.\n 6.33 Show that the tunneling probability through a barrier of width d is proportional to e-2qd for \nqd W 1.\n 6.34 If the tunneling current in a scanning tunneling microscope is 1 nA at 1 nm tip-surface \n separation, how much current will ﬂow at tip-surface separations of 0.8 nm, 1.2 nm, or 2 nm? \nAssume that the work functions of the metals are 5 eV and that the bias voltage is minimal.\nRESOURCES\nActivities\nThe bulleted activity is available at\nwww.physics.oregonstate.edu/qmactivities\n•  Time Evolution of a Gaussian Wave Packet: Students predict and study the time evolution of a \nGaussian wave packet.\nQuantum Tunneling and Wave Packets: This simulation experiment from the PHET group at the \nUniversity of Colorado animates wave functions tunneling through barriers: \nhttp://phet.colorado.edu/en/simulation/quantum-tunneling\nFurther Reading\nInterference experiments with particles are discussed in these articles:\nA. Tonomura, J. Endo, T. Matsuda, T. Kawasaki, and H. Ezawa, “Demonstration of single- \nelectron buildup of an interference pattern,” Am. J. Phys. 57, 117–120 (1989).\nO. Nairz, M. Arndt, and A. Zeilinger, “Quantum interference experiments with large molecules,” \nAm. J. Phys. 71, 319–325 (2003).\nD. E. Pritchard, A. D. Cronin, S. Gupta, D. A. Kokorowski, “Atom optics: Old ideas, current \n technology, and new results,” Ann. Phys. (Leipzig) 10, 35–54 (2001).\nThe Nobel Prize for scanning tunneling microscopy is described here: \nnobelprize.org/nobel_prizes/physics/laureates/1986/\n\nC H A P T E R  \n7\nAngular Momentum\nIn the last two chapters, we learned the fundamentals of solving quantum mechanical problems with \nthe wave function approach. We studied particles bound in idealized square potential energy wells \nand free particles. We are now ready to attack the most important problem in the history of quan-\ntum mechanics—the hydrogen atom. The ability to solve this problem and compare it with precision \nexperiments has played a central role in making quantum mechanics the best proven theory in physics.\nThe hydrogen atom is the bound state of a positively charged proton and a negatively charged \nelectron that are attracted to each other by the Coulomb force. Classically, we expect the electron \n(me = 9.11 * 10-31 kg) to orbit around the more massive proton (mp = 1.67 * 10-27 kg), in the \nsame manner that the earth orbits around the sun, as depicted in Fig. 7.1(a). However, the uncertainty \nprinciple dictates that we cannot know the position of the electron well enough for Fig. 7.1(a) to be a \nvalid representation, but rather, the electron is represented by a probability cloud as in Fig. 7.1(b). By \nthe end of the next chapter, we will be able to predict the details of the many different possible shapes \nof the electron cloud.\nAs always in quantum mechanics, we begin by identifying the Hamiltonian of the system of inter-\nest because of its role in determining the dynamics of the system through the Schrödinger equation\n \niU d\ndt 0 c9 = H0 c9. \n(7.1)\nFIGURE 7.1 (a) A classical atom and (b) a quantum atom.\n\n7.1 Separating Center-of-Mass and Relative Motion \n203\nHsysΨsys(R,r) \u0004 EsysΨsys(R,r)\nΨsys(R,r) \u0004 ΨCM(R)Ψrel(r)\n7.30\n7.24, 7.27\n7.20\n7.21\nHCMΨCM(R)\u0004 ECMΨCM(R)\n7.24, 7.28\nHrelΨrel(r) \u0004 ErelΨrel(r)\nΨCM(X,Y,Z)\nr\nΘ\nΦ\nΨsys(R,r) \u0004 ΨCM(X,Y,Z)Ψrel(r,Θ,Φ)\nΨrel(r,Θ,Φ)\u0004R(r)Θ(Θ)Φ(Φ)\n8.69\nFIGURE 7.2 Flowchart for solving the hydrogen atom energy eigenvalue problem by reducing the \ntwo-body problem to a one-body problem and by separation of the spherical coordinate variables.  \nThe numbers in the corners of the boxes refer to the relevant equation numbers in the text.\nOnce we know the Hamiltonian, we ﬁnd the energy eigenstates by solving the energy eigenvalue equation\n \nH0 E9 = E0 E9. \n(7.2)\nThe energy eigenstates form the preferred basis for expanding any initial state and applying the \nSchrödinger time evolution recipe, so solving the energy eigenvalue equation is the primary task \nrequired to solve most quantum mechanical problems.\nCompared to the problems in the last two chapters, the hydrogen atom system presents us with \ntwo major complications: two particles and three dimensions. The goal of this chapter is to simplify \nboth these aspects of the problem. Analogous to the approach taken in classical mechanics, we reduce \nthe two-body problem to a ﬁctitious one-body problem and we separate the three spatial degrees of \nfreedom in a way that each spherical coordinate can be treated independently. A ﬂowchart depicting \nthese two simpliﬁcations is shown in Fig. 7.2. In this chapter, we perform all the steps of Fig. 7.2 except \nthe radial coordinate part. In particular, we focus on the two angular degrees of freedom because they \nrelate to the angular momentum, which is a conserved quantity. In the next chapter, we solve the radial \naspect of the problem for a 1>r Coulomb potential energy, which leads to the quantized energy levels \nof the hydrogen atom. The journey through the next two chapters requires some mathematics that may \nappear daunting; we provide the roadmaps in Figs. 7.2 and 7.6 so you can see the forest for the trees.\n\n204 \nAngular Momentum\nFor a three-dimensional system of two particles, the Hamiltonian is the sum of the kinetic energies \nof the two individual particles and the potential energy that describes the interaction between them:\n \nHsys =\np2\n1\n2m1\n+\np2\n2\n2m2\n+ V1r1, r22. \n(7.3)\nParticle 1 has mass m1, position r1, and momentum p1; particle 2 has mass m2, position r2, and \nmomentum p2, and the interaction of the two particles is characterized by the potential energy \nV1r1, r22. We assume that the potential energy depends only on the magnitude of the separation of \nthe two particles\n \nV1r1, r22 = V10 r1 - r202, \n(7.4)\nwhich we refer to as a central potential. In this chapter, we do not need to know the actual form of \nthe central potential. In fact, the quantum mechanical angular wave functions we ﬁnd in this chapter \nare valid for any central potential, which is a very powerful result. We introduce the Coulomb potential \nenergy for the hydrogen atom system in the next chapter.\n 7.1 \u0002 SEPARATING CENTER-OF-MASS AND RELATIVE MOTION\nIn classical mechanics, we simplify the motion of a system of particles by separating the motion of the \ncomposite system into the motion of the center of mass and the motion about the center of mass. We \ntake this same approach to simplify the quantum mechanical description of the hydrogen atom. We will \nwork this through in some detail because the procedure of separating the motion is very common and \nneeds to be understood, but, in fact, we will not pursue the motion of the center of mass beyond this \nsection. In the next section, we’ll begin the discussion of the motion about the center of mass, which is \nwhere many treatments of the hydrogen atom start.\nAs illustrated in Fig. 7.3, we deﬁne the center-of-mass coordinate position vector for this two-\nbody system as\n \nR = m1r1 + m2r2\nm1 + m2\n \n(7.5)\nand the relative position vector as\n \nr = r2 - r1. \n(7.6)\nIn classical mechanics, we typically use velocities, which are obtained by differentiation of position \nwith respect to time. In quantum mechanics, we use momentum as the preferred quantity, so the appro-\npriate quantities to separate the two-body motion are the momentum of the center of mass\n \nP = p1 + p2 \n(7.7)\nand the relative momentum\n \nprel = m1p2 - m2p1\nm1 + m2\n. \n(7.8)\n\n7.1 Separating Center-of-Mass and Relative Motion \n205\nThe relative momentum takes the simpler form that looks like a relative velocity\n \nprel\nm\n= p2\nm2\n- p1\nm1\n \n(7.9)\nif we deﬁne the reduced mass m:\n \n 1\nm =\n1\nm1\n+ 1\nm2\n \n \n m =\nm1m2\nm1 + m2\n . \n \n(7.10)\nWith the deﬁnitions in Eqs. (7.7) and (7.8), the two-body Hamiltonian in Eq. (7.3) becomes \n(Problem 7.1)\n \nHsys = P 2\n2M +\np 2\nrel\n2m + V1r2, \n(7.11)\nwhere the relative particle separation r is the magnitude 0 r2 - r10 . This procedure has separated the \nsystem Hamiltonian into two independent parts:\n \nHsys = HCM + Hrel, \n(7.12)\nwith a center-of-mass term\n \nHCM = P2\n2M \n(7.13)\nrepresenting the motion of a particle of mass M = m1 + m2 located at position R with momentum \nP = p1 + p2, and a relative term\n \nHrel =\np 2\nrel\n2m + V1r2 \n(7.14)\nx\ny\nr \u0004 r2 \u0002 r1\nz\nm2 (x2,y2,z2)\nm1 (x1,y1,z1)\nR\u000b\n\u000b\n\u000b\n\u000b\n\u000b\n\u000b\n(X,Y,Z)\nr1\nr2\nFIGURE 7.3 The center-of-mass and relative coordinates for a two-body system.\n\n206 \nAngular Momentum\nrepresenting the motion of a single ﬁctitious particle of mass m located at position r = r2 - r1 with \nmomentum prel subject to a potential energy V1r2 created by a force-center that is ﬁxed at the origin. \nNotice that the center-of-mass Hamiltonian HCM does not depend on the relative motion variables prel \nand r, and the relative Hamiltonian Hrel does not depend on the center-of-mass motion variables P \nand R; this is what we mean by “separable.” In contrast, Eq. (7.3) presents the same Hamiltonian in \nterms of p1 and r1 and p2 and r2, but the potential energy V contains both r1 and r2, so H is not sepa-\nrable in those coordinates. Notice also that the center-of-mass position vector R does not appear in \nthe Hamiltonian at all, which, classically, is a reﬂection of the fact that the momentum of the center \nof mass is conserved because there are no external forces. For the hydrogen atom system, the reduced \nmass is m = 0.9995me and the center of mass is located very near the proton.\nThe separation of the Hamiltonian into center-of-mass motion and relative motion can also be \ndone using the explicit position representation of the momentum operators as differentials. In the posi-\ntion representation, the one-dimensional momentum operator is\n \np \u0003 -i U d\ndx . \n(7.15)\nIn three dimensions, the momentum operator is cast in terms of the gradient operator \u0002:\n \np \u0003 -i U a 0\n0x\n in +\n0\n0y\n jn +\n0\n0z\n kn b = -i U\u0002. \n(7.16)\nFor a two-particle system, the momentum operators for the two particles are\n \n p1 \u0003 -i U ¢ 0\n0x1\n in +\n0\n0 y1\n jn +\n0\n0 z1\n kn ≤= -i U\u00021  \n \n p2 \u0003 -i U ¢ 0\n0x2\n in +\n0\n0 y2\n jn +\n0\n0z2\n kn ≤= -i U\u00022. \n \n(7.17)\nSubstituting these position representations into the Hamiltonian in Eq. (7.3) leads to the same separa-\ntion as in Eq. (7.11), where the center-of-mass momentum operator has the position representation \n(Problem 7.1)\n \nP \u0003 -i U a 0\n0X\n in +\n0\n0Y\n jn +\n0\n0Z\n kn b = -i U\u0002R. \n(7.18)\nX, Y, and Z are the Cartesian coordinates of the center-of-mass vector R, and \u0002R is the gradient opera-\ntor corresponding to the center-of-mass coordinates. The relative momentum operator has the position \nrepresentation\n \nprel \u0003 -i U a 0\n0x\n in +\n0\n0 y\n jn +\n0\n0z\n kn b = -i U\u0002r, \n(7.19)\nwhere x, y, and z are the Cartesian coordinates of the relative position vector r = r2 - r1 and \u0002r is the \ngradient operator corresponding to the relative coordinates.\nWith the Hamiltonian separated into center-of-mass motion and relative motion, we expect that \nthe quantum state vector can also be separated. This is not always the case, as we saw in the discussion \nof entanglement in Chapter 4, but it is a valid assumption for the hydrogen atom problem we want to \nsolve because the potential energy is a function only of the relative coordinate r. Hence, we write the \nwave function for the system as\n \ncsys1R, r2 = cCM1R2 crel1r2. \n(7.20)\n\n7.1 Separating Center-of-Mass and Relative Motion \n207\nThe energy eigenvalue equation for the system is\n \nHsys csys1R, r2 = Esys  csys1R, r2, \n(7.21)\nand substituting the separated Hamiltonian [Eq. (7.12)] and separated wave function [Eq. (7.20)] gives\n \n1HCM + Hrel2cCM1R2 crel1r2 = Esys  cCM1R2 crel1r2. \n(7.22)\nThe separate center-of-mass and relative Hamiltonians act only on their respective wave functions \nbecause the gradients \u0002R and \u0002r are independent, so Eq. (7.22) becomes\n \ncrel1r2HCM cCM1R2 + cCM1R2Hrel crel1r2 = Esys  cCM1R2 crel1r2. \n(7.23)\nWe assert that the separate center-of-mass and relative Hamiltonians satisfy their own energy eigen-\nvalue equations (Problem 7.2)\n \n HCM cCM1R2 = ECM cCM1R2 \n \n Hrel crel1r2 = Erel crel1r2 \n \n(7.24)\nand arrive at the energy eigenvalue equation for the system \n \nHsys cCM1R2 crel1r2 = 1ECM + Erel2cCM1R2 crel1r2, \n(7.25)\nwhich demonstrates that the system energy is the additive energy of the two parts\n \nEsys = ECM + Erel. \n(7.26)\nUsing the separate Hamiltonians in Eqs. (7.13) and (7.14), the separated energy eigenvalue \nequations are\n \nP 2\n2M\n cCM1R2 = ECM cCM1R2 \n(7.27)\nand\n \na\np 2\nrel\n2m + V1r2b  crel1r2 = Erel crel1r2. \n(7.28)\nThe center-of-mass energy eigenvalue equation (7.27) is the free particle eigenvalue equation we \nencountered in Chapter 6, while the relative motion energy eigenvalue equation (7.28) contains the \ninteraction potential and so has the interesting physics of the hydrogen atom. Using the position rep-\nresentation of the momentum operator in Eq. (7.18), the center-of-mass energy eigenvalue equation is\n \n-  U2\n2M\n a 0 2\n0X 2 + 0 2\n0Y 2 + 0 2\n0Z 2b cCM1X, Y, Z2 = ECM cCM1X, Y, Z2. \n(7.29)\nThe solution to Eq. (7.29) is the three-dimensional extension of the free-particle eigenstates we stud-\nied in Chapter 6\n \ncCM1X, Y, Z2 =\n1\n12pU2\n3>2 e i1PXX+PYY+PZZ2>U \n(7.30)\nwith energy eigenvalues\n \nECM =\n1\n2M AP 2\nX + P 2\nY + P 2\nZB. \n(7.31)\n\n208 \nAngular Momentum\nFor measurements of observables associated with the relative motion, the center-of-mass wave func-\ntion contributes only an overall phase to the system wave function and so has no effect on calculat-\ning probabilities of relative motion quantities. We can therefore leave the center-of-mass motion and \nconcentrate only on the relative motion dictated by the energy eigenvalue equation (7.28). That is the \nproblem we want to solve for the hydrogen atom. Remember that the angular momentum discusssion \nthat will follow in this chapter is valid for any central potential. In Chapter 8, we will insert the speciﬁc \nform of the potential for the hydrogen atom.\n7.2 \u0002 ENERGY EIGENVALUE EQUATION IN SPHERICAL COORDINATES\nThe relative motion Hamiltonian that governs the hydrogen atom is\n \nH = p 2\n2m + V1r2, \n(7.32)\nwhere we drop the “relative” subscripts because we are now focusing exclusively on the relative \nmotion and ignoring the center-of-mass motion. Using the position representation of the momentum \noperator from Eq. (7.19), the Hamiltonian is represented by\n \nH \u0003 -  U2\n2m\n \u00022 + V1r2 \n(7.33)\nand the energy eigenvalue equation is the differential equation\n \na-  U2\n2m\u00022 + V1r2b c1r2 = Ec1r2. \n(7.34)\nBecause the potential energy in Eq. (7.34) depends on the parameter r only, this problem is clearly \nasking for the use of spherical coordinates centered at the origin of the central potential. The system of \nspherical coordinates is shown in Fig. 7.4(a) and the relations between the spherical coordinates r, u, f \nand the Cartesian coordinates x, y, z are\n \n x = r sin u cos f \n \n y = r sin u sin f  \n \n(7.35)\n \n z = r cos u.\n \nThe differential volume element dV = dx dy dz expressed in spherical coordinates is\n \ndV = r2 sin u d u d f dr. \n(7.36)\nThis volume element is shown in Fig. 7.4(b), leading one to consider the grouping\n \ndV = 1r d u21r sin u d f21dr2. \n(7.37)\nHowever, for calculating the normalization of wave functions, we will group the terms as\n \ndV = 1sin u d u21d f21r2 dr2 \n(7.38)\n\n7.2 Energy Eigenvalue Equation in Spherical Coordinates \n209\nand normalize each coordinate piece of the wave function separately. It is also convenient to express \nthe volume element as\n \ndV = r2 dr d\t, \n(7.39)\nwhere\n \nd\t = sin u d u d f \n(7.40)\nis the differential solid angle element.\nIn spherical coordinates, the gradient operator is\n \n\u0002 = rn 0\n0r + un 1\nr\n  0\n0 u + fn \n1\nr sin u\n  0\n0 f \n(7.41)\nand the Laplacian operator \u00022 = \u0002~\u0002 is\n \n\u00022 = 1\nr2  0\n0r\n ar2\n 0\n0r b +\n1\nr2 sin u\n  0\n0 u\n asin u 0\n0 u b +\n1\nr2 sin2 u\n  0 2\n0 f2 . \n(7.42)\nUsing this spherical coordinate representation, the energy eigenvalue equation (7.34) becomes the dif-\nferential equation\n \n-  U2\n2m\n c 1\nr2  0\n0r\n ar2\n 0\n0r b +\n1\nr2 sin u\n  0\n0 u\n asin u 0\n0 u b +\n1\nr2 sin2 u\n  0 2\n0 f2 d c1r, u, f2 \n \n+ V1r2c1r, u, f2 = Ec1r, u, f2 .  \n(7.43)\nThis looks formidable, so it is worth remembering that this is just the position representation of the \nenergy eigenvalue equation\n \nH0 E9 = E0 E9. \n(7.44)\nSolving Eq. (7.43) for the energy E and the eigenstates 0 E9 \u0003 c1r, u, f2 is our primary task, but ﬁrst \nlet’s discuss the important role that angular momentum plays in this equation.\n(a)\nx\ny\nz\nr\u0002\nΘ\nΦ\ndΘ\ndΦ\ndr\nx\ny\nz\n(b)\nFIGURE 7.4 (a) Spherical coordinates and (b) the differential volume element.\n\n210 \nAngular Momentum\n7.3 \u0002 ANGULAR MOMENTUM\n 7.3.1 \u0002 Classical Angular Momentum\nThe classical angular momentum is deﬁned as\n \nL = r * p. \n(7.45)\nIn the case of central forces, the torque r * F is zero and angular momentum is a conserved quantity:\n \nt = d L\ndt = 0  1   L = constant. \n(7.46)\nA central force F1r2 depends only on the distance of the reduced mass from the center of force \n(i.e., the separation of the two particles) and not on the angular orientation of the system. Therefore, \nthe system is spherically symmetric; it is invariant (unchanged) under rotations. Noether’s theorem \nstates that whenever the laws of physics are invariant under a particular motion or other operation, \nthere will be a corresponding conserved quantity. In this case, the conservation of angular momentum \nis related to the invariance of the physical system under rotations.\n 7.3.2 \u0002 Quantum Mechanical Angular Momentum\nIn quantum mechanics, the Cartesian components of the angular momentum operator L = r * p in \nthe position representation are\n \n Lx = ypz - zpy \u0003 -i U ay 0\n0z - z 0\n0y b  \n \n Ly = zpx - xpz \u0003 -i U az 0\n0x - x 0\n0z b  \n(7.47)\n \n Lz = xpy - ypx \u0003 -i U ax 0\n0y - y 0\n0x b. \nPosition and momentum operators for a given axis do not commute 13x, px4 = iU, etc.2, whereas posi-\ntion and momentum operators for different axes do commute 13x, py4 = 0, etc.2. We can use these \ncommutators to calculate the commutators of the components of the angular momentum operator. For \nexample,\n \n 3Lx, Ly4 = 3ypz - zpy, zpx - xpz4\n \n \n = ypz  z px - ypz  x pz - z py \n z px + z py \n x pz - z px  ypz + z px  z py + x pz  ypz - x pz\n z py\n . \n(7.48)\nNow use the commutation relations to move commuting operators through each other (e.g., \nypz  z px = ypx pz z) and cancel terms:\n \n3Lx, Ly4 = ypx  pz\n z - x ypz  pz - zz px py + x py z pz - ypx  z pz + zz px py + x ypz  pz - x py  pz\n z \n \n= ypx pzz + x py  z pz - ypx  z pz - x py  pz\n z . \n(7.49)\nFinally, collect terms and use the commutator relation 3z, pz4 = iU :\n \n 3Lx, Ly4 = x py1z pz - pz\n z2 - ypx1z pz - pz\n z2 \n \n = x py3z, pz4 - ypx3z, pz4\n \n(7.50)\n \n = i U1xpy - ypx2\n \n \n = i ULz .\n \n\n7.3 Angular Momentum \n211\nCyclic permutations of this identity give the three commutation relations\n \n 3Lx, Ly4 = i ULz \n \n 3Ly, Lz4 = i ULx  \n \n(7.51)\n \n 3Lz, Lx4 = i ULy  . \nThese are exactly the same commutation relations that spin angular momentum obeys (Section 2.4)! \nSo orbital and spin angular momentum appear to have something in common, as you might expect. \nIndeed, this is why the physical property of spin angular momentum was given this name.\nWhen we studied spin, we found it useful to consider the S2 = S~S operator. The corresponding \noperator for orbital angular momentum is\n \nL 2 = L~L = L2\nx + L2\ny + L2\nz . \n(7.52)\nIn the spin case, the operator S2 commutes with all three component operators. Let’s try the same with \norbital angular momentum. For example,\n \n 3L 2, Lx4 = 3L2\nx + L2\ny + L2\nz , Lx4\n \n \n = 3L2\nx, Lx4 + 3L2\ny, Lx4 + 3L2\nz , Lx4  \n \n(7.53)\n \n = L2\ny Lx - Lx L2\ny + L2\nzLx - Lx L2\nz . \nAdd zero to this equation, but choose the terms that sum to zero cleverly so they help:\n \n 3L 2, Lx4 = Ly Ly Lx - Ly Lx Ly + Ly Lx Ly - Lx Ly Ly + Lz Lz Lx - Lz Lx Lz + Lz Lx Lz - Lx Lz Lz \n \n=0\n \n=0\n \n = Ly3Ly, Lx4 + 3Ly, Lx4Ly + Lz3Lz, Lx4 + 3Lz, Lx4Lz \n(7.54)\n \n = -i ULy Lz - i ULz Ly + i ULz Ly + i ULy Lz\n \n \n = 0.\n \nThe other two components also commute with L 2 (Problem 7.4):\n \n 3L 2, Lx4 = 0  \n \n 3L 2, Ly4 = 0  \n \n(7.55)\n \n 3L 2, Lz4 = 0  . \nSo orbital and spin angular momentum obey all the same commutation relations.\nThough we did not do it that way in Chapter 1, the eigenvalues and the eigenstates of spin angular \nmomentum can be derived solely from the commutation relations of the operators (see Section 11.3). \nThe spin eigenvalue equations are\n \n S20 sms9 = s1s + 12U20 sms9 \n \n(7.56)\n \n Sz0 sms9 = ms U0 sms9.\n \nThe states 0 sms9 are simultaneously eigenstates of S2 and Sz, which is possible because the two opera-\ntors commute with each other. Because orbital angular momentum obeys the same commutation rela-\ntions as spin, the eigenvalue equations for L 2 and Lz have the same form:\n \n L 20 /m/9 = /1/ + 12U20 /m/9 \n \n Lz0 /m/9 = m/ U0 /m/9\n \n \n(7.57)\n\n212 \nAngular Momentum\nand the states 0 /m/9 are simultaneously eigenstates of L 2 and Lz. Hence, we can draw on all the work \nwe did in the spins chapters to help us understand orbital angular momentum. The quantum number / \nis the orbital angular momentum quantum number and gives a measure of the “size” of the angular \nmomentum vector in that the magnitude is 2/1/ + 12U. The quantum number m/ is the orbital magnetic \nquantum number and indicates that the magnitude of the z-component of the angular momentum is m/ U.\nThere is one crucial difference between spin angular momentum and orbital angular momentum. \nIn the spin case, the allowed quantized values of the spin angular momentum quantum number s are \nthe integers and half integers:\n \ns = 0, 1\n2, 1, 3\n2, 2, 5\n2, 3, 7\n2, 4, ... . \n(7.58)\nIn Chapters 1–3 we studied spin-1/2 and spin-1 systems. In the case of orbital angular momentum, the \nquantum number / is allowed to take on only integer values\n \n/ = 0, 1, 2, 3, 4, ...  . \n(7.59)\nOther than this important distinction, spin and orbital angular momentum behave the same in quantum \nmechanical calculations of probabilities, expectation values, etc. The spin magnetic quantum number \nms spans the range from -s S +s in integer steps. The orbital magnetic quantum number m/ is similarly \nrestricted to the 2/ + 1 values\n \nm/ = -/, -/ + 1, ..., -1, 0, 1, ..., / - 1, /  . \n(7.60)\nIn the spin-1/2 system, we represent the spin operators as matrices:\n \n S2 \u0003 3\n4\n U2 a1\n0\n0\n1b  Sz \u0003 U\n2\n a1\n0\n0\n-1b  \n \n Sx \u0003 U\n2\n a0\n1\n1\n0b   Sy \u0003 U\n2\n a0\n-i\ni\n0 b  ,\n \n \n(7.61)\nwhere the basis states of the representation are the eigenstates of S2 and Sz as deﬁned in Eq. (7.56). For \norbital angular momentum, we also represent the operators as matrices, with the exception that only \ninteger values of / are allowed. For example, the matrix representations of the orbital angular momen-\ntum operators for / = 1 are\n \n L 2 \u0003 2U2 °\n1\n0\n0\n0\n1\n0\n0\n0\n1\n¢  L z \u0003 U\n  °\n1\n0\n0\n0\n0\n0\n0\n0\n-1\n¢\n \n \n Lx \u0003\nU\n22\n °\n0\n1\n0\n1\n0\n1\n0\n1\n0\n¢    Ly \u0003\nU\n22\n °\n0\n-i\n0\ni\n0\n-i\n0\ni\n0\n¢  , \n \n(7.62)\nwhere the basis states of the representation are the eigenstates of L 2 and Lz as deﬁned in Eq. (7.57). \nThese matrices are exactly the same as the spin-1 matrices we deﬁned in Chapter 2.7.\n\n7.3 Angular Momentum \n213\nExample 7.1 A particle with orbital angular momentum / = 1 is in the state\n \n0 c9 = 4\n1\n3 0119 + 4\n2\n3 0109. \n(7.63)\nFind the probability that a measurement of Lz yields the value U for this state and calculate the \nexpectation value of Lz.\nThe eigenstate of Lz with eigenvalue Lz = +U Aand eigenvalue L 2 = 2U2B is \n0 / = 1, m/ = 19 = 0 119, so the probability of measuring Lz = +U is\n \n PU = 08110 c90\n2\n \n \n = @  8110  A4\n1\n3 0119 + 4\n2\n3 0109B @\n2\n \n(7.64)\n \n = @4\n1\n3 8110119 + 4\n2\n3 8110109@\n2\n. \nThe states 0 /m/9 form an orthonormal basis, so 8110119 = 1 and 8110109 = 0, and the probability is\n \n PU = @4\n1\n3\n @\n2\n \n(7.65)\n \n = 1\n3.\n \nThe expectation value of Lz is\n \n8Lz9 = 8c0 Lz0 c9. \n(7.66)\nLet’s calculate this with matrices. Using the matrix (column) representation of 0 c9:\n \n0 c9 \u0003\n1\n23\n °\n1\n22\n0\n¢ , \n(7.67)\nwe get\n \n 8Lz9 =\n1\n23\n 11\n \n22\n02 U °\n1\n0\n0\n0\n0\n0\n0\n0\n-1\n¢ 1\n23\n °\n1\n22\n0\n¢\n \n(7.68)\n \n = U\n3\n 11\n \n22\n02°\n1\n0\n0\n¢\n \n \n = U\n3.\n \nThese calculations are no different than if this were a spin-1 problem.\nSo it looks like we can solve orbital angular momentum problems using our spin knowledge, and \nyou may well ask: Is that all there is to it? Yes and no! If you can solve a problem like Example 7.1 \nusing the bra-ket or matrix notation we developed in the spins chapters, then do that. But there are \nproblems where we need to do more.\n\n214 \nAngular Momentum\nIn Chapters 1–3 we never discussed a position representation of spin operators or eigenstates, \nbecause it is not possible to describe spin angular momentum using the wave function language we \ndeveloped in Chapter 5. In contrast, it is possible to represent orbital angular momentum operators and \neigenstates in the position representation. We have already presented the position representation of the \norbital angular momentum operators Lx, Ly, and Lz in Eq. (7.47), and the end result of this chapter is a \nposition representation of the angular momentum eigenstates 0 /m/9. In solving for the allowed spatial \nwave functions, we will prove that the orbital angular momentum is quantized according to Eqs. (7.59) \nand (7.60).\nArmed with wave functions detailing the spatial dependence of orbital angular momentum, we \nwill then be able to visualize the angular probability distribution of the electron around the proton \nin the hydrogen atom. We will be able to understand why two hydrogen atoms form a molecule and \nwhy the carbon bonds in a diamond lattice are oriented in such a way to make diamond so unique. For \nexample, Fig. 7.5 shows the angular orientation of the four tetrahedral bonds that one carbon atom \nmakes within the diamond lattice.\nTo see the importance of orbital angular momentum in solving the hydrogen atom energy eigen-\nvalue equation, we change the angular momentum operators in Eq. (7.47) to spherical coordinates. \nUsing the relations in Eq. (7.35), one can show that the angular momentum operator Lz has the spheri-\ncal coordinate representation (Problem 7.8)\n \nLz \u0003 -i U 0\n0 f \n(7.69)\nand depends on f alone. Likewise, we convert Lx and Ly to spherical coordinates (Problem 7.8) and \nobtain the operator L 2 = L~L = L 2\nx + L 2\ny + L 2\nz:\n \nL2 \u0003 -U2 c\n1\n sin u\n  0\n0 u\n asin u 0\n0 u b +\n1\nsin2u\n  0 2\n0 f2d  , \n(7.70)\nwhich depends on u and f, and not on r. We now have the expressions for the two operators L 2 and Lz \nthat we need to express the angular momentum eigenvalue equations (7.57) in the spherical coordinate \nrepresentation, which we do later in this chapter.\nNow compare the L 2 operator in Eq. (7.70) with the energy eigenvalue equation (7.43). You \nnotice that the L 2 operator is part of the differential operator in the energy eigenvalue equation. Hence, \nwe can rewrite the energy eigenvalue equation H0 c9 = E0 c9 with the L 2 operator\n \n-  U2\n2m\n c 1\nr2 0\n0r\n ar2\n 0\n0r b -\n1\nU2r2 L2 d c1r, u, f2 + V1r2c1r, u, f2 =  Ec1r, u, f2  . \n(7.71)\nAll of the angular part of the Hamiltonian is contained in the L 2 angular momentum operator. In this \nform, it is clear that the central force Hamiltonian commutes with the orbital angular momentum oper-\nators L 2 and Lz (Problem 7.9)\n \n 3H, L 24 = 0 \n \n 3H, Lz4 = 0, \n \n(7.72)\nwhich implies that we can ﬁnd simultaneous eigenstates of all three operators.\n\n7.4 Separation of Variables: Spherical Coordinates \n215\nFIGURE 7.5 Angular dependence of the four sp3 hybrid orbitals in a diamond lattice.\n7.4 \u0002 SEPARATION OF VARIABLES: SPHERICAL COORDINATES\nWe have already simpliﬁed the two-body nature of the hydrogen atom problem to an effective one-\nbody problem by separating the relative motion (interesting) from the center-of-mass motion (not so \ninteresting). We now proceed to simplify the three-dimensional aspect of the problem by separating \nthe three spherical coordinate dimensions from each other. To do this, we apply the standard tech-\nnique of separation of variables to the energy eigenvalue differential equation (7.71). This technique \nis reviewed in Appendix E, where six steps detail the process in its general form. The ﬂowchart in \nFig. 7.6 shows how the separation and recombination process will progress over the remainder of this \nchapter and through the next chapter.\nIn the ﬁrst instance, we apply the six steps of the separation of variables procedure to isolate the \nradial r dependence and the angular u, f dependence into two separate equations.\nStep 1: Write the partial differential equation in the appropriate coordinate system. We have done \nthis already in Eq. (7.71)\n \n-  U2\n2m\n c 1\nr2 0\n0r\n ar2\n 0\n0r b -\n1\nU2r2 L2 d c1r, u, f2 + V1r2c1r, u, f2 =  Ec1r, u, f2. (7.73)\nStep 2:  Assume that the solution c1r, u, f2 can be written as the product of functions, at least one of \nwhich depends on only one variable, in this case r. The other function(s) must not depend at \nall on this variable, that is, assume\n \nc1r, u, f2 = R1r2Y1u, f2. \n(7.74)\n \n Plug this assumed solution into the partial differential equation (7.73) from Step 1. Because \nof the special form of c, the partial derivatives each act on only one of the functions in c. Any \n\n216 \nAngular Momentum\npartial derivatives that act only on a function of a single variable may be rewritten as total \nderivatives, yielding\n \n-  U2\n2m\n c Y 1\nr2 d\ndr\n ar2 dR\ndr b -\n1\nU2r2 R1L2Y2d + V1r2RY = ERY. \n(7.75)\nY(Θ,Φ) \u0004 Θ(Θ)Φ(Φ)\nΦm(Φ)\nHΨnlm(r,Θ,Φ) \u0004 EnΨnlm(r,Θ,Φ)\nLzΦm(Φ) = mhΦm(Φ)\nB(m)\nA(l)\nHΨ(r,Θ,φ) \u0004 EΨ(r,Θ,Φ)\nΨ(r,Θ,Φ) \u0004 R(r)Y(Θ,Φ)\nΘl\nm(cosΘ)\nL2Yl\nm(Θ,Φ) \u0004 l(l+1)h2Yl\nm(Θ,Φ) \nYl\nm(Θ,Φ) \nΨnlm(r,Θ,Φ) \u0004 Rnl(r )Yl\nm(Θ,Φ)\nRnl(r)\n7.83\n7.100\n7.82\n7.79\n8.67\n7.156\n7.161\n8.69\n7.81\n7.74\n7.43\nd\ndΦ eqn \u0004 BΦ\nd\ndΘ eqn \u0004 AΘ\nd\ndr eqn \u0004 ER\nFIGURE 7.6 Flowchart of the separation of variables procedure applied to the hydrogen atom. \nThe numbers in the corners of the boxes refer to the relevant equation numbers in the text.\n\n7.4 Separation of Variables: Spherical Coordinates \n217\n \n Note that the orbital angular momentum operator L2 acts only on angular spatial functions \n[Eq. (7.70)].\n Step 3: Divide both sides of the equation by c = RY:\n \n-  U2\n2m\n c 1\nR 1\nr2 d\ndr\n ar2 dR\ndr b - 1\nY 1\nU2r2 1L2Y2d + V1r2 = E. \n(7.76)\n Step 4: Isolate all of the dependence on one coordinate on one side of the equation. To isolate the r \ndependence, we multiply Eq. (7.76) by r 2 to clear the r dependence from the denominator \nof the angular term (involving angular derivatives in L 2 and angular functions in Y). Further \nrearranging Eq. (7.76) to get all of the r dependence on the left-hand side, we obtain:\n \n1\nR1r2 d\ndr\n ar2 \ndR1r2\ndr\nb - 2m\nU2  1E - V1r22r2 = 1\nU2 \n1\nY1u, f2 L2Y1u, f2. \n(7.77)\n \nfunction of r only\n \nfunction of u, f only\n \n The left-hand side of Eq. (7.77) is a function of r only, while the right-hand side is a function \nof u, f only.\n Step 5: Now imagine changing the isolated variable r by a small amount. In principle, the left-hand \nside of Eq. (7.77) could change, but nothing on the right-hand side would. Therefore, if the \nequation is to be true for all values of r, the particular combination of r dependences on the \nleft-hand side must result in no overall dependence on r—the left-hand side must be a con-\nstant. We thus deﬁne a separation constant, which we call A in this case:\n \n1\nR1r2 d\ndr\n ar2 \ndR1r2\ndr\nb - 2m\nU2  1E - V1r22r2 = 1\nU2 \n1\nY1u, f2\n L2Y1u, f2 K A. \n(7.78)\n Step 6: Write each equation in standard form by multiplying each equation by its unknown function \nto clear it from the denominator. Rearranging Eq. (7.78) slightly, we obtain the radial and \nangular equations in the more standard forms:\n \nc -  U2\n2mr2 d\ndr ar2 d\ndr\n b + V1r2 + A U2\n2mr2 d R1r2 = ER1r2 \n(7.79)\n \nL2Y1u, f2 = A U2 Y1u, f2. \n(7.80)\nNotice that the only place that the central potential V1r2 enters the set of differential equations is in \nthe radial equation (7.79), which is not yet in the form of an eigenvalue equation because it contains \ntwo unknown constants, E and A. Equation (7.80) is an eigenvalue equation for the orbital angular \nmomentum operator L 2 with eigenvalue AU2. It has the same form as Eq. (7.57), so we fully expect \nthat the separation constant A = /1/ + 12, which we will prove shortly. The angular momentum \neigenvalue equation is independent of the central potential V1r2, so once we have solved for the \norbital angular momentum eigenstates, we will have solved that aspect of the problem for all central \npotentials. Only the radial equation need be solved again for different potentials.\nThe separation of variables procedure can be applied again to separate the u dependence from the \nf dependence in the angular equation (7.80). If we let\n \nY1u, f2 = \u00121u2\u00131f2, \n(7.81)\n\n218 \nAngular Momentum\nthen the separated equations are (Problem 7.10)\n \nc\n1\nsin u d\nd u\n asin u d\nd ub - B \n1\n sin2 u d \u00121u2 = -A \u00121u2 \n(7.82)\n \nd2\n \u00131f2\ndf2\n= -B \u0013(f), \n(7.83)\nwhere we have deﬁned the new separation constant as B. Equation (7.83) is an eigenvalue equation for \nthe operator d2>df2 with eigenvalue -B. Equation (7.82) is not yet in the form of an eigenvalue equa-\ntion because it contains two unknown constants A and B.\nWe started with a partial differential equation in three variables and we ended up with three ordi-\nnary differential equations by introducing two separation constants A and B. You should always get \none fewer separation constant than the number of variables you started with; each separation constant \nshould appear in two equations of the ﬁnal set.\nSo in turn we have identiﬁed a radial differential equation for R1r2, a polar angle differential \nequation for \u00121u2, and an azimuthal differential equation for \u00131f2. But note that the radial equation \ncontains the polar separation constant A and the polar equation contains the azimuthal separation \nconstant B. So we must solve the azimuthal equation ﬁrst, then the polar equation, and ﬁnally the \nradial equation. The azimuthal solution to Eq. (7.83) determines the constant B, which then goes \ninto Eq. (7.82) to determine the polar angle solution and the constant A. The combined azimuthal and \npolar solutions also satisfy the eigenvalue equation (7.80) for the orbital angular momentum operator L2. \nFinally, the constant A goes into the radial equation (7.79) and the energy eigenvalues are determined.\nRather than simply solving these mathematical equations, we will place each of these three \neigenvalue equations in some physical context by identifying situations that isolate the different equa-\ntions from the original energy eigenvalue equation H0 E9 = E0 E9. In this chapter, we focus on the two \nangular equations, which are independent of the central potential energy V1r2. In the next chapter, we \nsolve the radial equation for the special case of the hydrogen atom with the Coulomb potential energy \nfunction.\n7.5 \u0002 MOTION OF A PARTICLE ON A RING\nTo isolate the azimuthal eigenvalue problem in Eq. (7.83), we consider a system with no radial or \npolar angle dependence. This system comprises a particle of mass m conﬁned to move on a ring of \nconstant radius r0, as shown in Fig. 7.7. We assume that the ring lies in the x, y plane, so that in spheri-\ncal coordinates u = p>2. Thus, the motion takes place at constant r and constant u, with the azimuthal \nangle f as the sole degree of freedom. The wave function c is independent of r and u, so derivatives \nwith respect to those variables are zero. Hence, the energy eigenvalue equation [Eq. (7.43)] reduces to\n \n-U2\n2m  1\nr2\n0\n 0 2\n0 f2 c + V1r02c = Eringc, \n(7.84)\nwhich is the position representation of\n \nHring0 Ering9 = Ering0 Ering9. \n(7.85)\n\n7.5 Motion of a Particle on a Ring \n219\nFollowing our notation in the previous section, we call the wave function \u00131f2 and we change the \npartial derivative in Eq. (7.84) to a total derivative because there is only one variable. For this simpli-\nﬁed ring problem, the potential energy is a constant V1r02, which we choose to be zero, but we have to \nremember that we cannot make this choice when we are working on the full hydrogen atom problem. \nWe also identify mr2\n0 = I as the moment of inertia of a classical particle of mass m traveling in a ring \nabout the origin. With these choices, the energy eigenvalue equation becomes\n \n-  U2\n2I d2\ndf2 \u00131f2 = Ering\u00131f2. \n(7.86)\nThis is the same eigenvalue equation we found in Eq. (7.83) for the azimuthal function \u00131f2 as long \nas we identify the separation constant B as\n \nB = 2I\nU2 Ering \n(7.87)\nin this problem of a particle on a ring. Thus, this idealized particle-on-a-ring example has the same dif-\nferential equation, and hence the same wave function solutions, as the separated azimuthal equation in \nthe three-dimensional hydrogen atom problem.\nIf we compare the azimuthal differential equation (7.86) with the orbital angular momentum \noperator in Eq. (7.69), we note that the energy eigenvalue equation can be expressed as\n \nL2\nz\n2I\n \u00131f2 = Ering\u00131f2, \n(7.88)\nwhich again emphasizes the importance of angular momentum. This energy eigenvalue equation is \nwhat you would expect for a classical particle rotating in a circular path in the x, y plane with kinetic \nenergy T = Iv2>2 = L2\nz >2I and resultant Hamiltonian\n \nHring = T =\nL2\nz\n2I , \n(7.89)\nx\ny\nΦ\nr0\nΜ\nKnown quantities\nΜ, r0, I, \u0002\nParameters\nΦ\nUnknown quantities\nEring,\u0007Ψ\nFIGURE 7.7 Particle conﬁned to move on a ring.\n\n220 \nAngular Momentum\nassuming zero potential energy. We noted earlier that eigenstates of Lz obey an eigenvalue equation\n \nLz0 m9 = m U0 m9, \n(7.90)\nwhere we suppress the / quantum number (for the moment) because it is not applicable to this ideal-\nized one-dimensional particle-on-a-ring problem. The 0 m9 states are also eigenstates of L2\nz:\n \nL2\nz 0 m9 = m2U20 m9 \n(7.91)\nand hence of the Hamiltonian of the particle on a ring:\n \n Hring0 m9 = Ering0 m9  \n \n \nL2\nz\n2I 0 m9 = m2 U2\n2I 0 m9. \n \n(7.92)\nSo it looks like we already know the answer; that the energy eigenvalues are E = m2 U2>2I and the \nseparation constant is B = m2. However, we know the properties of the 0 m9 states in the abstract only; \nwe do not know their spatial representation. That comes from solving the differential equation (7.86), \nwhich is the position representation of the abstract equation (7.92). Let’s solve it and conﬁrm our \nexpectations about the energy eigenvalues.\n 7.5.1 \u0002 Azimuthal Solution\nThe azimuthal differential equation written in terms of the separation constant is\n \nd2\u00131f2\ndf2\n= -B\u00131f2. \n(7.93)\nThe solutions to this differential equation are the complex exponentials\n \n\u00131f2 = Ne{i2Bf, \n(7.94)\nwhere N is the normalization constant. Mathematically B could have any value, but the physics \nimposes some constraints.\nThere is no “boundary” on the ring, so we cannot impose boundary conditions like we did for the \npotential energy well problems in Chapter 5. However, there is one very important property of the wave \nfunction that we can invoke: it must be single-valued. The variable f is the azimuthal angle around the \nring, so that f + 2p is physically the same point as f. If we go once around the ring and return to our \nstarting point, the value of the wave function must remain the same. Therefore, the solutions must sat-\nisfy the periodicity condition \u00131f + 2p2 = \u00131f2. In order for the eigenstate wave function \u00131f2 \nto be periodic, the value of 1B must be real (complex 1B would result in real exponential solutions). \nFurthermore, the solutions must have the correct period, which requires that 1B be an integer:\n \nm = 0, {1, {2, ... . \n(7.95)\nSo we see that there are many solutions, each corresponding to a different integer (which can be zero, \npositive, or negative). We write the solutions as\n \n\u0013m1f2 = Neimf. \n(7.96)\n\n7.5 Motion of a Particle on a Ring \n221\nThe quantum number m is the orbital magnetic quantum number we introduced in Section 7.3. We \ndon’t use a subscript on m here because there is no need to distinguish it from spin for now.\nIf we operate on the eigenstate wave function \u0013m1f2 with the derivative form of the Lz operator, \nwe obtain\n \n Lz\u0013m1f2 = -i U 0\n0 f\n 1Neimf2  \n \n = -i U1im21Neimf2 \n \n = m U1Neimf2\n \n \n = m U\u0013m1f2.\n \n \n(7.97)\nAs expected, we have found that the energy eigenstates for the particle on a ring are the states 0 m9 that \nsatisfy the Lz eigenvalue equation (7.90).\nAs usual, we ﬁnd the normalization constant N in Eq. (7.94) by requiring that the probability of \nﬁnding the particle somewhere on the ring is unity:\n \n1 =\n \nL\n2p\n0\n\u0013*\nm1f2\u0013m1f2df =\n \nL\n2p\n0\nN*e-imf Neimf df = 2p0 N0\n2. \n(7.98)\nWe are free to choose the constant to be real and positive:\n \nN =\n1\n22p\n. \n(7.99)\nWe have thus found the position representation \u0013m1f2 = 8f0 m9 of the 0 m9 states:\n \n0 m9 \u0003 \u0013m1f2 =\n1\n22p\n eimf   . \n(7.100)\nThe eigenfunctions of the ring form an orthonormal set (Problem 7.11):\n \nL\n2p\n0\n\u0013*\nk1f2\u0013m1f2df = dkm. \n(7.101)\nTo reiterate, these functions are eigenstates of the ring Hamiltonian\n \n Hring0 m9 = Ering0 m9\n \n \n Hring\u0013m1f2 = Ering\u0013m1f2 \n \n(7.102)\nas well as eigenstates of the z-component of orbital angular momentum\n \n Lz0 m9 = m U0 m9\n \n \n Lz\u0013m1f2 = m U\u0013m1f2  . \n \n(7.103)\n\n222 \nAngular Momentum\nThe allowed values of the separation constant B are B = m2, so the possible energy eigenvalues \nusing Eq. (7.87) are\n \nE0m0 = m2 U2\n2I, \n(7.104)\nwhich is exactly what we expected from Eq. (7.92). The spectrum of allowed energies is shown in \nFig. 7.8. The eigenstates corresponding to + 0 m0  and - 0 m0  states have the same energy, so there are \ntwo energy states at every allowed energy except for the one corresponding to m = 0. Thus the \nparticle-on-a-ring system exhibits degeneracy, which we ﬁrst encountered in the free-particle system \nin Section 6.1.1. For the particle-on-a-ring system, all states are two-fold degenerate except for m = 0, \nwhich is nondegenerate. The {m degeneracy of the energy eigenstates corresponds to the angular \nmomentum states with Lz = +mU and Lz = -mU. That is, the two degenerate energy states represent \nstates with opposite components of the angular momentum along the z-axis. The energy is the same \nregardless of the direction of rotation, which is analogous to the free particle in one dimension where \nthe energy is independent of the direction of travel.\nThe particle on a ring is a one-dimensional system even though it exists in a two-dimensional \nspace. This is because there is only one degree of freedom f, similar to the particle-in-a-box system \nwe studied in Chapter 5, where the single degree of freedom was x. The solutions to both problems \nhave the same oscillatory form. As in the particle-in-a-box problem, the energy eigenvalues of the par-\nticle-on-a-ring system are discrete because of a boundary condition. The difference is that the bound-\nary condition appropriate to the ring problem is periodicity because f is a physical angle, rather than \nc1x2 = 0 at the boundaries, which is appropriate to an inﬁnite potential.\n0\n5\n10\n15\nE/E1\n\u0002m\u0002 =\u00074\n\u0002m\u0002 =\u00073\n\u0002m\u0002 =\u00072\n\u0002m\u0002 =\u00071\nm =\u00070\nFIGURE 7.8 Energy spectrum for a particle on a ring.\n\n7.5 Motion of a Particle on a Ring \n223\n 7.5.2 \u0002 Quantum Measurements on a Particle Conﬁned to a Ring\nMany of the aspects of quantum measurement applied to this new system are similar to the spin and \nparticle-in-a-box examples we studied previously (e.g., Examples 2.3, 5.5, and 7.1). However, the \ndegeneracy of energy levels presents a new aspect. Because the states 0 m9 and 0 -m9 have the same \nenergy, the probability of measuring the energy E0m0 is the sum\n \nPE0m0 = 08m0 c90\n2 + 08-m0 c90\n2, \n(7.105)\nexcept for the m = 0 state. On the other hand, the state 0 m9 uniquely speciﬁes the orbital angular \nmomentum component along the z-direction, so the probability of measuring the angular momentum \ncomponent is\n \nPLz=m U = 08m0 c90\n2. \n(7.106)\nExample 7.2 A particle on a ring is in the superposition state\n \n0 c9 =\n1\n17 10 09 + 20 19 + 0\n -19 + 0 292. \n(7.107)\nIf we measure the energy, what is the probability of measuring the value E1 = U2>2I and what is \nthe state of the system after measuring that value?\nThe probability of measuring the value E1 = U2>2I is obtained using Eq. (7.105):\n \n PE1 = 0810 c90\n2 + 08-10 c90\n2\n \n \n = @ H1@  1\n17  A @  0I + 2@  1I + @  -1I + @  2IB @\n2\n+ @ H-1@  1\n17  A @  0I + 2@  1I + @  -1I + @2IB @\n2\n \n \n = @  2\n17\n @\n2\n+ @  1\n17\n @\n2\n \n(7.108)\n \n = 5\n7.\n \nAfter the measurement, the new state vector is the normalized projection of the input state onto the \nkets corresponding to the result of the measurement (postulate 5, Chapter 2):\n \n@cafter E0m09 = 0 m98m0 + 0 -m98-m0\n2PE0m0\n0 c9, \n(7.109)\nwhich in this case is\n \n @cafter E19 = 0 19810 + 0  -198-10\n2PE1\n 1\n17 10 09 + 20 19 + 0  -19 + 0 292 \n(7.110)\n \n =\n1\n15 120 19 + 0  -192.\n \nUsing Stern-Gerlach analyzers, measurements of the angular momentum component Lz could \nbe made after the energy measurement, and would yield the results shown in Fig. 7.9 (Problem 7.12).\n\n224 \nAngular Momentum\n 7.5.3 \u0002 Superposition States\nThe eigenstate wave functions for the particle on a ring are complex, so we must plot both the real \nand imaginary components for a proper graphical representation of the wave function. Plots of three \n\u0013m1f2 eigenstates are shown in Fig. 7.10. The probability density of an eigenstate is\n \nPm1f2 = 0 \u0013m1f20\n2. \n(7.111)\nSubstituting in the eigenstate wave function from Eq. (7.100), we obtain\n \nPm1f2 = 2\n1\n12p eimf 2\n2\n=\n1\n2p , \n(7.112)\nwhich is a constant independent of the quantum number m. So there is no measurable spatial depen-\ndence of the 0 m9 eigenstates.\nHowever, there is spatial dependence in the probability density for superposition states. For \nexample, consider a state of the system with an initial wave function comprising two eigenstates:\n \nc1f, 02 = c1\u0013m11f2 + c2eiu\u0013m21f2. \n(7.113)\nm \u0004\u00070\nΠ\n2Π\nΦ\n\t(Φ)\n1\n2Π\n\n1\n\u000b\n2Π\n\u000b\nΦ\nm \u0004\u00071\nΠ\n2Π\n\t(Φ)\n1\n2Π\n\n1\n\u000b\n2Π\n\u000b\nΦ\nm \u0004\u00072\nΠ\n2Π\n\t(Φ)\n1\n2Π\n\n1\n\u000b\n2Π\n\u000b\nFIGURE 7.10 Eigenstate wave functions for a particle on a ring. The real part of the wave function is the solid \nline and the imaginary part is the dashed line.\n14\nH\n2\n1\n0\n\n1\n\n2\n2\nLz\nLz\n1\n0\n\n1\n\n2\n2\n1\n0\n\n1\n\n2\n58\n14\n14\nLz\n\u0002Ψ\u0003\nE2\nE1\nE0\nFIGURE 7.9 Energy measurement and orbital angular momentum component measurements.\n\n7.5 Motion of a Particle on a Ring \n225\nWe assume that this function is already properly normalized Aso that c2\n1 + c2\n2 = 1B, and we assume \nthat the constants c1 and c2 are real. An overall phase has no physical meaning (cannot be measured), \nso we can always choose one coefﬁcient to be real. Relative phases play a crucial role in measurement, \nso we have made the relative phase explicit by separating the phase eiu from the coefﬁcient of the sec-\nond term. Using the Schrödinger time-evolution recipe from Chapter 3, the initial state in Eq. (7.113) \nbecomes\n \n c1f, t2 = c1\u0013m11f2e-iE0m10t>U + c2eiu\u0013m21f2e-iE0m20t>U\n \n \n = c1 \n1\n12p eim1fe-iE0m10t>U + c2eiu\n1\n12p eim2fe-iE0m20t>U.\n \n \n(7.114)\nFor this state, the probability density for measuring the position of the particle on the ring is\n \n P1f, t2 = 0 c1f, t20\n2 = c*1f, t2c1f, t2\n \n =\n1\n2p\n 1c1e-im1fe+iE0m10t>U + c2e-iue-im2fe+iE0m20t>U21c1eim1fe-iE0m10t>U + c2eiueim2fe-iE0m20t>U2\n \n =\n1\n2p\n 3c2\n1 + c2\n2 + c1c21e-im1fe+iE 0\n  m10\n t>Ueiueim2fe-iE 0\n  m20\n t>U + eim1fe-iE 0\n  m10\n t>Ue-iue-im2fe+iE 0\n  m20\n t>U24 \n \n=\n1\n2p\n 31 + 2c1c2 cos 51m1 - m22f - u - 1E0m10 - E0m202t>U64. \n(7.115)\nThis probability density exhibits spatial dependence and time dependence in the form of a wave mov-\ning around the ring. There are four measurable properties of this probability density wave: the spatial \nfrequency, the temporal frequency, the amplitude, and the phase of the wave. These four quantities are \ndetermined by the factors 1m1 - m22, 1E0m10 - E0m202, c1c2, and u, respectively, in Eq. (7.115). Using \nthe measured values for these four quantities, the direction of the wave, and the normalization condi-\ntion c2\n1 + c2\n2 = 1 allows us to determine the ﬁve constants c1, c2, m1, m2, and u that specify the wave \nfunction superposition in Eq. (7.113) (Problem 7.17).\nExample 7.3 Calculate and plot the probability density for the initial superposition state\n \nc1f, 02 = 4\n1\n3 \u001331f2 + i 4\n2\n3 \u0013-11f2. \n(7.116)\nThe time-evolved wave function is\n \nc1f, t2 =\n1\n12p\n  4\n1\n3\n ei3f e-i9Ut>2I + i \n1\n12p\n  4\n2\n3\n e-if e-iUt>2I \n(7.117)\nand the probability density is\n \n P1f, t2 =\n1\n2p\n c 1 + 222\n3  cos a4f - p\n2 - 8U\n2I\n tb d  \n(7.118)\n \n =\n1\n2p\n c 1 + 222\n3   sin a4f - 4U\nI\n tb d .\n \n\n226 \nAngular Momentum\nThe probability density varies around the ring and at t = 0 is a maximum where  sin 4f = +1, or \nf = p>8, 5p>8, 9p>8, and 13p>8. The spatial dependence of the probability density is plotted in \nFig. 7.11 in three different graphical representations. The traditional plot in Fig. 7.11(a) is similar \nto the particle-in-a-box plots and conveys the idea of a varying density, but the single dimension \nfails to make it clear that the left and right ends are connected on the ring and must have the same \ndensity. The plot in Fig. 7.11(b) makes the connection between f = 0 and f = 2p clear by \nplotting the probability density using grayscale (color) as a parameter along the ring. The plot in \nFig. 7.11(c) combines the ideas of the previous two plots by using both the vertical scale and gray-\nscale to represent the probability density. Because the probability density varies with time, each of \nthe plots in Fig. 7.11 moves (toward increasing f in this example) when they are animated. (See the \nactivity on a particle conﬁned to a ring.)\nFIGURE 7.11 Probability density of a superposition state for a particle on a ring displayed as \n(a) a linear plot, (b) grayscale around the ring, and (c) height and grayscale around the ring.\n\n7.6 Motion on a Sphere \n227\nFIGURE 7.12 Particle conﬁned to move on the surface of a sphere.\nWe have now completed our investigation of the particle on a ring. We have identiﬁed the Hamil-\ntonian, found the energy spectrum, found the position representation of the eigenstates, and studied the \nprobability distributions, including the time dependence. These eigenstates are the same ones we will \nuse as the azimuthal part of the three-dimensional wave function to solve the hydrogen atom problem.\n7.6 \u0002 MOTION ON A SPHERE\nWe have now solved for the azimuthal part of the hydrogen atom wave function, so we turn our atten-\ntion to the polar angle part of the wave function. This is best done in the context of a system that \ninvolves both angular variables u and f, so that we ﬁnd the solutions \u00121u2 to Eq. (7.82) and then com-\nbine them with the azimuthal states \u0013m1f2 to form the solutions Y1u, f2 to the angular momentum \neigenvalue equation (7.80). The system we choose to discuss angular wave functions is that of a par-\nticle of mass m conﬁned to the surface of a sphere of radius r0, as shown in Fig. 7.12, which is a natural \nextension of the ring problem. The results of this analysis yield predictions that can be successfully \ncompared with experiments on molecules and nuclei that rotate more than they vibrate. For this reason, \nthe problem of a mass conﬁned to a sphere is often called the rigid rotor problem. Furthermore, the \nsolutions Y1u, f2 that we ﬁnd, called spherical harmonics, occur whenever one solves a partial dif-\nferential equation that involves spherical symmetry.\nFor a particle conﬁned to a sphere, the wave function c is independent of r, so derivatives with \nrespect to r are zero and the energy eigenvalue equation (7.43) reduces to\n \n-  U2\n2mr 2\n0\n c\n1\n sin u 0\n0 u\n asin u 0\n0 u b +\n1\nsin2 u 0 2\n0 f 2 d c + V1r02c = Esphere c, \n(7.119)\nwhich is the position representation of\n \nHsphere0 Esphere9 = Esphere0 Esphere9. \n(7.120)\n\n228 \nAngular Momentum\nFollowing our previous notation, we call the wave function Y1u, f2 = \u00121u2\u00131f2. For this simpliﬁed \nsphere problem, we choose the potential energy V1r02 to be zero, as in the ring problem. We identify \nmr 2\n0 = I as the moment of inertia of a classical particle of mass m moving on a sphere. With these \nchanges, the energy eigenvalue equation is\n \n-  U2\n2I\n c\n1\n sin u 0\n0 u\n asin u 0\n0 u b +\n1\nsin2 u 0 2\n0 f2 d Y1u, f2 = EsphereY1u, f2. \n(7.121)\nUsing Eq. (7.70), we identify the angular differential operator as the position representation of the \nangular momentum operator L2 and write the energy eigenvalue equation in operator form:\n \nL2\n2I\n Y1u, f2 = EsphereY1u, f2. \n(7.122)\nThis eigenvalue equation appears similar to the ring problem but is actually very different, because \nnow the particle can move anywhere on the sphere and so the angular momentum is no longer con-\nﬁned to the z-direction. Equation (7.122) is the same eigenvalue equation we obtained in Eq. (7.80) \nthrough separation of variables for the angular function Y1u, f2 = \u00121u2\u00131f2, as long as we identify \nthe separation constant A as\n \nA = 2I\nU2 Esphere. \n(7.123)\nAs noted above, we expect that the separation constant A is equal to /1/ + 12 because the L2 oper-\nator obeys the eigenvalue equation (7.57). Now that we know that this sphere problem is equiva-\nlent to the angular momentum eigenvalue equation, we proceed to solve for the polar angle function \n\u00121u2 that we identiﬁed in the differential equation (7.82). We have already solved for the azimuthal \nangle wave function \u0013m1f2, so at the end we combine \u00121u2 and \u0013m1f2 to yield the eigenstates \nY1u, f2 = \u00121u2\u0013m1f2 for the particle on the sphere. In due course, we’ll ﬁnd that the \u00121u2 eigen-\nstates have their own quantum numbers, and so we’ll label the polar angle states as \u0012m\n/ 1u2 and the \nspherical harmonics as Y m\n/ 1u, f2 (the m label is a superscipt, not an exponent).\n 7.6.1 \u0002 Series Solution of Legendre’s Equation\nThe polar angle equation (7.82) is our ﬁrst encounter with a differential equation that requires a \nsophisticated solution method. The next two sections detail the series solution method and arrive at \nthe Legendre and associated Legendre functions that solve the polar angle equation. If you are already \nexperienced with this method and are knowledgeable about the Legendre functions, you may safely \nskip these two sections.\nThe solutions \u0013m1f2 to the f equation (7.83) that we found in the ring problem told us the pos-\nsible values of the separation constant B = m2, where m is any integer. We now substitute these \nknown values into the polar angle differential equation (7.82). The u equation becomes an eigenvalue \nequation for the unknown function \u00121u2 and the separation constant A:\n \nc\n1\nsin u d\ndu\n asin u d\ndub -\nm2\nsin2 u d \u00121u2 = -A\u00121u2. \n(7.124)\nTo solve this differential equation, we start with a change of independent variable z = cos u, where \nz is the rectangular coordinate for the particle, assuming a unit sphere. We also introduce a new function\n \nP1z2 = \u00121u2. \n(7.125)\n\n7.6 Motion on a Sphere \n229\nThis step is not mathematically necessary but resolves the difference between the required normaliza-\ntion properties of quantum mechanial wave functions [\u0012(u)] and the standard normalization used for \nthe solutions [P(z)] to Eq. (7.124). As u ranges from 0 to p, z ranges from 1 to -1. Using the chain \nrule for derivatives and  sin u = 21 - z2, the differential term becomes\n \nd\ndu = dz\ndu\n  d\ndz = -sin u d\ndz = - 21 - z2 d\ndz . \n(7.126)\nNotice, particularly, the last equality: we are trying to change variables from u to z, so it is important to \nmake sure we change all the u’s to z’s. Multiplying by sin u, we obtain:\n \n sin u d\ndu = -11 - z22 d\ndz . \n(7.127)\nBe careful ﬁnding the second derivative; it involves a product rule:\n \n 1\nsin u d\ndu\n asin u d\ndub = d\ndz\n a11 - z22 d\ndzb\n \n \n = 11 - z22 d 2\ndz2 - 2z d\ndz\n .\n \n \n(7.128)\nInserting Eq. (7.128) into Eq. (7.124), we obtain a standard form of the associated Legendre \nequation:\n \na11 - z22 d 2\ndz2 - 2z d\ndz + A -\nm2\n11 - z22b P1z2 = 0. \n(7.129)\nOnce we solve this equation for the eigenfunctions P1z2, we substitute z = cos u everywhere to ﬁnd \nthe quantum mechanical eigenfunctions \u00121u2 of the original equation (7.124).\nIt is easiest to begin the solution of Eq. (7.129) with the m = 0 case, which corresponds to the \nsimplest possible f dependence: \u001301f2 = 1> 12p. Setting m = 0 in equation (7.129) gives us the \nspecial case known as Legendre’s equation:\n \n¢11 - z22 d 2\ndz2 - 2z d\ndz + A≤ P1z2 = 0. \n(7.130)\nBy dividing this equation by 11 - z22, we express it as\n \na d 2\ndz2 -\n2z\n11 - z22\n d\ndz +\nA\n11 - z22\nb P1z2 = 0, \n(7.131)\nwhich emphasizes the mathematical singularities at z = {1.\nWe use the series method to ﬁnd a solution of Legendre’s equation; that is, we assume that the \nsolution can be written as a series\n \nP1z2 = a\n\u0005\nn=0\na n zn \n(7.132)\n\n230 \nAngular Momentum\nand solve for the coefﬁcients an. The differentials\n \n dP\ndz = a\n\u0005\nn=0\na n nz n-1\n \n(7.133)\n \n d 2P\ndz2 = a\n\u0005\nn=0\na n n1n - 12z n-2 \n(7.134)\nsubstituted into Eq. (7.130) yield\n \n 0 = a\n\u0005\nn=0\na n n1n - 12z n-2 - z 2 a\n\u0005\nn=0\na n n1n - 12z n-2 - 2z a\n\u0005\nn=0\na n nz n-1 + Aa\n\u0005\nn=0\na n z n \n \n = a\n\u0005\nn=0\na n n1n - 12z n-2 - a\n\u0005\nn=0\na n n1n - 12z n - 2a\n\u0005\nn=0\na n nz n + Aa\n\u0005\nn=0\na n z n.\n \n(7.135)\nTo combine the sums, we must collect terms of the same powers. To do this, we note that the ﬁrst two \nterms of the ﬁrst sum are zero:\n \na01021-12z-2 + a1112102z-1 = 0 + 0, \n(7.136)\nso we shift the dummy variable n S n + 2 in the ﬁrst sum, giving\n \n a\n\u0005\nn=0\nan n1n - 12z n-2 =\na\n\u0005\nn=-2\nan+21n + 221n + 12z n \n \n = a\n\u0005\nn=0\nan+21n + 221n + 12z n.\n \n \n(7.137)\nNow all the sums in Eq. (7.135) have the same power and we group the sums together to yield\n \na\n\u0005\nn=0\n3an+21n + 221n + 12 - an n1n - 12 - 2an n + Aan4z n = 0. \n(7.138)\nNow comes the magic part. Because Eq. (7.138) is true for all values of z, the coefﬁcient of zn for \neach term in the sum must separately be zero:\n \nan+21n + 221n + 12 - an n1n - 12 - 2an n + Aan = 0. \n(7.139)\nTherefore, we can solve Eq. (7.139) for the recurrence relation, giving the later coefﬁcient an+2 in \nterms of the earlier coefﬁcient an:\n \nan+2 =\nn1n + 12 - A\n1n + 221n + 12\n an. \n(7.140)\nPlugging successive even values of n into the recurrence relation Eq. (7.140) allows us to ﬁnd a2, a4, \netc. in terms of the arbitrary constant a0, and successive odd values of n allow us to ﬁnd a3, a5, etc. in \nterms of the arbitrary constant a1. Thus, for the second-order differential equation (7.130), we obtain \ntwo solutions as expected. The coefﬁcient a0 becomes the normalization constant for a solution with \n\n7.6 Motion on a Sphere \n231\nonly even powers of z, and a1 becomes the normalization constant for a solution with only odd powers \nof z. For example, some even coefﬁcients are\n \n a2 = -  A\n2\n a0\n \n \n a4 = 6 - A\n12\n a2 = - a6 - A\n12\nba A\n2 b a0 \n \n(7.141)\nand some odd coefﬁcients are\n \n a3 = 2 - A\n6\n a1\n \n \n a5 = 12 - A\n20\n a3 = a 12 - A\n20\nba2 - A\n6\nb a1 \n(7.142)\nso that\n \nP1z2 = a0 c z0 - aA\n2 b z 2 + ...d + a1c z1 + a2 - A\n6\nb z 3 + ...d . \n(7.143)\nWe seek solutions that are normalizable, so we must address the convergence of the series solu-\ntion. Note that for large n, the recurrence relation gives\n \nan+2\nan\n\u0002 1, \n(7.144)\nwhich implies that the series solution we have assumed does not converge at the end points where \nz = {1. This is to be expected because the coefﬁcients of Eq. (7.131) are singular at z = {1, which \ncorrespond to the north and south poles u = 0, p. But there is nothing special about the physics at \nthese points, only the choice of coordinates is special here. This is an important example of a problem \nwhere the choice of coordinates for a partial differential equation ends up imposing boundary con-\nditions on the ordinary differential equation which comes from it. To ensure convergence, we thus \nrequire that the series not be inﬁnite, but rather that it terminate at some ﬁnite power nmax. Inspection \nof the recurrence relation in Eq. (7.140) tells us that the series terminates if we choose\n \nA = nmax1nmax + 12, \n(7.145)\nwhere nmax is a non-negative integer. When we started this problem, we expected the separation con-\nstant to be A = /1/ + 12 and we have found just that, as long as we identify the termination index \nnmax with the orbital angular momentum quantum number /. We have now succeeded in ﬁnding the \nquantization condition for orbital angular momentum, and it is just as we expected from our work \nwith spin angular momentum. But we came to it from a very different perspective, which is one of the \nbeautiful aspects of physics. We have now found that the orbital angular momentum quantum number \n/ must be a non-negative integer:\n \n/ = 0, 1, 2, 3, 4, ...  . \n(7.146)\nThe solutions to Eq. (7.130) for these special values of A are polynomials of degree /, denoted P/1z2, \nand are called Legendre polynomials.\nThe Legendre polynomials can also be calculated using Rodrigues’ formula:\n \nP/1z2 =\n1\n2//! d /\ndz/ 1z 2 - 12/. \n(7.147)\n\n232 \nAngular Momentum\nThe ﬁrst few Legendre polynomials are shown in Table 7.1 and are plotted in Fig. 7.13. There are \nseveral useful patterns to the Legendre polynomials:\n• The overall coefﬁcient for each solution is conventionally chosen so that P/112 = 1. As \ndiscussed in the next section, this is an inconvenient convention that we are stuck with.\n• P/1z2 is a polynomial of degree /.\n• Each P/1z2 contains only odd or only even powers of z, depending on whether / is even \nor odd. Therefore, each P/1z2 is either an even or an odd function.\n• Because the differential operator in Eq. (7.130) is Hermitian, we are guaranteed that \nthe Legendre polynomials are orthogonal for different values of / ( just as with Fourier \nseries), that is,\n \nL\n1\n-1\nP*\nk 1z2P/1z2dz =\n2\n2/ + 1 dk/. \n(7.148)\nNote that the Legendre polynomials are not normalized to unity, rather the “squared norm” of P/ is \n2>12/ + 12.\nNotice that when we substitute the separation constant A = /1/ + 12 back into the original dif-\nferential equation (7.130)\n \n11 - z 22 d 2P\ndz 2 - 2z dP\ndz + /1/ + 12P = 0, \n(7.149)\nTable 7.1 Legendre Polynomials\nP01z2 = 1\nP11z2 = z\nP21z2 = 1\n2 13z  2 - 12\nP31z2 = 1\n2 15z  3 - 3z2\nP41z2 = 1\n8 135z  4 - 30z  2 + 32\nP51z2 = 1\n8 163z  5 - 70z  3 + 15z2\n\n1\n1\nz\n\n1\n\n0.5\n0.5\nPl(z)\n1\nl\u00040\nl\u00041\nl\u00042\nl\u00043\nl\u00044\nFIGURE 7.13 Legendre polynomials.\n\n7.6 Motion on a Sphere \n233\nthe result is a different equation for different values of /. For a given value of /, you should expect \ntwo solutions of Eq. (7.149), but we have only given one. The “other” solution for each value of / is \nnot regular (i.e., it blows up) at z = {1. In cases where the separation constant A does not have the \nspecial value /1/ + 12 for non-negative integer values of /, it turns out that both solutions blow up. \nWe discard these irregular solutions as unphysical for the problem we are solving.\n 7.6.2 \u0002 Associated Legendre Functions\nWe now return to Eq. (7.129) to consider the cases with m \u0002 0. We need a slightly more sophisticated \nversion of the series technique from the m = 0 case, and we do not detail this here. We again ﬁnd solu-\ntions that are regular at z = {1 whenever we choose A = /1/ + 12 for / \u0002 50, 1, 2, 3, ...6. With \nthese values for A, we obtain the standard form of the associated Legendre equation, namely\n \na11 - z 22 d 2\ndz 2 - 2z d\ndz + /1/ + 12 -\nm2\n11 - z 22b P1z2 = 0. \n(7.150)\nSolutions of this equation that are regular at z = {1 are called associated Legendre functions, and \nare calculated from the Legendre functions by differentiation:\n \n P m\n/ 1z2 = P -m\n/ 1z2 = 11 - z 22\nm>2\n d m\ndz m P/1z2  \n \n =\n1\n2//!\n 11 - z 22\nm>2\n d m+/\ndz m+/ 1z 2 - 12/,\n \n \n(7.151)\nwhere m Ú 0. In Eq. (7.151), the integer m is a superscript label—not an exponent—on the associated \nLegendre function Pm\n/ 1z2, but m is an exponent on the right hand side of the equation. The associated \nLegendre equation (7.150) is independent of the sign of the integer m, so\n \nP-m\n/ 1z2 = Pm\n/ 1z2. \n(7.152)\nThe Legendre function P/1z2 is a polynomial of order /, so the mth derivative in Eq. (7.151), and hence \nthe associated Legendre function Pm\n/ 1z2, vanishes if m 7 /. In the ring problem, we learned that m \nmust be an integer, but there was no limit on the possible values of those integers. Now we have dis-\ncovered an additional constraint on the magnetic quantum number for the sphere problem\n \nm = -/, -/ + 1, ..., -1, 0, 1, ..., / - 1, /  . \n(7.153)\nAgain, this is consistent with our expectations from the spin problem.\nIt is more useful for us to express the Legendre polynomials and the associated Legendre functions \nin terms of the polar angle u rather than the variable z, so we substitute z = cos u into the functions. \nThe Legendre polynomial P/1cos u2 is a polynomial in cos u, while the associated Legendre function \nP m\n/ 1cos u2 is a polynomial in cos u times a factor of sinm u because of the additional term\n \n11 - z 22\nm>2 = 1sin2 u2\nm>2 =  sinm u \n(7.154)\n\n234 \nAngular Momentum\nin Eq. (7.151). Some of the associated Legendre functions are shown in Table 7.2 and are plotted in \nFig. 7.14. The plots in Fig. 7.14 are polar plots where the “radius” r at each angle u is the absolute \nvalue of the function P m\n/ 1cos u2, as illustrated further in Fig. 7.15. The associated Legendre functions \nare deﬁned over the interval 0 … u … p, but the convention is to plot the functions reﬂected in the \nz-axis in anticipation of their application to the full three-dimensional hydrogen atom.\nSome useful properties of the associated Legendre functions are:\n• P m\n/ 1z2 = 0 if 0m0 7 /\n• P -m\n/ 1z2 = P m\n/ 1z2\n• P m\n/ 1{12 = 0 for m \u0002 0 Acf. factor of 11 - z 22\nm>2B\n• P m\n/ 1-z2 = 1-12\n/-m P m\n/ 1z2 (behavior under parity)\n• \nL\n1\n-1\nP m\n/ 1z2P m\nq1z2dz =\n2\n12/ + 12 1/ + m2!\n1/ - m2! d/q.\nP0\n0\nP1\n0\nP1\n1\nP2\n0\nP2\n1\nP2\n2\nP3\n0\nP3\n1\nP3\n2\nP3\n3\nFIGURE 7.14 Polar plots of associated Legendre functions.\n\n7.6 Motion on a Sphere \n235\n\n1\n\n0.5\n0.5\n1\n\n0.5\n0.5\nz\nr \u0004 Pl\nm(Θ)\u0002\nΘ\nP1\n1 (Θ)\n\u0002\nFIGURE 7.15 Polar plot of an associated Legendre function.\nTable 7.2 Associated Legendre Functions\nP 0\n0 = 1 \nP 0\n1 = cos u \nP 0\n3 = 1\n2 15 cos3 u - 3 cos u2\nP 1\n1 = sin u \nP 1\n3 = 3\n2 sin u15 cos2 u - 12\nP 0\n2 = 1\n2 13 cos2 u - 12 \nP 2\n3 = 15 sin2 u cos u\nP 1\n2 = 3 sin u cos u \nP 3\n3 = 15 sin3 u\nP 2\n2 = 3 sin2 u\nThe last property shows that for each given value of m, the associated Legendre functions form an \northogonal basis on the interval -1 … z … 1. Any function on this interval can be expanded in terms \nof any one of these bases. The associated Legendre functions are not normalized to unity, but by multi-\nplying by the appropriate factor we construct the eigenstates \u0012 m\n/ 1u2 that solve the eigenvalue equation \n(7.124) and are normalized to unity over the interval 0 … u … p:\n \nL\np\n0\n\u0012 m\n/ 1u2\u0012 m\nq1u2sin u du = d/q. \n(7.155)\nThese eigenstates are\n \n\u0012m\n/ 1u2 = 1-12\nm\n 12/ + 12\n2\n 1/ - m2!\n1/ + m2!\n P m\n/ 1cos u2, m Ú 0, \n(7.156)\nwith the negative m states deﬁned by\n \n\u0012 -m\n/ 1u2 = 1-12\nm \u0012 m\n/ 1u2, m Ú 0. \n(7.157)\n\n236 \nAngular Momentum\n 7.6.3 \u0002 Energy Eigenvalues of a Rigid Rotor\nWe now know the separation constant A in Eq. (7.124), which determines the energy of the parti-\ncle bound to the sphere through Eq. (7.123). Substituting A = /1/ + 12 into Eq. (7.123) gives the \nallowed energy eigenvalues\n \nE/ = U2\n2I\n  /1/ + 12. \n(7.158)\nThe energy is independent of the magnetic quantum number m, so each energy level is degenerate, \nwith 12/ + 12 possible m states for a given /. The free particle and the particle on a ring both exhib-\nited degeneracy because the kinetic energy was independent of the direction of the motion. Similarly, \nthe rotational kinetic energy of the particle on a sphere is independent of the orientation of the angular \nmomentum. The spectrum of energy levels is shown in Fig. 7.16. The selection rule for transitions \nbetween these levels is \u0006/ = {1, yielding the emission lines in Fig. 7.16. The transition energies are\n \n \u0006E = E/+1 - E/\n \n \n = U2\n2I\n  1/ + 121/ + 22 - U2\n2I\n /1/ + 12 \n \n = U2\n2I\n  21/ + 12\n \n \n(7.159)\n \n = U2\n2I\n  52, 4, 6, 8, 10, ...6.\n \n0\n1\n2\n3\n4\n5\nEnergy (\u00022/2l\u0007)\nE (\u00022/2l\u0007)\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n22\n24\n26\n28\n30\n0\n2\n4\n6\n8\n10\nl\nFIGURE 7.16 Energy spectrum and transitions of a rigid rotor.\n\n7.6 Motion on a Sphere \n237\nz\nm1\nm2\nr1\n\u0002\nr2\n\u0002\nL\u0002\nFIGURE 7.17 A diatomic molecule is the simplest example of a rigid rotor. The two-atom \nsystem rotates around an axis perpendicular to the symmetry axis of the molecule.\nA physical example of this particle-on-a-sphere model is the rigid rotor. The simplest rigid rotor is \na diatomic molecule, as illustrated in Fig. 7.17. The two atoms with a separation r0 have a moment \nof inertia about the center of mass of I = mr2\n0, just as we have assumed in our particle-on-a-sphere \nmodel. Molecular spectroscopists call the energy U2>2I the rotational constant of the molecule.\nFor example, consider the diatomic molecule hydrogen chloride HCl. The equilibrium bond \nlength is r0 = 0.127 nm, which gives a rotational constant\n \nU2\n2I `\nHCl\n= 1.32 meV = 10.7 cm-1. \n(7.160)\nThe experimentally measured value is 10.4 cm-1. That seems close, but is in fact a clue that something \nis missing from the model. It turns out that the coupling of the vibrational motion (Chapter 9) to the \nrotational motion changes the energy levels of a real molecule. Reﬁning simple models leads to better \nunderstanding; our job here is to gain basic understanding.\n 7.6.4 \u0002 Spherical Harmonics\nWe have in hand the eigenfunctions of the two angular equations, so we can construct the energy \neigenstates of the particle on the sphere. The normalized solutions of the f equation (7.83) that satisfy \nperiodic boundary conditions are the \u0013m1f2 states in Eq. (7.100) with the restriction that the magnetic \nquantum number m be an integer. The normalized solutions of the u equation (7.82) that are regular at \nthe poles are the \u0012 m\n/ 1u2 states in Eq. (7.156) with the restriction that / = 0, 1, 2, ... and m = -/, ..., / \nin integer steps. The product \u0012 m\n/ 1u2\u0013m1f2 of the two solutions yields the function Y m\n/ 1u, f2 that we \nassumed when we applied the separation of variables procedure to the angular equation (7.80). These \nangular functions are the spherical harmonics\n \nY m\n/ 1u, f2 = 1-12\n1m+ 0m02>2\nC\n12/ + 12\n4p\n \n1/ - 0 m02!\n1/ + 0 m02! P m\n/ 1cos u2eimf, \n(7.161)\nthe ﬁrst few of which are listed in Table 7.3. The somewhat peculiar choice of sign is conventional and \ngives the useful result\n \nY -m\n/ 1u, f2 = 1-12\nm Y m*\n/ 1u, f2. \n(7.162)\n\n238 \nAngular Momentum\nLet’s now discuss the important properties of the spherical harmonics.\n• Orthonormality\nThe spherical harmonics are orthonormal on the unit sphere\n \n8/1m10 /2m29 =\n \nL\n2p\n0\nL\np\n0\nY m1*\n/1 1u, f2 Y m2\n/2 1u, f2 sin u du df = d/1/2dm1m2  , \n(7.163)\nwhich means that two wave functions must have the same angular momentum (/1 = /2) and the \nsame z-component (m1 = m2) or else the overlap integral is zero. The / orthogonality comes from the \nassociated Legendre u functions and the m orthogonality comes from the complex exponential f func-\ntions. The orthonormality condition is also written compactly as an integral over the full solid angle\n \nL\nY m1*\n/1 1u, f2Y m2\n/2 1u, f2d\t = d/1/2dm1m2 \n(7.164)\nfor those common occasions when there is no need to consider separate angular integrals.\n• Completeness\nThe spherical harmonics are complete in the sense that any sufﬁciently smooth function c1u, f2 \non the unit sphere can be expanded in a Laplace series as\n \nc1u, f2 = a\n\u0005\n/=0\n a\n/\nm=-/\nc/mY m\n/ 1u, f2. \n(7.165)\nThe c/m expansion coefﬁcients are found by projecting the superposition wave function onto the \n0 /m9 eigenstates:\n \nc/m = 8/m0 c9 =\n \nL\n2p\n0\nL\np\n0\nY m*\n/ 1u, f2c1u, f2sin u du df. \n(7.166)\nTable 7.3 Spherical Harmonics\n/ \nm \nY  m\n/ 1u, f2\n0 \n0 \nY  0\n0 = 4\n1\n4p\n1 \n0 \nY  0\n1 = 4\n3\n4p cos u\n \n{1 \nY {1\n1\n= <4\n3\n8p sin ue{if\n2 \n0 \nY  0\n2 = 4\n5\n16p 13 cos2 u - 12\n \n{1 \nY {1\n2\n= <4\n15\n8p sin u cos ue{if\n \n{2 \nY {2\n2\n= 4\n15\n32p sin2 ue{i2f\n3 \n0 \nY  0\n3 = 4\n7\n16p 15 cos3 u - 3 cos u2\n \n{1 \nY {1\n3\n= <4\n21\n64p sin u15 cos 2 u - 12e{i f\n \n{2 \nY {2\n3\n= 4\n105\n32p sin2 u cos ue{i2f\n \n{3 \nY {3\n3\n= 4\n35\n64p sin3 ue{i3f\n\n7.6 Motion on a Sphere \n239\n• Parity\nThe behavior of the spherical harmonics under the parity operation r S -r is determined by the \nangular momentum quantum number /. Spherical harmonics with even / have even parity and \nthose with odd / have odd parity:\n \nY m\n/ 1p - u, f + p2 = 1-12\n/\n Y m\n/ 1u, f2. \n(7.167)\nTo summarize, we have found that the spherical harmonics Y m\n/ 1u, f2 are eigenstates of the Ham-\niltonian for the particle on a sphere [Eq. (7.121)]. Because the Hamiltonian for this problem is pro-\nportional to the L2 orbital angular momentum operator [Eq. (7.122)], the spherical harmonics are also \neigenstates of L2 [Eq. (7.80)]. The spherical harmonics contain the \u0013m1f2 eigenstates, so they are also \neigenstates of the Lz operator (Problem 7.24). These three eigenvalue equations are\n \nHsphereY m\n/ 1u, f2 = U2\n2I\n /1/ + 12Y m\n/ 1u, f2\n \nL 2Y m\n/ 1u, f2 = /1/ + 12U2Y m\n/ 1u, f2 \n.\n \n(7.168)\n \nLzY m\n/ 1u, f2 = m UY m\n/ 1u, f2\nThese three operators share eigenstates because they commute with each other (Problem 7.9).\nFor a particle on a sphere, the measurement probabilities are complicated by the degeneracy, \njust as we saw in the particle on a ring [Eq. (7.105)]. For a state 0 c9, the probability of measuring the \nenergy E/ is a sum over all the degenerate states:\n \nPE/ =\na\n/\nm=-/\n08/m0 c90\n2. \n(7.169)\nThe probability of measuring the L2 angular momentum observable to be /1/ + 12U2 is also given by \nEq. (7.169) because the energy eigenstates and the L2 eigenstates exhibit the same degeneracy. The \nprobability of measuring the Lz angular momentum observable to be m U is the sum over all the / states \nfor which that value of m is allowed:\n \nPLz=m U = a\n\u0005\n/=m\n08/m0 c90\n2. \n(7.170)\nLet’s practice using the spherical harmonics.\nExample 7.4 A particle on a sphere is in the state\n \nc1u, f2 = 4\n15\n16p sin 2 u cos f. \n(7.171)\nWhat are the probabilities of energy (H) and angular momentum AL2 and LzB measurements?\nThis wave function looks almost like a spherical harmonic eigenstate, so we try to do this \nproblem by inspection. Using trigonometric identities, rewrite the wave function as\n \n c1u, f2 = 4\n15\n16p 12 sin u cos u2aeif + e-if\n2\nb\n \n \n = 4\n15\n16p sin u cos ueif + 4\n15\n16p sin u cos ue-if\n \n(7.172)\n \n = -  1\n12 A-4\n15\n8p sin u cos ueifB +\n1\n12 A4\n15\n8p sin u cos ue-ifB, \n\n240 \nAngular Momentum\nwhich we recognize from Table 7.3 of spherical harmonics as the superposition\n \nc1u, f2 = -  1\n12 Y 1\n11u, f2 +\n1\n12 Y \n -1\n1 1u, f2. \n(7.173)\nIn Dirac notation, this state is\n \n0 c9 = -  1\n12 0 119 +\n1\n12 0 1, -19. \n(7.174)\nSo, without doing any integrals, we obtain the expansion coefﬁcients\n \nc/m = 8/m0 c9 = - 1\n12 d/1dm1 +\n1\n12 d/1dm,-1 \n(7.175)\nand the energy measurement probabilities\n \n PE/ =\na\n/\nm=-/\n08/m0 c90\n2\n \n \n = A-  1\n12 d/1B\n2\n+ A 1\n12 d/1B\n2\n \n \n(7.176)\n \n = d/1.\n \nThe probability of measuring the energy to be E1 = U2>I is 100%, as is the probability for measur-\ning L2 = 2U2.\nThe probability of measuring Lz = U is\n \n PL z=\n U = a\n\u0005\n/=m\n08/10 c90\n2 \n \n = A-  1\n12B\n2\n \n \n(7.177)\n \n = 1\n2.\n \nSimilarly PL z=-U = 1>2.\nSolution by inspection is nice when it works, but sometimes we must bite the bullet and integrate, \nas we’ll see in the example in the next section.\n 7.6.5 \u0002 Visualization of Spherical Harmonics\nVisualization of spherical harmonics is a challenge because of the two-dimensional structure of the \nwave functions and the fact that they are represented by complex numbers. To overcome the com-\nplex problem, it is common to plot the complex square, which is the probability density, or to plot \nthe absolute value. In either case, the azimuthal dependence vanishes as we saw with the ring prob-\nlem earlier. A two-dimensional polar plot, like we used for the Legendre polynomials, is therefore \nsufﬁcient to display the polar angle dependence, as shown in Fig. 7.18(a). To convey the uniform \nazimuthal dependence, one should visualize the polar plot as rotated around the vertical z-axis, as \ndisplayed in the three-dimensional polar plot in Fig. 7.18(b). In this plot, the “radius” at each angle \nu, f is the complex square of the spherical harmonic function. In the ring case, we also displayed the \nprobability density as a grayscale on the ring itself, which suggests plotting the spherical harmonic \n\n7.6 Motion on a Sphere \n241\nFIGURE 7.18 Spherical harmonic 0 Y 1\n31u, f20\n2 displayed as (a) a two-dimensional polar plot, \n(b) a three-dimensional polar plot, (c) grayscale on a sphere, (d) grayscale on a ﬂat rectangular  \nprojection, and (e) grayscale on a ﬂat Mollweide projection.\nprobability density as grayscale (or color) on the sphere, as shown in Fig. 7.18(c). The grayscale \nsphere can then also be projected onto a ﬂat surface, as mapmakers do, yielding the two-dimensional \nrepresentations in Figs. 7.18(d) and (e). Note that these plots do not yet give the three-dimensional \nelectron probability density because the  spherical harmonics are not functions of the radius r. We \nstill have to learn about the radial wave function in the next chapter.\nThe three-dimensional polar plots for the ﬁrst four sets of spherical harmonics are shown in \nFig. 7.19. The standard convention is to label the spherical harmonics, or orbitals, with a letter \n corresponding to the value of the orbital angular momentum quantum number /:\n \n / = 0   1   2   3   4   5   6   7  ...  \n \n(7.178)\n \n letter = s   p   d    f    g    h    i    k  ... . \nThe plots in Fig. 7.19 show angular momentum eigenstate wave functions. In many cases, such as the \ncarbon atom in Fig. 7.5, the actual orbitals are superpositions, or hybrids, of the angular momentum \neigenstates.\n\n242 \nAngular Momentum\nExample 7.5 Given the angular wave function for a particle on a sphere\n \nc1u, f2 = 4\n60060\n139301p a 1\n4 + cos3 2 u + sin2 fb, \n(7.179)\ngenerate the histogram of possible energy measurements.\nTo ﬁnd the probabilities of energy measurements\n \nPE/ =\na\n/\nm=-/\n08/m0 c90\n2, \n(7.180)\nFIGURE 7.19 Three-dimensional polar plots of some spherical harmonics.\n\n7.6 Motion on a Sphere \n243\nwe must ﬁnd the overlap integrals\n \nc/m = 8/m0 c9 =\n \nL\n2p\n0\nL\np\n0\nY m*\n/ 1u, f2c1u, f2sin u d u d f. \n(7.181)\nThis wave function looks like it could be a ﬁnite sum of spherical harmonics, but the wild nor-\nmalization constant is a clue that an inﬁnite sum is required. You could try to calculate the c/m \n coefﬁcients by hand, but this problem is a good chance to explore the power of mathematical pack-\nages such as Mathematica, Maple, or Matlab. Mathematica, for example, has the spherical harmon-\nics built into its system and the overlap integral requires one command line\n \nTable3Integrate3Conjugate3SphericalHarmonicY3l,m,u,f44 \n \nc3u,f4Sin3u4,5u,0,p6,5f,0,2p64,5l,0,76,5m,-l,l64, \n(7.182)\nwhich generates a table of the c/m coefﬁcients for / = 0 S 7 and m = -/ S /, assuming c1u, f2 \nhas been deﬁned previously. A subset of the results is presented in Table 7.4. The last column of \nthe table is the probability of measuring the energy E/. From the explicit square roots in the results, \nit is evident that Mathematica does the integral analytically, not numerically. The results also indi-\ncate the symmetries of the wave function. Only m = -2, 0, 2 states contribute nonzero terms to \nthe expansion because of the symmetry of the azimuthal dependence of the wave function:\n \n  sin2 f = 31eif - e-if2>2i4\n2\n \n \n = 1\n4 1ei 2f + e-i 2f - 22. \n \n(7.183)\nFor m = 0, the coefﬁcients beyond / = 6 are zero because the polar angle term cos3 2 u has no \ncos/ u or  sin/ u terms beyond / = 6. The m = {2 coefﬁcients extend to / = \u0005.\nTable 7.4 Coefﬁcients of Spherical Harmonic Expansion\nc/m\nm =\na\n/\nm=-/\n0 c/m0\n2\n-3\n-2\n-1\n0\n1\n2\n3\n0\n1\n2\n3\nO = 4\n5\n6\n7\n0\n0\n0\n0\n0\n-5 4\n1001\n278602\n0\n4\n3003\n278602\n0\n-  13\n2  4\n11\n139301\n0\n0\n0\n0\n0\n0\n0\n0\n69 4\n429\n4875535\n0\n80 4\n143\n2925321\n0\n-128 4\n39\n53630885\n0\n512 4\n5\n32178531\n0\n0\n0\n0\n0\n0\n0\n0\n5 4\n1001\n278602\n0\n4\n3003\n278602\n0\n 13\n2  4\n11\n139301\n0\n0\n0\n0\n0\n0\n2042469\n4875535\n0\n1440725\n2925321\n0\n1795131\n5360885\n0\n3050869\n64357062\n0\n\n244 \nAngular Momentum\nA partial histogram of energy measurement probabilities is shown in Fig. 7.20. The energy \nprobabilities for the states up to / = 6 shown in Table 7.4 and Fig. 7.20 sum to 0.9923, so we \nexpect that the ﬁnite spherical harmonic expansion\n \ncfinite1u, f2 = a\n6\n/=0\n a\n/\nm=-/\nc/mY m\n/ 1u, f2 \n(7.184)\nshould be a good approximation to the actual wave function. The original wave function and the \nﬁnite spherical harmonic expansion are shown in Fig. 7.21. The match between the two is good, \nexcept at the endpoints u = 0,p, which is a phenomenon similar to that seen in Fourier series \nexpansions. Note that this wave function exhibits azimuthal dependence because it is a superposi-\ntion of different m states.\nFIGURE 7.21 (a) Original wave function and (b) 6-term spherical harmonic expansion.\nE0 E1 E2\nE3\nE4\nE5\nE6\nE\n0.5\nPEi\n\u0002\u0004E2\u0002Ψ\u0003\u00022\n\u0002\u0004E4\u0002Ψ\u0003\u00022\n\u0002\u0004E6\u0002Ψ\u0003\u00022\n\u0002\u0004E0\u0002Ψ\u0003\u00022\nFIGURE 7.20 Histogram of energy measurements.\n\nProblems \n245\nSUMMARY\nIn this chapter, we introduced the idea of orbital angular momentum and illustrated its importance in \nsolving the three-dimensional differential equation that is the energy eigenvalue equation for the hydro-\ngen atom. By separating variables in the eigenvalue equation H0 E9 = E0 E9, we isolated the differential \nequations for the angular variables u and f from the differential equation for the radial variable r. Only \nthe radial differential equation includes the potential energy, so the solutions to the angular equations are \nvalid for all central potentials. The f equation yielded the azimuthal wave functions\n \n\u0013m1f2 =\n1\n22p\n eimf \n(7.185)\nand the u equation yielded the polar wave functions\n \n\u0012 m\n/ 1u2 = C\n12/ + 12\n2\n \n1/ - 0 m02!\n1/ + 0 m02! P m\n/ 1cos u2. \n(7.186)\nThe products of these two are the total angular wave functions, which are the spherical harmonics\n \n0 /m9 \u0003 Y m\n/ 1u, f2 = 1-121m+ 0  m 02>2\nC\n12/ + 12\n4p\n \n1/ - 0 m02!\n1/ + 0 m02! P m\n/ 1cos u2eimf. \n(7.187)\nThe spherical harmonics are eigenstates of the angular momentum operators L2 and Lz. In Dirac nota-\ntion, the eigenvalue equations are\n \n L 20 /m9 = /1/ + 12U20 /m9 \n \n Lz0 /m9 = m U0 /m9.\n \n(7.188)\nIn wave function notation, the eigenvalue equations are\n \n L 2Y m\n/ 1u, f2 = /1/ + 12U2Y m\n/ 1u, f2 \n \n LzY m\n/ 1u, f2 = m UY m\n/ 1u, f2.\n \n \n(7.189)\nThe limitations on the quantum numbers m and / arise from requiring the wave function to be periodic \nin f and ﬁnite at u = 0, p, respectively. The quantum numbers m and / must be integers with the \nlimitations\n \n m = -/, -/ + 1, ... 0, ..., / - 1, / \n \n / = 0, 1, 2, 3, ...\u0005.\n \n(7.190)\nPROBLEMS\n 7.1 Show that the two-body Hamiltonian in Eq. (7.3) can be separated into center-of-mass and \nrelative Hamiltonians, as in Eq. (7.11). Do this in two ways: (a) with momentum operators in \nthe abstract, and (b) momentum operators in the position representation.\n 7.2 Use the separation of variables procedure in Appendix E to separate the two-body energy eigen-\nvalue equation into the center-of-mass and relative energy eigenvalue equations in Eq. (7.24).\n 7.3 Use the separation of variables procedure in Appendix E to separate equation Eq. (7.29) into \nthree ordinary differential equations for each Cartesian coordinate.\n\n246 \nAngular Momentum\n 7.4 Verify the angular momentum commutation relations in Eqs. (7.51) and (7.55).\n 7.5 An angular momentum system with / = 1 is prepared in the state\n0 c9 =\n2\n129 0 119 + i 3\n129 0 109 -\n4\n129 0 1, -19.\na) What are the possible results of a measurement of the angular momentum component Lz, \nand with what probabilities would they occur?\nb) What are the possible results of a measurement of the angular momentum component Lx, \nand with what probabilities would they occur?\nc) Plot histograms of the predicted measurement results from parts (a) and (b).\n 7.6 An angular momentum system with / = 1 is prepared in the state\n0 c9 =\n1\n114 0 119 -\n3\n114 0 109 + i 2\n114 0 1, -19.\na) What are the possible results of a measurement of the angular momentum component Lz, \nand with what probabilities would they occur?\nb) Suppose that the Lz measurement on the system yields the result Lz = -U. Subsequent to \nthat result, a second measurement is performed to measure the angular momentum com-\nponent Lx. What are the possible results of that measurement, and with what probabilities \nwould they occur?\nc) Draw a schematic diagram depicting the successive measurements in parts (a) and (b).\n 7.7 An angular momentum system is prepared in the state\n0 c9 =\n1\n110 0 119 -\n2\n110 0 109 + i 2\n110 0 229 + i 1\n110 0 209.\na) What are the possible results of a measurement of the angular momentum observable L2, \nand with what probabilities would they occur?\nb) What are the possible results of a measurement of the angular momentum component Lz, \nand with what probabilities would they occur?\nc) Plot histograms of the predicted measurement results from parts (a) and (b).\n 7.8 Using Eqs. (7.35) and (7.47), show that the orbital angular momentum operators Lx, Ly, and Lz \nare represented in spherical coordinates as\n \n Lx \u0003 i U asin f 0\n0 u + cos f cot u 0\n0 fb\n \n \n Ly \u0003 i U a-cos f 0\n0 u + sin f cot u 0\n0 fb \n \n Lz \u0003 -i U 0\n0 f\n \n \n and verify that the operator L 2 = L~L = L 2\nx + L 2\ny + L 2\nz is represented in spherical coordi-\nnates as in Eq. (7.70).\n 7.9 Verify that the angular momentum operators L 2 and Lz commute with the central force \nHamiltonian.\n 7.10 Use the separation of variables procedure in Appendix E on the angular equation (7.80) to obtain \nEq. (7.82) and Eq. (7.83) for the polar and azimuthal angles.\n\nProblems \n247\n 7.11 Show by direct integration that the azimuthal eigenstates \u0013m1f2 are orthonormal.\n 7.12 Consider the particle-on-a-ring state in Example 7.2. What are the possible values of a mea-\nsurement of the observable Lz? Calculate the measurement probabilities and show that they \nagree with the results indicated in Fig. 7.9.\n 7.13 Consider the normalized state 0 c9 for a quantum mechanical particle of mass m constrained to \nmove on a circle of radius r0, given by:\n0 c9 = 23\n2 0 39 + i\n2 0 -29.\na) What is the probability that a measurement of Lz will yield 2U? 3U?\nb) What is the probability that a measurement of the energy yields E = 2U2>I?\nc) What is the expectation value of Lz in this state?\nd) What is the expectation value of the energy in this state?\n 7.14 A particle on a ring is in the normalized state\n0 c9 =\n1\n115\n 1 0 09 + i0 19 - 2i0 29 + 30 -292.\na) What are the possible results of an energy measurement and what are the corresponding \nprobabilities? Calculate the expectation value of the energy.\nb) What are the possible results of an Lz measurement and what are the corresponding prob-\nabilities? Calculate the expectation value of Lz.\n 7.15 Consider the normalized state 0 c9 for a quantum mechanical particle of mass m constrained to \nmove on a circle of radius r0, given by\n0 c9 \u0003\nN\n2 + cos 13f2 ,\n \n where N is the normalization constant.\na) Find the normalization constant N.\nb) Plot the wave function.\nc) What is the expectation value of Lz in this state?\n 7.16 A particle on a ring is prepared in the initial state\n0 c9 = 4\n1\n5 0 29 - i  4\n4\n5\n 0 -19.\n \n Find the probability density as a function of time.\n 7.17 The time-dependent probability density for a particle on a ring is measured to be\nP1f, t2 =\n1\n2p\n c 1 - 22223\n13\n sin a3f + 3U\n2I\n tb d .\n \n Determine the initial state of the particle.\n 7.18 Calculate the moment of inertia of a diatomic molecule, as depicted in Fig. 7.17. Express the \nmoment two ways: (1) in terms of the individual masses m1 and m2 and the coordinates r1 and r2, \nand (2) in terms of the reduced mass m and the atom-atom separation r0.\n 7.19 Calculate the rotational constant for the hydrogen iodide (HI) molecule.\n\n248 \nAngular Momentum\n 7.20 In each of the following sums, shift the dummy index n S n + 2. Don’t forget to shift the lim-\nits of the sum as well. Then write out all of the terms in the sum (if the sum has a ﬁnite number \nof terms) or the ﬁrst ﬁve terms in the sum (if the sum has an inﬁnite number of terms) and con-\nvince yourself that the two different expressions for each sum are the same:\na) a\n3\nn=0\nn\nb) a\n5\nn=1\neinf\nc) a\n\u0005\nn=0\nan n1n - 12zn-2\n 7.21 Use Rodrigues’ formula, by hand, to generate the ﬁrst ﬁve Legendre polynomials. Show by \ndirect integration that P21cos u2 is orthogonal to P41cos u2, and that P21cos u2 is normalized \naccording to Eq. (7.148).\n 7.22 Generate the associated Legendre functions P 1\n21z2 and P 3\n31z2 by hand. Express each function \nboth as a function of the argument z and as a function of u.\n 7.23 Use the deﬁnitions in Eqs. (7.151) and (7.161) to generate the spherical harmonics Y 0\n11u, f2 \nand Y \n -2\n2 1u, f2. Ensure that they are normalized and orthogonal by direct integration.\n 7.24 Verify that the spherical harmonics are eigenstates of the orbital angular momentum component \noperator Lz by direct application of the position representation of Lz. What are the eigenvalues?\n 7.25 Verify that the spherical harmonics are eigenstates of the orbital angular momentum operator \nL2. What are the eigenvalues?\n 7.26 Consider the new operators L+ and L- deﬁned by L{ = Lx { iLy. Use the results of Problem 7.8 \nto show that the position representations of these operators in spherical coordinates are\nL{ = U e{i f a{ 0\n0 u + i cot u 0\n0 f b.\n \n Act with these new operators on all the / = 1 spherical harmonic wave functions and sum-\nmarize your results in Dirac notation. Based on your results, postulate the names of these new \noperators. This is a preview of Chapter 11.\n 7.27 Express the / = 1 spherical harmonics in Cartesian coordinates. Combine the m = {1 func-\ntions in two possible ways to make real functions that closely resemble the m = 0 function.\n 7.28 Use your favorite tool (e.g., Maple, Mathematica, Matlab, pencil) to generate the Legendre \npolynomial expansion of the function f 1z2 = sin 1pz2. How many terms do you need to \ninclude in a partial sum to get a “good” approximation to f 1z2 for -1 6 z 6 1? What do you \nmean by a “good” approximation? How about the interval -2 6 z 6 2? How good is your \napproximation then? Discuss your answers. Answer the same set of questions for the function \ng1z2 = sin 13pz2.\n 7.29 Consider the normalized state of a particle on a sphere given by:\n0 c9 =\n1\n12 0 1, -19 +\n1\n13 0 109 +\ni\n16 0 009.\na) What is the probability that a measurement of Lz will yield 2U? -U? 0 U?\nb) What is the expectation value of Lz in this state?\nc) What is the expectation value of L 2 in this state?\n\nResources \n249\nd) What is the expectation value of the energy in this state?\ne) What is the expectation value of Ly in this state?\n 7.30 A particle conﬁned to the surface of a sphere is in the state\nc1u, f2 = μ\nN ap2\n4 - u2b,\n   0 6 u 6 p\n2\n0,\n     p\n2 6 u 6 p,\n \n where the normalization constant is\nN =\n1\nB\np5\n8 + 2p3 - 24p2 + 48p\n.\na) Find the coefﬁcients for the 0 /m9 = 0 009, 0 1, -19, 0 109, and 0 119 terms in a spherical \nharmonics expansion of c1u, f2.\nb) What is the probability that a measurement of the square of the total angular momentum \nwill yield 2U2? 0 U2?\nc) What is the probability that the particle can be found in the region 0 6 u 6 p>6 and \n0 6 f 6 p>6? Repeat the question for the region 5p>6 6 u 6 p and 0 6 f 6 p>6. \nPlot your approximation from part (a) above on and check to see if your answers seem \nreasonable. (The activity on linear combinations of spherical harmonics has a Maple  \nworksheet ylmcombo.mws for plotting.)\nRESOURCES\nActivities\nThese activities are available at\nwww.physics.oregonstate.edu/qmactivities\nEigenstates of a Particle Conﬁned to a Ring: Students investigate eigenstates of a quantum particle \nconﬁned to a ring.\nGuessing the Legendre Polynomial Expansion of a Function: Students try to ﬁt a given function \nwith a linear combination of Legendre polynomials using the guess and check method.\nFinding Legendre Coefﬁcients: Students use Maple to ﬁnd the ﬁrst few coefﬁcients of a Legendre \nseries to approximate a function.\nParticle Conﬁned to a Ring: Students visualize linear combinations of eigenstates and study anima-\ntions of time evolution of the probability density.\nParticle Conﬁned to a Sphere: Students visualize the spherical harmonics.\nLinear Combinations of Spherical Harmonics: Students visualize states that are made up of linear \ncombinations of spherical harmonics.\n\nC H A P T E R \n8\nHydrogen Atom\nThe angular wave functions we found in the last chapter are independent of the particular form of \nthe central potential that binds the system. The remaining radial part of the wave function, however, \ndepends critically on the central potential you choose. The radial part of the problem determines the \nallowed energies of the system and hence the spectroscopic ﬁngerprint of the system that we observe \nin experiments. In this chapter, we solve for the quantized energies and the radial wave functions of \nthe bound states of the hydrogen atom, which is the simplest atomic system, comprising one electron \nbound to one proton in the nucleus. The electron and proton are bound together by the Coulomb poten-\ntial, which underlies the bonding in all atoms, molecules, liquids, and solids.\n8.1 \u0002 THE RADIAL EIGENVALUE EQUATION\nIn Chapter 7, we separated the three-dimensional energy eigenvalue equation into differential equa-\ntions for each of the spherical coordinates r, u, and f. We solved the f eigenvalue equation (7.83) and \nfound the azimuthal eigenstates \u0013m1f2 and eigenvalues m, which determined the separation constant \nB = m2. We then used the separation constant B to make the u differential equation (7.82) into an \neigenvalue equation and solved for the polar eigenstates \u0012 m\n/ 1u2 and the eigenvalues /1/ + 12, which \ndetermined the separation constant A = /1/ + 12. We now use the separation constant A to make the \nradial differential equation (7.79) into an eigenvalue equation for the energy E:\n \nc-  U2\n2mr 2 d\ndr\n ar 2 d\ndrb + V1r2 + /1/ + 12 U2\n2mr 2d R1r2 = ER1r2. \n(8.1)\nSolving this differential equation will give us the radial eigenstates R1r2 and the allowed ener-\ngies E. We then combine the three separated eigenstates into the three-dimensional eigenstate \nc1r, u, f2 = R1r2Y m\n/ 1u, f2, where the spherical harmonics Y m\n/ 1u, f2 = \u0012 m\n/ 1u2\u0013m1f2 are the prod-\nucts of the azimuthal and polar eigenstates that we found in Chapter 7.\nBefore we begin the solution, notice that the radial eigenvalue equation (8.1) resembles a one-\ndimensional eigenvalue equation with an effective potential energy Veff:\n \nVeff 1r2 = V1r2 +\nU2/1/ + 12\n2mr 2\n. \n(8.2)\nThe term U2/1/ + 12>2mr 2 in the effective potential energy is called the centrifugal barrier. It \nbehaves like a repulsive potential, and it increases with / in exact analogy with classical mechanics. In \nthis viewpoint, the effective potential energy that determines the radial motion of the electron is differ-\nent for each state with a different angular momentum quantum number /, as shown in Fig. 8.1.\n \n\n8.1 The Radial Eigenvalue Equation \n251\nFor the hydrogen atom, the Coulomb potential energy is responsible for attracting the electron to \nthe proton. This Coulomb potential energy is\n \nV1r2 = -  Ze2\n4pe0r , \n(8.3)\nwhere we assume that the nucleus has a charge +Ze so that our solution applies to the general case of a \nhydrogenic atom: H, He+, Li++, etc. With this choice of V1r2, the radial differential equation is\n \nd 2R\ndr 2 + 2\nr dR\ndr + 2m\nU2  cE +\nZe2\n4pe0r -\nU2/1/ + 12\n2mr 2\nd R = 0. \n(8.4)\nThe potential energy at r = \u0005 is V1\u00052 = 0, so bound states have energy E 6 0 while unbound \nstates have energy E 7 0.\nIt is convenient at this point to rewrite the radial differential equation in terms of dimensionless \nenergy and position parameters. The angular differential equations in Chapter 7 were treated similarly \nbecause the separation constants A and B were dimensionless. We deﬁne a characteristic length scale \nof the hydrogenic atom as a, such that the dimensionless radius is\n \nr = r\na. \n(8.5)\nWithout knowing what this scale is yet, we write the differential equation for R1r2 as\n \n1\na2 d 2R\ndr2 + 1\na2 2\nr dR\ndr + 2m\nU2  cE +\nZ e2\n4pe0 ar - U2/1/ + 12\n2ma2r2\nd R = 0. \n(8.6)\nMultiplying Eq. (8.6) by a2, we obtain\n \nd 2R\ndr2 + 2\nr dR\ndr + c\n2ma2\nU2\n E + a mZ e2\n4pe0\n U2b 2a\nr -\n/1/ + 12\nr2\nd R = 0. \n(8.7)\n2\n4\n6\n8\n10\nr\na0\n\n8\n\n6\n\n4\n\n2\n0\n2\n4\nVeff(r) (eV)\n\u0002 \u0004\u00070\n\u0002 \u0004\u00071\n\u0002 \u0004\u00072\nFIGURE 8.1 The effective potential for different values of the angular momentum quantum number /.\n\n252 \nHydrogen Atom\nThe terms inside the square brackets of Eq. (8.7) are now dimensionless, so we identify the hydrogen \ncharacteristic length scale as\n \na = 4pe0  U2\nm Z e2  \n(8.8)\nand the characteristic energy scale as U2>2ma2. We deﬁne a dimensionless energy parameter as\n \n-g2 =\nE\na U2\n2ma2b\n   , \n(8.9)\nwhere we assume that E 6 0 because we are seeking bound-state solutions. Using U2>2ma2 as the \nenergy scale is reasonable in light of the ground state energy being E1 = p2\n U2>2ma2 for a particle in \na box of size a. With the dimensionless length and energy parameters, the radial differential equation \nbecomes\n \nd 2R\ndr2 + 2\nr dR\ndr + c-g2 + 2\nr -\n/1/ + 12\nr2\nd R = 0. \n(8.10)\nIn this dimensionless form, the eigenvalue we are seeking is g2 and the eigenfunction is R1r2.\n8.2 \u0002 SOLVING THE RADIAL EQUATION\n8.2.1 \u0002 Asymptotic Solutions to the Radial Equation\nTo solve the radial eigenvalue equation (8.10), it is instructive to ﬁrst get some clues about the form \nof the solution by looking at the limiting behavior of the solutions for large and small r (i.e., large \nand small r). For large r, the terms in Eq. (8.10) involving r-1 and r-2 can be neglected, so Eq. (8.10) \nbecomes approximately\n \nd 2R\ndr2 - g2R = 0. \n(8.11)\nThis equation has the familiar exponential solutions R1r2 = e{gr, where the { symbol is required \nbecause Eq. (8.11) involves the second derivative of R1r2. We eliminate one of these signs by not-\ning that the solution e+gr blows up as r goes to inﬁnity. We want solutions for the wave functions to \nyield reasonably behaved probability densities (that is, they must be ﬁnite everywhere), and we must \ntherefore discard any solution that leads to an inﬁnite probability. Our solution for the radial wave \nfunction in this limit then becomes:\n \nR1r2\u0003\n e-  gr 1large r2. \n(8.12)\nNow let’s look at the behavior of the solutions when r is small. Now the r-2 term dominates and \nwe neglect the other terms in the square brackets in Eq. (8.10). In this case, we obtain the approximate \nequation\n \nd 2R\ndr2 + 2\nr dR\ndr - /1/ + 12\nr2\n R = 0. \n(8.13)\n\n8.2 Solving the Radial Equation \n253\nWe see by inspection that a solution of the form R1r2 = rq satisﬁes Eq. (8.13). For this choice of \nR1r2, each term in Eq. (8.13) is proportional to rq-2, and the three terms sum to zero for all values of \nr when\n \nq1q - 12rq-2 + 2\nr\n qrq -1 - /1/ + 12\nr2\n rq = 0, \n(8.14)\nwhich leads to\n \nq1q + 12 - /1/ + 12 = 0. \n(8.15)\nThis quadratic equation for q yields two solutions: q = / and q = -/ - 1. For small r, the solution \nr-/-1 blows up, so we discard this solution. We then have the limiting form\n \nR1r2\u0007r/  1small r2. \n(8.16)\nCombining Eqs. (8.12) and (8.16), we expect the radial solution to look something like \nR1r2\u0007r/e-  gr. We have not violated the proper behavior at the limits by combining these two  solutions; \nR1r2 remains well-behaved for r = 0 and r S \u0005. What else do we need to complete the solution? We \nneed to know the radial dependence at intermediate r, so let’s try an additional function H1r2 that is well-\nbehaved by remaining ﬁnite at r = 0 Aor blowing up more slowly than r-l B and as r S \u0005 (or blowing \nup more slowly than egr). We therefore seek solutions to the radial equation of the form\n \nR1r2 = r/e-  gr H1r2, \n(8.17)\nand our next goal is to determine the function H1r2.\n8.2.2 \u0002 Series Solution to the Radial Equation\nWe substitute the trial function R1r2 = r/e-  gr H1r2 into the radial differential equation (8.10) in \norder to ﬁnd the differential equation for the new function H1r2. Immediately, we ﬁnd that we need \nthe ﬁrst two derivatives of R1r2:\n \ndR\ndr = r/-1e-  gr 3/H1r2 - grH1r2 + rH\u00041r24, \n(8.18)\nwhere H\u00041r2 = dH>dr, and\n \nd 2R\ndr2 = r/-1e-  gr 312 - 2g - 2g/2 H 1r2 + 12 + 2/ - 2gr2  H\u00041r2 + rH\u000e1r24. \n(8.19)\nNow we substitute Eqs. (8.18) and (8.19) into Eq. (8.10) and collect terms to obtain the differential \nequation for H1r2:\n \nr d 2H\ndr2 + 21/ + 1 - gr2 dH\ndr + 211 - g - g/2  H 1r2 = 0. \n(8.20)\nJust as we did with the u differential equation in Chapter 7 [Eq. (7.132)], we use a power series \nexpansion to solve the radial equation (8.20). We assume that H1r2 has the form\n \nH1r2 = a\n\u0005\nj=0\ncj\n r j, \n(8.21)\n\n254 \nHydrogen Atom\nand now our job is to ﬁnd the cj coefﬁcients. The derivatives of H1r2 in this series form that we need are\n \n dH\ndr = a\n\u0005\nj =  0\n jcj r j -1 = a\n\u0005\nj =  0\n1 j + 12cj+1 r j \n(8.22)\n \n d 2H\ndr2 = a\n\u0005\nj=0\n j1 j + 12cj+1 r j-1,\n \nwhere we have shifted indices in the ﬁrst equation, as we did in the angular solutions in Section 7.6.1. \nSubstituting Eq. (8.22) into Eq. (8.20), we obtain\n \n a\n\u0005\nj=0\n j1 j + 12cj+1 r j + 21/ + 12 a\n\u0005\nj=0\n1 j + 12cj +1 r j\n \n \n -2ga\n\u0005\nj=0\n jcj r j + 211 - g - g/2 a\n\u0005\nj=0\n cj r j = 0. \n(8.23)\nIn order for all terms of the series in Eq. (8.23) to sum to zero for any and all values of r, the coef-\nﬁcient of each power of r must be zero, just as for the Legendre equation solution. The coefﬁcient of \nthe general term r j is\n \nj1 j + 12cj +1 + 21/ + 121 j + 12cj +1 - 2gjcj + 211 - g - g/2cj = 0, \n(8.24)\nwhich leads to the recurrence relation\n \ncj +1 =\n2g11 + j + /2 - 2\n1 j + 121 j + 2/ + 22\n cj. \n(8.25)\nThe recurrence relation shows us that the starting coefﬁcient c0 determines all of the remaining expan-\nsion coefﬁcients in the function H1r2. The normalization requirement determines c0, as you have \nprobably already realized, and we’ll return to this point in Section 8.4.\nIn our study of the polar angle wave functions \u00121u2, we found that we had to force the series to \nterminate to prevent the wave function from becoming inﬁnite. So far, we have assumed that the series \nexpansion of H1r2 includes an inﬁnite number of terms 1 j S \u00052. We have forced the asymptotic \nforms of R1r2 to remain ﬁnite, so let’s see how the new part of the solution, H1r2, behaves for large \nvalues of j and how that affects the radial function R1r2 = r/e-gr H1r2.\nFor large j, the recurrence relation in Eq. (8.25) is\n \ncj +1 \u0002 2gj\nj 2  cj = 2g\nj\n cj. \n(8.26)\nThis is exactly the same recurrence relation we ﬁnd for the exponential function! The series expansion \nof the exponential function\n \nea\n x = a\n\u0005\nn=0\n a n\nn!\n x n = 1 + a\n1!\n x + a2\n2!\n x 2 + a3\n3!\n x 3 + ... \n(8.27)\nhas a recurrence relation cj+1 = 1a>1 j + 122cj \u0002 1a>j2cj for large j. Hence, the large j limit in \nEq. (8.26) implies that for large r,\n \nH1r2 \u0002 e2gr, \n(8.28)\n\n8.2 Solving the Radial Equation \n255\nwhich leads to an asymptotic radial function\n \nR1r2 \u0002 r/e -gre2gr = r/egr. \n(8.29)\nThis asymptotic behavior has the same exponential pathology that we rejected in arriving at Eq. (8.12), \nso we must reject it once again. We do that by forcing the series expansion of H1r2 to terminate at a \nﬁnite value of j, just as we did for the Legendre polynomials.\nHence, the requirement that the wave function be normalizable leads us to deﬁne a value jmax \nsuch that the numerator of the recurrence relation, Eq. (8.25), goes to zero and terminates the series:\n \n2g11 + jmax + /2 - 2 = 0. \n(8.30)\nBecause j and / are integers, 11 + jmax + /2 is also an integer, which we denote as n:\n \nn = jmax + / + 1. \n(8.31)\nThis new integer is the principal quantum number of the hydrogen atom. The deﬁnition of the prin-\ncipal quantum number in Eq. (8.31) leads us to three important conclusions.\n• The integers j and / both start at 0 (make sure you know why), so the principal quantum number \nn starts at 1 and continues to inﬁnity because / can go to inﬁnity:\n \n n = 1, 2, 3, ... \u0005  . \n(8.32)\n• The dimensionless energy parameter g has discrete values! We learn this by substituting the \nnew quantum number n into Eq. (8.30) and solving:\n \ng = 1\nn. \n(8.33)\nFurthermore, the energy itself takes on only discrete values, and we ﬁnd those values by \nsubstituting Eq. (8.33) into the deﬁnition of g in Eq. (8.9). We also need the length scale in \nEq. (8.8) and arrive at\n \n-  1\nn2 =\nE\na U2\n2ma2b\n=\nE\na U2\n2mb\n a4pe0\n U2\nm Z e2 b\n2\n. \n(8.34)\nSo the requirement that the radial wave function be well behaved has led us to the quanti-\nzation condition on the allowed energies of the hydrogen atom. Solving Eq. (8.34) for the \nallowed energy yields\n \nEn = -  1\n2n2 a Z e2\n4pe0\nb\n2m\nU2 ,   n = 1, 2, 3, ...   , \n(8.35)\nwhich relates the hydrogen energy to the newly deﬁned principal quantum number n. We’ll \nsay more about the energy spectrum in the next section.\n• The angular momentum quantum number / is limited to a ﬁnite set of values for every n. We \nlearn this by solving Eq. (8.31) for /:\n \n/ = n - jmax - 1. \n(8.36)\n\n256 \nHydrogen Atom\nThe polar angle eigenstate solution in Chapter 7 told us that the angular momentum quantum \nnumber / had a range from 0 to inﬁnity. The lower limit of 0 is consistent with Eq. (8.36), in \nwhich case n = jmax + 1. However, the upper limit of inﬁnity is consistent with Eq. (8.36) \nonly for the special case of n = \u0005. For ﬁnite values of n, / cannot exceed n - jmax - 1, \nwhich is largest for the case of jmax = 0, implying that /max = n - 1. Thus, the radial \neigenvalue solution places a new limit on the allowed values of the angular momentum quan-\ntum number / that came from the polar eigenvalue equation:\n \n/ = 0, 1, 2, ... n - 1  . \n(8.37)\nWe now know all the quantum numbers for the hydrogen atom, so let’s take a moment to summa-\nrize our journey. We solved the f eigenvalue equation and found that the magnetic quantum number \nm was any integer from negative inﬁnity to positive inﬁnity. We then solved the u eigenvalue equation \nand found that the angular momentum quantum number / was an integer from 0 to inﬁnity, but that \nthe absolute value of the magnetic quantum number m could be no larger than /. Finally, we have now \nsolved the r eigenvalue equation and found that the principal quantum number n is an integer from 1 to \ninﬁnity, but the angular momentum quantum number / can be no larger than n - 1. In summary, the \nhydrogen atom quantum numbers are\n \n n = 1, 2, 3, ...\u0005\n \n \n / = 0, 1, 2, ..., n - 1\n \n \n(8.38)\n \n m = -/, -/ + 1, ... 0, ..., / - 1, /  . \n8.3 \u0002  HYDROGEN ENERGIES AND SPECTRUM\nThe solution to the radial eigenvalue equation has now given us the quantized energy eigenvalues of \nthe hydrogenic atom:\n \nEn = -  1\n2n2 a Ze2\n4pe0\nb\n2\n m\nU2 ,   n = 1, 2, 3, ...  . \n(8.39)\nThe principal quantum number n ranges from 1 to inﬁnity and is sometimes referred to as the shell \nnumber. The quantized energies are less than zero because the zero of potential energy is taken to be \nwhere the electron and nucleus are separated to inﬁnity—also called the ionization limit. Note that E \ndepends only on n and not on /, even though the radial wave function Rn/1r2 depends on both n and / \nthrough the jmax  in Eq. (8.31).\nIt is common to express the hydrogen energy in different forms that are more instructive than the \njumble of constants in Eq. (8.39). To simplify our discussion, we focus on the hydrogen atom itself \nand set Z = 1. We also follow the convention of using the electron mass me rather than the reduced \nmass m at this stage, and then using the correct reduced mass in later calculations. With these simpliﬁ-\ncations and a few rearrangements of constants, the hydrogen energy levels are\n \nEn = -  1\n2n2 mec2 a\ne2\n4pe0\n Ucb\n2\n. \n(8.40)\n\n8.3 Hydrogen Energies and Spectrum \n257\nThis form is useful because it contains the electron rest mass energy Erest = me\n c2 and a collection of \nfundamental constants, which must be dimensionless. The dimensionless constant inside the parenthe-\nses is the ﬁne structure constant\n \na =\ne 2\n4pe0\n U c , \n(8.41)\nso named because of its role in the ﬁne structure of the hydrogen spectra that we’ll study in Chapter 12. \nMore important, the ﬁne structure constant is a measure of the fundamental strength of the electromag-\nnetic interaction, and is also called the electromagnetic coupling constant. In terms of the ﬁne structure \nconstant, the hydrogen energy levels take on the simple form\n \nEn = -  1\nn2 1\n2\n a2me\n c2  . \n(8.42)\nThe ﬁne structure constant has the approximate value\n \na =\n1\n137\n   . \n(8.43)\nThe electron rest mass energy has the approximate value\n \nme\n c2 = 511 keV  . \n(8.44)\nAt this level of precision, the hydrogen energy levels are\n \nEn = -  1\nn2 13.6 eV  . \n(8.45)\nYou should commit the three numerical values in Eqs. (8.43), (8.44), and (8.45) to memory.\nAnother common and convenient form of the hydrogen energy level formula is obtained by using \nthe length scale we deﬁned in Eq. (8.8). In the case of hydrogen, the nuclear charge is Z = 1, and \nusing the electron mass rather than the reduced mass, we deﬁne the quantity\n \na 0 = 4pe0\n U2\nmee2  \n(8.46)\nas the Bohr radius, with the approximate value\n \na 0 = 0.0529 nm = 0.529 A \n\b  . \n(8.47)\nIn terms of the Bohr radius, the hydrogen energy levels are\n \nEn = -  1\n2 n2 a\n1\n4pe0\n e 2\na 0\nb  , \n(8.48)\nwhich emphasizes the Coulomb binding of the atom.\n\n258 \nHydrogen Atom\nThe spectrum of hydrogen energy states is shown in Fig. 8.2. There are several noteworthy fea-\ntures of the hydrogen energies:\n• There are an inﬁnite number of bound states in the hydrogen atom because the Coulomb \npotential energy falls off slowly for r S \u0005. In contrast, a three-dimensional ﬁnite square \nwell has a ﬁnite number of bound states, similar to the one-dimensional case.\n• The hydrogen energy levels are degenerate with respect to the / and m quantum numbers \nbecause the energy depends on n only. For each energy level En, there are n possible / states \nranging from / = 0  to  / = n - 1 in unit steps. For each of those / states, there are 2/ + 1 \npossible m states ranging from m = -/ to m = +/ in unit steps. The total number of states \nat each energy level En is the sum of these possibilities:\n \na\nn -1\n/=0\n12/ + 12 = 2a\nn-1\n/=0\n/ + a\nn-1\n/=0\n1 = 2 n1n - 12\n2\n+ n = n2. \n(8.49)\n1\n2\n3\n4\n5\nn\nLyman\nBalmer\nPaschen\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\nE\nf\nΛ\n\r\nEnergy (eV)\nFIGURE 8.2 Hydrogen energy levels and emission spectrum.\n\n8.3 Hydrogen Energies and Spectrum \n259\nWhen we include the two spin possibilities of the electron—spin up and spin down along the \nz-axis—then there are 2n2 possible states per energy level. The m degeneracy is a result of the \nspherical symmetry of the hydrogen atom and is removed if we break this symmetry, for exam-\nple by applying an electric or magnetic ﬁeld in a given direction (Chapter 10). The / degeneracy \nis a result of a special symmetry of the 1>r Coulomb potential and is removed when we account \nfor non-Coulomb interactions in the atom (Chapter 12).\n• The results we have obtained for the hydrogen energy levels are the same as those obtained \nwith the semi-classical Bohr model. That is a bit surprising because the Bohr model used \nsome incorrect physics. Because of this equality of results, the energy levels we have derived \nhere are often still referred to as the Bohr energies.\n• The Bohr energies require corrections due to relativity and internal magnetic ﬁelds \nthat change the energies at the level of about 1 part in 104, and considering that today’s \nspectroscopic techniques permit a precision of 1 part in 1014, 1 part in 104 is huge! This \nmeans that hydrogen is a wonderful playground to test reﬁnements of the simplest \nmodels. We will study some of these effects in Chapter 12.\nHydrogen atoms absorb or emit light when electrons make transitions between energy levels. \nWhen an electron transitions from a higher-lying to a lower-lying level, a photon is emitted. Some of \nthese emission lines are shown in Fig. 8.2. Transitions to the n = 1 ground state comprise the Lyman \nseries, with the lowest energy transition 1n = 2 S 12 referred to as the Lyman-A line or L a , the next \none Lb , etc. Transitions from higher levels down to the n = 2 level comprise the Balmer series and \ntransitions down to the n = 3 level comprise the Paschen series. The wavelengths of some of these \ntransitions are listed in Table 5.1. Transitions to higher-lying levels require the absorption of light.\nWhether the photon is emitted or absorbed, its energy matches the energy difference between the \ntwo atomic states involved:\n \nEphoton = \u0006Efi = 0 Ef - Ei0 = 1\n2\n me\n c2 a\ne 2\n4pe0\n Ucb\n2\n2 1\nn2\ni\n- 1\nn2\nf\n2 . \n(8.50)\nThe energy of the photon is related to its wavelength via\n \nEphoton = U v = hf = hc\nl , \n(8.51)\nso the wavelength of the photon obeys the relation\n \n1\nl = R\u0005 2 1\nn2\ni\n- 1\nn2\nf\n2 , \n(8.52)\nwhere we deﬁne the Rydberg constant as\n \nR\u0005 =\nme\n4pU3c\n a e2\n4pe0\nb\n2\n  . \n(8.53)\nThe Rydberg constant was discovered empirically in the nineteenth century through experimental \nmeasurements of the spectrum of hydrogen. The subscript \u0005 refers to our use of the electron mass \nin Eq. (8.53) as opposed to the reduced mass, which must be done to get accurate results. If we use \nthe reduced mass for hydrogen in Eq. (8.53), then the result is referred to as RH. RH and R\u0005 differ \nby 5 parts in 104 (huge!), so in precision measurement it’s important to be clear which is being used. \nToday the Rydberg constant is the second most precisely measured fundamental constant (the g-factor \nof the electron being the most precise). The latest measured value is\n \nR\u0005 = 109 737.315 685 271732 cm-1. \n(8.54)\n\n260 \nHydrogen Atom\nIt is also common to use the term Rydberg (without the word “constant”) in reference to the energy \ninstead of the inverse wavelength. For example, one often writes the hydrogen energies in the form\n \nEn = -  1\nn2 Ryd  , \n(8.55)\nwhere one Rydberg (Ryd) is equal to 13.6 eV.\nNot all transitions between states are allowed in the hydrogen atom. As we discussed in Chapter 3, \nthe probability of a transition is proportional to the matrix element of the light interaction between \nthe two states: 8cnf    /f  mf 0 Vint 0 cni /i\n mi9 [Eq. (3.109)]. The general properties of these matrix elements \ndetermine the selection rules that tell us which transitions are allowed and which are forbidden. For \nthe electromagnetic interaction that characterizes the emission and absorption of light, the selection \nrules for transitions in the hydrogen atoms are\n \n \u0006/ = /f - /i = {1\n \n \u0006m = mf - mi = 0, {1  .   \n (8.56)\nThese selection rules are primarily due to the conservation of angular momentum. The photon has \nspin angular momentum 1, so when an atom absorbs or emits light, the atom must change its angular \nmomentum by one unit. Some of the allowed transitions in hydrogen are shown in Fig. 8.3 where the \ndifferent angular momentum states s, p, d, etc. are identiﬁed in order to emphasize the \u0006/ = {1 tran-\nsitions. We will study these transitions and selection rules further in Chapter 14.\n1s\n2s\n2p\n3s\n3p\n3d\n4s\n4p\n4d\n4f\n\r\nEnergy (eV)\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\nFIGURE 8.3 Transitions between states in hydrogen, emphasizing the \u0006/ = {1 selection rule.\n\n8.4 The Radial Wave Functions \n261\n8.4 \u0002 THE RADIAL WAVE FUNCTIONS\nLet’s now return to the radial wave function solution R1r2 = r/e-gr H1r2 [Eq. (8.17)]. We have \ndetermined that g = 1>n, established that n and / are restricted integers, and found the recurrence \nrelation for the coefﬁcients in the series H1r2. The next thing to do is to put the dimensions back into \nthe problem. In terms of the Bohr radius a0, the length scale parameter a is\n \na = 4pe0\n U2\nme\n Ze2 = a0\nZ  , \n(8.57)\nand we have continued the convention of using the electron mass me rather than the reduced mass m. \nThe dimensionless radial position r is then\n \nr = r\na = Zr\na0\n. \n(8.58)\nThe radial wave function with the dimensions back in place is\n \nRn/1r2 = ¢Zr\na0\n≤\n/\n e-Zr\u0006na0 H ¢Zr\na0\n≤. \n(8.59)\nWe label the radial wave functions as Rn/ using the two quantum numbers n and / that affect the radial \ndependence. Now we’re ready to use our knowledge of the allowed quantum numbers and the recur-\nrence relation to ﬁnd the polynomial H1Zr>a02 for each state. The polynomial terminates at the value\n \njmax = n - / - 1. \n(8.60)\nLet’s look at solutions for a few particular values of n and /, and then we’ll discuss the general results \nfor the radial wave function.\nThe ground state of hydrogen has the principal quantum number n = 1 and the angular momen-\ntum quantum number / = 0, so Eq. (8.60) tells us that the polynomial terminates at jmax = 0. That’s \nthe simplest polynomial possible! Hence, we have H1Zr>a02 = c0 and the radial wave function is\n \nR101r2 = c0 e-Zr>a0. \n(8.61)\nThe constant c0 is determined from the normalization requirement (Problem 8.1).\nThe ﬁrst excited state of hydrogen has n = 2 and two possible values for /: / = 0 and / = 1. \nFor the 2s state 1/ = 02, Eq. (8.60) tells us that the polynomial terminates at jmax = 1. The polyno-\nmial is therefore H1Zr>a02 = c0 + c11Zr>a02. The coefﬁcients c0 and c1 are related by the recurrence \nrelation Eq. (8.25):\n \nc1 = -  1\n2\n c0 \n(8.62)\nso that H1Zr>a02 = c011 - Zr>2a02. The radial wave function is therefore\n \nR201r2 = c0 e-Zr>2a0 11 - Zr>2a02. \n(8.63)\nAgain, the constant c0 is determined from the normalization requirement, and it must be emphasized \nthat the coefﬁcients for different sets of quantum numbers n and / are not related to each other.\nFor the 2p state 1/ = 12, the polynomial terminates at jmax = 0, so H1Zr>a02 = c0 . The radial \nwave function is therefore\n \nR211r2 = c0 re-Zr>2a0. \n(8.64)\n\n262 \nHydrogen Atom\nContinuing this procedure results in the complete set of radial wave functions, some of which are \nshown in Table 8.1 and illustrated graphically in Fig. 8.4.\nIt turns out that the radial wave functions can also be written in terms of a common set of functions \nknown as the associated Laguerre polynomials L p\nq1x2, which are deﬁned as\n \nL p\nq1x2 = d p\ndx p Lq1x2. \n(8.65)\nTable 8.1 Radial Wave Functions of Hydrogenic Atoms\nR101r2 = 2a Z\na0\nb\n3>2\ne-Zr>a0\nR201r2 = 2a Z\n2a0\nb\n3>2\n c1 - Zr\n2a0\nd e-Zr>2a0\n R211r2 =\n1\n13\n a Z\n2a0\nb\n3>2\n Zr\na0\n e-Zr>2a0\n R301r2 = 2a Z\n3a0\nb\n3>2\n £1 - 2Zr\n3a0\n+ 2\n27 aZr\na0\nb\n2\n§  e-Zr>3a0\nR311r2 = 412\n9\n a Z\n3a0\nb\n3>2\n Zr\na0\n a1 - Zr\n6a0\nb e-Zr>3a0\n  R321r2 = 212\n2715\n a Z\n3a0\nb\n3>2\n aZr\na0\nb\n2\n e-Zr>3a0\n1\n2\n3\n4\n0\n0.5\n1.\n1.5\nR(r)\nR(r)\nR(r)\nR(r)\nR(r)\nR(r)\n1s\n2\n4\n6\n8\n10\n\n0.2\n0\n0.2\n0.4\n0.6\n2s\n2\n4\n6\n8\n10\nr\na0\n0\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\n2p\n5\n10\n15\n\n0.1\n0\n0.1\n0.2\n0.3\n3s\n5\n10\n15\n\n0.04\n\n0.02\n0\n0.02\n0.04\n0.06\n3p\n5\n10\n15\n0\n0.01\n0.02\n0.03\n0.04\n3d\nr\na0\nr\na0\nr\na0\nr\na0\nr\na0\nFIGURE 8.4 Radial wave functions for hydrogen energy eigenstates.\n\n8.5 The Full Hydrogen Wave Functions \n263\nThe ordinary Laguerre polynomials Lq1x2 are deﬁned as \n \nLq1x2 = e x\n d q\ndx q 1x qe-x2. \n(8.66)\nThe Laguerre polynomials Lq1x2 are of degree q, so the associated Laguerre polynomials L p\nq1x2 are of \ndegree q - p. Using these deﬁntions, the radial wave functions are\n \nRn/1r2 = - b a 2Z\nna0\nb\n3\n 1n - / - 12!\n2n31n + /2!43 r\n1>2\n e-Zr/na0 a2Zr\nna0\nb\n/\nL n+/\n2/+112Zr>na02  . \n(8.67)\nThe \nassociated \nLaguerre \npolynomial \nL 2/+1\nn+/ 12Zr>na02 \nis \na \npolynomial \nof \ndegree \n1n + /2 - 12/ + 12 = n - / - 1, as expected from the value of jmax  given by Eq. (8.60). Be \naware that there are differing deﬁnitions of the Laguerre polynomials, so the expression for the radial \nwave function may look different in other texts.\nIn Chapter 7, we normalized each of the angular wave functions separately, and we do the same \nhere with the radial function. This isn’t mathematically or physically necessary; it’s just a convenient \nway to do it. The radial normalization condition is\n \nL\n\u0005\n0\nr 2 dr 3Rn/1r24\n2 = 1, \n(8.68)\nwhich includes the r 2 term we discussed in Eq. (7.38). The normalization condition in Eq. (8.68) is \nwhat we need to ﬁnd the c0 coefﬁcients in Eqs. (8.61), (8.63), and (8.64) and was used to normalize the \nradial wave functions in Eq. (8.67).\n8.5 \u0002 THE FULL HYDROGEN WAVE FUNCTIONS\nFinally, we’re ﬁnished! We’ve solved each of the separated differential equations, we’ve found the \nthree quantum numbers n, /, and m for the hydrogen atom, and we’ve found the allowed energies. \nWe’re now ready to recombine the three separated parts of the wave function to form the full three-\ndimensional energy eigenstate wave functions of the hydrogen atom\n \n0 n/m9 \u0003 cn/m1r, u, f2 = Rn/1r2Y /\nm1u, f2  . \n(8.69)\nThe full eigenstates for the ﬁrst few energy levels of a hydrogenic atom are given in Table 8.2; the \nradial part comes from Eq. (8.67) and the angular part from Eq. (7.161). These states are also eigen-\nstates of the angular momentum operators L2 and Lz. They can be eigenstates of H, L2, and Lz simul-\ntaneously because these three operators commute with each other. The three eigenvalue equations are:\n \n H cn/m1r, u, f2 = -  13.6 eV\nn2\n cn/m1r, u, f2\n \n L2 cn/m1r, u, f2 = /1/ + 12U2 cn/m1r, u, f2   \n(8.70)\n \n Lz cn/m1r, u, f2 = m U cn/m1r, u, f2     . \n\n264 \nHydrogen Atom\nThe normalization condition for the full wave function is the three-dimensional  integral\n \n1 = 8n/m0 n/m9 =\n \nL\n0 cn/m1r, u, f20\n2 dV \n \n=\n \nL\n\u0005\n0\nL\n2p\n0\nL\np\n0\n0 Rn/1r20\n2 0 Y m\n/ 1u, f20\n2 r 2 sin u d u df dr. \n(8.71)\nIt is instructive to rewrite Eq. (8.71) to emphasize our choice to normalize the radial and angular parts \nof the wave function independently:\n \n1 = 8n/m0 n/m9 = b\nL\n\u0005\n0\nr 20 Rn/1r20\n2 dr r b\nL\n2p\n0\nL\np\n0\n@ Y m\n/ 1u, f2@\n2\n sin u d u dfr. \n(8.72)\n \n=1\n \n=1\nWe could break this down further into u and f pieces, but that step is not generally necessary. Note \nagain that the r 2 part of the differential volume element goes with the radial integral.\nTable 8.2 Energy Eigenstate Wave Functions of Hydrogenic Atoms\nc1001r, u, f2 =\n1\n1p\n a Z\na0\nb\n3>2\n e -Zr>a0\nc2001r, u, f2 =\n1\n1p\n a Z\n2a0\nb\n3>2\n c1 - Zr\n2a0\nd e -Zr>2a0\nc2101r, u, f2 =\n1\n21p\n a Z\n2a0\nb\n3>2\n Zr\na0\n e -Zr>2a0 cos u\nc21,{11r, u, f2 = <  \n1\n212p\n a Z\n2a0\nb\n3>2\n Zr\na0\n e -Zr>2a0 sin ue{if\nc3001r, u, f2 =\n1\n1p\n a Z\n3a0\nb\n3>2\n £ 1 - 2Zr\n3a0\n+ 2\n27 aZr\na0\nb\n2\n§  e -Zr>3a0\nc3101r, u, f2 =\n212\n313p\n a Z\n3a0\nb\n3>2\n Zr\na0\n a1 - Zr\n6a0\nb e -Zr>3a0 cos u\nc31,{11r, u, f2 = <  \n2\n313p\n a Z\n3a0\nb\n3>2\n Zr\na0\n a1 - Zr\n6a0\nb e -Zr>3a0 sin ue{if\nc3201r, u, f2 =\n1\n2712p\n a Z\n3a0\nb\n3>2\n aZr\na0\nb\n2\n e -Zr>3a0 13 cos2\n u - 12\nc32,{11r, u, f2 = <  13\n271p\n a Z\n3a0\nb\n3>2\n aZr\na0\nb\n2\n e -Zr>3a0 sin u cos ue{if\nc32,{21r, u, f2 =\n13\n541p\n a Z\n3a0\nb\n3>2\n aZr\na0\nb\n2\n e -Zr>3a0 sin2\n ue{i2f\n\n8.5 The Full Hydrogen Wave Functions \n265\nThe probability density is the absolute square of the wave function, so for an energy eigenstate\n \n P1r, u, f2 = 0 cn/m1r, u, f20\n2\n \n \n = 0 Rn/1r2Y m\n/ 1u, f20\n2. \n(8.73)\nMultiplying the probability density by the inﬁnitesimal volume element dV = r 2 dr sin u du df gives \nthe probability of measuring the electron to be within that volume element:\n \n P1r, u, f2dV = 0 cn/m1r, u, f20\n2 r 2 dr sin u du df\n \n \n = 0 Rn/1r2Y m\n/ 1u, f20\n2\n r 2dr sin u du df. \n(8.74)\nTo calculate the probability of ﬁnding the electron within some ﬁnite volume, we integrate Eq. (8.74) \nover that region.\nBecause the probability density is three dimensional, it is difﬁcult to represent graphically on a \nﬂat piece of paper. We needed three dimensions to properly visualize the two-dimensional spherical \nharmonic probability densities, so we would need four dimensions to visualize the three-dimensional \natomic probability density. A variety of different visualization schemes are possible, many aided by \nthe power of modern computers.\nLet’s start with the ground state of the hydrogen atom. The wave function is\n \nc1001r, u, f2 =\n1\n4pa 3\n0\n e-r>a0 \n(8.75)\nand the probability density is\n \nP1001r, u, f2 = 0 c1001r, u, f20\n2 =\n1\npa 3\n0\n e-2r>a0. \n(8.76)\nThe dimensions of the probability density are 1>length3 as you would expect for a three-dimensional \ndensity. For the hydrogen ground state, the probability density is independent of the angles u and f,\nwhich means that the electron cloud around the nucleus is spherically symmetric. The three-dimen-\nsional electron probability distribution of the 1s state is illustrated in Fig. 8.5. In Fig. 8.5(a) the three \naxes represent physical space and the value of the probability density is represented by a grayscale \n(white is high, black is low). Just three parallel planes are shown, allowing us to “peek” at the distribu-\ntion. In Fig. 8.5(b), the grayscale density plot in the x-z plane 1y = 02 is shown. On a computer, you \ncan animate the motion of the slicing planes in Fig. 8.5(a) to visualize the full electron cloud, and you \ncan also use color while you’re at it (see the activity on hydrogen probability densities). Figure 8.6(a) \nrepresents the 1s probability density in the x-z plane using height above the plane as the indicator of \nprobability density, and Fig. 8.6(b) shows the probability density in a one-dimensional plot as a function \nof r, the distance from the nucleus. All of these representations demonstrate that the probability density \nfor measuring the electron position in the 1s state is largest at the origin.\nGrayscale density plots in the x-z plane for the eigenstates in the ﬁrst three energy levels of the \nhydrogen atom are shown in Fig. 8.7. The density plots for negative values of m are indistinguishable \nfrom those for positive m, so they are not included. In the grayscale plots in Fig. 8.7, we plot the abso-\nlute value of the wave function, which is the square root of the probability density, to provide a better \nvisual representation of the electron distribution. The spatial scales are different for each value of n. \nEach plot has a range of -3n2a0 to +3n2a0 .\n\n266 \nHydrogen Atom\nHere are some important features of the radial wave functions and the probability densities.\n• All the radial functions have an r/ dependence, so the wave function vanishes at the origin \nexcept for the s states 1/ = 02. This is caused by the centrifugal barrier that “repels” the \nelectron from the nucleus for / Ú 1, as we saw in the effective potential in Fig. 8.1. For s \nstates, the probability density at the origin is\n \n Pns10, u, f2 = 0 cn 0010, u, f20\n2 = 0 Rn 0102Y 0\n01u, f20\n2 =\n1\n4p 0 Rn 01020\n2 \n \n = 1\np a Z\nna0\nb\n3\n.\n \n(8.77)\n \nThis nonzero probability density is important because it means that the electron has some ﬁnite \nprobability of being inside the nucleus, which affects the real energy levels when we consider \nthe nucleus not to be a point particle, as well as some other effects we address in Chapter 12.\nFIGURE 8.6 Probability density of the ground state of hydrogen (a) represented as the height \nabove the x-z plane and (b) plotted as a function of radius.\nFIGURE 8.5 (a) Two-dimensional slices of the three-dimensional electron distribution of the ground \nstate of hydrogen. In each slice, the probability density is represented by grayscale (black = 0, \nwhite = maximum). (b) The particular two-dimensional probability density slice at y = 0.\n\n8.5 The Full Hydrogen Wave Functions \n267\n• Each radial wave function Rn/ (r) has n - / - 1 nodes and n - / antinodes. The particle-\nin-a-box energy eigenstates also have more nodes as the energy increases. The hydrogen \nradial functions for a given n have fewer nodes for higher / states, but the angular wave func-\ntions compensate for that by having more nodes.\n• The full wave function has parity (-1)/ (recall that the parity operation is r S -r). The par-\nity of the wave function derives from the parity of the spherical harmonics, which we noted in \nEq. (7.167). The parity is important later in calculating matrix elements.\n• The probability densities are independent of the azimuthal angle f, which we have already \nseen in Chapter 7 from the nature of the spherical harmonics.\nThe probability plots we have shown are informative, but ultimately we need to calculate prob-\nabilities or expectation values to compare with experiments. These are often done with computers, but \nyou need to know what to tell the computer to do. Let’s work an example that is analytically tractable.\nExample 8.1 Find the probability that the electron in the ground state of hydrogen is measured to \nbe within one Bohr radius of the nucleus and calculate the expectation value of the radial position r.\nThe probability is the integral of the probability density over a sphere of radius a0, so we limit \nthe r integral to r 6 a0 and integrate over the full range of u and f:\n \n Pr6a0 =\nL\nsphere r6a0\nP1r, u, f2dV \n \n=\nL\na0\n0\nL\n2p\n0\nL\np\n0\nP1r, u, f2  r 2 sin u du df dr \n(8.78)\n \n=\nL\na0\n0\nL\n2p\n0\nL\np\n0\n0 Rn/1r2Y m\n/ 1u, f20\n2\n r 2 sin u du df dr. \nFIGURE 8.7 Grayscale density plots in the x-z plane of the absolute value of the \nwave function for hydrogen energy eigenstates 0 n/m9 denoted by the labels above \neach plot. The spatial range of each plot is -3n2a0 to +3n2a0.\n\n268 \nHydrogen Atom\nWe separate the radial and angular integrals\n \nPr6a0 = e\nL\na0\n0\nr 20 Rn/ 1r20\n2\n drf e\nL\n2p\n0\nL\np\n0\n0 Y m\n/ 1u, f20\n2\n sin u du dff. \n(8.79)\nThe angular integral is unity because the spherical harmonics are normalized (See! The separate \nnormalization is useful!), leaving\n \nPr6a0 =\n \nL\na0\n0\nr 20 Rn/1r20\n2\n dr. \n(8.80)\nNow we put in the radial ground state wave function to get\n \nPr6a0 =\n \nL\na0\n0\nr 2 4Z 3\na 3\n0\n e-2Zr>a0 dr. \n(8.81)\nSubstituting x = 2Zr>a0 and integrating gives\n Pr6a0 = 1\n2 L\n2Z\n0\nx2e-x dx = 1\n2\n 1-x2 - 2x - 22e-x 2\n2Z\n0\n= 1\n2\n 31-4Z2 - 4Z - 22e-2Z + 24. (8.82)\nFor the hydrogen case, Z = 1, and the probability is\n \n Pr6a0 = 31 - 12 + 2 + 12e-24 = 1 - 5e-2 \n \n = 0.323.\n \n(8.83)\nIn a set of radial position measurements, 32% of the results will be within one Bohr radius of the \nnucleus.\nThe expectation value of the radius is\n \n 8r9 = 8n/m0 r0 n/m9 = 81000 r0 1009\n \n \n =\n \nL\n r0 cn/m1r, u, f20\n2\n dV\n \n(8.84)\n \n =\n \nL\n\u0005\n0\nL\n2p\n0\nL\np\n0\nr0 Rn/1r2Y m\n/ 1u, f20\n2\n r 2 sin u du df dr. \nAgain, we separate the radial and angular integrals\n \n8r9 = b\nL\n\u0005\n0\nr 30 Rn/1r20\n2\n dr r b\nL\n2p\n0\nL\np\n0\n@ Y m\n/ 1u, f2@\n2\n sin u du dfr. \n(8.85)\nThe angular integral is unity and we get\n \n8r9 =\n \nL\n\u0005\n0\nr 30 Rn/1r20\n2\n dr. \n(8.86)\nSubstituting in the radial ground state wave function, we get\n \n8r9 =\n \nL\n\u0005\n0\nr 3 4Z 3\na 3\n0\n e-2Zr>a0 dr. \n(8.87)\n\n8.5 The Full Hydrogen Wave Functions \n269\nSubstituting x = 2Zr>a0 and integrating gives\n \n 8r9 = a0\n4Z L\n\u0005\n0\nx 3e-x dx = a0\n4Z\n 1-x 3 - 3x 2 - 6x - 62e-x `\n\u0005\n0\n \n \n = 3a0\n2Z\n .\n \n(8.88)\nFor the hydrogen atom, the mean value of the radius is 3a0>2. The integrand r 20 Rn/1r20\n2 of the \nintegral in Eq. (8.80) is plotted in Fig. 8.8. The hatched area under the curve represents the prob-\nability we calculated above that the electron is measured to be in the region 0 … r … a0. The arrow \nindicates the expectation value of the radius, which is beyond the peak because the integrand is \nnot symmetric.\nExpectation values of the radial position are useful for many calculations we will do later. We \nquote here without proof the expectation values 8n/m0 r k0 n/m9 for different powers:\n \n 8r9 = a0\n2Z\n 33n2 - /1/ + 124\n \n \n 8r 29 =\na 2\n0\n n2\n2Z 2  35n2 + 1 - 3/1/ + 124 \n \n h1\nr i =\nZ\na0\n n2\n \n(8.89)\n \n h 1\nr 2 i =\nZ 2\na 2\n0\n n31/ + 1\n22\n \n \n h 1\nr 3 i =\nZ 3\na 3\n0\n n3/1/ + 1\n221/ + 12\n .\n \nThe result in Example 8.1 agrees with the general expression in the ﬁrst equation above.\n1\n2\n3\n4\nr\na0\n0.1\n0.2\n0.3\n0.4\n0.5\nr 2R2(r)\n1s\n\u0004r\u0003\nFIGURE 8.8 Radial probability integrand for the hydrogen 1s ground state. The hatched \nregion indicates the probability Pr…a0 and the arrow indicates the expectation value 8r9.\n\n270 \nHydrogen Atom\n8.6 \u0002 SUPERPOSITION STATES\nHaving solved the energy eigenvalue equation for the hydrogen atom and found the allowed energies \nand allowed wave functions, we can now use them to ﬁnd the time evolution of the atom with arbitary \ninitial conditons using the Schrödinger time-evolution recipe we developed in Chapter 3. If the atom \nstarts in one of the energy eigenstates, then the time evolution of the system is\n \n0 c1t29 \u0003 c1r, u, f, t2 = Rn/1r2Y m\n/ 1u, f2e-iEnt>U, \n(8.90)\nwhere En are the energy eigenvalues given in Eq. (8.39). The wave function acquires an overall time-\ndependent phase factor, but that does not affect any measurements we make on the system, so this is a \nstationary state, as we have seen in previous chapters.\nMore interesting time-dependent behavior occurs if the system starts in a superposition of energy \neigenstates. In this case, the time evolution of the wave function is\n \n0 c1t29 \u0003 c1r, u, f, t2 = a\nn,/,m\ncn/m Rn/1r2Y m\n/ 1u, f2e-iEnt>U, \n(8.91)\nwhere the expansion coefﬁcients are obtained from the projections of the initial state 0 c1t = 029 onto \nthe energy eigenstates\n \ncn/m = 8n/m0 c1029 =\n \nL\n\u0005\n0\nr 2 dr\nL\np\n0\nsin u du\nL\n2p\n0\ndf R*\nn/ 1r2 Y m*\n/ 1u, f2 c1r, u, f, 02. \n(8.92)\nExample 8.2 Find the time evolution of an equal superposition of the 1s ground state and the \n2p01m = 02 excited state:\n \n0 c1029 =\n1\n12 0 1009 +\n1\n12 0 2109. \n(8.93)\nThese states are both energy eigenstates, so the time evolution is obtained by application of the \nSchrödinger recipe:\n \nc1r, u, f, t2 =\n1\n12 c1001r, u, f2e-iE1t>U +\n1\n12 c2101r, u, f2e-iE2t>U \n \n=\n1\n42pa 3\n0\n e-r>a0\n e-iE1t>U +\n1\n4pa 3\n0\n r cos u\n8a0\n e-r>2a0\n e-iE2t>U \n(8.94)\n \n=\n1\n42pa 3\n0\n e-iE1t>U ae-r>a0 + r cos u\n412a0\n e-r>2a0\n e-iv21tb, \nwhere the Bohr frequency is v21 = 1E2 - E12>U. Noting that z = r cos u, we rewrite the wave \nfunction as\n \nc1r, u, f, t2 =\n1\n42pa 3\n0\n e-iE1t>U ae-r>a0 +\nz\n412a0\n e-r>2a0\n e-iv21tb, \n(8.95)\nwhich emphasizes the z-dependence of the state. The probability amplitude (absolute value of \nthe wave function) is displayed in Fig. 8.9(a) at time t = 0. The electron cloud is displaced in the \npositive z-direction, but as time evolves, animation of Fig. 8.9(a) shows that the cloud moves up \nand down along z. This is a model of the oscillating electric dipole moment that is responsible for \nthe radiation that the atom emits at the Bohr frequency (Problem 8.13).\n\nExample 8.3 Find the time evolution of an equal superposition of the 2s excited state and the\n2p0 1m = 02 excited state:\n \n0 c1029 =\n1\n12 0 2009 +\n1\n12 0 2109. \n(8.96)\nThe time-evolved state is\n \n c1r, u, f, t2 =\n1\n12 c2001r, u, f2e-iE2t>U +\n1\n12 c2101r, u, f2e-iE2t>U\n \n \n =\n1\n24pa 3\n0\n a1 -\nr\n2a0\nb e-r>2a0 e-iE2t>U +\n1\n4pa 3\n0\n r cos u\n8a0\n e-r>2a0 e-iE2t>U \n(8.97)\n \n =\n1\n24pa 3\n0\n e-iE2t>U a a1 -\nr\n2a0\nb e-r>a0 +\nz\n4a0\n e-r>2a0b.\n \nIn this case, the two states are degenerate in energy and there is no relative time-dependent phase \nfactor. The probability amplitude (absolute value of the wave function) is displayed in Fig. 8.9(b) \nat time t = 0. The electron cloud is displaced in the negative z-direction in this case because of the \ndifferent radial wave function for the 2s state, and as time evolves, the cloud does not move. This \nis a model of a static electric dipole moment that we will use again when we study the response of \nthe atom to an applied electric ﬁeld—the Stark effect—in Chapter 10. Such an s-p superposition is \na hybrid orbital that can be used to explain molecular bonding. Two atoms with displaced electron \nclouds facing each other reduce the electrostatic repulsion of the positively charged nuclei and \nstabilize the system.\nFIGURE 8.9 Probability amplitude (wave function) densities for (a) 1s-2p and \n(b) 2s-2p superposition states.\n8.6 Superposition States \n271\n\n SUMMARY \nThe radial part of the energy eigenvalue equation contains the crucial physics of the Coulomb interac-\ntion that determines the energies of the bound hydrogen atom. Solving the radial differential equation \nyields the quantization condition on the energy. The new quantum number is the principal quantum \nnumber n = 1, 2, 3, ... . The resultant energies of the hydrogen atom states are\n \nEn = -  1\nn2 13.6 eV. \n(8.98)\nThe length scale of the hydrogen atom is set by the Bohr radius\n \na0 = 0.0529 nm. \n(8.99)\nThe radial wave functions Rn/1r2 combine with the spherical harmonics from Chapter 7 to give \nthe full three-dimensional wave functions of the hydrogen atom\n \n0 n/m9 \u0003 cn/m1r, u, f2 = Rn/1r2Y m\n/ 1u, f2. \n(8.100)\nThe allowed values of the three quantum numbers are\n \nn = 1, 2, 3, ...\u0005\n \n/ = 0, 1, 2, ..., n - 1 \n(8.101)\n \nm = -/, -/ + 1, ...0, ..., / - 1, /.\nThe hydrogen atom states cn/m1r, u, f2 are simultaneously eigenstates of the Hamiltonian H, and the \nangular momentum operators L2 and Lz:\n \n Hcn/m1r, u, f2 = En cn/m1r, u, f2\n \n \n L2\n cn/m1r, u, f2 = /1/ + 12U2 cn/m1r, u, f2 \n \n(8.102)\n \n Lz \n cn/m1r, u, f2 = m U cn/m1r, u, f2.\n \n PROBLEMS \n \n8.1 Calculate the coefﬁcient c0 that normalizes the radial wave function R101r2 in Eq. (8.61) and \nconﬁrm the wave function shown in Table 8.1.\n \n8.2 Use the recurrence relation for the radial wave function to construct the n = 3 radial states of \nhydrogen. Calculate the normalization constant for the R321r2 state.\n \n8.3 Use the deﬁnition of the radial wave function in terms of the associated Laguerre polynomials \n[Eq. (8.67)] to construct the radial wave function R421r2.\n \n8.4 Show that the wave functions representing the 01009 and 0 2109 states are orthogonal.\n \n8.5 By direct application of the differential operators, verify that the state 0 3219 \u0003 c3211r, u, f2 is \nan eigenstate of H, L2, and Lz and determine the corresponding eigenvalues.\n \n8.6 Calculate the probability that the electron is measured to be within one Bohr radius of the \nnucleus for the n = 2 states of hydrogen. Discuss the differences between the results for the \n/ = 0 and / = 1 states.\n272 \nHydrogen Atom\n\n \n8.7 Calculate the probability that the electron is measured to be in the classically forbidden region \nfor the n = 2 states of hydrogen. Discuss the differences between the results for the / = 0 and \n/ = 1 states.\n \n8.8 Calculate by direct integration the expectation values 8r 29 and 81>r9 of the radial position for \nthe ground state of hydrogen. Compare your results to the quoted expressions in Eq. (8.89) \nand discuss your results. Did you expect that 81>r9 \u0002 1>8r9? Use your result for 81>r9 to \nﬁnd the expectation value of the kinetic energy of the ground state of hydrogen and discuss \nyour result.\n \n8.9 Calculate by direct integration the expectation value of the radial position for each of the \nn = 3 states of hydrogen. Compare your results to the quoted expression in Eq. (8.89) and \ndiscuss your results.\n 8.10 Calculate the probability that the electron in the ground state of a hydrogenic atom of nuclear \ncharge Z is measured to be inside the nucleus. A nucleus with A nucleons (Z protons and \nA-Z neutrons) has an approximate radius of r \u0002 11.2 * 10-15 m2A1>3. Calculate the prob-\nabilities for hydrogen and uranium-238.\n 8.11 Tritium is an isotope of hydrogen, with a nucleus comprising one proton and two neutrons. The \ntritium nucleus (triton) is radioactive, decaying by beta (electron) emission to the helium-3 \nnucleus comprising two protons and one neutron. An electron is initially in the ground state of \na tritium atom. After the instantaneous beta decay, what is the probability that the electron is in \nthe ground state of the new atom?\n 8.12 Find the ground state energy, the effective Bohr radius [using Eq. (8.8)], and the Lyman-alpha \nwavelength of the following hydrogenic systems:\na) deuterium: electron and nucleus with one proton and one neutron\nb) positive helium ion: 4He+\nc) positronium: electron 1q = -e,  m = me2 and positron 1q = +e,  m = me2\nd) muonium: electron and antimuon 1q = +e,   m = mm \u0002 207me2\ne) muonic hydrogen: muon and proton\nf ) hydrogen-like uranium: 235U 91+\n 8.13 Consider the one-dimensional probability density  P1z2 along the z-axis obtained by integrating \nover a plane perpendicular to the z-axis, either in Cartesian coordinates\n \nP1z2 =\n \nL\n\u0005\n- \u0005 L\n\u0005\n- \u0005\n0 cn/m1x, y, z2 0\n2\n dx dy \n \n or in cylindrical coordinates\n \nP1z2 =\n \nL\n2p\n0\nL\n\u0005\n0\n0 cn/m1r, f, z20\n2\n r d r df. \n \n Calculate this probability density for the superposition states 0 c19 = 10 1009 + 021092> 12 \nand 0 c29 = 10 2009 + 0 21092> 12. Use these probability densities to ﬁnd the expectation \nvalue of the electric dipole moment d = qr and verify that the moments for these two states \nare oppositely oriented as indicated by Fig. 8.9. Plot and animate the probability densities to \nverify that one state is oscillating and one state is static.\n Problems \n273\n\n274 \nHydrogen Atom\n 8.14 A hydrogen atom is initially in the superposition state\n \n0 c1029 =\n1\n114 0 2119 -\n2\n114 0 32, -19 +\n3i\n114 0 4229. \na) What are the possible results of a measurement of the energy and with what probabilities \nwould they occur? Plot a histogram of the measurement results. Calculate the expectation \nvalue of the energy. \nb) What are the possible results of a measurement of the angular momentum operator L2 and \nwith what probabilities would they occur? Plot a histogram of the measurement results. \nCalculate the expectation value of L2.\nc) What are the possible results of a measurement of the angular momentum component oper-\nator Lz and with what probabilities would they occur? Plot a histogram of the measurement \nresults. Calculate the expectation value of Lz.\nd) How do the answers to (a), (b), and (c) depend upon time?\n 8.15 Consider a particle of mass m bound in an inﬁnite square potential energy well in three \n dimensions:\n \nV1z2 = e\n 0,\n\u0005,   0 6 x 6 L, 0 6 y 6 L, 0 6 z 6 L\nother wise.\n \n Use separation of variables in Cartesian coordinates to ﬁnd the energy eigenvalues and eigen-\nstates of this particle in a cubical box. Find the degeneracy of the ﬁrst 6 energy levels.\nRESOURCES \nActivities\nThese activities are available at\nwww.physics.oregonstate.edu/qmactivities\nRadial Wavefunctions: Students visualize the radial part of the probability density of the hydrogen \natom.\nHydrogen Probability Densities: Students visualize the probability density of the electron in the \nhydrogen atom.\nFurther Reading \nHigh resolution spectroscopy of the hydrogen atom is discussed in this article:\nT. W. Hänsch, A. L. Schawlow, and G. W. Series, “The spectrum of atomic hydrogen,” \nScientiﬁc American, 240(3), 94–110 (1979).\n\n \n275\nC H A P T E R \n9\nHarmonic Oscillator \nIn the last four chapters, you have learned the tools for analyzing the motion of particles in quantum \nmechanics. You applied these tools to three important problems: (1) a particle bound in an inﬁnite \nsquare potential energy well in one dimension, (2) a free particle in one dimension, and (3) the hydro-\ngen atom in three dimensions. In this chapter we will solve another system with bound states in a one-\ndimensional potential energy well: the harmonic oscillator. This system resembles the inﬁnite square \nwell or particle-in-a-box system—the harmonic oscillator box just has a different shape. To solve the \nharmonic oscillator problem, we introduce a new method and some new tools in the process. Then we \nuse the solutions to the harmonic oscillator problem as a means to review the fundamental tools and \nconcepts of quantum mechanics. \n9.1 \u0002 CLASSICAL HARMONIC OSCILLATOR\nLet’s ﬁrst review the classical harmonic oscillator before we study the quantum mechanical case. A \nprototypical classical harmonic oscillator system is a mass m connected to a spring that is ﬁxed to a \nwall at its other end. The spring force is governed by Hooke’s law, which says that the force F is a \nrestoring force and is proportional to the displacement x of the mass from equilibrium:\n \nF = -kx , \n(9.1)\nwhere k is the spring constant. This linear restoring force is derivable from the quadratic potential \nenergy function V1x2 = 1\n2 kx 2 .\nThe beauty of the mass-on-a-spring system is that it is a model for many other systems in nature \nthat behave as harmonic oscillators. To see why this is so, consider the generic potential energy curve \nshown in Fig. 9.1. We are typically interested in ﬁnding the motion in the ground state or other low \nenergy states of the system. As the dashed line suggests, near the minimum at x0 of the potential \nenergy function that governs the system, the potential energy has the shape of a parabola, (i.e., it looks \nlike a harmonic oscillator). This parabolic shape is also evident if we consider a Taylor series expan-\nsion of the function about the minimum:\n \nV1x - x02 = V1x02 + 1x - x02 dV\ndx\n `\nx=x0\n+ 1\n2\n 1x - x02\n2 d 2V\ndx 2 `\nx=x0\n+ ... . \n(9.2)\nThe leading term in Eq. (9.2) is the quadratic term because the ﬁrst two terms are zero: (1) the \npotential energy offset V1x02  can be deﬁned to be to zero because a constant potential energy \ndoes not affect the motion, and (2) the linear term is zero because the potential derivative \n\n276 \nHarmonic Oscillator\n(i.e., slope) is zero at the minimum. Hence the motion of the system is that of a harmonic oscilla-\ntor in the vicinity of the potential energy minimum, and we identify the spring constant k as the \nsecond derivative of the potential energy evaluated at the minimum x0. If the motion takes the \nsystem too far from the minimum, the shape may deviate slightly from a parabola, and the motion \nwill be altered, but we still ﬁnd it useful to start by considering the motion as harmonic and then \nasking how that motion is perturbed. For these reasons, you will study harmonic oscillators as \nlong as you do physics.\nThe motion of the classical harmonic oscillator is solved by using Newton’s second law:\n \n F = ma\n \n -kx = m d 2x\ndt 2 .  \n(9.3)\nIt is convenient to deﬁne a new constant\n \nv = B\nk\nm \n(9.4)\nand rewrite the equation of motion as\n \nd 2x\ndt 2 = -v2 x1t2. \n(9.5)\nThis is a standard differential equation that you have likely encountered many times before. The \nsolution is the sinusoidal function\n \nx1t2 = A cos 1vt + f2, \n(9.6)\nwhere the amplitude A and phase constant f are determined by the initial state of the motion of the \nsystem. The motion is characterized by a single angular frequency (i.e., a single harmonic—hence the \nname) given by v.\n0\nx0\nx\nV(x)\nFIGURE 9.1 A general potential energy function (solid) \nis approximated by a quadratic harmonic  potential (dashed) \nin the vicinity of the potential minimum.\n\n9.2 Quantum Mechanical Harmonic Oscillator \n277\n9.2 \u0002 QUANTUM MECHANICAL HARMONIC OSCILLATOR\nThe procedure for ﬁnding the quantum mechanical Hamiltonian of any system is to ﬁrst ﬁnd the clas-\nsical energy and then rewrite that in terms of quantum mechanical operators. The potential energy of \nthe harmonic oscillator is\n \nV1x2 = 1\n2\n kx2. \n(9.7)\nThe total mechanical energy of the system is the sum of kinetic and potential energies:\n \nE = p2\n2m + 1\n2\n kx2. \n(9.8)\nThe oscillator frequency v plays an important role in quantum mechanics, so it is common to rewrite \nthe potential energy using v in place of k. From Eq. (9.4) we have k = mv2, so that the quantum \nmechanical Hamiltonian for the harmonic oscillator is\n \nH = pn  2\n2m + 1\n2 mv2xn  2   . \n(9.9)\nWe denote the operators xn and pn with carets to distinguish them from the variables x and p, but we \noften don’t use the caret notation if there is no ambiguity.\nAs always, our goal when presented with a new potential energy system is to solve the energy \neigenvalue equation H0 E9 = E0 E9 to ﬁnd the allowed energies in the system. Then we use the energy \neigenstates as the preferred basis to apply the recipe for Schrödinger time evolution. In the previous \npotential energy well problems, the square wells and the hydrogen atom, we expressed the energy \neigenvalue equation H0 E9 = E0 E9 as a differential equation in the wave function picture (i.e., the posi-\ntion representation). For the harmonic oscillator, the energy eigenvalue differential equation is\n \n-  U2\n2m \nd 2wE1x2\ndx 2\n+ 1\n2\n mv2x 2wE1x2 = EwE1x2. \n(9.10)\nWe can solve Eq. (9.10) using a power series solution, similar to the approach taken in the hydrogen \natom solutions in Chapters 7 and 8. Rather than do that here, we present a new method of solution that \nis more elegant and is known as the operator method or the algebraic method. Of course, we get the \nsame results either way.\nIf you haven’t seen it before, the operator method for solving the quantum mechanical harmonic \noscillator problem appears to be magic. We arrive at the solution by deﬁning some new quantities that \nyou would not imagine would be useful and by using minimal information about what how the opera-\ntors xn and pn behave. This operator method is also useful in describing angular momentum, and it is the \nbasis of quantum ﬁeld theory.\nTo make this discussion of the operator solution to the harmonic oscillator problem clearer, let’s \ngo ahead and present the energy spectrum answer to the problem. As we discussed in Chapter 5, the \nsolutions to bound state problems in different quantum mechanical systems share many features. The \nbound states in a potential energy well are discrete, with the ground state near, but not at, the bottom \nof the well. The positions of the energy levels depend upon the shape of the well. In the case of the \ninﬁnite square well that we studied in Chapter 5, the energy levels scale with n2, where n is the quan-\ntum number labeling the energy levels n = 1, 2, 3, ... . Hence the energy level spacing in the inﬁnite \nsquare well increases as n increases, as shown in Fig. 9.2(a). The hydrogen atom that we studied in \n\n278 \nHarmonic Oscillator\nChapter 8 has energy levels that scale as 1>n2 and so they get closer together as n increases, as shown \nin Fig. 9.2(b). The harmonic oscillator has a special potential energy well shape that gives rise to \nenergy levels that scale linearly with n and hence are evenly spaced, as shown in Fig. 9.2(c). The \nenergy eigenvalues of the harmonic oscillator are\n \nEn = U v1n + 1\n22,  n = 0, 1, 2, 3, ... . \n(9.11)\nThe convention is to label the ground state of the harmonic oscillator as n = 0, rather than n = 1 as \nin most other bound state problems. In Dirac notation, the energy eigenstates 0 n9 are labeled with the \nquantum number n and satisfy the energy eigenvalue equation\n \nH@ n9 = En@ n9 = U v1n + 1\n22@ n9. \n(9.12)\nIn the operator method of solving the harmonic oscillator problem, we deﬁne two new operators \nand use the properties of these operators to derive the energy eigenvalues given in Eq. (9.11). The new \noperators are the raising and lowering operators, a- and a, respectively, and they act to change the \nlabels n on the eigenstates. These new operators are built from the position and momentum opera-\ntors, xn and pn, that comprise the Hamiltonian in Eq. (9.9), and they simplify many of the calculations \nrequired in the harmonic oscillator problem.\nTo see where these new operators come from and why they are useful, ﬁrst note that the har-\nmonic oscillator Hamiltonian in Eq. (9.9) is a sum of squares. If it were a difference of squares, then \nwe could factor it as a product of the sum and difference Ci.e., u2 - v2 = 1u - v21u + v2D. But as a \nsum, we can still factor it if we use complex numbers, which we know are used quite often in quan-\ntum mechanics:\n \nu2 + v2 = 1u - iv21u + iv2. \n(9.13)\n(a)\nx\nE4\u000416E1\nE3\u00049E1\nE2\u00044E1\nE1\u0004E1\nE1\u0004E1\n(b)\nr\nE\n(c)\nx\nE\nE2\u0004\nE1\n4\nE3\u0004\nE1\n9\nE4\u0004\nE1\n16\nE3\u00047 \u0002Ω\n2\nE2\u00045 \u0002Ω\n2\nE1\u00043 \u0002Ω\n2\nE0\u0004 \u0002Ω\n2\nFIGURE 9.2 Spectra of energy eigenstates in (a) the inﬁnite square well, (b) the hydrogen atom, \nand (c) the harmonic oscillator well.\n\n9.2 Quantum Mechanical Harmonic Oscillator \n279\nSo let’s factor the Hamiltonian in the manner of Eq. (9.13), and while we’re at it, let’s make our life \neasier by using dimensionless quantities. We know that Planck’s constant times frequency has dimen-\nsions of energy, so we start by factoring out an energy term U v from the Hamiltonian\n \n H = 1\n2 mv2cxn2 +\npn2\nm2v2d\n \n = U v e mv\n2U\n cxn2 +\npn2\nm2v2d f \n(9.14)\nsuch that the expression inside the curly brackets is dimensionless. We now deﬁne a new dimension-\nless operator, called the lowering operator, to help us factor the Hamiltonian\n \na = A\nmv\n2U\n axn + i pn\nmvb. \n(9.15)\nNote that the lowering operator is not Hermitian, because it is not equal to its Hermitian conjugate\n \n a- = A\nmv\n2U\n axn - - i pn -\nmvb \n \n = A\nmv\n2U\n axn - i pn\nmvb, \n(9.16)\nwhich is the raising operator. Recall that xn and pn are Hermitian, xn = xn - and pn = pn -, because they \nrepresent physical observables. Because the raising and lowering operators are not Hermitian, they do \nnot correspond to measurable observables. Nonetheless, they are very useful.\nOur attempt to factor the Hamiltonian is complicated by the fact that quantum mechanical opera-\ntors do not in general commute with each other. In Eq. (9.13), we implicitly assumed that u and v com-\nmute with each other, so that the cross terms  -ivu and iuv cancel. However, the quantum mechanical \noperators xn and pn that we use to deﬁne the raising and lowering operators do not commute with each \nother. As a result, we must take care in ﬁnding the product of the two new operators:\n \n a-a = mv\n2 U\n axn - i pn\nmvbaxn + i pn\nmvb\n \n = mv\n2 U\n axn 2 +\npn 2\nm2v2 +\ni\nmv\n 3xnpn - pnxn4b \n(9.17)\n \n = mv\n2 U\n axn 2 +\npn 2\nm2v2 +\ni\nmv3xn, pn4b.\nHence, the product a-a of the raising and lowering operators gives us what we want—the term in the \ncurly brackets in Eq. (9.14)—but with an extra additive term proportional to the commutator of xn and pn.\nRecall [Problem 5.1 and Eq. (6.66)] that the commutator of xn and pn is\n \n3xn, pn4 = i U. \n(9.18)\n\n280 \nHarmonic Oscillator\nSubstituting into Eq. (9.17), we obtain\n \na-a = mv\n2 U\n axn  2 +\npn2\nm2v2b - 1\n2, \n(9.19)\nso that the Hamiltonian written in terms of these new operators is\n \nH = U v1a-a + 1\n22  . \n(9.20)\nWe need one more thing before we proceed. Go back to Eq. (9.17) and note that if we had reversed \nthe order of a- and a, then we would have obtained a similar result with one difference: the commutator\nwould be reversed in sign (Problem 9.1). Thus the reverse product of the raising and lowering \noperators is\n \naa- = mv\n2U\n axn  2 +\npn2\nm2v2b + 1\n2. \n(9.21)\nIf we now subtract Eq. (9.19) from Eq. (9.21), we ﬁnd the commutator of the two new operators:\n \n3a, a-4 = aa- - a-a = 1  . \n(9.22)\nThis commutator equation deﬁnes the algebra of these new operators and provides the key to ﬁnding \nthe eigenvalue spectrum.\nArmed with the commutator relation in Eq. (9.22), we can now demonstrate that the new opera-\ntors a- and a do act to raise and lower, respectively, the energy eigenstates, as we said at the beginning. \nTo see how the raising and lowering operators act on energy eigenstates, we ﬁrst calculate the com-\nmutator of the lowering operator with the Hamiltonian:\n \n  3H, a4 = Ha - aH\n \n = U v1a-a + 1\n22a - a U v1a-a + 1\n22 \n(9.23)\n \n = U v1a-aa - aa-a2.\nNow use the commutator of the raising and lowering operators to obtain\n \n 3H, a4 = U v1a-aa - 1a-a + 12a2 \n \n = -U v a .\n \n(9.24)\nLikewise, you can show that the commutator of the raising operator with the Hamiltonian is\n \n3H, a-4 = +U va-. \n(9.25)\nTo show that the lowering operator deserves its name, act with a on an energy eigenstate 0 E9, \nwhere we assume that 0 E9 is a normalized energy eigenstate that satisﬁes the energy eigenvalue equa-\ntion H0 E9 = E0 E9, but we don’t yet know the eigenvalue E. To learn about the energy of the new ket \na0 E9, consider what happens when the Hamiltonian H acts on a0 E9:\n \nH1a0 E92 = Ha0 E9. \n(9.26)\n\n9.2 Quantum Mechanical Harmonic Oscillator \n281\nThe commutator in Eq. (9.24) tells us that Ha = aH - U va, so Eq. (9.26) becomes\n \n H1a0 E92 = 1a H - U va20 E9\n \n = a H0 E9 - U va0 E9. \n(9.27)\nNow use the energy eigenvalue equation H0 E9 = E0 E9 to obtain\n \n H1a0 E92 = 1a E0 E9 - U va0 E2\n \n = 1E - U v21a0 E92.  \n(9.28)\nThis looks like algebraic gymnastics, but there is something useful buried here! Equation (9.28) tells \nus that when the new ket a0 E9 is acted on by the Hamiltonian H, the result is the same ket a0 E9 multi-\nplied by the factor 1E - U v2, which means that the new ket a0 E9 is also an eigenstate of H, but with \nan energy eigenvalue 1E - U v2 that is smaller than the eigenvalue E of the original ket 0 E9 by one \nquantum of energy U v. The eigenvalue equation for this new state is\n \nH0 E - U v9 = 1E - U v2 0 E - U v9. \n(9.29)\nSo a has earned the name “lowering operator.” The only tricky point is that the state a0 E9 may not be \nnormalized (in fact it is not), assuming that the eigenstates 0 E9 are normalized, so we cannot say that \na0 E9 is equal to 0 E - U v9, merely that they are proportional.\nThe result is that we have now learned what happens when the operator a acts on an eigenstate \n0 E9 of H: it produces another eigenstate of H with the eigenvalue lowered by one quantum U v. Like-\nwise, one can show that the action of a- on an eigenstate of H produces an eigenstate with the eigen-\nvalue raised by one quantum of energy (Problem 9.2). Now you see why we call the operators a and \na- lowering and raising operators. We also refer to these operators collectively as ladder operators \nbecause they take us up and down a ladder of energy eigenstates, as depicted schematically in Fig. 9.3. \nWe don’t yet know where the rungs of the ladder are (i.e., what the energy eigenvalues are) or whether \nthere are many interleaved ladders. But the importance of the ladder operators is that if we can ﬁnd just \none eigenstate 0 E9, then the ladder operators can be used to ﬁnd other eigenstates of the system, with \neach level separated by the energy quantum U v.\nFrom the discussion so far and from the schematic in Fig. 9.3, you probably have the impression \nthat the ladder of energy states goes up and down symmetrically. But the commutator in Eq. (9.22) \nalready gives us a hint that there is a built-in asymmetry in the ladder, which we can use to ﬁnd the \nenergy spectrum. Because a and a- do not commute, we have aa- \u0002 a-a, which we can express \nabstractly as\n \n1down21up2 \u0002 1up21down2. \n(9.30)\nBut that is not how you might expect ladder operators to behave. If you stand on a rung and go up then \ndown, you are in the same position as if you had gone down then up. The asymmetry of the harmonic \noscillator is also evident if we note that the potential energy well that deﬁnes the harmonic oscillator \nhas a minimum level at V = 0 from which it goes only upward. A classical particle cannot have a \ntotal energy below the potential energy minimum because kinetic energy cannot be negative. Though \nquantum mechanics does allow for negative kinetic energies (in the classically forbidden regions), it is \nalso true in quantum mechanics that the total energy of a particle cannot be below the potential energy \nminimum (quantum mechanics may be weird, but it is not that weird). So we conclude that the ladder \nof energy states in Fig. 9.3 must not go below E = 0.\n\n282 \nHarmonic Oscillator\nIf the ladder of energy states in Fig. 9.3 cannot go below zero, then there must be a lowest energy \nstate @ Elowest9. But how can that be consistent with the idea of the ladder operators? Wouldn’t the low-\nering operator take that “lowest” state to a state with lower energy, below zero? Not if we don’t let it! \nWe do that by requiring that when we operate on the lowest possible energy state with the lowering \noperator we get zero:\n \na @ Elowest9 = 0. \n(9.31)\nWe refer to this as the ladder termination condition. We now use this condition to ﬁnd the energy of \nthat lowest state. Act with the Hamiltonian H on the lowest state:\n \nH 0 Elowest9 = U v1a-a + 1\n220 Elowest9 \n(9.32)\nand note that the ladder termination condition in Eq. (9.31) means that the ﬁrst term on the right-hand \nside of Eq. (9.32) becomes zero, giving\n \nH 0 Elowest9 = 1\n2\n U v0 Elowest9. \n(9.33)\nThis is nothing but the energy eigenvalue equation H 0 E9 = E 0 E9 for the lowest state, so the energy is\n \nElowest = 1\n2 U\n v. \n(9.34)\nWow! This operator gymnastics has led us to the ground state energy of the quantum mechanical \nharmonic oscillator, using just the form of the Hamiltonian and the commutator of position and \nmomentum. Note that the ground state does not have zero energy, in contrast to the classical harmonic \noscillator. Rather, the quantum mechanical ground state has a zero-point energy of U v>2. This is not \nsurprising if we recall the other potential well systems we have studied such as the square well poten-\ntial, where the ground state is not at the bottom of the well. The zero-point energy is also consistent \nwith the uncertainty principle in that we expect there to be residual energy associated with the spread \nin momentum.\nTo generate the next energy eigenstate up the ladder of states, we act with the raising operator on \nthe ground state 0 Elowest9, which produces a new energy eigenstate with the energy increased by one \na\u0002\na\u0002\na\u0002\na\u0002\na\na\na\na\nE \u000f\u00072\u0002Ω\nE \n\u00072\u0002Ω\nE \u000f\u00071\u0002Ω\nE \n\u00071\u0002Ω\nE\nFIGURE 9.3 Part of the ladder of energy eigenstates, with the action of the raising \nand lowering  operators shown.\n\n9.2 Quantum Mechanical Harmonic Oscillator \n283\nquantum U v. We repeat the action of the raising operator to generate the complete ladder of energy \nvalues, as shown in Fig. 9.4:\n \nE = 1\n2\n U\n v, 3\n2\n U\n v, 5\n2\n U\n v, 7\n2  U\n v, ... . \n(9.35)\nWe write the energy spectrum compactly as\n \nEn = U v1n + 1\n22,  n = 0, 1, 2, 3, ...  , \n(9.36)\nwhich is the result we quoted at the beginning. The quantum number n is used to label the energy \neigenstates 0 En9 = 0 n9. These states satisfy the energy eigenvalue equation\n \nH 0 n9 = En0 n9 = 1n + 1\n22 U\n v0 n9, \n(9.37)\nare normalized to unity\n \n8n0 n9 = 1, \n(9.38)\nand are orthogonal to each other\n \n8m0 n9 = dmn. \n(9.39)\nThus we have found the complete spectrum of energy eigenstates of the harmonic oscillator, using \nminimal information about the operator properties.\nAs shown in Fig. 9.4, the energy eigenstates are evenly spaced by the energy quantum U v. The \nselection rule for the quantum mechanical harmonic oscillator (coming soon in Section 9.8) restricts \ntransitions to those between adjacent energy states, so the uniform spacing implies that a spectroscopy \nexperiment would yield only one possible value for an energy difference, no matter which levels were \ninvolved. This is similar to the classical case where there is only one frequency that characterizes a \nharmonic oscillator. \nIn addition to the ladder operators, it is useful to deﬁne one more new operator that will help us \n“count” energy quanta. The energy eigenvalue equation for the harmonic oscillator\n \n H 0 n9 = En\n 0 n9\n \n \n U v1a-a + 1\n22@ n9 = U v1n + 1\n22@ n9 \n(9.40)\ncan be simpliﬁed to obtain a new eigenvalue equation\n \na-a 0 n9 = n0 n9. \n(9.41)\nThis equation suggests that we deﬁne the operator a-a as the number operator N:\n \nN = a-a. \n(9.42)\nThe number operator N is dimensionless and obeys the eigenvalue equation\n \nN0 n9 = n0 n9. \n(9.43)\nThe eigenvalues of the number operator are the same integers n that we use to label the energy eigen-\nstates 0 n9. We can write the harmonic oscillator Hamiltonian in terms of the number operator:\n \nH = U v1N + 1\n22. \n(9.44)\n\n284 \nHarmonic Oscillator\nThe number operator is Hermitian, even though the ladder operators that comprise it are not. The \neigenvalues of the number operator represent the number of energy quanta U v there are in the system \nabove the ground state.\nThe mathematics of the quantum mechanical harmonic oscillator system can be applied to other \nquantum mechanical systems, even though they do not appear to be harmonic oscillators. All that is \nrequired is that the Hamiltonian be the sum of squares of operators. For example, the Hamiltonian \nrepresenting the electromagnetic ﬁeld energy can be written as the sum of squares of operators \nrepresenting the electric and magnetic ﬁelds (see any E&M text). Hence, when we apply quantum \nmechanics to the electromagnetic ﬁeld, the energy eigenstate 0 n9 represents a state of the system with \nn photons (particles or quanta of light), each with an energy U v. The ground state 0 09 represents the \nstate of the system with no photons, also known as the vacuum. Thus the zero-point energy represents \nthe electromagnetic energy of the vacuum state, which is a bit surprising because we usually associate \nthe vacuum with the absence of all “stuff.” Even though spectroscopic measurements determine only \nenergy differences, there are observable effects of this zero-point energy in the Lamb shift, which we \nwill learn about in Chapter 12. Because the raising and lowering operators change the number of pho-\ntons in the system, they are often referred to as creation and annihilation operators, respectively.\n9.3 \u0002 WAVE FUNCTIONS\nThough we have solved the energy eigenvalue equation, we are not quite done. We don’t yet know the \nspatial wave functions corresponding to the energy eigenstates. That is to say, we know that the states \n0 n9 are the energy eigenstates, but we don’t know their spatial representation 0 n9 \u0003 wn1x2 = 8x0 n9.\nAs we did for the particle in a box and the hydrogen atom, we could solve the differential equation \nform of the energy eigenvalue equation, which in this case is\n \n-  U2\n2m d 2wn1x2\ndx2\n+ 1\n2\n  mv2x2wn1x2 = Enwn1x2. \n(9.45)\nAs we mentioned earlier, this can be solved with a power series technique that would yield the ener-\ngies En and the states wn1x2. Rather, let’s continue our operator approach to ﬁnd the wave functions.\nWe said before that if we know one of the harmonic oscillator eigenstates, then we can use the \nladder operators to generate the other energy eigenstates. We used this idea to discover the spectrum \na\u0002\na\u0002\na\u0002\na\u0002\na\na\na\na\n\u00020\u0003\n\u00021\u0003\n\u00022\u0003\n\u00023\u0003\n\u00024\u0003\nE3 \u0004\n\u0002Ω\n\u0002Ω\n7\n2\nE2 \u0004 5\n2\nE1 \u0004\n\u0002Ω\n\u0002Ω\n3\n2\nE0 \u0004 1\n2\nE4 \u0004\n\u0002Ω\n9\n2\nFIGURE 9.4 The ladder of harmonic oscillator states has its lowest rung at n = 0.\n\n9.3 Wave Functions \n285\nof energy levels by noting that the ground state is unique in that there are no states below it, which led \nus to the ladder termination equation\n \na 0 09 = 0. \n(9.46)\nLet’s now use this same termination condition to ﬁnd the wave function representing the ground state, \nand then use the raising operator to generate all the other wave functions. In the x–representation, the \nladder termination equation is\n \n a w01x2 = 0  \n \n A mv\n2U  axn + i pn\nmvb w01x2 = 0  \n \n(9.47)\n \n A\nmv\n2U  ax +\nU\nmv\n  d\ndxb w01x2 = 0, \nwith the result\n \nd\ndx\n w01x2 = -  mv\nU\n xw01x2. \n(9.48)\nWe now have a ﬁrst-order differential equation for the ground state wave function. This equation tells \nus that we want a function whose derivative is equal to the function itself times a constant and x. We \nknow that the derivative of the exponential function ex is itself, so to get the extra factor of x we need \nan x2 in the exponent. To get the multiplicative factor correct, the function must be e-mvx2>2U. The prop-\nerly normalized solution to Eq. (9.48) is (Problem 9.3)\n \nw01x2 = amv\np Ub\n1>4\ne-mvx2>2U, \n(9.49)\nwhich is a Gaussian function. This ground state wave function is plotted in Fig. 9.5(a). The wave func-\ntion has a single antinode as we expect for the ground state. A classical particle with the same energy \n(U v>2) has classical turning points at {x0 where the energy is all potential energy:\n \n 1\n2 U v = 1\n2 mv2x2\n0\n \n \n x0 = B\nU\nmv.\n \n(9.50)\n(a)\n\n4 \n2\n2\n4\nx/x0\nΨ(x)\nn \u0004\u00070\n(b)\nx/x0\nΨ(x)\n\n4 \n2\n2\n4\nn \u0004\u00071\n(c)\nx/x0\nΨ(x)\n\n4 \n2\n2\n4\nn \u0004\u00072\nFIGURE 9.5 Energy eigenstate wave functions for the ﬁrst three states of the harmonic oscillator. \nThe dashed lines enclose the classically allowed region.\n\n286 \nHarmonic Oscillator\nFrom the plot in Fig. 9.5(a), you see that there is a ﬁnite probability that the particle is in the classically \nforbidden region beyond {x0 (Problem 9.4).\nTo ﬁnd the other energy eigenstates we act on the ground state with the raising operator. But \nwe have already mentioned that the ladder operators do not preserve the normalization of the energy \neigenstates, so we must determine the proper scaling factor. Let’s ﬁrst look at the lowering operator. \nConsider the norm of the state a0 n9. The rules of Hermitian conjugation allows us to write the norm as\n \n@  a 0 n9@\n2\n= 18n0 a-21a0 n92 = 8n0 a-a0 n9. \n(9.51)\nThe product a-a is the number operator N, so we get\n \n @  a 0 n9@\n2\n= 8n0 N0 n9 = 8n0 n0 n9 = n8n0 n9 \n \n = n,\n \n(9.52)\nwhere we have used the normalization 18n0 n9 = 12 of the energy/number eigenstates 0 n9. Let c be \nthe proportionality factor between the state a0 n9 and the eigenstate 0 n - 19:\n \na 0 n9 = c0 n - 19. \n(9.53)\nBecause both 0 n9 and 0 n - 19 are normalized to unity, we can use Eq. (9.52) to ﬁnd the constant c:\n \n @  a 0 n9@\n2\n= @  c0 n - 19@\n2\n \n \n n = 0 c0\n2.\n \n(9.54)\nBy convention, we choose the proportionality constant to be real and positive (an overall phase is not \nmeasurable) and obtain\n \na 0 n9 = 1n0 n - 19  . \n(9.55)\nLikewise you can show that the raising operator equation is (Problem 9.5)\n \na-0 n9 = 2n + 10 n + 19  . \n(9.56)\nA simple mnemonic to remember which operator gives which factor (n or n + 1) in Eqs. (9.55) and \n(9.56) is that the index under the square root is the larger value of the two eigenstates on the two sides \nof the equations. The different scale factors in Eqs. (9.55) and (9.56) are a reﬂection of the asymmetry \nof the raising and lowering operations that is embodied in the commutator relation in Eq. (9.22).\nTo generate states above the ground state we use Eq. (9.56) to formulate the raising operator \nequation\n \n0 n + 19 =\n1\n2n + 1\n a-0 n9. \n(9.57)\nApply Eq. (9.57) to the ground state and the resulting states to obtain\n \n0 19 =\n1\n21\n a-0 09\n \n0 29 =\n1\n22\n a-0 19 =\n1\n22 # 1\n 1a-2\n20 09\n \n(9.58)\n \n0 39 =\n1\n23\n a-0 29 =\n1\n23 # 2 # 1\n 1a-2\n30 09\n\n9.3 Wave Functions \n287\nand generalize to ﬁnd\n \n0 n9 =\n1\n2n!\n 1a-2\nn0 09. \n(9.59)\nProjected onto the spatial basis, this general result is\n \nwn(x) =\n1\n1n!\n c A\nmv\n2U\n ax -\nU\nmv d\ndxb d\nn\n w0 1x2. \n(9.60)\nExample 9.1 Use the eigenstate generating expression in Eq. (9.60) to determine the ﬁrst excited \nstate of the harmonic oscillator.\nTake Eq. (9.60) and set n = 1, which means that the raising operator acts only one time to \nyield the ﬁrst eigenstate above the ground state:\n \n w11x2 =\n1\n21!\n c A\nmv\n2 U  ax -\nU\nmv d\ndxb d w01x2\n \n = c A\nmv\n2 U  ax -\nU\nmv d\ndxb d amv\np U b\n1>4\ne-mvx2>2U \n \n = amv\np U b\n1>4\nA\nmv\n2 U  c ax -\nU\nmv a- mv\nU\n xbb d e-mvx2>2U\n \n(9.61)\n \n = amv\np U b\n1>4\nA\nmv\n2 U\n 12 x2 e-mvx2>2U. \nThis result is already normalized. Note that the wave function dimensions are 1/2length to ensure \nthat the normalization condition is dimensionless.\nThe general wave function expression in Eq. (9.60) can be difﬁcult to use in practice because it \nrequires n derivatives. When we apply the raising operator to the Gaussian function in w01x2 n times, \nwe obtain the Gaussian function multiplied by a polynomial of order n. The resultant polynomials are \nHermite polynomials. To simplify the general wave function expression, it is common to write the \nharmonic oscillator wave functions in terms of a dimensionless variable\n \nj K A\nmv\nU\n x. \n(9.62)\nIn this case, the ground state and the general states are written as\n \nw0 1x2 = amv\np U b\n1>4\ne-  j2>2 \n(9.63)\nand\n \nwn (x) = amv\np U b\n1>4\n1\n22nn!\n Hn (j) e-  j2>2. \n(9.64)\n\n288 \nHarmonic Oscillator\nThe ﬁrst several Hermite polynomials Hn 1j2 are:\n \n H0 1j2 = 1\n \n \n H1 1j2 = 2j\n \n \n H2 1j2 = 4j2 - 2\n \n \n(9.65)\n \n H3 1j2 = 8j3 - 12j\n \n \n H4 1j2 = 16j4 - 48j2 + 12. \nYou can easily verify that for n = 1, Eq. (9.64) agrees with the result we found in Example 9.1.\nThe ﬁrst three harmonic oscillator energy eigenstate wave functions are plotted in Fig. 9.5. As \nwe expected, the harmonic oscillator energy eigenstates are similar in many ways to the energy eigen-\nstates of the other bound state systems we have studied—the inﬁnite and ﬁnite square wells and the \nhydrogen atom. On a superﬁcial level, we can consider each of these bound state systems as a particle-\nin-a-box system—the boxes just have different shapes. Common features of these energy eigenstates \nare (1) the wave functions are oscillatory inside the well and exponential decaying outside the well, \nwhere the edge of the well is deﬁned by the classical turning points; (2) the wave functions of sym-\nmetric wells are alternately even and odd with respect to inversion about the center of the well, reﬂect-\ning the spatial symmetry of the well; and (3) the number of nodes and antinodes in the wave function \nincreases with energy.\nAs we have done in the previous bound state problems, we combine the schematic diagrams \ndepicting (i) the potential energy well, (ii) the energy spectrum, and (iii) the energy eigenstates in a \nsingle uniﬁed diagram, shown in Fig. 9.6(a). This single diagram is commonly used to represent the \npotential energy well problem and its quantum mechanical solution. In this uniﬁed schematic diagram, \nthe vertical scale measures energy (i and ii) or wave function (iii), and the zero of each wave function \nis placed at the corresponding energy level of that state in the well.\nThe spatial probability density is given by the absolute square of the wave function\n \nPn1x2 = 0 wn1x20\n2. \n(9.66)\n(a)\n(b)\nn=0\nn=1\nn=2\nn=3\nx\nE,Ψ\nx\nE,\u0002Ψ\u00022\nFIGURE 9.6 Energy eigenstate (a) wave functions and \n(b) probability densities of the harmonic oscillator.\n\n9.4 Dirac Notation \n289\nIn Fig. 9.6(b) we plot the probability densities of the ﬁrst four energy eigenstates in a uniﬁed diagram. \nThe ground state probability density is largest at the center of the well, but the location of the prob-\nability density maximum gets increasingly close to the classical turning points as the energy level \nincreases. The probability density for a large value of the quantum number n is shown in Fig. 9.7. For \nsuch a high energy state, the probability density is similar, at least when locally averaged, to the prob-\nability distribution of a classical harmonic oscillator.\nLet’s summarize how the harmonic oscillator illustrates the ﬁrst three basic postulates of quan-\ntum mechanics. The ﬁrst postulate tells us that quantum states are represented by kets, such as the \nenergy eigenstates 0 n9 \u0003 wn1x2. The second postulate tells us that observables are represented by \noperators, such as the Hamiltonian H, the position xn and the momentum pn. The third postulate tells us \nthat the eigenvalues of an operator are the only possible results of measurements, such as the energies \nEn = 1n + 1>22U v.\n9.4 \u0002 DIRAC NOTATION\nLet’s use the harmonic oscillator problem as a framework for reviewing Dirac notation. We use the \nDirac kets 0 n9 to represent the energy eigenstates. Recall that the labeling of the kets does not affect \nthe properties of the kets, so we are free to use whatever labeling is most convenient. The convention \nis to be as brief as possible without losing speciﬁcity. We label the harmonic oscillator energy eigen-\nstates states with the energy eigenvalue index n alone, but it is also common for w or c to be used as a \nlabel with the eigenvalue index as a subscript. Or one could use the energy value itself. These are all \nequally valid notations:\n \n 0 n9 = 0 wn9 = 0 En9 = @1n + 1\n22U\n v9 \n \n 0 09 = 0 w09 = 0 E09 = @ 1\n2  U\n v9.\n \n(9.67)\n\nxcl\nxcl\nx\n\u0002\u0010n(x)\u00022\nFIGURE 9.7 Quantum mechanical probability density for the n = 30 state. The \nclassical probability distribution (thin line) peaks at the classical turning points.\n\n290 \nHarmonic Oscillator\nIn Section 9.3, we found the energy eigenstate wave functions wn1x2. The connection between wave \nfunctions and abstract kets is expressed as\n \nwn1x2 = 8x0 n9. \n(9.68)\nIn words, Eq. (9.68) says that the wave function wn1x2 is the projection of the abstract ket 0 n9 onto the \nposition eigenstates 0 x9. Or using the representation notation\n \n0 n9 \u0003 fn1x2, \n(9.69)\nwe say that wn1x2 is the representation of the quantum state 0 n9 in the position representation.\nThe energy eigenstates of the harmonic oscillator obey the three important properties that we \nhave discussed previously: normalization, orthogonality, and completeness. The normalization condi-\ntion is expressed in wave function notation as\n \nL\n\u0005\n- \u0005\n0 wn1x2 0\n2dx = 1 \n(9.70)\nor in Dirac notation as\n \n8n0 n9 = 1. \n(9.71)\nThe connection between the normalization condition in the position representation [Eq. (9.70)] and \nin abstract Dirac notation [Eq. (9.71)] is evident if we use the completeness relation for the position \neigenstates, which form a complete continuous basis:\n \nL\n\u0005\n- \u0005\n0 x98x0 dx = 1. \n(9.72)\nBecause the right hand side of Eq. (9.72) is the unity operator, it can be inserted into an expression \nwithout altering the value of the expression. Inserting Eq. (9.72) into Eq. (9.71) yields\n \n 1 = 8n0 n9\n \n \n = 8n0 e\nL\n\u0005\n- \u0005\n0 x98x0 dx f 0 n9 \n \n =\nL\n\u0005\n- \u0005\n8n0 x98x0 n9dx\n \n \n(9.73)\n \n =\nL\n\u0005\n- \u0005\nw*\nn 1x2wn 1x2 dx\n \n \n =\nL\n\u0005\n- \u0005\n0 wn1x2 0\n2\n dx , \nwhich shows that Eq. (9.70) and Eq. (9.71) are equivalent.\nThe energy eigenstates of the harmonic oscillator are orthogonal because they are the eigenvec-\ntors of an Hermitian operator. The orthogonality condition is expressed in wave function notation as\n \nL\n\u0005\n- \u0005\nw*\nm1x2wn1x2dx = dmn  \n(9.74)\n\n9.4 Dirac Notation \n291\nor in Dirac notation as\n \n8m0 n9 = dmn\n . \n(9.75)\nBy using a Kronecker delta, the orthogonality condition also includes the normalization, so Eqs. (9.74) \nand (9.75) are called the orthonormality condition. You can check that the harmonic oscillator energy \neigenstate wave functions are orthogonal by doing the explicit integrals in Eq. (9.74) (Problem 9.7).\nThe harmonic oscillator eigenstates form a complete discrete basis, which is expressed in terms \nof the closure relation\n \na\n\u0005\nn=0\n0 n98n0 = 1, \n(9.76)\nwhere the right hand side is the unity operator. Completeness of the energy basis means that any \narbitrary state vector 0 c9 can be written in terms of the energy eigenstates, either in wave function \nnotation\n \n0 c9 \u0003\n c1x2 = a\n\u0005\nn=0\ncn wn 1x2 \n(9.77)\nor in Dirac notation\n \n0 c9 = a\n\u0005\nn=0\ncn0 n9. \n(9.78)\nTo ﬁnd the value of a particular expansion coefﬁcient, we use the closure relation in Eq. (9.76) to \nrewrite the state 0 c9 in terms of the energy eigenstates:\n \n 0 c9 = 10 c9\n \n \n = e a\n\u0005\nn=0\n0 n98n0 f 0 c9 \n(9.79)\n \n = a\n\u0005\nn=0\n0 n98n0 c9.\n \nBy comparing Eqs. (9.78) and (9.79), we conclude that the expansion coefﬁcient cn is the projection of \nthe wave function 0 c9 onto the particular basis state 0 n9, which in Dirac notation is\n \ncn = 8n0 c9 \n(9.80)\nand in wave function notation is\n \ncn =\nL\n\u0005\n- \u0005\nw*\nn 1x2c1x2dx. \n(9.81)\nThe normalization requirement on the general state 0 c9 \u0003  c1x2 in wave function notation is\n \nL\n\u0005\n- \u0005\n0 c1x2 0\n2dx = 1 \n(9.82)\n\n292 \nHarmonic Oscillator\nor in Dirac notation is\n \n8c0 c9 = 1. \n(9.83)\nWe can also use the energy eigenstate closure relation Eq. (9.76) to write the normalization require-\nment in terms of the eigenstate expansion\n \n 1 = 8c0 c9 = 8c0 e a\n\u0005\nn=0\n0 n98n0 f 0 c9\n \n = a\n\u0005\nn=0\n8c0 n98n0 c9 = a\n\u0005\nn=0\n08n0 c9 0\n2 \n(9.84)\n \n = a\n\u0005\nn=0\n0 cn0\n2.\nThe square of each expansion coefﬁcient is the probability that the state 0 c9is measured to be in state \n0 n9, that is, to have energy eigenvalue En:\n \nPEn = 08n0 c9 0\n2 = 0 cn0\n2. \n(9.85)\nThus the requirement that the state be normalized is a requirement that the total probability sum to \nunity, (i.e., there is unit probability that some value of energy is measured).\nExample 9.2 A quantum mechanical harmonic oscillator is in the state\n \n0 c9 = 12\n4 0 09 + i 2\n4 0 19 - i 1\n4 0 29 + 3\n4 eip>30 39. \n(9.86)\nWhat are the possible results of an energy measurement and with what probabilities do they occur? \nFind the expectation value of the energy.\nThe possible results of an energy measurement are the energy eigenvalues En = 1n + 1\n22U v. \nFor this superposition of four energy eigenstates, the probabilities calculated from Eq. (9.85) are \nzero except for the four energies E0, E1, E2, and E3. These probabilities are\n \nPE0 = 0800 c9 0\n2 = @H0@A12\n4 @0I + i 2\n4@1I - i 1\n4@2I + 3\n4 eip>3@3IB @\n2\n= @12\n4 @\n2\n=\n2\n16\n \nPE1 = 0810 c9 0\n2 = @H1@A12\n4 @0I + i 2\n4@1I - i 1\n4@2I + 3\n4 eip>3@3IB @\n2\n= @ i 2\n4@\n2 =\n4\n16\n \n(9.87)\n \nPE2 = 0820 c9 0\n2 = @H2@A12\n4 @0I + i 2\n4@1I - i 1\n4@2I + 3\n4 eip>3@3IB @\n2\n= @ -i 1\n4@\n2\n=\n1\n16\n \nPE3 = 0830 c9 0\n2 = @H3@A12\n4 @0I + i 2\n4@1I - i 1\n4@2I + 3\n4 eip>3@3IB @\n2\n= @3\n4 eip>3@\n2\n=\n9\n16.\nThe expectation value of the energy is\n \n 8E9 = a\n\u0005\nn=0\nEnPEn = 11\n2 U v2 2\n16 + 13\n2 U v2 4\n16 + 15\n2 U v2 1\n16 + 17\n2 U v2 9\n16\n \n= 41\n16 U v _ 2.56 U v.\n  \n(9.88)\n\n9.5 Matrix Representations \n293\nThe expectation value can also be calculated as 8E9 = 8c0 H0 c9, with the same result. A his-\ntogram of the energy measurements is shown in Fig. 9.8.\nLet’s continue the summary of how the harmonic oscillator illustrates the basic postulates of \nquantum mechanics. The fourth postulate tells us that the probability of a measurement is the complex \nsquare of the projection onto the measured eigenstate, such as the energy probability PEn = 08n0 c9 0\n2 \nor the position probability density P1x2 = 0 c1x2 0\n2. The ﬁfth postulate tells us that the quantum state \nvector after a measurement is the measured eigenstate, such as 0 c9 collapsing to 0 n9 after the energy \nEn is measured. The sixth postulate tells us how to ﬁnd the time evolution of states, which we’ll \naddress in Section 9.8.\n9.5 \u0002 MATRIX REPRESENTATIONS\nSo far we have presented the operators and states of the harmonic oscillator in abstract Dirac nota-\ntion and in wave function or position representation. However, we found a matrix representation to \nbe useful previously, for example in the discussion of spin states. Can we use a matrix representation \nfor the harmonic oscillator case? It turns out that we can. A matrix representation is a collection of \nnumbers that represents states and operators in terms of a chosen basis set. So we must ﬁrst choose \na basis for the matrix representation. We have just solved for the energy basis states of the harmonic \noscillator, so that choice seems reasonable, especially in light of the importance of the energy basis in \nthe Schrödinger time evolution recipe. So how do we ﬁnd the numbers we need to represent states and \noperators as matrices in the energy representation? We do it by inspection!\nWe learned in Section 2.1 that an operator is always diagonal in its own basis, and eigenvec-\ntors are unit vectors in their own basis. So the Hamiltonian is diagonal in the energy basis and the \nenergy eigenstates are unit vectors in the energy basis. The diagonal elements of the Hamiltonian \nare the energy eigenvalues, so by inspection of our energy result in Eq. (9.36), we ﬁnd the Ham-\niltonian matrix\n \nH \u0003 •\n1\n2 U v\n0\n0\n0\ng\n0\n3\n2 U v\n0\n0\ng\n0\n0\n5\n2 U v\n0\ng\n0\n0\n0\n7\n2 U v\ng\nf\nf\nf\nf\nf\nμ,  \n(9.89)\nFIGURE 9.8 Histogram of energy measurements.\nE\n0.5\n1.0\nP\nPE0\nPE1\nPE2\nPE3\n\n294 \nHarmonic Oscillator\nwhere we use the convention of ordering the rows and columns starting with the ground state energy. \nThere are an inﬁnite number of energy eigenstates, so the matrix representation of the Hamiltonian is \ninﬁnite, but discrete. In this matrix representation, the energy basis states are the unit vectors\n \n0 09 \u0003 •\n1\n0\n0\n0\nf\nμ, 0 19 \u0003 •\n0\n1\n0\n0\nf\nμ, 0 29 \u0003 •\n0\n0\n1\n0\nf\n μ, g. \n(9.90)\nThat’s all there is to it!\nFinding the matrix representation of other states and operators takes more work, but not too much. \nWe already found the expansion coefﬁcients cn = 8n0 c9 required to represent an arbitrary state 0 c9 \nin terms of the energy eigenstates in Eq. (9.80), now we just order them in a column vector:\n \n0 c9 \u0003 •\nc0\nc1\nc2\nc3\nf\nμ  . \n(9.91)\nThe matrix representation of other operators requires us to know how they act upon the energy eigen-\nstates. For the ladder operators, we learned this in Eqs. (9.55) and (9.56):\n \n a0 n9 = 1n0 n - 19\n \n a-0 n9 = 2n + 10 n + 19.\n \n(9.92)\nTo ﬁnd the individual matrix elements of the ladder operators, project each of these equations onto a \ndifferent eigenstate to obtain\n \n 8m0 a0 n9 = 8m0 1n0 n - 19  8m0 a-0 n9 = 8m0 2n + 10 n + 19 \n \n = 1n dm, n-1  \n = 2n + 1dm, n+1 .\n \n(9.93)\nBecause the ladder operators take one state to an adjacent state, the matrix elements connect only adja-\ncent states, as the Kronecker deltas indicate. Hence, the matrices for the ladder operators are\n \na \u0003 •\n0\n21\n0\n0\ng\n0\n0\n22\n0\ng\n0\n0\n0\n23\ng\n0\n0\n0\n0\ng\nf\nf\nf\nf\nf\nμ       a- \u0003 •\n0\n0\n0\n0\ng\n21\n0\n0\n0\ng\n0\n22\n0\n0\ng\n0\n0\n23\n0\ng\nf\nf\nf\nf\nf\nμ.  (9.94)\nNote that these operators are dimensionless, as expected. They are each nondiagonal and they are not \nHermitian. However, they are Hermitian conjugates of each other, as required by their deﬁnitions.\n\n9.5 Matrix Representations \n295\nThe ladder operators were deﬁned in Eqs. (9.15) and (9.16) in terms of the position and momen-\ntum operators. Hence, the position and momentum operators are related to the ladder operators by\n \nxn = B\nU\n2mv 1a- + a2  \n \npn = iB\nUmv\n2  1a- - a2\n \n(9.95)\nand their matrix representations are\n \nxn \u0003 B\nU\n2mv •\n0\n11\n0\n0\ng\n11\n0\n12\n0\ng\n0\n12\n0\n13\ng\n0\n0\n13\n0\ng\nf\nf\nf\nf\nf\nμ  pn \u0003 B\nUmv\n2  •\n0\n-i11\n0\n0\ng\ni11\n0\n-i12\n0\ng\n0\ni12\n0\n-i13 g\n0\n0\ni13\n0\ng\nf\nf\nf\nf\nf\nμ.\n \n(9.96)\nThese matrices are Hermitian, as they must be because position and momentum are observables. The \nposition and momentum matrix elements connect only adjacent states, but in this case, states above and \nbelow. This is important in determining the selection rules for transitions, as discussed in Section 9.8. \nThe position and momentum matrices are both nondiagonal in the energy basis, so they do not com-\nmute with the Hamiltonian.\nExample 9.3 Find the expectation value of position in the ground state of the harmonic oscillator.\nThere are three ways to calculate this.\n(1) The expectation value of position is the matrix element\n \n8xn9 = 8c0 xn 0 c9. \n \n(9.97)\nThe expectation value of position in the ground state is the speciﬁc matrix element\n \n8xn9 = 800 xn 0 09 = xn00 \n(9.98)\nwhich is zero by inspection of the position matrix in Eq. (9.96). \n(2) We can also calculate the expectation value using explicit Dirac notation and the ladder \noperators [Eq. (9.95)]:\n \n 8xn9 = 800 xn 0 09\n \n = B\nU\n2mv\n 8001a- + a20 09.\n \n(9.99)\nThe raising operator acting on the state 0 09 produces the state 0 19 in the ﬁrst term and the lowering \noperator acting on the state 0 09 yields the value 0 in the second term. The result\n \n 8xn9 = B\nU\n2mv\n 3800 19 + 04 \n(9.100)\n \n = 0\n \n is again zero.\n\n296 \nHarmonic Oscillator\n(3) Finally, we can calculate the expectation value in the position representation by doing an \nintegral\n \n 8xn9 = 800 xn 0 09\n \n \n =\nL\n\u0005\n- \u0005\nw*\n01x2xw01x2dx \n(9.101)\n \n =\nL\n\u0005\n- \u0005\nx0 w01x2 0\n2dx.\n \nThis integral is zero because the probability density is spatially symmetric (even) about the origin \nand the function x is antisymmetric (odd) about the origin, yielding an antisymmetric (odd) inte-\ngrand. The integral of an antisymmetric (odd) integrand over a symmetric (even) interval is zero.\nThis particular calculation is simple using any of these methods. More detailed calculations, such \nas the expectation value of the square of the position are most easily done using the operator method \nin Eq. (9.99) (Problem 9.9).\n9.6 \u0002 MOMENTUM SPACE WAVE FUNCTION\nThe matrices for position and momentum in Eq. (9.96) have the same form, with different constants \nto get the dimensions correct. This suggests some symmetry between the position and momentum \nrepresentation that does not exist in the other bound state problems we have solved. To explore \nthis symmetry, let’s ﬁnd the momentum space representation (see Section 6.1.2) of the energy \neigenstates 0 n9 \u0003 fn1p2 = 8p0 n9. There are three ways we can ﬁnd the momentum space wave \nfunctions.\n(1) We can take the same operator approach we used above to ﬁnd the position representation \nwave functions. We start with the ladder termination equation\n \na0 09 = 0 \n(9.102)\nand express this in the momentum representation, where the position operator is a derivative with \nrespect to momentum and the momentum operator is a multiplicative factor:\n \n a f01p2 = 0  \n \n A\nmv\n2U\n axn + i pn\nmvbf01p2 = 0  \n(9.103)\n \n A\nmv\n2U\n aiU d\ndp +\ni\nmv\n pbf01p2 = 0. \nThis leads to a differential equation\n \nd\ndp\n f01p2 = -  1\nmvU\n pf01p2 \n(9.104)\n\n9.6 Momentum Space Wave Function \n297\nthat has the same form as the differential equation for the ground state wave function in the position \nrepresentation [see Eq. (9.48)]. It is a ﬁrst-order differential equation whose solution is a Gaussian \nfunction [see Eq. (9.49)], but in this case, momentum is the argument of the function. Hence, the prop-\nerly normalized ground state energy eigenstate in the momentum representation is (Problem 9.18):\n \nf01p2 = a\n1\npmvUb\n1>4\ne-  p2>2mvU. \n(9.105)\nThe excited states can be found with the raising operator approach that we used in the position rep-\nresentation. The momentum representation result includes the same Hermite polynomials as in the \nposition representation:\n \nfn1p2 = a\n1\npmvUb\n1>4\n1\n22nn!\n Hna\np\n2mvU\nbe-  p2>2mvU. \n(9.106)\n(2) We can also go back to the energy eigenvalue equation and express it in the momentum repre-\nsentation. In this case, we get a second-order differential equation in momentum space\n \n H0 n9 = En0 n9\n \n \n 1\n2m\n 3 pn 2 + m2v2xn 24@ n9 = En0 n9\n \n \n 1\n2m\n c  p2 - m2v2U2 d 2\ndp2dfn1p2 = Enfn1p2\n \n(9.107)\n \n -  mv2U2\n2\n \nd\n 2fn1p2\ndp2\n+\n1\n2m\n p2fn1p2 = Enfn1p2. \nOnce again this differential equation has the same form as the spatial differential equation [see \nEq. (9.45)] and leads to the momentum space solutions in Eq. (9.106) with the same functional \ndependence as the position representation solutions [Eq. (9.64)] with momentum as the argument \nrather than position.\n(3) We can transform the position representation solutions to the momentum representation using \nthe Fourier transform. In Chapter 6, we found that the position representation wave function and the \nmomentum representation wave function are connected by the Fourier transform\n \nf1p2 =\n1\n22pU L\n\u0005\n- \u0005\nc1x2e-ipx>Udx. \n(9.108)\nBecause the Fourier transform of a Gaussian function is another Gaussian function, the ground state \nmomentum space wave function is the Gaussian function in Eq. (9.105). The Fourier transform of an \nHermite polynomial times a Gaussian function is also an Hermite polynomial times a Gaussian func-\ntion, so the excited states are given by Eq. (9.106).\nThus we ﬁnd the interesting result that the momentum space wave functions representing the \nenergy eigenstates have the same functional dependence on momentum as the position representation \nwave functions have on position. This similarity is visible in the momentum space probability density \nfor one particular energy eigenstate shown in Fig. 9.9. In this case, the limits {pn = {212n + 12mvU \nrepresent the limits of the classical momentum for a particle with energy En = 1n + 1>22U v. Note \nthe similarity with Fig. 9.7.\n\n298 \nHarmonic Oscillator\n9.7 \u0002 THE UNCERTAINTY PRINCIPLE\nThe Heisenberg uncertainty principle places a lower limit on the product of the uncertainties of posi-\ntion and momentum\n \n\u0006x\u0006p Ú U\n2, \n(9.109)\nwhere the quantum mechanical uncertainties are deﬁned as the standard deviations\n \n \u0006x = 481x - 8x9229 = 48x29 - 8x92\n \n \u0006p = 481p - 8p9229 = 48p29 - 8p92. \n \n(9.110)\nNow that we know the position and momentum probability distributions, these uncertainties are \nstraightforward to calculate by integration or by operator methods. For the ground state of the har-\nmonic oscillator, these uncertainties can be found by inspection because the Gaussian functional form \nof the ground state wave function is a standard probability function.\nThe standard way of writing a Gaussian function for use in probability analysis is\n \nf 1x2 =\n1\n22ps\n e-  1x-x22>2s2. \n(9.111)\nwhere x is the mean or average of the distribution and s is the standard deviation of the distribution. \nFor the harmonic oscillator ground state, the spatial probability density distribution is\n \nP01x2 = 0 w01x2 0\n2 = A\nmv\npU\n e-  mvx 2>U. \n(9.112)\n\npn\npn\np\n\u0002Φn(p)\u00022\nFIGURE 9.9 Momentum space probability density for the n = 30 harmonic \noscillator state.\n\n9.7 The Uncertainty Principle \n299\nThis is identical to the standard form in Eq. (9.111). By comparing the quantum mechanical probabil-\nity density in Eq. (9.112) and the standard probability expression in Eq. (9.111), we ﬁnd by inspection \nthat the mean and standard deviation are\n \n x = 0\n \n \n s = B\nU\n2mv.\n \n(9.113)\nThe mean or average is what we call the expectation value 8x9 in quantum mechanics, and the stan-\ndard deviation is the quantum mechanical uncertainty \u0006x. Hence, we have the results for the ground \nstate\n \n 8x9 = 0\n \n \n \u0006x = B\nU\n2mv.\n \n(9.114)\nWe already found that the expectation value 8x9 is zero in Example 9.3, and now we have found the \nuncertainty \u0006x by inspection.\nThe momentum probability density distribution also has a Gaussian form for the harmonic oscil-\nlator ground state\n \nP01p2 = \u0004f01p2 0\n2 = A\n1\npmvU\n e-  p2>mvU. \n(9.115)\nIf we also compare this to the standard Gaussian function, we ﬁnd by inspection that the expectation \nvalue 8p9 and the uncertainty \u0006p are \n \n 8p9 = 0\n \n \u0006p = B\nmvU\n2\n.\n \n(9.116)\nWe expect the expectation value 8p9 to be zero, based upon inspection of the momentum matrix in \nEq. (9.96).\nWe can now check that the uncertainty principle is obeyed. Using the results in Eqs. (9.114) and \n(9.116), we obtain\n \n\u0006x\u0006p = B\nU\n2mvB\nmvU\n2\n= U\n2.  \n(9.117)\nNot only is the uncertainty principle obeyed, but the uncertainty product has its minimum value, so we \nrefer to the harmonic oscillator ground state as a minimum uncertainty state. We found in Chapter 6 \nthat the Gaussian wave packet for a free particle is also a minimum uncertainty state. However, the \nfree particle wave packet evolves with time in a way that causes it to spread out in space, and so it \nis only a minimum uncertainty state at one time. The harmonic oscillator ground state is an energy \neigenstate and so its time evolution produces only a multiplicative overall phase factor, which does not \nchange the probability density in position or momentum space. Hence, the harmonic oscillator ground \nstate remains a minimum uncertainty state for all time. The shape of the harmonic oscillator potential \nenergy well is just right to counter the spreading of the wave packet.\n\n300 \nHarmonic Oscillator\n9.8 \u0002 TIME DEPENDENCE\nNow let’s study some examples of time dependence in the harmonic oscillator. These examples dem-\nonstrate the manifestation of the sixth postulate regarding Schrödinger time evolution. They also illus-\ntrate the power of the operator approach for the harmonic oscillator, in contrast with the wave function \napproach. A general state of the system is expressed as a superposition of energy eigenstates\n \n0 c1029 = a\nq\nn=0\ncn0 n9. \n(9.118)\nIn the energy basis, the Schrödinger time evolution recipe tells us that the time dependence is found by \nmultiplying each energy eigenstate coefﬁcient by an energy dependent phase factor, giving:\n \n 0 c1t29 = a\nq\nn=0\ncne-  i En t>U0 n9 \n \n = a\nq\nn=0\ncne-  i 1n+ 1\n22vt0 n9  \n(9.119)\n \n = e-  i vt>2 a\nq\nn=0\ncne-invt0 n9.\n \nThus we see that each successive term acquires an additional relative phase of e-ivt from the \nSchrödinger time evolution.\nExample 9.4 A harmonic oscillator system starts in an equal superposition of the ground state \nand the ﬁrst excited state\n \n0 c1029 =\n1\n12 0 09 +\n1\n12 0 19. \n(9.120)\nFind the probability as a function of time of measuring the system to have energy U v>2, the prob-\nability density as a function of time, and the expectation value of position.\nThe time-evolved state function is found from the Schrödinger recipe:\n \n0 c1t29 = e-  i vt>2 C 1\n12@0I +\n1\n12e-  ivt @1ID, \n(9.121)\nwhere we factor out the common phase because only the relative phase is important. The probability \nof ﬁnding the oscillator in the ground state is\n \nP0 = @80@ c1t29@\n2 = @H0@e-  i vt>2 C 1\n12@0I +\n1\n12 e-  ivt@1ID@\n2\n \n= @e-  i vt>2 1\n12 H0@0I + e-  i vt>2 1\n12 e-  ivt H0@1I@\n2\n \n(9.122)\n \n= @ e-  i vt>2 1\n12 @\n2\n \n \n = 1\n2. \nThis probability is time independent, as is the probability of making any particular measurement of \nthe energy. This is why we refer to energy states as stationary states.\n\n9.8 Time Dependence \n301\nThe spatial probability density of this two-state superposition is\n \nP1x, t2 = 08x0 c1t29 0\n2 = 0 c1x, t2 0\n2 = @Hx@e-  i vt>2 C 1\n12@0I +\n1\n12 e-  ivt @1ID@\n2\n \n \n= 1\n2 0 w01x2 + e-  ivtw11x2 0\n2 \n(9.123)\n \n= 1\n23 0 w01x2 0\n2 + 0 w11x2 0\n2 + w01x2w*\n11x2  e+ivt + w*\n01x2w11x2e-ivt4.\nIf the position is measured, then the result is time dependent because the position operator does \nnot commute with the Hamiltonian. For the harmonic oscillator, the wave functions are real and \nEq. (9.123) simpliﬁes to\n \nP1x, t2 = 1\n2\n 3w2\n01x2 + w2\n11x2 + 2w01x2w11x2cos vt4. \n(9.124)\nThis probability density oscillates with time, as depicted in the animation frames shown in \nFig. 9.10, where the constant t is the oscillation period t = 2p/v of the harmonic oscillator (see \nthe activity on time evolution of harmonic oscillator states). The probability distribution of this \nsuperposition sloshes back and forth in the well.\nWe calculate the expectation value of the position using the raising and lowering operators \n[see Eq. (9.99)]\n \n 8xn9 = 8c1t2 0 xn 0 c1t29 = B\nU\n2mv\n 8c1t2 0 a- + a0 c1t29\n \n = B\nU\n2mv\n  C 1\n12 H0@ +\n1\n12 e+ivt H1@D Aa- + aBC 1\n12@0I +\n1\n12 e-ivt@1ID\n \n = 1\n2B\nU\n2mv\n 380@1a- + a2@19e-ivt + 81@1a- + a2@ 09e+ivt4\n \n = 1\n2B\nU\n2mv\n 3180@  a-@ 19 + 80@  a @ 192e-ivt + 181@  a-@ 09 + 81@  a @ 092e+ivt4. \n(9.125)\nSuperposition of n\u00040 and n\u00041 states\nt/Τ \u0004 0.0\nt/Τ \u0004 0.1\nt/Τ \u0004 0.2\nt/Τ \u0004 0.3\nt/Τ \u0004 0.4\nt/Τ \u0004 0.5\nFIGURE 9.10 Time dependence of the spatial probability density for the superposition state com-\nposed of equal probabilities of n \u0003 0 and 1 states. The frames represent half of the oscillation period t.\n\n302 \nHarmonic Oscillator\nEach matrix element is found using the ladder operator matrix elements in Eq. (9.93) or by inspec-\ntion of the matrix in Eq. (9.94), yielding\n \n 8xn9 = 1\n2B\nU\n2mv CAH0@2I 22 + H0@0I 21Be -ivt + AH1@1I 21 + 0Be +ivtD \n \n = 1\n2B\nU\n2mv\n 3e -ivt + e +ivt4\n \n(9.126)\n \n = B\nU\n2mv cos vt.\nHence, the expectation value of position oscillates with time, which is evident in the animation \nframes shown in Fig. 9.10. This calculation of matrix elements was simpliﬁed greatly by using the \nladder operators. If we were to use wave functions, then we would need to calculate spatial inte-\ngrals. The moral of the story is: use the ladder operators wherever you can and do not do an integral \nif you don’t have to.\nNow consider a general two-state superposition, such as\n \n0 c1029 = cm0 m9 + cn0 n9. \n(9.127)\nIn this case, the expectation value of x is equal to zero if the states 0 m9 and 0 n9 that comprise the \nsuperposition are not adjacent energy states because the xn matrix [Eq. (9.96)] only connects adjacent \nstates. This means that a measurement of 8x9 can oscillate only at the frequency v, not at 2v, 3v, etc. \nThis result is similar to the classical oscillator where only a single harmonic is observed. Thus it is true \nin both quantum mechanics and classical mechanics that a linear oscillator has no higher harmonics. \nWe need nonlinearity in the restoring force to achieve anharmonicity and to observe other frequencies.\nNote, however, that the probability density does exhibit higher harmonics. For example, the \nprobability density of the state 0 c9 =\n1\n12 1 0 09 + 0 292  oscillates with time at the frequency 2v, but \nit does so in a manner that preserves the zero value of 8x9. As shown in the animation frames in \nFig. 9.11(a), the probability distribution “breathes” symmetrically such that 8x9 = 0. For the state \n0 c9 =\n1\n12 1 0 09 + 0 392, the probability distribution [Fig. 9.11(b)] has two lobes that pass through each \nother at frequency 3v, while preserving 8x9 = 0.\nThe presence of only a single Bohr frequency in the expectation value of the position is related to \nthe selection rule for transitions between energy levels. We know from Chapter 3 that the probability \nfor a system to make a transition is proportional to the matrix element of the interaction between the \ntwo states. Assuming that the bound particle has a charge q, then the relevant electric dipole  interaction \nis governed by the matrix element 8ni0 qxn 0 nf9 of the electric dipole operator dn = qxn. Because the \nmatrix for position connects only adjacent states, the matrix elements are\n \n8ni0 qxn 0 nf9 \f dni, nf {1 \n(9.128)\nand the selection rule for harmonic oscillator transitions is\n \n\u0006n = nf - ni = {1. \n(9.129)\nNow consider measurements of the momentum of a superposition state of the harmonic oscillator. \nThe similarity of the position and momentum operators means that the momentum probability distri-\nbution and the expectation value of momentum8p9show similar results to those for position obtained \n\n9.8 Time Dependence \n303\nabove (Problem 9.11). In particular, the expectation values of position and momentum follow Ehren-\nfest’s theorem (see Chapter 6), which tells us that expectation values obey classical laws. The classical \nrelation between position and momentum is p = mv = mdx>dt, so the quantum mechanical superpo-\nsition states obey the relation:\n \n8p9 = m d8x9\ndt . \n(9.130)\nThough the superposition state presented in Fig. 9.10 exhibits the classical behavior of Eq. (9.130), \nthe time evolution does not really “look” classical. Classically, we expect to see a well-localized “par-\nticle” oscillate between the turning points. We saw in Fig. 9.7 that higher energy states exhibit more \nclassical behavior, so we might ask if the time evolution would appear more classical if the states \n0 m9 and 0 n9 that comprise the superposition were higher in energy. An example of this is shown in \nFig. 9.12. The wave is more localized but now exhibits interference fringes that would not be expected \nfor a classical particle. One way to make the time evolution appear classical is to build a superposition \nstate known as a coherent state.\nA coherent state is a wave packet that moves within the quadratic harmonic oscillator potential \nin such a way that it retains its shape, unlike the two-state superpositions in Figs. 9.10, 9.11, and 9.12. \nWave packets in free space distort as they propagate, so this is a new phenomenon. The coherent state \nis an inﬁnite superposition of harmonic oscillator energy eigenstates with a particular choice ofampli-\ntudes and phases (hence the name coherent). The form of these coefﬁcients is not so important for now, \nbut what is interesting is that the wave function of the coherent state is identical to the ground state \nGaussian wave function, except that it is not centered at the origin. As shown in Fig. 9.13(a), this dis-\nplaced Gaussian state oscillates about the origin and does not change its shape (see the activity on time \nevolution of harmonic oscillator states). Because the ground state has a minimum uncertainty product, \nthe coherent states also minimize the uncertainty product and do so even as they move. Figure 9.13(b) \nshows that if we choose a displaced Gaussian wave packet with the wrong width it does not move with-\nout distortion. It remains a Gaussian, but changes it size (it breathes as it moves).\nSuperposition of n=0 and n=2 states\nSuperposition of n=0 and n=3 states\n(a)\n(b)\nt/Τ \u0004 0.0\nt/Τ \u0004 0.0\nt/Τ \u0004 0.0333\nt/Τ \u0004 0.05\nt/Τ \u0004 0.0667\nt/Τ \u0004 0.1\nt/Τ \u0004 0.2\nt/Τ \u0004 0.1\nt/Τ \u0004 0.133\nt/Τ \u0004 0.15\nt/Τ \u0004 0.167\nt/Τ \u0004 0.25\nFIGURE 9.11 Time dependence of the spatial probability density for the superposition states \ncomposed of equal probabilities of (a) the n = 0 and 2 states, and (b) the n = 0 and 3 states.\n\n304 \nHarmonic Oscillator\nCoherent State Superposition\nt/Τ\u0007= 0.0\nt/Τ\u0007= 0.1\nt/Τ\u0007= 0.2\nt/Τ\u0007= 0.3\nt/Τ\u0007= 0.4\nt/Τ\u0007= 0.5\nt/Τ\u0007= 0.0\nt/Τ\u0007= 0.1\nt/Τ\u0007= 0.2\nt/Τ\u0007= 0.3\nt/Τ\u0007= 0.4\nt/Τ\u0007= 0.5\nDisplaced Gaussian Superposition\n(a)\n(b)\nFIGURE 9.13 Time dependence of the spatial probability density \n(a) for a coherent state and (b) for a displaced Gaussian state that is  \nnot the ground state.\nSuperposition of n=19 and n=20 states\nt/Τ \u0004 0.0\nt/Τ \u0004 0.1\nt/Τ \u0004 0.2\nt/Τ \u0004 0.3\nt/Τ \u0004 0.4\nt/Τ \u0004 0.5\nFIGURE 9.12 Time dependence of the spatial probability density \nfor the superposition state composed of equal probabilities of the  \nn \u0003 19 and 20 states.\n\n9.9 Molecular Vibrations \n305\n9.9 \u0002 MOLECULAR VIBRATIONS\nOne of the most common applications of the quantum mechanical harmonic oscillator is found in the \nvibrations of the nuclei of molecules. In a diatomic molecule, the Coulomb attraction between the nuclei \nand the electrons is balanced by the Coulomb repulsion between the nuclei in a way that results in the \npotential energy diagram shown in Fig. 9.14. This diagram shows the Coulomb potential energy of a \ndiatomic molecule as a function of the nuclear separation for a given electron conﬁguration (in this case, \nthe ground state). The minimum of the potential energy -De occurs at the bond length R0 of the diatomic \nmolecule, and the zero of potential energy represents the separation of the two atoms to inﬁnite separa-\ntion, (i.e., the dissociation of the molecule). This potential energy curve determines the motion of the \nnuclei with respect to each other. Because this curve resembles a parabola near the minimum energy, the \nmotion of the nuclei resembles the motion of a quantum mechanical harmonic oscillator.\nThe harmonic oscillator potential energy that approximates the molecular potential energy is\n \nVHO1R2 =  -De + 1\n2 mv21R - R02\n2, \n(9.131)\nwhere m is the reduced mass of the two nuclei. However, the molecular potential energy curve resem-\nbles a parabola only near the minimum, as shown in Fig. 9.15. As the energy level approaches the \ndissociation limit, the difference between the parabolic harmonic oscillator potential and the true \nmolecular potential becomes quite dramatic. A better approximation to the molecular potential is \ngiven by the Morse potential\n \nVM1R2 = De1e -2a1R-R02 - 2e -a1R-R022, \n(9.132)\nwhere the constant a is\n \na = vA\nm\n2De\n. \n(9.133)\nR0\nR\n0\n−De\nV(R)\nFIGURE 9.14 Potential energy of a diatomic molecule as a function of the nuclear separation.\n\n306 \nHarmonic Oscillator\nThe energy levels shown in Fig. 9.15 are the solutions to the motion of the nuclei in the Morse poten-\ntial and show a marked deviation from the levels of an ideal harmonic oscillator. The Morse energy \nlevels become closer together near the top of the well, in contrast to the uniform spacing of the har-\nmonic oscillator. This difference has a clear signature in the spectra of molecular vibrations. An ideal \nharmonic oscillator has transitions only between adjacent energy levels 1\u0006n = {1 selection rule from \nSection 9.8), and all those possible transitions have the same energy difference U v. The transitions in a \nMorse oscillator exhibit a progression from the n = 0 4 1 transition at energy U v to smaller energies \nas we progress up the potential well. In addition, a Morse oscillator has allowed transitions between \nnonadjacent states 1\u0006n = {2, 3, ...2 at higher energies near to multiples of the harmonic energy U v. \nThe \u0006n = {1 selection rule is not obeyed because of the anharmonicity of the well, though these tran-\nsitions are typically weaker than the \u0006n = {1 transitions.\nThe spectra observed in molecules are further complicated by the rotation of the molecule that we \nstudied in Chapter 7, and by transitions between different electronic levels, similar to the transitions in \nthe hydrogen atom we studied in Chapter 8. The transitions due to changes in electronic, vibrational, \nand rotational levels are each characterized by a different energy scale. Electronic transitions are typi-\ncally in the 1–5 eV range, vibrational transitions are typically 500-5000 cm\u00111 (0.06–0.6 eV ), and \nrotational transitions are typically 0.2–60 cm\u00111 (0.02–7 meV ). Thus rotational transitions represent \nﬁner structure compared to vibrational transitions, and vibrational transitions represent ﬁner structure \ncompared to electronic transitions. A schematic of these different energy scales is shown in Fig. 9.16.\nWe discussed the hydrogen chloride molecule in Chapter 7 and noted that the rotational spec-\ntrum was affected by the vibrational motion. We can now explain this using Fig. 9.15. As a mol-\necule vibrates, it occupies higher lying vibrational levels within the potential energy well shown in \nFig. 9.15. Because the Morse potential is asymmetric, the average value of the nuclear separation \n(the “bond length”) deviates from the equilibrium value R0, with the deviation growing as the energy \nincreases. The deviation is always positive in the Morse potential, which implies that the moment of \ninertia of the diatomic molecule I = mR2 increases and the rotational constant U2>2I decreases. This \nnegative shift of the rotational constant explains the discrepancy between the calculated and observed \n spectra that we noted in Chapter 7. Note that this rotation-vibration coupling is present even in the \nn = 0 vibrational ground state, where one might be tempted to think that the molecule is not vibrat-\ning. This effect is another example of the effect of the zero-point energy of the harmonic oscillator.\nR0\nR\n0\n−De\nE\nn = 2\nn = 1\nn = 0\nn =\u00073\nFIGURE 9.15 Vibrational energy states of the nuclear motion in the molecular potential. The dashed \nline is the approximate harmonic potential and the solid line is the more accurate Morse potential.\n\nSummary \n307\nSUMMARY\nWe solved the quantum mechanical harmonic oscillator problem using an operator approach. We \ndeﬁned the lowering and raising operators\n \na = A\nmv\n2U\n axn + i pn\nmvb \n(9.134)\nand\n \na- = A\nmv\n2U\n axn - i pn\nmvb , \n(9.135)\nrespectively. Using these operators, we expressed the Hamiltonian as\n \nH = pn  2\n2m + 1\n2mv2xn  2 = U v1a-a + 1\n22. \n(9.136)\nWe solved the energy eigenvalue problem to ﬁnd\n \nEn = U v1n + 1\n22,  n = 0, 1, 2, 3, ... . \n(9.137)\nWe used the quantum mechanical harmonic oscillator to review the fundamental ideas of quan-\ntum mechanics. Table 9.1 summarizes the manifestations of the quantum mechanical postulates in the \ndifferent systems we have studied to this point.\nR\nE\nelectronic transition\nro-vibrational transition\nrotational transition\n FIGURE 9.16 Transitions in a diatomic molecule.\n\n308 \nHarmonic Oscillator\nPROBLEMS\n 9.1 Show that the product of a and a-, in that order, is given by Eq. (9.21).\n 9.2 Show that the action of a- on an eigenstate of H produces an eigenstate with the eigenvalue \nraised by one quantum.\n 9.3 Normalize the wave function e-mvx2>2U to get the correct ground state of the harmonic  oscillator, \nas given in Eq. (9.49).\n 9.4 Calculate the probability that a particle in the ground state of the harmonic oscillator is found \nin the classically forbidden region.\n 9.5 Show that the proper scale factor of the raising operation yields Eq. (9.56): \na-0 n9 = 1n + 10 n + 19.\n 9.6 Show that the raising and lowering operators would commute with each other if their action \non energy eigenstates preserved normalization. That is, assume a0 n9 = 0 n - 19 and \na-0 n9 = 0 n + 19, and use that information to show that a and a- commute.\n 9.7 Show by direct integration that the ground and ﬁrst excited states of the harmonic oscillator are \northogonal to each other.\n 9.8 Show that the spatial probability density of a classical harmonic oscillator is\n \nP1x2 =\n1\np2x2\n0 - x2 ,\n \n where x0 is the classical turning point (see Fig. 9.7).\n 9.9 a)  For the ground state of the harmonic oscillator, calculate 8x9, 8p9, 8x29, and 8p29 by \nexplicit spatial integration.\nb)  Calculate 8x9, 8p9, 8x29, and 8p29 for all the energy eigenstates 0 n9 of the harmonic \noscillator without doing integration (i.e., use the operators a and a†).\nc)  Check that the uncertainty principle is obeyed in both the above cases.\nTable 9.1 Manifestations of Quantum Mechanical Postulates\nPostulates\nSpin 1/2\nHydrogen atom\nHarmonic Oscillator\n1)  State deﬁned by \nket\n\u0004 +9, \u0004 -9\ncnlm1r,u,f2 = R nl 1r2Y m\nl 1u,f2\n\u0004 n9, wn1x2, fn 1p2\n2)  Observables as \noperators\nSz, S2, H\nH, L2, Lz\nH, xn, pn\n3)  Measure eigen-\nvalues\nSz = {U\n /2\nEn= -Z2R\nn2\nL2 = / (/ + 1)U2, Lz = mU\nEn = U v1n + 1\n22\n4)  Probability  \n(density)\n\u00048+ \u0004 c9\u0004\n2\n\u00048nlm \u0004 c9\u0004\n2,     \u0004 cnlm1r, u, f2 \u0004\n2\n\u00048n \u0004 c9\u0004\n2,    \u0004wn1x2\u0004\n2\n5) State reduction\n\u0004 c9 S \u0004 +9\n\u0004 c9 S \u0004 nlm9,     \u0004 c9 S \u0004 En9\n\u0004 c9 S \u0004 n9\n6)  Schrödinger time \nevolution\nLarmor  \nprecession\nDipole oscillation\nSuperposition oscillation\n\nProblems \n309\n 9.10 Discuss and show explicitly how Eq. (9.93) is used to ﬁnd the matrix representations of the \nladder operators.\n 9.11 A particle in the harmonic oscillator potential has the initial state\n \n0 c 1t = 029 = A3@ 09 + 2eip/2@ 194.\na) Find the normalization constant A.\nb) Find the time-evolved state 0 c(t)9.\nc) Calculate 8x9 and 8p9 as functions of time and verify that Ehrenfest’s theorem\n[Eq. (9.130)] is obeyed.\n 9.12 A particle is in the ground state of the harmonic oscillator potential V11x2 = 1\n2 mv2\n1 x2  when the \npotential suddenly changes to V21x2 = 1\n2 mv2\n2 x2  without initially changing the wave function.\na) What is the probability that a measurement of the particle energy yields the result 1\n2 U v2?\nb) Evaluate the result in (a) for the case v2 = 1.7v1.\n 9.13 Find the allowed energy levels of a particle of mass m moving in the one-dimensional potential \nenergy well\n \nV1x2 = e\n1\n2 mv2x2,     x 6 0\n  \u0005  ,     x 7 0.\n \n (Hint: The answer requires a qualitative argument rather than a calculation.)\n 9.14 A particle in the harmonic oscillator potential has the initial state\n \nc1x, 02 = A c 1 - 3A\nmv\nU\n x + 2 mv\nU\n x2 d e-mvx2>2U\n \n where A is the normalization constant.\na) Calculate the expectation value of the energy.\nb) At a later time T, the wave function is\n \nc1x, T2 = Bc 3 - 3iA\nmv\nU\n x - 2 mv\nU\n x2 d e-mvx2>2U\n \n for some constant B. What is the smallest possible value of T ?\n 9.15 A measurement of the energy of a harmonic oscillator system yields the results U v>2 and \n3U v>2 with equal probability. A measurement of the position (actually measurements on an \nensemble of identically prepared systems) yields the result 8x9 = - 1U>2mv sin vt. Calculate \nthe expectation value of the momentum.\n 9.16 A particle is in the harmonic oscillator potential V1x2 = 1\n2 mv2x2 and the energy is  measured. \n \n The probability that the energy measurement yields 3\n2 U v is 36% and the probability that \nthe energy measurement yields 5\n2 U v is 64%. The expectation value of the position 8x9 is a \n minimum at time t \u0003 0.\na) Find the time-dependent wave function.\nb) Calculate the expectation value 8p9 of the momentum for this particle, as a function of time.\nc) Calculate the expectation value 8E9 of the energy.\n\n310 \nHarmonic Oscillator\n 9.17 A particle in the harmonic oscillator potential V1x2 = 1\n2 mv2x2 starts out in the state\n \nc1x, 02 = A3w01x2 + 2w11x2 + 2w21x24,\n \n where wn1x2 are the normalized eigenfunctions of the Hamiltonian.\na) If you measure the energy of this particle, what values might you get, and with what \n probabilities?\nb) Calculate the expectation value 8p9 of the momentum for this particle, as a function of time.\nc) What is the expectation value 8E9 of the energy?\nd) What is the standard deviation \u0006E of the energy?\n 9.18 Solve the differential equation (9.104) and show that the properly normalized momentum \nspace wave function of the ground state of the harmonic oscillator is given by Eq. (9.105).\n 9.19 Find the momentum representation of the ground state of the harmonic oscillator using the \nFourier transform in Eq. (9.108).\n 9.20 Use your favorite software package to study the coherent states of the harmonic oscillator. \nAssume that the system has the initial wave function\n \nc1x, 02 = w01x - x02,\n \n where x0 is a constant representing the displacement of the Gaussian ground state waveform \nfrom the origin.\na) Plot the wave function, choosing x0 = 1U>mv\nb) Calculate the overlap integrals in cn = 8n0 c1029 necessary to express the initial wave \nfunction in terms of energy eigenstates. Do this for the ﬁrst 10 energy levels. Compare your \nresults to the expression\n \ncn =\nan\n2n!\n e-  a2>2,\n  \nwhere a = x01mv>2U. Check whether the 10 terms in the expansion are enough to prop-\nerly represent the wave function. Explain.\nc) Calculate the expectation value of the energy.\nd) Construct the time-dependent wave function. Animate the wave function and describe its \ntime evolution.\ne) Repeat the above for x0 = 41U>mv. You may need more that 10 terms!\n 9.21 Show that the Morse potential reduces to a parabolic potential for small displacements from \nthe equilibrium bond length. Find the cubic correction term to the harmonic potential that is \nincluded in the Morse potential.\n 9.22 Imagine a quantum system with an energy spectrum En = n3 U v for n = 1, 2, 3, ... . By \ninspection, write down the matrix representation of the Hamiltonian and the energy eigen-\nstates. Write down the matrix representing the operator A in this system that is deﬁned by \nA0 n9 = 3n20 n + 29.\n\nResources \n311\nRESOURCES\nActivities\nThese activities are available at\nwww.physics.oregonstate.edu/qmactivities\nHarmonic Oscillator Basis States: Students express the normalization, orthogonality, and complete-\nness conditions for harmonic oscillator states in Dirac notation and in wave function notation.\nTime Evolution of Harmonic Oscillator States: Students animate wave functions consisting of lin-\near combinations of eigenstates.\nFurther Reading\nMore details on treating light as a harmonic oscillator and coherent states of light can be found in \nthese texts:\nMark Fox, Quantum Optics: An Introduction, Oxford: Oxford University Press, 2006.\nChristopher Gerry and Peter Knight, Introductory Quantum Optics, Cambridge: Cambridge \nUniversity Press, 2005.\nRodney Loudon, Quantum Theory of Light, Oxford: Oxford University Press, 2000.\n\nC H A P T E R \n10\nPerturbation Theory\nThe quantum mechanics you have studied so far has entailed solving a few carefully chosen prob-\nlems exactly. Unfortunately, those problems represent a small fraction of the realistic problems that \nnature presents to us, and in most cases those exactly solvable problems are only approximations to \nreal problems. Now we must learn to solve more realistic problems that do not admit exact solutions. \nThe approach we take to solving these realistic problems is to make them look like problems we have \nalready solved exactly, with an additional part that represents the new, more realistic aspect of the \nproblem. We assume that this new part, the perturbation, is small so that we can use approximations \nto ﬁnd the corrections to the exact solutions. Our focus is to discover how energies and eigenstates are \naffected by small additional terms in the Hamiltonian. To guide us, we will take some exactly solv-\nable problems, solve them, and then expand the solutions. We will compare these results with the new \nperturbation methods that we learn.\nWe had a sneak peek at how a perturbation affects a system in Chapter 5 when we studied the \nasymmetric square well. We saw that an additional potential energy “shelf” in the inﬁnite square well \nchanged the energy levels, as shown in Fig. 10.1. For small values V0 of the potential energy shelf, \nthe energy of the ground state is shifted by an amount that is linearly dependent on V0, but as the \nperturbation increases, the energy begins to change quadratically. This linear-to-quadratic behavior \nis a common feature of perturbation theory and will be evident as we proceed. Our goal is to produce \nplots like Fig. 10.1(b) that demonstrate how energy levels shift when a perturbation is applied to a \nsystem.\n0\nL/2\nL\nx\nV0\nV(x)\n(a)\n(b)\n\u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003 \u0003\n0\n0.5\n1.0\nV0/E1\n0\n0.2\n0.4\n\u0012E/E1\n\r\nFIGURE 10.1 (a) Asymmetric square well and (b) energy shift of the ground state \nas a function of the perturbation V0, where the points are from the exact calculation in \nSection 5.9, and the straight line is from the perturbation calculation in Example 10.2.\n\n10.1  Spin-1/2 Example \n313\n10.1 \u0002 SPIN-1/2 EXAMPLE\nTo get a feel for what perturbation theory is and how it works, let’s go back to our old standby—the \nspin-1/2 problem. The usual Hamiltonian of a spin-1/2 system is the potential energy of the spin mag-\nnetic moment in an applied magnetic ﬁeld. For an applied magnetic ﬁeld in the z-direction B = B0zn , \nthe Hamiltonian is\n \nH0 = -M~B = v0Sz \u0003 U\n2\n ¢v0\n0\n0\n-v0 ≤, \n(10.1)\nwhere we have deﬁned the Larmor frequency v0 = eB0>me as we did in Chapter 3. The subscript zero \non the Hamiltonian in Eq. (10.1) indicates that this is the zeroth-order Hamiltonian (i.e., the Hamil-\ntonian before we apply a perturbation). The energy eigenstates of the zeroth-order Hamiltonian are the \nspin up and down states 0 {9 and the energy eigenvalues are\n \nE  (0)\n{ = {U v0\n2 , \n(10.2)\nwhere the superscript zero on the energy (not an exponent) denotes the order of the solution. The \nenergy spectrum of the zeroth-order energy eigenstates is shown in Fig. 10.2.\nThe goal of perturbation theory is to ﬁnd the higher-order corrections to the energy eigenvalues \nand eigenstates caused by the application of a perturbation to the system. For this spin-1/2 system, \nwe will solve the problem exactly and then expand the solutions to discover how perturbation series \nbehave. Our exact solution should contain the zeroth-order solutions we already know [Eq. (10.2)] and \nsmall corrections.\nThe simplest way to perturb this spin system is to change the magnetic ﬁeld. Any general change \nto the magnetic ﬁeld can be decomposed into an additional component along the original ﬁeld in the \nz-direction, and an additional component perpendicular to that, as shown in Fig. 10.3(a). We write \nthe new total ﬁeld as B = B0\n zn + B1zn + B2xn and characterize the two additional ﬁeld components \nby their respective Larmor frequencies v1 = eB1>me and v 2 = eB2>me. With this notation, the new \nHamiltonian is\n \nH = -M~B = v0 Sz + v1Sz + v2 Sx \u0003 U\n2\n av0+v1\nv2\nv2\n-v0 -v1\nb.  \n(10.3)\n\n0.50\n\n0.25\n0.00\n0.25\n0.50\nE/\u0002Ω0\n\u0002\u000f\u0003\n\u0002Ω0\nE\u000f\u0004 \u0002Ω0\n2\n\nE\n\u0004\n\u0002Ω0\n2\n\u0002\n\u0003\nFIGURE 10.2 Energy levels of a spin-1/2 particle in a uniform magnetic ﬁeld.\n\n314 \nPerturbation Theory\nIt is useful to separate the new Hamiltonian into the zeroth-order Hamiltonian H0 and the perturbation \nHamiltonian that we denote H\u0004:\n \nH = H0 + H\u0004. \n(10.4)\nThe zeroth-order Hamiltonian is given by Eq. (10.1) and the perturbation Hamiltonian is\n \nH\u0004 \u0003 U\n2\n ¢v1\nv2\nv2\n-v1\n≤. \n(10.5)\nThe perturbation Hamiltonian has terms along the diagonal and terms off the diagonal. These diagonal \nand off-diagonal terms play important roles in perturbation theory.\nWe now solve for the energy eigenvalues and eigenstates of the new Hamiltonian in Eq. (10.3) \nexactly by diagonalizing the matrix. But we have already done this in Chapter 2 for the general spin-1/2 \ncase of a magnetic ﬁeld at an angle \u0007 to the z-axis. We found there that the Hamiltonian is proportional \nto the spin component Sn along the new magnetic ﬁeld direction nn, and can be expressed in terms of \nthe angle \u0007 of the new ﬁeld as\n \n H = vnew Sn \u0003 U vnew\n2\n ¢cos u\nsin u\nsin u\n-cos u≤,  \n(10.6)\nwhere\n \ntan u =\nv2\nv0 + v1\n, \n(10.7)\nas shown in Fig. 10.3 (b). The new Larmor frequency vnew obeys the Pythagorean equation\n \nvnew = 4(v0 + v1)2 + v2\n2 \n(10.8)\ncorresponding to the total ﬁeld, as suggested by Fig. 10.3(b). The eigenstates of this new Hamiltonian \nare the spin states 0 {9n aligned along or against the new ﬁeld and the eigenenergies are (Problem 10.1)\n \nEnew = { U\n2\n vnew = { U\n2\n 4(v0 + v1)2 + v2\n2. \n(10.9)\nThis is the exact solution, which we now expand in a power series.\nB2\nB1\nB0\nBnew\nΩnew\nΘ\nΩ2\nΩ1\nΩ0 Θ\n(a)\n(b)\nFIGURE 10.3 (a) Perturbing magnetic ﬁelds and (b) the resultant Larmor frequencies.\n\n10.1  Spin-1/2 Example \n315\nThe basic idea of perturbation theory is to assume that the new terms in the Hamiltonian are \nsmall compared to the original Hamiltonian, (i.e., the perturbation Hamiltonian H\u0004 is much smaller \nthan the zeroth-order Hamiltonian H0). In this spin-1/2 example, that would imply that the added \nﬁelds B1 and B2 are small compared to the original ﬁeld B0, which means that the new Larmor fre-\nquencies v1 and v2 are small compared to the original Larmor frequency v0. Hence, we treat the \nratios v1>v0 and v2>v0 of the new to old Larmor frequencies as small dimensionless parameters and \nrewrite the energy in Eq. (10.9) as\n \n Enew = {U v0\n2\n B a1 + v1\nv0\nb\n2\n+\nv2\n2\nv2\n0\n \n \n = {U v0\n2\n B\n1 + 2v1\nv0\n+\nv2\n1\nv2\n0\n+\nv2\n2\nv2\n0\n. \n(10.10)\nSo far this is still the exact solution. Now we expand the exact energy to second order in a power \nseries in the small parameters v1>v0 and v2>v0, so as to reach some general conclusions about per-\nturbation theory:\n \n Enew = {U v0\n2\n c1 + 2v1\nv0\n+\nv2\n1\nv2\n0\n+\nv2\n2\nv2\n0\nd\n1>2\n \n \n = {U v0\n2\n c1 + v1\nv0\n+\nv2\n1\n2v2\n0\n+\nv2\n2\n2v2\n0\n- 1\n8\n a2v1\nv0\n+\nv2\n1\nv2\n0\n+\nv2\n2\nv2\n0\nb\n2\n+ ...d \n(10.11)\n \n = {U v0\n2\n c1 + v1\nv0\n+\nv2\n1\n2v2\n0\n+\nv2\n2\n2v2\n0\n-\nv2\n1\n2v2\n0\n+ ...d\n \n \n \u0002 {U v0\n2\n c1 + v1\nv0\n+\nv2\n2\n2v2\n0\nd .\n \nWe now conclude that the two energies of the perturbed system, to second order in the small quantities \ncharacterizing the perturbation, are\n \nE + \u0002 + U v0\n2\n+ U v1\n2\n+\nU v2\n2\n4v0\n \n \nE - \u0002 -  U v0\n2\n- U v1\n2\n-\nU v2\n2\n4v0\n. \n(10.12)\nIn both energies, we identify the ﬁrst term as the zeroth-order energy E (0)\n{  given by Eq. (10.2), and \nwe note two additional terms. The ﬁrst is linear or ﬁrst order in the perturbation and is equal to the \ncorresponding diagonal term {U v1>2 in the perturbation Hamiltonian [Eq. (10.5)]. The second addi-\ntional term is quadratic or second order in the perturbation and is proportional to the square of the \noff-diagonal term U v2>2 in the perturbation Hamiltonian. This general pattern of corrections is char-\nacteristic of perturbation theory, so we denote perturbed energies as the series\n \nEn = E (0)\nn\n+ E (1)\nn\n+ E (2)\nn\n+ ..., \n(10.13)\nwhere the superscript indicates the order of the perturbation. We found in this spin-1/2 example that \nthe linear corrections arose from the diagonal terms in the perturbation Hamiltonian and the quadratic \n\n316 \nPerturbation Theory\nterms arose from the off-diagonal terms, another characteristic pattern of general perturbation theory. \nIn Eq. (10.12), the second-order energy correction due to the off-diagonal terms has a factor of v0 in \nthe denominator, and it will diverge if the energy splitting U v0 is zero, (i.e., if the original levels are \ndegenerate in energy). This divergence violates the assumption that the perturbation corrections are \nsmall, which creates a problem that we will address in Section 10.5.\nIn addition to these features of the perturbed energies, we can also draw some conclusions about \nthe perturbation corrections to the eigenstates from our knowledge of the exact eigenstate solutions. \nThe eigenstates of the full Hamiltonian in Eq. (10.6) are the spin up and down eigenstates 0 {9n along \nthe direction nn :\n \n 0  +9n =  cos u\n2\n 0  +9 + sin u\n2\n 0  -9\n \n \n 0  -9n = -sin u\n2\n 0  +9 + cos u\n2\n 0  -9. \n(10.14)\nFrom Fig. 10.3(a), it is evident that the angle \u0007 is small for small perturbing magnetic ﬁelds, so we can \nalso use \u0007 as a small parameter for a series expansion:\n \n 0  +9n \u0002 a1 - u2\n8 b 0  +9 + u\n2\n 0  -9 = 0  +9 + u\n2\n 0  -9 - u2\n8\n 0  +9\n \n \n 0  -9n \u0002 - u\n2\n 0  +9 + a1 - u2\n8 b 0  -9 = 0  -9 - u\n2\n 0  +9 - u2\n8\n 0  -9. \n(10.15)\nTo second order in the angle \u0007, the new eigenstates have two correction terms: a ﬁrst-order term that is \northogonal to the original state, and a second-order term that is parallel (in a Hilbert space sense, not a \ngeometric sense) to the original state. If we neglect the parallel terms (we’ll see in Section 10.3.2 why \nwe do this), we get:\n \n 0  +9n \u0002 0  +9 + u\n2\n 0  -9  \n \n 0  -9n \u0002 0  -9 - u\n2\n 0  +9. \n(10.16)\nUsing the schematic in Fig. 10.3(b), we express the small angle u in terms of the Larmor frequencies. \nTo ﬁrst order [consistent with neglecting the second-order parallel terms in Eq. (10.15)], we obtain\n \nu \u0002\nv 2\n4(v0 + v1)2 + v2\n2\n\u0002 v 2\nv0\n. \n(10.17)\nThus, we arrive at the perturbation series expansion for the perturbed states, to ﬁrst order\n \n0  +9n \u0002 0  +9 + v2\n2v0\n 0  -9  \n \n0  -9n \u0002 0  -9 - v2\n2v0\n 0  +9. \n(10.18)\nWe conclude that the ﬁrst-order eigenstate correction depends only on the off-diagonal matrix ele-\nment, not on the diagonal elements. Note that the coefﬁcient of the original state remains one, which \n\n10.2 General Two-Level Example \n317\nmakes it appear that the state is no longer normalized. But if we check the normalization of the per-\nturbed state:\n \n n8  + @  +  9n = c8 + @ + v2\n2v0\n 8 -  @ d c @   +  9 + v2\n2v0\n @  -  9d  \n \n = 1 + a v2\n2v0\nb\n2\n \n(10.19)\n \n \u0002 1,\n \nwe see that it is normalized to ﬁrst order in the small perturbation parameters.\n 10.2 \u0002 GENERAL TWO-LEVEL EXAMPLE\nContinuing our introduction to perturbation theory, we consider a general two-level system that we \nsolve exactly to learn how the solutions depend on the perturbation and to practice the notation we will \nuse later. This example repeats the calculation of Section 10.1 with general notation rather than con-\nsidering a speciﬁc physical system. At each step in this section, refer back to Section 10.1 to identify \nwhat each term is in the speciﬁc case of a perturbing magnetic ﬁeld applied to a spin-1/2 system.\nIn the general two-level case, we assume a zeroth-order Hamiltonian of the form\n \nH0 \u0003 ¢E  (0)\n1\n0\n0\nE  (0)\n2\n≤ \n(10.20)\nand a general perturbation\n \nH\u0004 \u0003 ¢H =\n11\nH =\n12\nH =\n21\nH =\n22\n≤, \n(10.21)\nwhere the matrix elements of the perturbation Hamiltonian are written as\n \nH =\nij = 8i (0)0\n H\u00040  j (0)9. \n(10.22)\nNote that we use the energy eigenvectors of the zeroth-order Hamiltonian as the basis vectors for \nmatrix representation of the operators. It is useful to parameterize the strength of the perturbation \nwith a dimensionless quantity l that allows us to keep track of the order of the perturbation in a power \nseries solution (note that this l is not the l we often use as a placeholder when ﬁnding eigenvalues). In \nthe end we will set l equal to one, so it is used solely to keep track of the different orders in the power \nseries. Using this parameter, the full Hamiltonian is\n \nH = H0 + lH\u0004 \u0003 ¢E (0)\n1\n+ lH =\n11\nlH =\n12\nlH =\n21\nE (0)\n2\n+ lH =\n22\n≤. \n(10.23)\nNow let’s ﬁnd the exact eigenvalues of this Hamiltonian. To reduce the clutter in the algebra, \nredeﬁne the matrix values:\n \n¢E (0)\n1\n+ lH =\n11\nlH =\n12\nlH =\n21\nE (0)\n2\n+ lH =\n22\n≤K ¢ a\nc\nc*\nb≤, \n(10.24)\n\n318 \nPerturbation Theory\nnoting that because H\u0004 is Hermitian, H =\n12 = H =\n21\n*. Using the symbol E for the energy eigenvalue, we \ndiagonalize the Hamiltonian by ﬁnding the characteristic equation:\n \n ` a - E\nc\nc*\nb - E ` = 0 \n \n (a - E )(b - E ) - 0 c0\n2 = 0 \n(10.25)\n \n E 2 - E (a + b) + a b - 0 c0\n2 = 0 \nand solving to obtain\n \n E = 1\n2 (a + b){4\n1\n4 (a + b)\n2 - a b + 0 c0\n2 \n \n = 1\n2 (a + b){4\n1\n4 (a - b)2 + 0 c0\n2.\n \n(10.26)\nWe are considering the case where the perturbation is small  [i.e., c V (a - b)], so we factor and use \nthe binomial expansion:\n \n E = 1\n2 (a + b){1\n2 (a - b)c1 +\n40 c0\n2\n(a - b)2d\n1\n2\n \n \n \u0002 1\n2 (a + b){1\n2 (a - b)c1 +\n20 c0\n2\n(a - b)2d . \n(10.27)\nThis yields the two energies\n \n E1 \u0002 a +\n0 c0\n2\n(a - b)   \n \n E2 \u0002 b -\n0 c0\n2\n(a - b) . \n(10.28)\nNow rewrite these solutions in terms of the original parameters\n \n E1 = E (0)\n1\n+ lH =\n11 +\nl20 H =\n12 0\n2\n1E (0)\n1\n+ lH =\n11 - E (0)\n2\n- lH =\n222\n \n \n E2 = E (0)\n2\n+ lH =\n22 -\nl20 H =\n12 0\n2\n1E (0)\n1\n+ lH =\n11 - E (0)\n2\n- lH =\n222\n \n(10.29)\nand expand Eq. (10.29) to second order in the expansion parameter l :\n \n E1 \u0002 E (0)\n1\n+ lH =\n11 +\nl20 H =\n12 0\n2\n1E (0)\n1\n- E (0)\n2 2\n \n \n E2 \u0002 E (0)\n2\n+ lH =\n22 +\nl20 H =\n21 0\n2\n1E (0)\n2\n- E (0)\n1 2\n, \n(10.30)\n\n10.3 Nondegenerate Perturbation Theory \n319\nwhere we have written the results in a way to make it clear that the two energies have the same form \n(they are equivalent if we swap indices 1 4 2). We have left the expansion parameter in for now to \nmake the order of the expansion clear, but imagine it set to unity, as we will do later. The general \nconclusion is that an energy level En has a ﬁrst-order correction that is the matrix element H =\nnn of the \nperturbation in the state in question and a second-order correction that depends on the square of the \ncoupling H =\nnk to other states and inversely on the energy difference E (0)\nn\n- E (0)\nk  between states. This is \nthe same form that we saw in the spin example above and also what we will see as we develop pertur-\nbation theory in general. Note again that degeneracy of the two states (E (0)\nn\n- E (0)\nk\n= 0) creates prob-\nlems. For this reason we will study nondegenerate and degenerate perturbation theories separately.\n 10.3 \u0002 NONDEGENERATE PERTURBATION THEORY\nIn the examples above, we solved the problems exactly, even the “perturbed problems,” to ﬁnd the \nnew energy eigenvalues and eigenstates, and then we approximated these exact solutions to draw \nsome conclusions about the general behavior of perturbed energies and states. Now we tackle per-\nturbed problems that are not exactly solvable, but we assume that the nonperturbed part of the problem \nis exactly solvable. The exactly solvable part of the problem is called the zeroth-order problem and has \nan energy eigenvalue equation\n \nH0 @  n(0)9 = E (0)\nn @  n(0)9, \n(10.31)\nwhere we use a subscript on the energy to denote the quantum number and superscripts (not powers) \non the energy and eigenstates to denote the order of the solution. Now suppose that this system is per-\nturbed by the addition of a new term in the Hamiltonian that we call H\u0004. The new perturbed problem \nhas an energy eigenvalue equation\n \n(H0 + H\u0004) 0  n 9 = En0  n 9, \n(10.32)\nwhere En are the new energies and 0  n9 are the new eigenstates that we seek. As discussed in the pre-\nvious section, we parameterize the strength of the perturbation with a dimensionless quantity l and \nrewrite the energy eigenvalue equation as\n \n(H0 + lH\u0004) 0  n9 = En0  n9. \n(10.33)\nThe l parameter allows us to keep track of the order of the perturbation in a power series solution. In \nthe end we set it equal to one, so it is here solely to keep track of the different orders in the power series.\nThe essence of the perturbation technique is to assume that we can write the new eigenvalues and \neigenstates as power series expansions with ever-decreasing terms such that the series converge. There \nare some important examples where this does not work (e.g., superconductivity and quantum chromo-\ndynamics), but it does work in many cases. We use the dimensionless parameter l as the expansion \nparameter and write the desired eigenvalues and eigenstates as\n \n En = E (0)\nn\n+ lE (1)\nn\n+ l2E (2)\nn\n+ l3E (3)\nn\n+ ...  \n \n 0  n 9 = @  n(0)9 + l @  n(1)9 + l2 @  n(2)9 + l3 @  n(3)9 + ... . \n(10.34)\nTo ﬁnd the new solutions, substitute these series into the eigenvalue equation Eq. (10.33) and collect \nterms of the same order or power of the parameter l on each side of the equation. For the eigenvalue \nequation to hold for any value of l, the coefﬁcients of like orders on the two sides of the equation must \n\n320 \nPerturbation Theory\nbe equal, and we can isolate an equation for each order in the expansion parameter. The result is the \nfollowing set of equations: (Problem 10.2)\n \n\u00141l02:  1H0 - E (0)\nn 2@  n(0)9 = 0\n \n(10.35)\n \n\u00141l12:  1H0 - E (0)\nn 2@  n(1)9 = 1E (1)\nn\n- H\u00042@  n(0)9\n \n(10.36)\n \n\u00141l22:  1H0 - E (0)\nn 2@  n(2)9 = 1E (1)\nn\n- H\u00042@  n(1)9 + E (2)\nn @  n(0)9\n \n(10.37)\n \n\u00141l32:  1H0 - E (0)\nn 2@  n(3)9 = 1E (1)\nn\n- H\u00042@  n(2)9 + E (2)\nn @  n(1)9 + E (3)\nn @  n(0)9 \n(10.38)\nand so on. At this point, the parameter l has done its work and is not needed any more.\nEquation (10.35) is zeroth order in the expansion parameter and is simply the original eigenvalue \nequation [Eq. (10.31)]. That’s why it’s called the zeroth-order equation. We assume that the zeroth-\norder energies E (0)\nn  and the zeroth order eigenstates 0  n(0)9 have been solved for and are known.\n 10.3.1 \u0002 First-Order Energy Correction\nEquation (10.36) is the ﬁrst-order equation and contains the ﬁrst-order corrections E (1)\nn  and @  n(1)9 \nto the eigenvalues and eigenstates, respectively, as unknowns. For a system with N energy levels \n(i.e., N is the dimension of the Hilbert space), Eq. (10.36) represents N equations for n = 1, 2, ... N \nto be solved for each energy and each eigenstate. Moreover, for any given n, Eq. (10.36) is really a \nsystem of N equations because the Hamiltonian operators are represented by N * N matrices. To see \nour way through this morass of N2 equations, it is helpful to examine the full matrix representation of \nEq. (10.36) for one particular choice of n and then generalize from that result.\nOf course, to use matrices, we must choose a basis for representation. Given that we have solved \nonly the zeroth-order problem at this stage, the basis of zeroth-order energy eigenstates @  n(0)9 is the \nmost obvious basis at our disposal. So, we express each part of Eq. (10.36) in this basis. The matrices \nrepresenting the Hamiltonians H0 and H\u0004 do not depend on the choice of the state n and are\n \n H0 \u0003 •\nE (0)\n1\n0\n0\n0\ng\n0\nE (0)\n2\n0\n0\ng\n0\n0\nE (0)\n3\n0\ng\n0\n0\n0\nE (0)\n4\ng\nf\nf\nf\nf\nf\nμ  \n \n H\u0004 \u0003 •\nH =\n11\nH =\n12\nH =\n13\nH =\n14\ng\nH =\n21\nH =\n22\nH =\n23\nH =\n24\ng\nH =\n31\nH =\n32\nH =\n33\nH =\n34\ng\nH =\n41\nH =\n42\nH =\n43\nH =\n44\ng\nf\nf\nf\nf\nf\nμ  , \n(10.39)\nwhere the matrix elements of the perturbation Hamiltonian are deﬁned in the zeroth-order basis\n \nH =\nij = 8i (0) @H\u0004@\n  j (0)9. \n(10.40)\nAll of the elements of these two matrices are known quantities.\n\n10.3 Nondegenerate Perturbation Theory \n321\nThe other terms in Eq. (10.36) do depend on the choice of the state n. Let’s choose n = 3 for this \nexample, which means that the zeroth-order energy eigenstate 0  3(0)9 is\n \n0 3(0)9 \u0003 •\n0\n0\n1\n0\nf\nμ . \n(10.41)\nThe ﬁrst-order correction to the eigenstate @ 3(1)9 is not yet known, and we characterize it in terms of \na set of yet-to-be-found ﬁrst-order coefﬁcients c (1)\n3m = 8m(0)@ 3(1)9 in the zeroth-order basis\n \n0 3(1)9 \u0003 ß\nc(1)\n31\nc(1)\n32\nc(1)\n33\nc(1)\n34\nf\n∑. \n(10.42)\nThe other two ingredients in Eq. (10.36) are the known zeroth-order energy E (0)\n3  and the unknown \nﬁrst-order energy correction E (1)\n3 . We are now ready to construct the matrix form of Eq. (10.36) and \nsolve for the unknowns for this choice of n: the ﬁrst-order energy correction E (1)\n3  and the set of coef-\nﬁcients c(1)\n3m that determine the ﬁrst-order correction @ 3(1)9 to the eigenstate.\nFor the choice n = 3, the left-hand side of Eq. (10.36) is\n \n1H0 - E (0)\n3 2@ 3(1)9 \u0003 ß\nE (0)\n1\n- E (0)\n3\n0\n0\n0\ng\n0\nE (0)\n2\n- E (0)\n3\n0\n0\ng\n0\n0\n0\n0\ng\n0\n0\n0\nE (0)\n4\n- E (0)\n3\ng\nf\nf\nf\nf\nf\n∑ß\nc(1)\n31\nc(1)\n32\nc(1)\n33\nc(1)\n34\nf\n∑ \n \n\u0003 ß\n1E (0)\n1\n- E (0)\n3 2c(1)\n31\n1E (0)\n2\n- E (0)\n3 2c(1)\n32\n0\n1E (0)\n4\n- E (0)\n3 2c(1)\n34\nf\n∑ . \n(10.43)\n\n322 \nPerturbation Theory\nIn the matrix and the resultant vector, we have boxed the zero element that arises from subtracting the \nzeroth-order energy E (0)\n3  from the zeroth-order Hamiltonian in order to highlight the importance of \nthat element in the solution. The right-hand side of Eq. (10.36) is\n \n 1E (1)\n3\n- H =20 3(0)9 \u0003 ¶\nE (1)\n3\n- H =\n11\n-H =\n12\n-H =\n13\n-H =\n14\ng\n-H =\n21\nE (1)\n3\n- H =\n22\n-H =\n23\n-H =\n24\ng\n-H =\n31\n-H =\n32\nE 3\n(1) - H =\n33\n-H =\n34\ng\n-H =\n41\n-H =\n42\n-H =\n43\nE (1)\n3\n- H =\n44\ng\nf\nf\nf\nf\nf\n∂ ¶\n0\n0\n1\n0\nf\n∂\n \n\u0003 ¶\n-H =\n13\n-H =\n23\nE 3\n(1) - H =\n33\n-H =\n43\nf\n∂. \n(10.44)\nAgain we have boxed in the diagonal matrix element and the resultant vector component correspond-\ning to the state 0 3(0)9 that we are solving for. Equating the two sides of Eq. (10.36) gives\n \n¶\n1E (0)\n1\n- E (0)\n3 2c (1)\n31\n1E (0)\n2\n- E (0)\n3 2c (1)\n32\n0\n1E (0)\n4\n- E (0)\n3 2c (1)\n34\nf\n∂= ¶\n-H =\n13\n-H =\n23\nE 3\n(1) - H =\n33\n-H =\n43\nf\n∂ . \n(10.45)\nAs promised, we have N equations containing the unknown energy correction E\n (1)\n3  and the unknown \neigenstate correction @ 3(1)9 represented by the coefﬁcients c\n (1)\n3m. The third row of Eq. (10.45), which \nwe have been highlighting all along, yields the solution for the ﬁrst-order energy correction to the \nn = 3 state\n \nE (1)\n3\n- H =\n33 = 0 \n \nE (1)\n3\n= H =\n33.\n \n(10.46)\nWe conclude that the ﬁrst-order energy correction is the diagonal matrix element of the perturbation \nfor the state in question, which agrees with the results in Sections 10.1 and 10.2. The diagonal matrix \nelement of the perturbation is also what we call the expectation value of the perturbation. Note that \nno other states affect the energy correction of this state and the unperturbed states are used to ﬁnd the \nexpectation value of the perturbation; there is no need to know the correction to the state in order to \nﬁnd the ﬁrst-order correction to the energy. Having solved the ﬁrst-order perturbation equation for \n\n10.3 Nondegenerate Perturbation Theory \n323\nthis speciﬁc case of n = 3, we can now infer the result for the general n. The general result of nonde-\ngenerate ﬁrst-order perturbation theory is:\n \nE\n (1)\nn\n= H =\nnm = 8n(0)@H\u0004@ n(0)9  . \n(10.47)\nThe ﬁrst-order correction to the energy is the expectation value of the perturbation in the unper-\nturbed state. In wave function notation, the expectation value is expressed as an integral\n \nE (1)\nn\n= H =\nnn =\nL\nw(0)*\nn 1r2H\u0004w(0)\nn 1r2dV   , \n(10.48)\nwhere w(0)\nn (r) are the energy eigenstates of the zeroth-order Hamiltonian.\nThat’s the result. Now let’s use it.\nExample 10.1 The sodium nucleus has spin 3/2 and a magnetic moment MNa = (gNa\n e>2mp)S, \nwhere the gyromagnetic ratio is gNa = 1.48. The sodium nucleus is placed in a constant magnetic \nﬁeld in the z-direction B0 = B0\n zn. An additional, perturbative magnetic ﬁeld B\u0004 = B1zn  is applied to \nthe system. Find the ﬁrst-order energy shifts due to the perturbation.\nThis problem is a variation on the spin-1/2 example in Section 10.1. The zeroth-order Ham-\niltonian H0 is determined by the potential energy of the nuclear magnetic moment in the constant \nﬁeld B0 = B0\n zn :\n \nH0 = -M~B0 = v0Sz \u0003 §\n3\n2 U v0\n0\n0\n0\n0\n1\n2 U v0\n0\n0\n0\n0\n-  1\n2 U v0\n0\n0\n0\n0\n-  3\n2 U v0\n¥ , \n(10.49)\nwhere we have deﬁned the Larmor frequency v0 = gNa eB0>2mp. The zeroth-order energies are \nE (0)\n1\n= 3\n2 U v0 , E (0)\n2\n= 1\n2 U v0 , E (0)\n3\n= -  1\n2 U v0 , and E (0)\n4\n= -  3\n2 U v0 , labeled in order of decreasing \nenergy. The zeroth-order energy eigenstates are the eigenstates of the spin component operator Sz : \n@ 1(0)9= @ 3\n29, @ 2(0)9= @ 1\n29, @ 3(0)9= @ -1\n2 9, and @ 4(0)9= @ -3\n2 9, which are labeled with the magnetic quantum \nnumber m.\nThe perturbation Hamiltonian H\u0004 is determined by the ﬁeld B\u0004 = B1zn  and is characterized by \na different Larmor frequency v1 = gNa eB1>2mp :\n \nH\u0004 = -M~B\u0004 = v1Sz \u0003 §\n3\n2 U v1\n0\n0\n0\n0\n1\n2 U v1\n0\n0\n0\n0\n-  1\n2 U v1\n0\n0\n0\n0\n-  3\n2 U v1\n¥ . \n(10.50)\nSo far, all these are quantities known from the statement of the problem. Perturbation theory tells \nus that the ﬁrst-order correction to the energy is the expectation value of the perturbation in the \nunperturbed state:\n \nE (1)\nn\n= H =\nnn = 8n(0)@H\u0004@ n(0)9. \n(10.51)\n\n324 \nPerturbation Theory\nThese are the diagonal elements of the matrix representing H\u0004 in the basis of zeroth-order energy \neigenstates. The matrix in Eq. (10.50) thus yields the ﬁrst-order energy shifts due to the  perturbation:\n \n E\n (1)\n1\n= 3\n2 U v1\n \n \n E\n (1)\n2\n= 1\n2 U v1\n \n \n E\n (1)\n3\n= -  1\n2 U v1  \n \n E\n (1)\n4\n= -  3\n2 U v1. \n(10.52)\nThese energy shifts add to the zeroth-order energies to produce the results shown in Fig. 10.4 as \na function of the perturbing ﬁeld. The new energies exhibit the linear dependence we expect for \nﬁrst-order corrections. The constant B0 ﬁeld is assumed to be 2.35 Tesla, which is a standard ﬁeld \nin nuclear magnetic resonance spectroscopy because it produces a 100 MHz resonance for hydro-\ngen nuclei. For sodium nuclei, the resonance in this ﬁeld is 26.5 MHz (v0>2p), indicated by the \ntransition arrows in Fig. 10.4. As the perturbing ﬁeld increases, the resonance shifts in frequency. \nWhen the perturbing ﬁeld is produced by the local chemical environment of the nucleus, the res-\nonance shift is called a chemical shift. This technique is commonly used to identify  chemical \nmicrostructure.\n10.3.2 \u0002 First-Order State Vector Correction\nNow that we have the ﬁrst-order energy correction, we proceed to ﬁnd the ﬁrst-order correction to the \nenergy eigenstates. The other rows (nonhighlighted) of Eq. (10.45) yield equations of identical form \nfor the coefﬁcients c\n (1)\n3m that determine the ﬁrst-order correction to the state vector:\n \n1E\n (0)\nm\n- E\n (0)\n3 2c(1)\n3m = -H =\nm3 , \nm \u0002 3 \n \nc\n (1)\n3m =\nH =\nm3\nE\n (0)\n3\n- E\n (0)\nm\n \n, \nm \u0002 3. \n(10.53)\n0.5\n1.0\n1.5\n2.0\nB1(Tesla)\n\n40\n\n20\n20\n40\nE/h(MHz)\nm \u0004\u0007 3\n2\nm \u0004\u0007 1\n2\nm \u0004\u0007–3\n2\nm \u0004\u0007–1\n2\nFIGURE 10.4 First-order corrected energies of a sodium nucleus in a perturbing \nmagnetic ﬁeld that is parallel to the constant zeroth-order ﬁeld.\n\n10.3 Nondegenerate Perturbation Theory \n325\nEquation (10.53) determines all the coefﬁcients for m \u0002 3; however, there is no information about \nthe coefﬁcient c (1)\n33. This is not surprising. In solving energy eigenvalue equations before, we have \nalways found that one eigenstate coefﬁcient is undetermined by the equations. In those problems, we \nused the normalization requirement to determine the last coefﬁcient. We do the same here.\nUsing Eq. (10.34) (with l = 1) and Eq. (10.42), we write the corrected eigenstate to ﬁrst order as\n \n 0 39 = @ 3(0)9 + @ 3(1)9\n \n \n = @ 3(0)9 + a\nN\nm=1\nc\n (1)\n3m@ m(0)9. \n(10.54)\nSeparating out the undetermined coefﬁcient c (1)\n33, we obtain\n \n 0 39 = @ 3(0)9 + c (1)\n33 @ 3(0)9 + a\nm\u00023\nc (1)\n3m@ m(0)9 \n \n = 11 + c (1)\n33 2@ 3(0)9 + a\nm\u00023\nc (1)\n3m@ m(0)9.  \n(10.55)\nNow normalize this state to determine c (1)\n33 [all the other coefﬁcients are already speciﬁed by Eq. (10.53)]\n \n830 39 = e11 + c\n (1)*\n33 283(0)@ + a\nk\u00023\nc\n (1)*\n3k 8k(0)@ f e11 + c\n (1)\n33 2@ 3(0)9 + a\nm\u00023\nc\n (1)\n3m@ m(0)9f  \n \n= 11 + c\n (1)*\n33 211 + c\n (1)\n33 2 + a\nm\u00023\n@ c\n (1)\n3m @\n2 \n(10.56)\n \n= 1 + c\n (1)\n33 + c\n (1)*\n33\n+ @ c\n (1)\n33 @\n2 + a\nm\u00023\n@ c\n (1)\n3m @\n2,\nwhere we used the orthogonality 8k(0)@ m(0)9 = dkm of the zeroth-order states. We are working in the \nﬁrst-order perturbation approximation, so we must drop the second-order terms in the normalization \nequation for consistency, giving\n \n830 39 = 1 + c\n (1)\n33 + c\n (1)*\n33 . \n(10.57)\nUsing our freedom to choose the overall phase of the state vector, we choose c\n (1)\n33  to be real and con-\nclude that\n \nc\n (1)\n33 = 0. \n(10.58)\nHence, there is no component of the zeroth-order eigenstate @ 3(0)9 in the ﬁrst-order eigenstate correc-\ntion @ 3(1)9, which is the same conclusion we reached in the spin example in Eq. (10.15). This conclu-\nsion can be understood with an analogy between quantum state vectors and spatial vectors. Because all \nquantum state vectors must be normalized (in order to interpret the projections as probability ampli-\ntudes), all that really matters about a quantum state vector is its direction. Thus, as we look for changes \nin the vector, we must focus only on changes in direction. Figure 10.5 shows an analogy with spatial \nvectors, whereby we see that for small changes in direction, the change can be considered to be per-\npendicular to the original vector, and hence have no component along the  original vector.\n\n326 \nPerturbation Theory\nFor this n = 3 state, the ﬁrst-order eigenstate correction is\n \n@ 3(1)9 \u0003 ©\nH =\n13\nE\n (0)\n3\n- E\n (0)\n1\nH =\n23\nE\n (0)\n3\n- E\n (0)\n2\n0\nH =\n43\nE\n (0)\n3\n- E\n (0)\n4\nf\nπ, \n(10.59)\nand the corrected eigenstate to ﬁrst order is\n \n0 39 = @ 3(0)9 + @ 3(1)9 \u0003 ©\nH =\n13\nE\n (0)\n3\n- E\n (0)\n1\nH =\n23\nE\n (0)\n3\n- E\n (0)\n2\n1\nH =\n43\nE\n (0)\n3\n- E\n (0)\n4\nf\nπ. \n(10.60)\nFor the perturbation approach to be valid, we must have the new correction terms be small, which \nimplies that the matrix elements of the perturbation Hamiltonian are smaller than the unperturbed \nenergy differences. The absolute energies are not important because we can always shift the \nenergy axis.\n\u0002n(0)\u0003\n\u0002n(1)\u0003\n\u0002n\u0003\nFIGURE 10.5 Perturbed state corrections are orthogonal to the original state.\n\n10.3 Nondegenerate Perturbation Theory \n327\nFor a general state, the coefﬁcients for the eigenstate correction in Eqs. (10.53) and (10.58) \n generalize to\n \n c\n (1)\nnm =\nH =\nmn\nE\n (0)\nn\n- E\n (0)\nm\n , \nm \u0002 n \n \n c\n (1)\nnn = 0 .\n \n(10.61)\nWe conclude that the ﬁrst-order correction to the eigenstate is\n \n0 n(1)9 = a\nm\u0002n\n8m(0)0 H\u00040 n(0)9\n1E\n (0)\nn\n- E\n (0)\nm 2\n0 m(0)9    . \n(10.62)\nNote that the expansion coefﬁcients c(1)\nnm do not enter into the solution for the energy correction. This \nfeature is true in general for perturbation theory. We need the eigenstate correction only when solving \nfor the next order of the energy correction.\nTo illustrate the perturbation theory approach in the general case, let’s repeat the matrix calcula-\ntion we have just completed using Dirac bra-ket notation for the ﬁrst-order energy correction. We start \nwith the ﬁrst-order equation:\n \n1H0 - E\n (0)\nn 2@ n(1)9 = 1E (1)\nn\n- H\u00042@ n(0)9. \n(10.63)\nWe saw in Eq. (10.45) that the solution for the energy correction came from isolating the row for the \nstate of interest. In bra-ket notation, that means that we want to project Eq. (10.63) onto the zeroth-\norder nth eigenstate (as a bra):\n \n8n(0)@1H0 - E\n (0)\nn 2@ n(1)9 = 8n(0)@1E\n (1)\nn\n- H\u00042@ n(0)9.  \n(10.64)\nThe Hamiltonian H0 is Hermitian, so it can act backwards on the bra 8n(0)0 and give the energy \n18n(0)@H0 = 8n(0)@E\n (0)\nn 2, yielding zero on the left-hand side:\n \n 8n(0)@1E\n (0)\nn\n- E\n (0)\nn 2@ n(1)9 = E\n (1)\nn 8n(0)@ n(0)9 - 8n(0)@H\u0004@ n(0)9  \n \n 0 = E\n (1)\nn 8n(0)@ n(0)9 - 8n(0)@H\u0004@ n(0)9.  \n(10.65)\nSolving for the energy correction gives\n \nE\n (1)\nn\n= 8n(0)@  H\u0004@ n(0)9,  \n(10.66)\nwhich is the same as we obtained with the matrix approach above.\nIn summary, the new energy eigenvalues and eigenstates to ﬁrst order in the perturbation are\n \nEn = E\n (0)\nn\n+ 8n(0)@  H\u0004@ n(0)9\n \n \n0 n9 = @ n(0)9 + a\nm\u0002n\n8m(0)@H\u0004@ n(0)9\n1E\n (0)\nn\n- E\n (0)\nm 2\n@ m(0)9  . \n(10.67)\n\n328 \nPerturbation Theory\nThe eigenvalue and eigenstate corrections are independent of each other at this order. The corrections \nat this order will affect the corrections at the next order—the ﬁrst-order eigenstate corrections lead to \nsecond-order eigenvalue corrections, etc. The matrix elements of the perturbation Hamiltonian must \nbe smaller than the unperturbed energy differences for the perturbation approach to be valid.\nExample 10.2 An inﬁnite square well, shown in Fig. 10.6(a), is perturbed by an additional poten-\ntial energy term, with the resultant well shown in Fig. 10.6(b). Find the ﬁrst-order corrections to \nthe energies.\nWe solved the zeroth-order Hamiltonian for the inﬁnite square well in Chapter 5. The zeroth-\norder eigenvalues and eigenstates are\n \n E\n (0)\nn\n= n2 p2U2\n2mL2\n \n \n 0 n(0)9 \u0003 w(0)\nn (x) = A\n2\nL sin anpx\nL b. \n(10.68)\nIn the perturbed case, a potential energy shelf of value V0  is added to the right half of the well. \nThis is the asymmetric square well problem that we solved exactly in Section 5.9 as a sneak peek \nat perturbation theory. Now we solve it using our new perturbation theory tools and compare to the \nexact result. The shift in energy to ﬁrst order is the expectation value of the perturbation Hamilto-\nnian in the unperturbed eigenstates:\n \nE\n (1)\nn\n= 8n(0)@ H\u0004@ n(0)9. \n(10.69)\nThe perturbation Hamiltonian H\u0004 = V(x) has spatial dependence in this problem, and we must \nuse the wave function form of the ﬁrst-order energy correction in Eq. (10.48) involving a spatial \n integral—we cannot simply use bra-ket notation. More formally, we do not know how this new \nperturbation acts on kets. The ﬁrst-order energy correction is\n \nE\n (1)\nn\n=\nL\n\u0005\n- \u0005\nw(0)*\nn (x)V(x)w(0)\nn (x)dx. \n(10.70)\n0\nL\nx\n(a)\n(b)\n0\nL\nx\nV(x)\nV(x)\nV0\nL/2\nL/2\n\u0005\n\u0005\nFIGURE 10.6 (a) An inﬁnite square potential energy well perturbed by \n(b) a potential energy shelf in the right half of the well.\n\n10.4  Second-Order Nondegenerate Perturbation Theory \n329\nThe perturbation is different in the two halves of the well, so we break the integral into two pieces, \nwith the perturbation Hamiltonian equal to zero in the left half and V0 in the right half:\n \n E\n (1)\nn\n=\nL\nL>2\n0\nw(0)*\nn (x) 0  w(0)\nn (x)dx +\nL\nL\nL>2\n w(0)*\nn (x)V0 w(0)\nn (x)dx \n \n = V0 L\nL\nL>2\n@ w(0)\nn (x)@\n2dx.\n \n(10.71)\nThe remaining spatial integral is the integral of the probability density over the right half of the \nwell. All the energy eigenstate probability densities are symmetric about the middle of the well, so \nthe integral is 1/2, yielding (Problem 10.4)\n \nE\n (1)\nn\n= V0\n2\n \n(10.72)\nfor all states. This is the result we included as the perturbation theory prediction of the ground state \nenergy in Fig. 10.1, which shows that the ﬁrst-order perturbation correction is not exact. This per-\nturbation involves higher-order terms because the off-diagonal matrix elements are not zero, lead-\ning to eigenstate corrections [Eq. (10.62)] (Problem 10.5), which lead to second-order eigenvalue \ncorrections as we’ll see in the next section.\n10.4 \u0002 SECOND-ORDER NONDEGENERATE PERTURBATION THEORY\nNow let’s proceed and ﬁnd the second-order energy correction in nondegenerate perturbation theory. \nThe second-order perturbation equation (10.37) is\n \n1H0 - E (0)\nn 2@ n(2)9 = 1E (1)\nn\n- H\u00042@ n(1)9 + E (2)\nn @ n(0)9.  \n(10.73)\nFirst, we’ll use the matrix approach to solve this. The matrices representing the Hamiltonians H0 and \nH\u0004 are again given by Eq. (10.39). The ﬁrst-order correction to the eigenstate @ 3(1)9 is now known, and \nthe second-order correction @ 3(2)9 is unknown, which we again characterize in terms of a set of coef-\nﬁcients c(2)\n3m in the zeroth-order basis. The corrections to the unperturbed n = 3 state are\n \n@ 3(1)9 \u0003 ß\nc(1)\n31\nc(1)\n32\n0\nc(1)\n34\nf\n∑,  @ 3(2)9 \u0003 ß\nc(2)\n31\nc(2)\n32\nc(2)\n33\nc(2)\n34\nf\n∑. \n(10.74)\n\n330 \nPerturbation Theory\nThe left-hand side of Eq. (10.73) is\n \n 1H0 - E (0)\n3 2@ 3(2)9 \u0003 ß\nE (0)\n1\n- E (0)\n3\n0\n0\n0\ng\n0\nE (0)\n2\n- E (0)\n3\n0\n0\ng\n0\n0\n0\n0\ng\n0\n0\n0\nE (0)\n4\n- E (0)\n3\ng\nf\nf\nf\nf\nf\n∑ ß\nc(2)\n31\nc(2)\n32\nc(2)\n33\nc(2)\n34\nf\n∑\n \n \u0003 ß\n1E (0)\n1\n- E (0)\n3 2c(2)\n31\n1E (0)\n2\n- E (0)\n3 2c(2)\n32\n0\n1E (0)\n4\n- E (0)\n3 2c(2)\n34\nf\n∑,\n \n \n(10.75)\nwhere we have again boxed the zero matrix element and the resultant vector component to highlight \ntheir importance in the solution. The ﬁrst term on the right-hand side of Eq. (10.73) is\n \n1E (1)\n3\n- H\u00042@ 3(1)9 \u0003 ß\nE (1)\n3\n- H =\n11\n-H =\n12\n-H =\n13\n-H =\n14\ng\n-H =\n21\nE (1)\n3\n- H =\n22\n-H =\n23\n-H =\n24\ng\n-H =\n31\n-H =\n32\nE (1)\n3\n- H =\n33\n-H =\n34\ng\n-H =\n41\n-H =\n42\n-H =\n43\nE (1)\n3\n- H =\n44\ng\nf\nf\nf\nf\nf\n∑ ß\nc(1)\n31\nc(1)\n32\n0\nc(1)\n34\nf\n∑ \n \n\u0003 ß\n1E (1)\n3\n- H =\n112c(1)\n31 - H =\n12c(1)\n32 - H =\n14c(1)\n34 - g\n-H =\n21c(1)\n31 + 1E (1)\n3\n- H =\n222c(1)\n32 - H =\n24c(1)\n34 - g\n-H =\n31c(1)\n31 - H =\n32c(1)\n32 - H =\n34c(1)\n34 - g\n-H =\n41c(1)\n31 - H =\n42c(1)\n32 + 1E (1)\n3\n- H =\n442c(1)\n34\nf\n∑.   \n(10.76)\n\n10.4  Second-Order Nondegenerate Perturbation Theory \n331\nAgain we have boxed in the diagonal matrix element and the resultant vector component correspond-\ning to the state 0 3(0)9 that we are solving for. The second term on the right-hand side of Eq. (10.73) is\n \nE (2)\n3  0 3(0)9 \u0003 E (2)\n3  ¶\n0\n0\n1\n0\nf\n∂= ¶\n0\n0\nE (2)\n3\n0\nf\n∂. \n(10.77)\nEquating the two sides of Eq. (10.73) gives\n \nß\n1E (0)\n1\n- E (0)\n3 2c(2)\n31\n1E (0)\n2\n- E (0)\n3 2c(2)\n32\n0\n1E 4\n(0) - E (0)\n3 2c(2)\n34\nf\n∑= ß\n1E (1)\n3\n- H =\n112c(1)\n31 - H =\n12c(1)\n32 - H =\n14c(1)\n34 - c\n-H =\n21c(1)\n31 + 1E (1)\n3\n- H =\n222c(1)\n32 - H =\n24c(1)\n34 - c\n-H =\n31c(1)\n31 - H =\n32c(1)\n32 - H =\n34c(1)\n34 - c\n-H =\n41c(1)\n31 - H =\n42c(1)\n32 + 1E (1)\n3\n- H =\n442c(1)\n34\nf\n∑+ ¶\n0\n0\nE (2)\n3\n0\nf\n∂. (10.78)\nAs in the ﬁrst-order calculation, the highlighted elements yield the solution for the second-order \nenergy correction\n \n E (2)\n3\n= H =\n31c(1)\n31 + H =\n32c(1)\n32 + H =\n34c(1)\n34 + ... \n \n = a\nm\u00023\nH =\n3mc(1)\n3m.\n \n(10.79)\nAs we said earlier, the second-order energy correction depends on the ﬁrst-order state vector correc-\ntion through the coefﬁcients c(1)\n3m. We substitute for the coefﬁcients from Eq. (10.53) to get\n \n E (2)\n3\n= a\nm\u00023\nH =\n3m \nH =\nm3\nE (0)\n3\n- E (0)\nm\n \n \n = a\nm\u0002383(0)@H\u0004@ m(0)9 8m(0)0 H\u00040 3(0)9\nAE (0)\n3\n- E (0)\nm B\n \n(10.80)\n \n = a\nm\u00023\n083(0)0 H\u00040 m(0)9 0\n2\nAE (0)\n3\n- E (0)\nm B\n  .\n \nHaving solved for the speciﬁc n = 3 case, we now generalize this result to\n \nE (2)\nn\n= a\nm\u0002n\n08n(0)0 H\u00040 m(0)9 0\n2\n1E (0)\nn\n- E (0)\nm 2\n  . \n(10.81)\n\n332 \nPerturbation Theory\nThe second-order energy correction is proportional to the squares of matrix elements connecting states \nand inversely proportional to the energy differences between those states. Hence, states that are nearby \nin energy generally have a stronger inﬂuence on the perturbation, and because the connecting matrix \nelements are squared, the sign of the energy shift is determined solely by the sign of the energy dif-\nference in the denominator. Energy levels m that lie above the state n give a negative contribution \nand hence push the energy level n down, away from the states m. Energy levels m that lie below \nthe state n give a positive contribution and hence push the energy level n up, also away from the \nstates m. The take-away message is that in second order, energy levels tend to repel each other, as \nshown in Fig. 10.7. This result has the quadratic form we expected from our examples on two-level \nproblems at the beginning of the chapter.\nExample 10.3 A sodium nucleus in a constant magnetic ﬁeld in the z-direction B0 = B0zn is sub-\nject to a perturbation caused by an additional magnetic ﬁeld B\u0004 = B2xn applied to the system. Find \nthe second-order energy shifts due to the perturbation and the state vectors correct to ﬁrst order.\nThis problem is a variation on the spin-3/2 problem in Example 10.1 and also parallels the \nspin-1/2 problem in Section 10.1. Here, the perturbing ﬁeld is perpendicular to, rather than paral-\nlel to, the zeroth-order ﬁeld B0 = B0zn. As in Example 10.1, the zeroth-order Hamiltonian H0 is \ndetermined by the potential energy of the nuclear magnetic moment in the uniform ﬁeld B0 = B0zn :\n \nH0 = -M~B0 = v0Sz \u0003 •\n3\n2 U v0\n0\n0\n0\n0\n1\n2 U v0\n0\n0\n0\n0\n-  1\n2 U v0\n0\n0\n0\n0\n-  3\n2 U v0\nμ, \n(10.82)\nwhere we have deﬁned the Larmor frequency v0 = gNaeB0>2mp. The zeroth-order energies are \nE (0)\n1\n= 3\n2 U v0 , E (0)\n2\n= 1\n2 U v0 , E (0)\n3\n= -  1\n2 U v0 , and E (0)\n4\n= -  3\n2 U v0 , using the same state labeling as \nin Example 10.1.\nperturbation\nEnergy\nE(2)\n2\nE(0)\n2\nE(0)\n1\nE(2)\n1\nFIGURE 10.7 Energy levels repel each other in second-order perturbation theory.\n\n10.4  Second-Order Nondegenerate Perturbation Theory \n333\nThe perturbation Hamiltonian H\u0004 is determined by the ﬁeld B\u0004 = B1xn and is characterized by \na different Larmor frequency v2 = gNaeB2>2mp :\n \nH\u0004 = -M~B\u0004 = v2Sx \u0003 •\n0\n13\n2  U v2\n0\n0\n13\n2  U v2\n0\n14\n2  U v2\n0\n0\n14\n2  U v2\n0\n13\n2  U v2\n0\n0\n13\n2  U v2\n0\nμ. \n(10.83)\nThis perturbation Hamiltonian has no diagonal elements, so there are no ﬁrst-order energy cor-\nrections, in contrast to Example 10.1. The off-diagonal elements give ﬁrst-order state vector \ncorrections [Eq. (10.62)] and second-order energy corrections [Eq. (10.81)]. We’ll calculate the \nsecond-order energy corrections ﬁrst.\nThe second-order correction to the energy is proportional to the squares of matrix elements \nconnecting states and inversely proportional to the energy differences between those states:\n \nE (2)\nn\n= a\nm\u0002n\n@8n(0)@H\u0004@ m(0)9@\n2\n1E (0)\nn\n- E (0)\nm 2\n. \n(10.84)\nFor the @ 1(0)9 state, the energy shift is given by a sum, but there is only one term in the sum because \nonly H =\n12 \u0002 0 :\n \n E (2)\n1\n= a\nm\u0002n\n@81(0)@H\u0004@ m(0)9@\n2\n1E (0)\n1\n- E (0)\nm 2\n=\n@81(0)@H\u0004@ 2(0)9@\n2\n1E (0)\n1\n- E (0)\n2 2\n \n \n =\n@13\n2  U v2@\n2\n13\n2 U v0 - 1\n2 U v02\n \n(10.85)\n \n =\n3U v2\n2\n4v0\n.\n \nFor the @ 2(0)9 state, the energy shift is\n \n E (2)\n2\n= a\nm\u0002n\n@82(0)@H\u0004@ m(0)9@\n2\n1E (0)\n2\n- E (0)\nm 2\n=\n@82(0)@H\u0004@ 1(0)9@\n2\n1E (0)\n2\n- E (0)\n1 2\n+\n@82(0)@H\u0004@ 3(0)9@\n2\n1E (0)\n2\n- E (0)\n3 2\n \n =\n@13\n2  U v2@\n2\n11\n2 U v0 - 3\n2 U v02\n+\n@14\n2  U v2@\n2\n11\n2 U v0 - -1\n2  U v02\n \n(10.86)\n \n =\nU v2\n2\n4v0\n.\n\n334 \nPerturbation Theory\nSimilarly, the shifts for the @ 3(0)9 and @ 4(0)9 state are:\n \n E (2)\n3\n= -  \nU v2\n2\n4v0\n \n \n E (2)\n4\n= -  \n3U v2\n2\n4v0\n.\n \n(10.87)\nAdding these energy shifts to the zeroth-order energies gives the perturbed energies shown in \nFig. 10.8 as a function of the perturbing ﬁeld magnitude B2. The new energies exhibit the quadratic \ndependence we expect for second-order shifts and also illustrate the repulsion of levels that we \nexpect. The NMR transitions are indicated by the arrows in Fig. 10.8. The resonance shifts in this \ncase are different from the shifts in Fig. 10.4, because the perturbing ﬁeld is perpendicular to, rather \nthan parallel to, the constant B0 ﬁeld. Hence chemical shifts also provide information about the \nspatial alignment of the system with respect to the constant B0 ﬁeld.\nThe ﬁrst-order state vector correction is\n \n@ n(1)9 = a\nm\u0002n\n 8m(0)@H\u0004@ n(0)9\n1E (0)\nn\n- E (0)\nm 2\n0 m(0)9. \n(10.88)\nFor the @ 1(0)9 state, the ﬁrst-order correction again has only one term in the sum because only \nH =\n12 \u0002 0 :\n \n @ 1(1)9 = a\nm\u0002n\n 8m(0)@H\u0004@ 1(0)9\n1E (0)\n1\n- E (0)\nm 2\n @ m(0)9 = 82(0)@H\u0004@ 1(0)9\n1E (0)\n1\n- E (0)\n2 2\n @ 2(0)9\n \n =\n13\n2  U v 2\n13\n2 U v0 - 1\n2 U v02\n @ 2(0)9\n \n = 23v2\n2v0\n @ 2(0)9.\n \n(10.89)\n0.5\n1.0\n1.5\n2.0\nB2(Tesla)\n\u000640\n\u000620\n20\n40\nE/h(MHz)\nm \u0007 3\n2\nm \u0007 1\n2\nm \u0007 –3\n2\nm \u0007 –1\n2\nFIGURE 10.8 Energy shifts of a sodium nucleus in a perturbing magnetic ﬁeld \nthat is perpendicular to the constant zeroth-order ﬁeld.\n\n10.4  Second-Order Nondegenerate Perturbation Theory \n335\nThe state vector correct to ﬁrst order includes the zeroth-order state:\n \n0 19 = @ 1(0)9 + 23v 2\n2v0\n @ 2(0)9. \n(10.90)\nSimilarly, the other corrected states to ﬁrst order are (Problem 10.6)\n \n 0 29 = @ 2(0)9 - 23v 2\n2v0\n @ 1(0)9 + v 2\nv0\n @ 3(0)9\n \n 0 39 = @ 3(0)9 - v 2\nv0\n @ 2(0)9 + 23v 2\n2v0\n @ 4(0)9 \n \n(10.91)\n \n 0 49 = @ 4(0)9 - 23v 2\n2v0\n @ 3(0)9.\nTo conclude, we repeat the matrix calculation of the second-order energy correction using Dirac \nbra-ket notation. As before, we project out the desired state by taking the inner product of the second-\norder equation (10.37) with the nth zero-order eigenstate and get\n \n8n(0)@1H0 - E (0)\nn 2@ n(2)9 = 8n(0)@1E (1)\nn\n- H\u00042@ n(1)9 + 8n(0)@E (2)\nn @ n(0)9. \n(10.92)\nThe Hamiltonian H0 is Hermitian, so it can act backwards on the bra 8n(0)@ to give the energy \n18n(0)@H0 = 8n(0)@E (0)\nn 2, giving zero on the left-hand side:\n \n 8n(0)@1E (0)\nn\n- E (0)\nn 2@ n(2)9 = 8n(0)@1E (1)\nn\n- H\u00042@ n(1)9 + E (2)\nn  \n \n 0 = E (1)\nn 8n(0)@ n(1)9 - 8n(0)@H\u0004@ n(1)9 + E (2)\nn .\n \n(10.93)\nThe ﬁrst-order state vector correction @ n(1)9 is orthogonal to the original state @ n(0)9, making the ﬁrst \nterm on the right-hand side zero. Now solve for the second-order energy correction\n \n 0 = 0 - 8n(0)@H\u0004@ n(1)9 + E (2)\nn  \n \n E (2)\nn\n= 8n(0)@H\u0004@ n(1)9.\n \n(10.94)\nUse the previous result for the ﬁrst-order state vector correction to get the second-order energy \ncorrection\n \n E (2)\nn\n= a\nm\u0002n\ncmn8n(0)@H\u0004@ m(0)9\n \n \n E (2)\nn\n= a\nm\u0002n\n 8m(0)@H\u0004@ n(0)9\n1E (0)\nn\n- E (0)\nm 2\n 8n(0)@H\u0004@ m(0)9 \n(10.95)\n \n E (2)\nn\n= a\nm\u0002n\n \n@8n(0)@H\u0004@ m(0)9@\n2\n1E (0)\nn\n- E (0)\nm 2\n,\n \nwhich is the general result we found in Eq. (10.81). We haven’t bothered to ﬁnd the second-order \ncorrections to the eigenstates, because we are primarily concerned with energy corrections. If you are \nso inclined, you can derive them from the m\u00023 rows of Eq. (10.78).\n\n336 \nPerturbation Theory\n10.5 \u0002 DEGENERATE PERTURBATION THEORY\nAs we have pointed out several times, states that are degenerate in energy with respect to the zeroth-\norder Hamiltonian create a problem in perturbation theory. If two states have the same zeroth-order \nenergy, then the term E (0)\nn\n- E (0)\nm  in the denominator of the ﬁrst-order state vector correction in \nEq. (10.67) becomes zero and the correction is no longer small, but rather diverges. This same \nproblematic denominator appears in the second-order energy correction in Eq. (10.81). Though \ndegeneracy creates no problem with the ﬁrst-order energy correction, these divergences still call \ninto question our overall approach to the problem. So we must explicitly address systems with \nenergy degeneracy.\nIf we look back at the matrix solution to the ﬁrst-order perturbation equation in Section 10.3, we \ncan identify the source of the divergence problem and decide how to proceed. The terms that cause \nthe divergence are the diagonal terms E (0)\nn\n- E (0)\n3  in the matrix representing H0 - E (0)\n3  in Eq. (10.43). \nWe knew that the term E (0)\n3\n- E (0)\n3  was zero and we used that fact to ﬁnd the ﬁrst-order energy cor-\nrection E (1)\n3 , as we indicated with the boxed parts of the matrix equations. However, we didn’t expect \nany of the other diagonal terms to be zero and so we ended up dividing by them in Eq. (10.53). \nLet’s identify which diagonal terms are zero before we start the solution and then we can avoid the \ndivision- by-zero problem.\nAgain, it helps to choose a speciﬁc example to illustrate the basic idea and then generalize at the \nend. Let’s assume that the n = 2 and n = 3 states of a system are degenerate with the same energy \nE (0)\n2 . The zeroth-order Hamiltonian in that case is\n \nH0 \u0003 ß\nE (0)\n1\n0\n0\n0\ng\n0\nE (0)\n2\n0\n0\ng\n0\n0\nE (0)\n2\n0\ng\n0\n0\n0\nE (0)\n4\ng\nf\nf\nf\nf\nf\n∑. \n(10.96)\nThe ﬁrst-order perturbation equation we want to solve is [see Eq. (10.36)]\n \n1H0 - E (0)\nn 2@n(1)9 = 1E (1)\nn\n- H\u00042@n(0)9, \n(10.97)\nwhere we are trying to ﬁnd the correction E (1)\n2 . The matrix 1H0 - E (0)\n2 2 on the left-hand side of \nEq. (10.97) is then\n \n1H0 - E (0)\n2 2 \u0003 ß\nE (0)\n1\n- E (0)\n2\n0\n0\n0\ng\n0\n0\n0\n0\ng\n0\n0\n0\n0\ng\n0\n0\n0\nE (0)\n4\n- E (0)\n2\ng\nf\nf\nf\nf\nf\n∑. \n(10.98)\n\n10.5 Degenerate Perturbation Theory \n337\nNow we have two zeros along the diagonal instead of one. More important, there is a whole submatrix, \nindicated by the central boxed item, that is equal to zero, instead of a single number [Eq. (10.43)], and \nthat submatrix is isolated from the rest of the matrix by virtue of the zeros in all the corresponding \nrows and columns.\nTurning now to the right-hand side of Eq. (10.97), the matrix 1E (1)\n2\n- H\u00042 is\n \n1E (1)\n2\n- H\u00042 \u0003 ß\nE (1)\n2\n- H =\n11\n-H =\n12\n-H =\n13\n-H =\n14\ng\n-H =\n21\nE (1)\n2\n- H =\n22\n-H =\n23\n-H =\n24\ng\n-H =\n31\n-H =\n32\nE (1)\n2\n- H =\n33\n-H =\n34\ng\n-H =\n41\n-H =\n42\n-H =\n43\nE (1)\n2\n- H =\n44\ng\nf\nf\nf\nf\nf\n∑, \n(10.99)\nwhere we have identiﬁed the submatrix corresponding to the zero submatrix in Eq. (10.98). In the \nnondegenerate case, we equated the two sides of Eq. (10.97) and found the solution for the ﬁrst-order \nenergy correction in the row corresponding to the energy level of interest [Eq. (10.45)]. But now the \nenergy level of interest is degenerate and corresponds to two rows and two columns of the matrices, \nas indicated in Eqs. (10.98) and (10.99). So instead of a single equation for the energy correction, we \nhave a matrix equation. To ﬁnd this matrix equation, we must include the column vectors representing \nthe states in Eq. (10.97). On the left-hand side is the unknown state correction @ 2(1)9 or @ 3(1)9:\n \n@ 2(1)9 \u0003 ß\nc(1)\n21\nc(1)\n22\nc(1)\n23\nc(1)\n24\nf\n∑, @ 3(1)9 \u0003 ß\nc(1)\n31\nc(1)\n32\nc(1)\n33\nc(1)\n34\nf\n∑.  \n(10.100)\nOn the right-hand side is the known eigenstate @ 2(0)9, or its degenerate partner @ 3(0)9:\n \n@ 2(0)9 \u0003 ¶\n0\n1\n0\n0\nf\n∂, @ 3(0)9 \u0003 ¶\n0\n0\n1\n0\nf\n∂. \n(10.101)\nBut the energy degeneracy of these two states creates an ambiguity. Both @ 2(0)9 and @ 3(0)9 satisfy \nthe zeroth-order energy eigenvalue equation for the energy E (0)\n2 , but so does any linear combination of \nthe two states. If we are trying to ﬁnd the energy correction to the state with zeroth-order energy E (0)\n2 ,\nhow do we know whether to use the state @ 2(0)9 or the state @ 3(0)9 in the perturbation equation? We don’t \n\n338 \nPerturbation Theory\nknow which one to use. We have no information that would help us decide which linear combination is \n“correct,” so we let the solution to the problem tell us! We leave the state unspeciﬁed and write it as\n \n@ 2(0)\nnew9 \u0003 ¶\n0\na\nb\n0\nf\n∂. \n(10.102)\nThis ambiguity turns out to be the answer to our degeneracy problem in perturbation theory.\nNow the left-hand side of Eq. (10.97) is\n \n 1H0 - E (0)\nn 2@ n(1)9 \u0003 ß\nE (0)\n1\n- E 2\n(0)\n0\n0\n0\ng\n0\n0\n0\n0\ng\n0\n0\n0\n0\ng\n0\n0\n0\nE (0)\n4\n- E (0)\n2\ng\nf\nf\nf\nf\nf\n∑ ß\nc(1)\n21\nc(1)\n22\nc(1)\n23\nc(1)\n24\nf\n∑ \n \n \u0003 ß\n1E (0)\n1\n- E (0)\n3 2c(1)\n21\n0\n0\n1E (0)\n4\n- E (0)\n3 2c(1\n24\n)\nf\n∑\n \n(10.103)\nand the right-hand side is\n \n 1E (1)\nn\n- H\u00042@ n(0)9 \u0003 ß\nE (1)\n2\n- H =\n11\n-H =\n12\n-H =\n13\n-H =\n14\ng\n-H =\n21\nE (1)\n2\n- H =\n22\n-H =\n23\n-H =\n24\ng\n-H =\n31\n-H =\n32\nE (1)\n2\n- H =\n33\n-H =\n34\ng\n-H =\n41\n-H =\n42\n-H =\n43\nE (1)\n2\n- H =\n44\ng\nf\nf\nf\nf\nf\n∑ ß\n0\na\nb\n0\nf\n∑\n \n \u0003 ß\n-H =\n12a - H =\n13b\n1E 2\n(1) - H =\n222a - H =\n23b\n-H =\n32a + 1E 2\n(1) - H =\n332b\n-H =\n42a - H =\n43b\nf\n∑.  \n(10.104)\n\n10.5 Degenerate Perturbation Theory \n339\nWe equate the two rows of interest to get the set of equations\n \n 1E (1)\n2\n- H =\n222a - H =\n23b = 0  \n \n -H =\n32a + 1E (1)\n2\n- H =\n332b = 0. \n(10.105)\nWe write these equations in matrix form\n \naE 2\n(1) - H =\n22\n-H =\n23\n-H =\n32\nE 2\n(1) - H =\n33\nb aa\nbb = 0, \n(10.106)\nwhich shows that we have isolated the submatrix of the full perturbation equation corresponding to the \ntwo degenerate levels. Now rewrite Eq. (10.106) in a simpler form\n \naH =\n22\nH =\n23\nH =\n32\nH =\n33\nb aa\nbb = E 2\n(1) aa\nbb . \n(10.107)\nThis equation looks surprisingly like a standard energy eigenvalue equation. However, this equation \nis limited to only the two states comprising the degenerate energies, with the perturbation Hamilto-\nnian playing the role of the Hamiltonian, while the energy eigenvalue is the ﬁrst-order correction that \nwe are seeking. We commonly refer to this isolated subspace of the whole system as the  degenerate \nsubspace.\nWe solve Eq. (10.107) by the standard procedure of diagonalization for eigenvalue equations, \nwhich yields two eigenvalues and two eigenstates. The eigenvalues are the two corrections E (1)\n2  to the \nzeroth-order energy E (0)\n2 . If we label the two energy solutions E (1)\n2a and E (1)\n2b , then the energies correct \nto ﬁrst order are\n \n E 2a = E (0)\n2\n+ E (1)\n2a  \n \n E 2b = E (0)\n2\n+ E (1)\n2b . \n(10.108)\nThe two sets of a and b coefﬁcients from solving Eq. (10.107) give the two eigenstate solutions @ 2a9 \nand  @ 2b9 that form a new basis in the degenerate subspace that was originally deﬁned by @ 2(0)9 and \n@ 3(0)9. In this new basis, the perturbation Hamiltonian is diagonal.\nNow let’s generalize our speciﬁc solution. The result of the twofold degenerate example was the \neigenvalue equation (10.107) for the perturbation Hamiltonian within the degenerate subspace. The \nperturbation corrections are found by solving that eigenvalue equation through the diagonalization \nprocedure we use throughout quantum mechanics. So there is no silver bullet formula for degenerate \nperturbation theory. There is just the mantra:\nDiagonalize the perturbation Hamiltonian in the degenerate subspace.\nThat’s it! Let’s see how it works.\nExample 10.4 An electron is bound to move on the surface of a sphere. Find the energy correc-\ntions caused by a perturbing magnetic ﬁeld B\u0004 = B1xn , limiting your consideration to the ﬁrst two \nenergy levels.\n\n340 \nPerturbation Theory\nThe zeroth-order Hamiltonian of a particle on a sphere is the kinetic energy, as we found in \nSection 7.6:\n \nHsphere = L2\n2I . \n(10.109)\nIn that problem, we found the zeroth-order eigenstates to be the angular momentum eigenstates\n \n0 /m9 \u0003 Y m\n/ 1u, f2 \n(10.110)\nand the zeroth-order energies to be\n \nE/ = U2\n2I\n /1/ + 12, \n(10.111)\nas illustrated in the energy spectrum of Fig. 7.16. The energy is independent of the magnetic quan-\ntum number m, so each energy level is degenerate except the / = 0 ground state, with (2/ + 1) \npossible m states for a given /. Hence the need for degenerate perturbation theory.\nThe perturbation Hamiltonian is the potential energy of interaction H\u0004 = -ML~B\u0004 between the \napplied magnetic ﬁeld and the magnetic moment of the electron due to its orbital angular momen-\ntum (we ignore spin angular momentum here). The electron magnetic moment associated with the \norbital motion is\n \nML = -  e\n2me\n L \n(10.112)\nand the resultant perturbation Hamiltonian is\n \n H\u0004 =\ne\n2me\n L~B\u0004 =\ne\n2me\n B1Lx \n \n = v1Lx.\n \n(10.113)\nThe Larmor frequency in this case is v1 = eB1>2me.  \nWe limit ourselves to the ﬁrst two energy levels: E (0)\n0\n= 0 and E (0)\n1\n= U2>I. The ground state \nis nondegenerate and the ﬁrst excited state is threefold degenerate, with the zeroth-order Hamilto-\nnian for these states\n \nH0 \u0003 •\n0\n0\n0\n0\n0\nU2>I\n0\n0\n0\n0\nU2>I\n0\n0\n0\n0\nU2>I\nμ \n/\nm\n0\n0\n1\n1\n1\n0\n1\n-1\n. \n(10.114)\nUsing the matrix representation of Lx in Eq. 7.62 for / = 1, we ﬁnd the perturbation Hamiltonian:\n \nH\u0004 \u0003 •\n0\n0\n0\n0\n0\n0\nU v1> 12\n0\n0\nU v1> 12\n0\nU v1> 12\n0\n0\nU v1> 12\n0\nμ \n/\nm\n0\n0\n1\n1\n1\n0\n1\n-1\n. \n(10.115)\n\n10.5 Degenerate Perturbation Theory \n341\nThe / = 0 level is nondegenerate, but the diagonal and off-diagonal matrix elements for \n/ = 0 are zero, so there is neither a ﬁrst-order nor a second-order correction to that energy. The \n/ = 1 level is threefold degenerate, so we must diagonalize the perturbation Hamiltonian in the \ndegenerate subspace:\n \nH =\n/=1 \u0003 §\n0\nU v1> 12\n0\nU v1> 12\n0\nU v1> 12\n0\nU v1> 12\n0\n¥ . \n(10.116)\nThe characteristic equation is\n \n 0 = 4  \nl\n-U v1> 12\n0\n-U v1> 12\nl\n-U v1> 12\n0\n-U v1> 12\nl\n 4\n \n \n = l1l2 - U2v2\n1>22 - 1-U v1> 1221-U v1> 122l \n \n = l1l2 - U2v2\n12\n \n(10.117)\nwith solutions\n \nl = U v1, 0, -U v1. \n(10.118)\nThis result is to be expected because the eigenvalues of Lx for / = 1 are U, 0, -U and the perturba-\ntion is H\u0004 = v1Lx . The resultant energy shifts are\n \nE (1)\n1a = U v1\n \n \nE (1)\n1b = 0\n \n(10.119)\n \nE (1)\n1c = -U v1. \nThe perturbed energy spectrum of the ﬁrst two levels is shown in Fig. 10.9.\nThe eigenstates of the perturbation Hamiltonian are the Lx eigenstates @ 1mx9 because the per-\nturbation is H\u0004 = v1Lx . If we had written the perturbation Hamiltonian in the Lx basis instead of \nthe usual Lz basis, then the matrix representing the perturbation Hamiltonian would have already \nbeen diagonal and the problem would be solved by inspection. The Lx eigenstates are thus the “cor-\nrect” basis for the perturbation problem. The solution for higher-order states now becomes clear. \nEach energy state is split into the (2/ + 1) possible mx states for that / state.\n\n342 \nPerturbation Theory\nAs we pointed out in the example above, if the perturbation Hamiltonian is already diagonal \nin the degenerate subspace, then the solution is obtained by inspection of the perturbation matrix: \nthe energy corrections are the diagonal elements H =\nnn = 8n(0)@H\u0004@ n(0)9. But this is exactly what \nﬁrst-order nondegenerate perturbation theory tells us to do! So if the perturbation is diagonal in the \ndegenerate subspace, then degenerate perturbation theory reduces to nondegenerate theory. In this \ncase, the problem with the divergent denominators fades because the off-diagonal elements that \nappear in those same numerators [Eqs. (10.67) and (10.81)] are now zero. If the perturbation is not \ndiagonal, then we make it diagonal by applying the diagonalization procedure to solve Eq. (10.107). \nThe resultant eigenvalues (i.e., the diagonal elements in the new basis) are the desired energy cor-\nrections and the resultant eigenstates form the “correct” basis that avoids the divergence problems.\nGiven the ambiguity of the zeroth-order eigenstates in the degenerate subspace, we can try to be \nclever enough to start the problem by choosing the “correct” or “good” basis that makes the pertur-\nbation diagonal. We can do that by using eigenstates of some operator that commutes with both the \nzeroth-order Hamiltonian and the perturbation Hamiltonian. In Example 10.4, the Lx basis is the “cor-\nrect” basis because the perturbation Hamiltonian is diagonal in that basis.\nLet’s conclude with some remarks about what is really going on with degenerate perturbation the-\nory. We know that the general method for ﬁnding the energy eigenvalues of a system is to diagonalize \nthe Hamiltonian. When we add a perturbation to a system, we must rediagonalize the full Hamiltonian. \nIf that is possible, by all means do it and you will have the exact answer to the problem. But perturba-\ntion theory is designed to tackle problems where we cannot, for whatever reason, diagonalize the full \nHamiltonian. So we diagonalize the zeroth-order Hamiltonian and then try to ﬁnd a way to account for \nthe perturbation Hamiltonian as best we can.\nIn the case of degenerate energy levels, we found that the nondegenerate perturbation theory \ndeveloped in Section 10.3 led to divergences when the degenerate energy states were involved. The \nsolution we presented above is to diagonalize the perturbation Hamiltonian within the degenerate sub-\nspace. However, one might ask whether we are justiﬁed in ignoring the original Hamiltonian and only \ndiagonalizing the perturbing Hamiltonian because the diagonalization procedure amounts to a trans-\nformation of bases, which one would expect to disturb the original basis and hence “ undiagonalize” the \nB1\n0\n1\n2\nE(\u00022/2I)\n0\n0\n1 \n1\n1\n0\n1\n1\nmx\nl\nFIGURE 10.9 Perturbed energies of an electron bound to a sphere with \nan applied magnetic ﬁeld.\n\n10.6  More Examples \n343\noriginal Hamiltonian. But the original Hamiltonian is degenerate within the subspace  corresponding \nto the degenerate energy, so it is proportional to the identity matrix within that subspace [Eq. (10.96)], \nand a transformation of the identity matrix leaves it unchanged. This is the mathematical consequence \nof the arbitrariness of choice of basis in the original problem. Hence, we are able to diagonalize the \nperturbation Hamiltonian within the degenerate subspace without undiagonalizing the zeroth-order \nHamiltonian.\nIn some cases, the perturbing Hamiltonian is already diagonal, in which case there is not much \nwork to do. The ﬁrst-order energy corrections are obtained by inspection of the matrix as the diagonal \nelements H =\nnn , which are the expectation values of the perturbation in the degenerate subspace. This \nresult is the same as we obtained with nondegenerate perturbation theory, so if we choose the original \nbasis correctly, then degenerate and nondegenerate perturbation theory are the same.\nIf the perturbing Hamiltonian is not diagonal, then we must go through the diagonalization proce-\ndure to ﬁnd the new eigenvalues and eigenstates. The energy results are the same that we would have \nobtained if we had done nondegenerate perturbation theory using the “correct” basis, (i.e., the one we \nhave found to diagonalize the perturbation). So in some sense we have merely found the “right” basis \nin which nondegenerate perturbation theory is valid.\nYou might ask whether we have found the exact solution and not just an approximation. The \nanswer is no. By diagonalizing the perturbation Hamiltonian only within the degenerate subspace, we \nneglect other terms in the perturbation Hamiltonian that connect the degenerate states with other states \nin the system [Eq. (10.104)]. That we can safely neglect these other states in degenerate perturbation \ntheory is suggested by the second-order energy correction in Eq. (10.81), which says that states farther \naway contribute less to perturbation. Degenerate states are the closest to each other and hence contrib-\nute the most to the corrections.\n10.6 \u0002 MORE EXAMPLES\nLet’s apply perturbation theory to some of our favorite systems.\n10.6.1 \u0002 Harmonic Oscillator\nThe harmonic oscillator has zeroth-order Hamiltonian\n \nH0 = pn 2\n2m + 1\n2 mv2xn  2 = U v1a-a + 1\n22,  \n(10.120)\nwhere we have written it using ladder operators because that makes the calculations easier. Now con-\nsider adding a perturbation to the Hamiltonian of the form\n \nH\u0004 = e 1\n2 mv2 xn  2, \n(10.121)\nwhere \u0015 is a small dimensionless term parameterizing the strength of the perturbation. This perturba-\ntion has a parabolic spatial dependence, which is the same as the zeroth-order Hamiltonian, so the \nsolution is known exactly, but we proceed with the example to see how the method is applied. For \na positive value of \u0015, the strength of the harmonic well is increased, which conﬁnes the particle to a \nsmaller region of space. The uncertainty principle then leads us to expect an increase in the energy.\n\n344 \nPerturbation Theory\nThe energy levels of the harmonic oscillator are nondegenerate, so we use nondegenerate pertur-\nbation theory. The ﬁrst-order correction to the energy is the expectation value\n \nE (1)\nn\n= 8n(0)@H\u0004@ n(0)9. \n(10.122)\nTo calculate this, it is most convenient to express the perturbation Hamiltonian using ladder  operators \n[Eq. (9.95)]:\n \n H\u0004 = e 1\n2 mv2 a\nU\n2mvb1a- + a2\n2\n \n \n H\u0004 = e 1\n4 U v 1a-a- + a-a + aa- + aa2. \n(10.123)\nThe expectation value of the perturbation is\n \nE (1)\nn\n= e 1\n4 U v8n(0)@1a-a- + a-a + aa- + aa2@ n(0)9. \n(10.124)\nThe operators a-a- and aa contribute zero because they raise or lower the state @ n(0)9 twice and produce \na new state that is orthogonal to @ n(0)9. The remaining terms are calculated using a@ n9 = 2n0 n - 19\n and a-0 n9 = 2n + 10 n + 19 :\n \n E (1)\nn\n= e 1\n4 U v8n(0)@1a-a + aa-2@ n(0)9\n \n \n = e 1\n4 U v8n(0)@A 2n2n + 2n + 12n + 1B @ n(0)9 \n \n = e 1\n4 U v1n + n + 12\n \n \n = e 1\n2 U v1n + 1\n22.\n \n(10.125)\nThe resultant energy of level n to ﬁrst order in the perturbation is\n \n En = E (0)\nn\n+ E (1)\nn\n \n \n = U v1n + 1\n22 + e 1\n2 U v1n + 1\n22 \n \n = U v1n + 1\n2211 + 1\n2 e2.\n \n(10.126)\nEach state is shifted upwards, with the shift larger for larger states. The original and ﬁrst-order per-\nturbed energy levels are shown in Fig. 10.10(a).\nNow consider the second-order energy correction\n \nE (2)\nn\n= a\nm\u0002n\n@8n(0)@H\u0004@ m(0)9@\n2\n1E (0)\nn\n- E (0)\nm 2\n. \n(10.127)\nThis looks like an inﬁnite sum, which would be problematic, but we plow ahead and ﬁnd that the sum \nis reduced for the harmonic oscillator case. The matrix elements are\n \n 8n(0)@H\u0004@ m(0)9 = e 1\n4 U v8n(0)@1a-a- + a-a + aa- + aa2@ m(0)9\n \n \n = e 1\n4 U vC\n2m + 12m + 2dn,m+2 + 2m2mdn,m\n+ 2m + 12m + 1dn,m + 2m2m - 1dn,m-2\nS. \n(10.128)\n\n10.6  More Examples \n345\nFor a given energy level n, only two terms in the sum (m \u0002 n) contribute, yielding \n \n E (2)\nn\n=\nC1\n4 e U v2n - 12n D  \n2\nE (0)\nn\n- E (0)\nn - 2\n+\nC1\n4 e U v2n + 22n + 1 D  \n2\nE (0)\nn\n- E (0)\nn  + 2\n \n \n = 11\n4 eU v2\n2c n(n - 1)\n2U v\n+ (n + 1)(n + 2)\n-2U v\nd\n \n =\n1\n32 e2U v3n2 - n - 1n2 + 3n + 224\n \n = -1\n8 e2U v1n + 1\n22.\n \n(10.129)\nNote that the second-order contribution is negative. Only the two levels m = n + 2 and m = n - 2 \ncontribute to the energy correction in Eq. (10.129). They each have the same magnitude energy \ndenominators, but the matrix element is larger for the m = n + 2 state above the state of interest, so \nthe level is pushed down. The resultant energy of level n to second order in the perturbation is\n \n En = E (0)\nn\n+ E (1)\nn\n+ E (2)\nn\n \n \n = U v1n + 1\n2211 + 1\n2 e - 1\n8 e22. \n(10.130)\nThe perturbed energies to ﬁrst and second order and the exact result are plotted in Fig. 10.10(b) as a \nfunction of the perturbation strength.\nx\nE\n0.0\n0.5\n1.0\nΕ\n0.0\n0.2\n0.4\n(a)\n(b)\nE(1)\n2\nE(0)\n2\nE(1)\n1\nE(0)\n0\nE(1)\n0\nE(0)\n1\n\u0012E/E0\nFIGURE 10.10 Perturbation of the harmonic oscillator. (a) Shifts of the ﬁrst \nthree energy levels. (b) Dependence of the shift of the ground state energy on  \nthe perturbation strength to ﬁrst order (dotted), second order (dashed), and  \nexact (solid).\n\n346 \nPerturbation Theory\nIn this example, we can ﬁnd the exact answer, so we can check the perturbation result and conﬁrm \nthat perturbation theory works. The exact Hamiltonian is\n \n H = H0 + H\u0004\n \n \n = pn 2\n2m + 1\n2 mv2xn  2 + e 1\n2 mv2xn  2 \n \n = pn 2\n2m + 1\n2 mv2xn  211 + e2\n \n \n = pn 2\n2m + 1\n2 mv2\np\n xn  2,\n \n(10.131)\nwhere we have deﬁned a new perturbed harmonic frequency\n \nvp = v21 + e. \n(10.132)\nThis new Hamiltonian has the same form as the original harmonic oscillator problem we have already \nsolved, but with a new characteristic frequency. Hence, we know the energy eigenvalues exactly. \nThey are\n \nEn = 1n + 1\n22U vp = 1n + 1\n22U v21 + e. \n(10.133)\nThe perturbation theory result in Eq. (10.130) was obtained to second order in the perturbation param-\neter e, so we must compare it to the exact result at this same order. Expanding the exact result in powers\nof e gives\n \n En = 1n + 1\n22U v11 + e2\n1>2\n \n \n \n = 1n + 1\n22U v11 + 1\n2 e - 1\n8 e2 + ...2. \n(10.134)\nThus we see that the two results agree, at least to second order.\nNote that the operator approach made our life very easy here—there was no need to do any spatial \nintegrals. However, this is not always the case. For the harmonic oscillator problem, one can always \nuse the operator approach because the spatial dependence of the perturbing potential energy, no matter \nhow complicated, can be expressed as a polynomial in x and written in terms of the ladder operators. \nBut for other potential wells, we do not know how to write the spatial function in terms of any simple \noperators and a spatial integral is often required.\n10.6.2 \u0002 Stark Effect in Hydrogen\nNow consider a perturbation of the hydrogen atom. The Stark effect is the perturbation of energies \ncaused by an external electric ﬁeld. For hydrogen, this example gives us a chance to practice degener-\nate perturbation theory. The unperturbed Hamiltonian is the sum of kinetic and Coulomb potential \nenergies that we studied in Chapter 8:\n \n H0 = p2\n2m + V1r2\n \n \n = p2\n2m -\nZe2\n4pe0r . \n(10.135)\n\n10.6  More Examples \n347\nThe eigenstate solutions to this zeroth-order problem are labeled with quantum numbers n/m\n \n@ c(0)9 = @ n/m(0)9 \u0003 cn/m\n(0) 1r, u, f2 = Rn/1r2Y /\nm1u, f2 \n(10.136)\nand the eigenenergies are\n \nE (0)\nn\n= -  Z 2\nn2 Ryd . \n(10.137)\nThese energy levels are degenerate because the energy is not a function of / and m. Each energy level \nhas n2 states, so only the ground state with n \u0003 1 is nondegenerate. Thus, we expect to have to use both \nnondegenerate and degenerate perturbation theory.\nTo perturb the system, we apply a uniform electric ﬁeld of magnitude E. When we solved the \nhydrogen atom problem in Chapter 8, we used spherical coordinates because of the spherical symme-\ntry of the problem. The applied electric ﬁeld has a speciﬁc direction in space and breaks the spherical \nsymmetry of the problem, but we continue to use spherical coordinates because we use the original \nbasis states in the perturbation solutions. Because the z-axis in spherical coordinates is special (the \npolar angle is measured from it and the azimuthal angle is measured about it), it is simplest to assume \nthat the applied ﬁeld is aligned along the z-axis: E = E zn.  The perturbation Hamiltonian is the poten-\ntial energy of the hydrogen atom in the applied ﬁeld. The electron and nucleus form an electric dipole, \nwhich the ﬁeld tries to orient along its direction. An electric dipole d has a magnitude given by the \nproduct of the charge (the positive and negative charges are assumed equal) and the displacement \nbetween the charges, and a direction pointing from negative to positive charge. We have placed the \norigin of our coordinate system at the nucleus, so r represents the location of the electron with respect \nto the nucleus. Thus the dipole moment of the atom is\n \nd = -er. \n(10.138)\nThe classical potential energy of an electric dipole in an electric ﬁeld is\n \nU = -d~E. \n(10.139)\nThe quantum mechanical potential energy is obtained by using this same expression as long as we \nclarify what the proper operators are. In this case only the position r is an operator. The charge and the \napplied electric ﬁeld are parameters. Thus the Hamiltonian representing the perturbation is\n \n H\u0004 = -d~E\n \n \n = -1-er2~E zn \n \n = eE z\n \n \n = eE r cos u.\n \n(10.140)\nLet’s ﬁrst consider the ground state of hydrogen. This state is nondegenerate, so we use nonde-\ngenerate perturbation theory. The ﬁrst-order perturbed energy is the expectation value of the perturba-\ntion in the state:\n \n E (1)\n1\n= 8100(0)@H\u0004@ 100(0)9\n \n \n = 8100(0)@eE z@ 100(0)9\n \n \n = eE 8100(0)@ z@ 100(0)9\n \n \n = eE\nL\nz@ c(0)\n100 1r, u, f2 @\n2dV. \n(10.141)\n\n348 \nPerturbation Theory\nThe expectation value of z is zero in the ground state because the function z has odd parity and the \nsquare of the wave function has even parity. The resultant integrand has odd parity and so yields zero \nwhen integrated over all space. To formally do the integral, one would use the substitution z = r cos u \nbecause the wave functions are in r, u, f, coordinates. The theta integral is the one that is zero, because \ncos u is odd with respect to u = p>2 (Problem 10.12).\nThe result of this calculation is that there is no ﬁrst-order (i.e., linear) Stark effect in the ground \nstate of hydrogen. As we saw in Chapter 8, all hydrogen atom eigenstates have deﬁnite parity, odd \nor even, yielding even wave function squares and zero expectation values of the electric dipole per-\nturbation. However, the degeneracy in the excited states of hydrogen means that a given energy state \nincludes states of differing parity, which permits a linear Stark effect, as we will see shortly. The \nabsence of a linear Stark effect in the ground state implies that the atom does not have a permanent \nelectric dipole moment in its ground state, because the expectation value of the perturbation that we \ncalculated in Eq. (10.141) is just the expectation value of the dipole moment d = -er times the value \nof the applied ﬁeld. Given the calculation in Eq. (10.141), we attribute that lack of dipole moment \nto the deﬁnite parity of the atomic wave function, which arises from the symmetry of the atomic \n system. We can now turn this whole argument on its end and say that if we measure the atom to have \na permanent electric dipole moment (by observing a linear Stark effect or by other means), then we \ncan conclude that parity is not an obeyed property of the atom. There is a whole cottage industry of \nexperiments designed to search for such effects because they indicate “parity violation.” Parity violat-\ning effects are attributed to the weak nuclear interaction and are usually studied in high-energy particle \ncollision experiments. Atomic parity violation experiments provide a unique opportunity for “low-\nenergy” physicists to do “high-energy”  measurements.\nThere is a second-order (i.e., quadratic) Stark effect in the ground state of hydrogen, but the \ncalculation is tedious because it involves an inﬁnite sum. We’ll skip that calculation and move on to \nexcited hydrogen states that require degenerate perturbation theory.\nThe n \u0003 2 state of the hydrogen atom is fourfold degenerate, with one 2s state and three 2p states. \nDegenerate perturbation theory tells us to diagonalize the perturbation Hamiltonian in the degenerate \nsubspace. To do this, we ﬁrst need to ﬁnd the matrix representing the Stark effect perturbation within \nthe subspace of the four degenerate states. The four n \u0003 2 states of hydrogen are \n \n0 2009 \u0003 c(0)\n2001r, u, f2 = R201r2Y 0\n01u, f2 =\n2\n12a023>2 a1 - r\na0\nbe -r>2a0 \n1\n24p\n \n \n0 2109 \u0003 c (0)\n2101r, u, f2 = R211r2Y 0\n11u, f2 =\n1\n4312a023>2 r\na0\n e -r>2a0\nB\n3\n4p cosu \n \n(10.142)\n \n0 21,{19 \u0003 c 21{1\n(0) 1r, u, f2 = R211r2Y 1\n{11u, f2 =\n{1\n4312a023>2 r\na0\n e -r>2a0\nB\n3\n8p e {ifsinu, \nand we need to calculate the 16 matrix elements\n \n82/m(0)@H\u0004@ 2/\u0004m\u0004(0)9 \n(10.143)\nto construct the matrix of the perturbation in the degenerate subspace.\nAs we saw in the ﬁrst-order ground state calculation above, the consideration of parity is impor-\ntant in evaluating the required matrix elements. The parity of the hydrogen atom wave functions is \ndetermined by the parity of the spherical harmonics Y m\n/ , which is (-1)/, independent of m. The Ham-\n\n10.6  More Examples \n349\niltonian for the perturbation has odd parity, so the matrix elements between states of the same parity \ngive an integrand that is odd and hence a zero integral. The only nonzero matrix elements are those \nbetween states of different parity, which are the s and p states. Thus we expect to do three integrals, \nbetween the s state and each of the three p states. However, we can reduce our task even further by \nconsidering the f part of the integrals for these cases. There is no f dependence in the perturbation \nHamiltonian, so the matrix elements in Eq. (10.143) have azimuthal integrals\n \nL\n2p\n0\ne -imfeim\u0004fdf =\nL\n2p\n0\nei(m\u0004-m)fdf. \n(10.144)\nThis integral is zero unless the magnetic quantum numbers m and m\u0004 are the same, which only  happens \nfor the matrix element between the 2s state and the 2p0 state. Thus the only nonzero matrix element in \nthe degenerate subspace is\n \n8210(0)@H\u0004@ 200(0)9 \u0002 0, \n(10.145)\nwhere the O’s are different but the m’s are the same. Thus the matrix representing the perturbation \nHamiltonian in the degenerate subspace has the form\n \nn\n2\n2\n2\n2\n/\n0\n1\n1\n1\nm\n 0\n1\n0\n-1\n \n \nH\u0004 \u0003 §\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n¥, \n(10.146)\nwhere the columns are labeled with n/m (of course one must use the same labeling order for both rows \nand columns), and the nonzero elements are boxed.\nNow we do the integral to ﬁnd the nonzero matrix elements\n \n8200(0)@ H\u0004@210(0)9 =\n \nL\nc (0)*\n2001r, u, f2eE r cosu c (0)\n2101r, u, f2r 2 sinu dr du df\n \n= eE \n2\n12a023>2 \n1\n24p\n \n1\n2312a023>2 B\n3\n4p L\n\u0005\n0\n a1 - r\na0\nb e-r>a0 r 4dr \n \nL\np\n0\ncos2 u sinu du\nL\n2p\n0\n df.  \n(10.147)\nThe u and f angular integrals are straightforward and give 2/3 and 2\u000b , respectively (Problem 10.13). \nDoing the radial integral yields the ﬁnal result\n \n 8200@ H\u0004@ 21094 = eE \n2\n12a02\n3 1\n4p 2\n3 2pc\nL\n\u0005\n0\nr4e-r>a0 dr -\n1\n2a0 L\n\u0005\n0\nr 5e-r>a0 drd\n \n = eE 1\n6a 2\n0\n c4!a 5\n0 -\n1\n2a0\n5!a6\n0d\n \n(10.148)\n \n = -3eEa0.\n\n350 \nPerturbation Theory\nNow we have the matrix representing the perturbation Hamiltonian in the original basis within \nthe n \u0003 2 subspace, and the recipe we have for degenerate perturbation theory tells us to diagonalize \nthis matrix. It is convenient to reorder the rows and columns of the matrix in a way that makes the \nmathematics of diagonalization easier and the physics of the perturbation more obvious:\n \nH\u0004 \u0003 §\n0\n-3eEa0\n0\n0\n-3eEa0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n¥ \n200\n210\n211\n21,-1\n. \n(10.149)\nBe careful to note the new labeling of rows and columns. Only the 2s and 2p0 states are connected by \nthe perturbation; the 2p{1 states are not affected by the perturbation and their energies are unchanged. \nWe diagonalize the perturbation Hamiltonian to get the energies and states:\n \n4\n-l\n-3eEa0\n0\n0\n-3eEa0\n-l\n0\n0\n0\n0\n-l\n0\n0\n0\n0\n-l\n4 = 0 \n \nCl2 - A3eEa0B\n2Dl2 = 0\n \n \nl = {3eEa0, 0, 0.\n \n(10.150)\nAs we said, the 2p{1 states are not shifted, and those eigenstates remain the same. The 2s and 2p0 \nstates are mixed by the perturbation, with the normalized states from the diagonalization being \n(Problem 10.14):\n \n@ c+I =\n1\n12 C @200I  - @210ID  \n \n@ c-I =\n1\n12 C @200I  + @210ID , \n(10.151)\nwhere the { subscript on the states corresponds to the energies E { = {3eEa0 . The perturbed \nenergy states are shown in Fig. 10.11. Note that the perturbation has lifted some of the degeneracy, but \nnot all of it; two states remain degenerate.\nE\n\bE\n\u00073eEa0\n\u00070\n\u0007\t3eEa0\nE(1)\n+\nE(1)\n0\nE(1)\n\nFIGURE 10.11 Stark effect in the hydrogen n = 2 state.\n\nSummary \n351\nThe shifts of the two superposition states in Eq. (10.151) are linear in the applied ﬁeld because the \ncombined s and p state has an electric dipole moment. This is allowed because it is not a state of deﬁ-\nnite parity. The 0 c-9 state of lower energy is the state we studied in Fig. 8.9(b), which has an electric \ndipole moment pointing in the ﬁeld direction (Problem 10.15). This is what we expect from the clas-\nsical model of an electric dipole moment that aligns with the electric ﬁeld in its lowest energy state.\nSUMMARY\nPerturbation theory allows us to calculate the effects of adding new terms to the Hamiltonian of a \nsystem we have already solved exactly. The zeroth-order Hamiltonian with exact solutions obeys the \neigenvalue equation\n \nH0@ n(0)9 = E (0)\nn @ n(0)9. \n(10.152)\nThe system is perturbed by the addition of a new term H\u0004, and the new Hamiltonian has the energy \neigenvalue equation\n \n1H0 + H\u000420 n9 = En0 n9. \n(10.153)\nThe approximate solutions to this equation are expressed as a series in increasing orders of the strength \nof the perturbation:\n \n En = E (0)\nn\n+ E (1)\nn\n+ E (2)\nn\n+ ...\n \n \n 0 n9 = @ n(0)9 + @ n(1)9 + @ n(2)9 + ... . \n(10.154)\nThe ﬁrst-order energy correction in nondegenerate perturbation theory is the expectation value of \nthe perturbation in the state\n \nE (1)\nn\n= H =\nnn = 8n(0)@H\u00040 n(0)9. \n(10.155)\nIn wave function notation, the expectation value is expressed as an integral\n \nE (1)\nn\n= H =\nnn =\nL\nw(0)*\nn 1r2 H\u0004w(0)\nn 1r2dV , \n(10.156)\nwhere w(0)\nn 1r2 are the energy eigenstates of the zeroth-order Hamiltonian. The ﬁrst-order eigenstate \ncorrection is\n \n@ n(1)9 = a\nm\u0002n\n 8m(0)@H\u0004@ n(0)9\nAE (0)\nn\n- E (0)\nm B\n @ m(0)9.  \n(10.157)\nThe second-order energy correction is\n \nE (2)\nn\n= a\nm\u0002n\n \n@8n(0)@H\u0004@ m(0)9@\n2\nAE (0)\nn\n- E (0)\nm B\n. \n(10.158)\nFor degenerate states, we must use degenerate perturbation theory, which tells us to diagonalize \nthe perturbation Hamiltonian in the degenerate subspace.\n\n352 \nPerturbation Theory\nPROBLEMS\n 10.1 Diagonalize the Hamiltonian in Eq. (10.3) and conﬁrm the energy eigenvalues in Eq. (10.9).\n 10.2 Show that the assumed power series expansions in Eq. (10.34) lead to the set of equations \n(10.35) through (10.38).\n 10.3 Assume a 3-state quantum mechanical system and use the matrix approach of Section 10.3 \nto explicitly show that the ﬁrst-order energy shift of the n = 2 state is given by \nE\n (1)\n2\n= H =\n22 = 82(0)@H\u0004@ 2(0)9.\n 10.4 Do the explicit integral in Eq. (10.71) to conﬁrm the result in Eq. (10.72).\n 10.5 Find the ﬁrst-order eigenstate corrections to the ground state of the asymmetric square well of \nExample 10.2.\n 10.6 Show that the eigenstates correct to ﬁrst order in Example 10.3 are given by Eq. (10.91).\n 10.7 The nitrogen nucleus has spin 1 and a gyromagnetic ratio gN = 0.404. A nitrogen nucleus is \nplaced in a constant magnetic ﬁeld in the z-direction B0 = B0zn . An additional, perturbative \nmagnetic ﬁeld B\u0004 = B1zn is applied to the system. Find the ﬁrst-order energy shifts due to the \nperturbation. Plot your results as a function of the perturbing ﬁeld strength, assuming that the \nconstant ﬁeld is B0 = 2.35 Tesla.\n 10.8 The nitrogen nucleus has spin 1 and a gyromagnetic ratio gN = 0.404. A nitrogen nucleus is \nplaced in a constant magnetic ﬁeld in the z-direction B0 = B0zn . An additional, perturbative \nmagnetic ﬁeld B\u0004 = B2xn is applied to the system. Find the second-order energy shifts due to \nthe perturbation. Plot your results as a function of the perturbing ﬁeld strength, assuming that \nthe constant ﬁeld is B0 = 2.35 Tesla.\n 10.9 An electron is bound to move on the surface of a sphere. Find the energy corrections caused \nby a perturbing magnetic ﬁeld B\u0004 = B1yn . Identify the “correct” zeroth-order basis.\n 10.10 Consider a particle bound in the harmonic oscillator potential V1x2 = 1\n2 mv2x 2. A perturbation \nH\u0004 = gx 3 is applied to the system.\na) Calculate the ﬁrst-order corrections to the energies.\nb) Calculate the second-order corrections to the ﬁrst three energy levels.\nc) Find the ﬁrst-order corrections to the eigenstates for these three states.\n10.11 Consider a particle bound in the harmonic oscillator potential V1x2 = 1\n2 mv2x 2. A perturba-\ntion H\u0004 = h x4 is applied to the system. Calculate the ﬁrst-order corrections to the energies.\n10.12 Conﬁrm by explicit integration of Eq. (10.141) that the linear Stark shift of the hydrogen \nground state is zero.\n10.13 Do the angular integrals in Eq. (10.147) and conﬁrm the results quoted in the text.\n10.14 Find the eigenstates of the perturbation Hamiltonian in Eq. (10.149) and verify the results in \nEq. (10.151).\n10.15 Calculate the expectation value of the electric dipole moment for the lower energy state 0 c-9 \nin Eq. (10.151) and verify that the moment is aligned with the ﬁeld.\n10.16 Consider the inﬁnite square well with the shelf perturbation shown in Fig. 10.6(b). Calculate \nthe second-order energy shift of the ground state.\n10.17 Consider the inﬁnite square well shown in Fig. 10.6(a). Add a linear “ramp”  perturbation \nH\u0004 = V1x2 = bx for 0 6 x 6 L to the system and ﬁnd the ﬁrst-order energy shift of the \nground state.\n\nProblems \n353\n10.18 Consider an inﬁnite square well potential with walls at x = 0 and x = L; that is, \nV1x2 = 0 for 0 6 x 6 L; V1x2 = \u0005 otherwise. Now impose a perturbation on this poten-\ntial of the form H\u0004 = L V0 d1x - L>2), where d(x) is the Dirac delta function.\na) Calculate the ﬁrst-order correction to the energy of the nth state of the inﬁnite well.\nb) Give some physical insight into why your answer is different for even and odd values of n.\nc) The ground state wave function is modiﬁed under the inﬂuence of the perturbation. \n Calculate the largest contribution to the ﬁrst-order correction. (In other words, which state \nis mixed in the most?)\n \n Now consider the case where we impose a perturbation on the inﬁnite square well potential as \nshown in Fig. 10.12, with \u0015 a small number.\nd) Calculate the ﬁrst-order correction to the energy of the ground state of the inﬁnite well.\ne) In the limit where \u0015 goes to zero, compare your answer to (d) with the answer in (a). Discuss.\n10.19 Calculate the ﬁrst-order energy corrections for all levels of an inﬁnite square well potential \nwith a perturbation H\u0004 = V0 sin1px>L2.\n10.20 Calculate the ﬁrst-order energy corrections for all levels of an inﬁnite square well potential \nwith a perturbation H\u0004 = gx1L - x 2. \n10.21 Consider a charged particle bound in the harmonic oscillator potential V1x2 = 1\n2 mv2x2. A \nweak electric ﬁeld E is applied to the system such that the potential energy is shifted by an \namount H\u0004 = -qE x.\na) Calculate the energy levels of the perturbed system to second order in the small  perturbation.\nb) Show that the perturbed system can be solved exactly by completing the square in the \nHamiltonian. Compare the exact energies with the perturbation results found in (a).\n10.22 Extend the Stark effect calculation in Section 10.6.2 to the n = 3 state of hydrogen. The \n symmetry and azimuthal integral arguments allow you to reduce the 81 = 9 * 9 required \nmatrix elements to only 8 non-zero matrix elements and 4 necessary integrals. Find the \n perturbed energies and the new preferred basis. Discuss the electric dipole moments of the \nnew states.\n0\nL\n2\nL\nL\n2 \u0006 2\nL\n2 \u000b \fL\n\fL\n2\nx\nV(x)\nV0\n\u0005\n\f\nFIGURE 10.12 Perturbed square well.\n\n354 \nPerturbation Theory\n10.23 Consider a quantum system with three states and a Hamiltonian given by\n \nH \u0003 V0\n £\n1\n2e\n0\n2e\n1\n3e\n0\n3e\n4\n≥, \n \n where V0 is a constant and e is a small number (\u0015 V 1) that characterizes the perturbation of \nthe system.\na) Write down the eigenvectors and eigenvalues of the unperturbed Hamiltonian (\u0015 \u0003 0).\nb) Find the leading correction to the energy of the state that is nondegenerate in the \n zeroth-order Hamiltonian.\nc) Use degenerate perturbation theory to ﬁnd the ﬁrst-order corrections to the two initially \ndegenerate energies.\nd) Plot the results of (b) and (c) as a function of the parameter e and discuss your results.\n10.24 Consider a quantum system with four states and a Hamiltonian given by\n \nH \u0003 V0\n §\n3\ne\n0\n0\ne\n3\n2e\n0\n0\n2e\n5\ne\n0\n0\ne\n7\n¥ , \n \n where V0 is a constant and \u0015 is a small number (\u0015 V 1) that characterizes the perturbation of \nthe system.\na) Write down the eigenvectors and eigenvalues of the unperturbed Hamiltonian (\u0015 \u0003 0).\nb) Use perturbation theory to ﬁnd corrections to the energy of each energy eigenstate. Find \nthe ﬁrst nonvanishing order for each state. \n\n \n355\nC H A P T E R \n11\nHyperﬁne Structure and the \nAddition of Angular Momenta\nSpectroscopy of the hydrogen atom reveals structure in the energy levels beyond the 1/n2 pattern \ndetermined by the Coulomb interaction between the electron and proton. These additional energy lev-\nels arise from a variety of perturbations to the zeroth-order Coulomb interaction Hamiltonian. In this \nchapter, we focus on the hyperﬁne structure, so named because its effects are smaller than another \neffect called the ﬁne structure. Studying hyperﬁne structure in the hydrogen atom gives us a chance to \nuse perturbation theory to calculate an important energy and also gives us a chance to learn some new \nangular momentum tools. In Example 10.4, we found that we could have solved the problem more \neasily if we had chosen the “correct” basis at the start of the problem. The “correct” basis is not so \nmuch correct as it is convenient because the perturbation Hamiltonian is already diagonal in that basis \nand we avoid the tedious diagonalization required by degenerate perturbation theory. The hyperﬁne \nstructure calculation presents us with a similar scenario, but not yet knowing how to choose the most \nconvenient basis, we will solve the hyperﬁne problem by brute force in the “inconvenient” basis and \nthen analyze the results to learn how to choose bases. The two bases for the hyperﬁne problem are the \nuncoupled and coupled bases, which refer to the coupling or addition of angular momenta. The \ntheory of the addition of angular momenta complements perturbation theory to allow us to more easily \nsolve for the many rich details in the hydrogen atom, which we will continue in the next chapter.\n11.1 \u0002 HYPERFINE INTERACTION\nThe hyperﬁne interaction between the electron and the nucleus arises from higher electromagnetic \nmultipole moments of the nucleus, beyond the electric monopole moment (i.e., charge) that is already \nincluded in the Coulomb interaction. The dominant hyperﬁne effect is due to the magnetic moment of \nthe nucleus and its interaction with the internal magnetic ﬁelds in the atom caused by the electron’s \norbital motion and by the electron’s spin magnetic moment. The intrinsic magnetic moment of the \nelectron associated with its spin is\n \nMe = -ge e\n2me\n S = -ge\n mB S\nU , \n(11.1)\nwhere the gyromagnetic ratio ge is approximately 2 and the Bohr magneton is mB = e U>2me. The \nproton is also a spin-1/2 particle and has an associated magnetic moment. We label the proton spin, \nand nuclear spin in general, as I, so the intrinsic magnetic moment of the proton is\n \nMp = gp e\n2mp\n I = gp\n mN I\nU, \n(11.2)\n\n356 \nHyperﬁne Structure and the Addition of Angular Momenta\nwhere the nuclear magneton is mN = e U>2mp. The gyromagnetic ratio gp of the proton is 5.59, which \narises from the composite quark structure of the proton. The proton mass is 1836 times larger than the \nelectron mass, so the proton magnetic moment is approximately three orders of magnitude smaller \nthan the electron magnetic moment. This factor is responsible for the small scale of the hyperﬁne \nstructure.\nThe hyperﬁne interaction Hamiltonian is\n \nH =\nhf = Mp~ m0\n4p eL\nmr 3 + m0\n4p 1\nr 3 cMe~Mp - 3 \n1Me~r21Mp~r2\nr 2\nd - m0\n4p 8p\n3 Me~Mpd1r2. \n(11.3)\nThe ﬁrst term represents the interaction between the proton magnetic moment and the magnetic ﬁeld \narising from the electron’s orbital angular momentum. The second term is the interaction between the \ntwo magnetic dipoles for the case r \u0002 0. The third term is the same dipole-dipole interaction for the \ncase r = 0 and is often called the Fermi contact interaction. Substituting the electron and proton spin \noperators into Eq. (11.3), we obtain the hyperﬁne Hamiltonian for the hydrogen atom:\n \nH =\nhf = m0\n4p \nge\n mB\n gp\n mN\nU2\n c 1\nr 3 I~L - 1\nr 3 S~I + 3\nr 5 1S~r21I~r2 + 8p\n3\n S~Id1r2d . \n(11.4)\nWe limit our discussion to the 1s ground state of hydrogen. For s states, the ﬁrst three terms of \nEq. (11.4) are zero, and only the Fermi contact term of the hyperﬁne Hamiltonian need be considered. \nHence, the hyperﬁne Hamiltonian for the ground state of hydrogen is\n \nH =\nhf = m0\n4p \nge\n mB\n gp\n mN\nU2\n 8p\n3\n S~Id1r2. \n(11.5)\nPerturbation theory requires us to take matrix elements of the perturbation Hamiltonian. The hyperﬁne \nHamiltonian has space and spin dependence, so the matrix elements have the form\n \n8space08spin0  m0\n4p \nge\n mB\n gp\n mN\nU2\n 8p\n3\n S~Id1r2 0\n spin9 0\n space9, \n(11.6)\n which can be factored into spin and space parts\n \nm0\n4p \nge\n mB\n gp\n mN\nU2\n 8p\n3\n 8space 0 d 1r2 0\n space98spin 0 S~I0\n spin9. \n(11.7)\nThe spatial matrix element in the ground state of hydrogen\n \n 8space 0 d 1r2 0\n space9 =\nLspace\n c*\n1s\n 1r, u, f2d 1r2c1s\n 1r, u, f2d 3r \n(11.8)\n \n = 0 c1s\n 102 0\n2\n \n\n11.2 Angular Momentum Review \n357\nresults in the probability density at the origin, which we found in Chapter 8 [Eq. (8.77)]. But we don’t \nknow how to calculate the spin matrix elements because we neglected to include the spins of the \nelectron and proton in the solution of the energy eigenstates in Chapter 8. Developing the tools to ﬁnd \nthese spin matrix elements is one of the goals of this chapter.\nUsing the result in Eq. (11.8), we simplify the hyperﬁne Hamiltonian for the ground state of \nhydrogen to just the spin aspect\n \nH =\nhf = A\nU2 S~I  ,  \n(11.9)\nwhere the constant A has dimensions of energy and includes the spatial integral:\n \nA = 2m0\n3\n ge\n mB\n gp\n mN0 c1s\n 1020\n2. \n(11.10)\nFor hydrogen, A is less than one-millionth of the Rydberg energy. The hyperfine interaction \nHamiltonian in Eq. (11.9) has the form you might expect classically. Two classical magnetic dipoles \ntend to align themselves in the same direction when you put them on top of each other, as depicted in \nFig. 11.1. The electron magnetic moment is opposite to its spin, so the aligned magnetic dipoles cor-\nrespond to antialigned electron and proton spins, which according to the Hamiltonian in Eq. (11.9) \nlowers the energy.\n11.2 \u0002 ANGULAR MOMENTUM REVIEW\nBefore we embark on ﬁnding the matrix elements of the hyperﬁne Hamiltonian, a quick review of \nangular momentum is in order. You have studied spin angular momentum S and orbital angular \nmomentum L. They have different physical origins—spin angular momentum is an intrinsic prop-\nerty of fundamental particles while orbital angular momentum depends on the state of motion of a \nparticle—but they share many similarities. There are many instances where the physical origin of \nthe angular momentum is not important, and we refer to a generalized angular momentum, which we \nsymbolize by J.\nelectron\nproton\n\u0004e\n\u0004p\nI\nS\nFIGURE 11.1 Hyperﬁne interaction between electron and \nproton magnetic moments.\n\n358 \nHyperﬁne Structure and the Addition of Angular Momenta\nThe eigenvalue equations for a generalized angular momentum have the same form as spin and \norbital angular momentum eigenvalue equations\n \n J2@\n jmj9 = j1j + 12U2@\n jmj9  \n \n Jz@\n jmj9 = mj U@\n jmj9   ,  \n(11.11)\nwhere @\n jmj9 are simultaneous eigenstates of the commuting operators J2 and Jz and are labeled with the \nrespective eigenvalues. The angular momentum quantum number j can be any integer or half integer. \nThe magnetic quantum number mj is restricted to the values\n \nmj = -j, -j + 1,....., j -1, j \n(11.12)\nfor a given value of j. This yields 2j +1 possible mj states for each j value. Because all angular momenta \nobey the eigenvalue equations (11.11), they exhibit the same spectra of angular momentum quantum \nnumbers. For example, s = 1 and / = 1 both have three possible component states ms = 1,0,-1 and \nm/ = 1,0,-1. In the general case, a given value of j yields a spectrum or manifold of mj states, as \nshown in Fig. 11.2.\nThe mathematical rules for generalized angular momentum are the same as those we have learned \nfor spin and orbital angular momentum. Speciﬁcally, the rectangular components of angular momen-\ntum do not commute with each other:\n \n3Jx , Jy4 = iUJz  \n \n3Jy , Jz4 = iUJx  \n \n3Jz , Jx4 = iUJy. \n(11.13)\nNote that these commutation relations are cyclic in xyz. As we discussed in Section 2.5, the angular \nmomentum commutation relations imply that we cannot measure two different components simulta-\nneously, so we cannot know the direction of the angular momentum vector. We can, however, know \nthe square of its magnitude J2. The operator J2 commutes with each of the angular momentum com-\nponents (see Section 2.6)\n \n3J2, Jx4 = 3J2, Jy4 = 3J2, Jz4 = 0 \n(11.14)\nso we can simultaneously measure the magnitude of the vector and its projection along one axis.\nmj \u0004\u0007j\nmj \u0004\u0007j \n\u00071\nmj \u0004\u0007j \n\u00072\nmj \u0004\u0007\n\u0007j \n\u00071\nmj \u0004\u0007\n\u0007j\nFIGURE 11.2 Manifold of angular momentum component eigenstates for a given j.\n\n11.3 Angular Momentum Ladder Operators \n359\nWe can represent angular momentum operators as matrices in the angular momentum basis (vs. \nthe position or momentum basis). It is the standard convention to write separate matrices for each \nparticular value of j (i.e., each j subspace), and use the eigenstates of the angular momentum compo-\nnent operator Jz as the basis. Each j matrix has 2j +1 rows and columns, corresponding to the number \nof possible mj component states. The matrices do not distinguish between spin and orbital angular \nmomentum. Thus we get, for example:\n \n j = 1\n2 1  J2 \u0003 3\n4\n U2 a1\n0\n0\n1b,         Jz \u0003 U\n2 a1\n0\n0\n-1b \n \n j = 1 1  J2 \u0003 2U2 °\n1\n0\n0\n0\n1\n0\n0\n0\n1\n¢,  Jz \u0003 U °\n1\n0\n0\n0\n0\n0\n0\n0\n-1\n¢.\n \n(11.15)\n11.3 \u0002 ANGULAR MOMENTUM LADDER OPERATORS\nThe manifold of angular momentum component states in Fig. 11.2 is very similar to the harmonic \noscillator problem (see Fig. 9.4), where the energy levels are labeled with n, which is the eigenvalue of \nthe number operator N. Similarly, the angular momentum states in Fig. 11.2 differ by one unit of the \nmagnetic quantum number mj (we don’t know anything about the energy in the angular momentum \ncase yet). In the harmonic oscillator problem, we found a pair of ladder operators that connected the \ndifferent energy states and that proved very useful. For angular momentum, there are similar ladder \noperators that connect the states within a given j manifold (see Problem 7.26). The angular momentum \nladder operators are deﬁned as\n \nJ+ = Jx + iJy\n \nJ- = Jx - iJy  . \n(11.16)\nThese new operators are analogous to the raising and lowering operators a- and a of the harmonic \noscillator. Like a- and a, J+ and J- are not Hermitian, so they do not represent physical observables. \nThey are Hermitian conjugates of each other\n \nJ+ = J -\n-, \n(11.17)\nand they do not commute with each other:\n \n3J+, J-4 = 2UJz, \n(11.18)\nbut they do commute with J2:\n \n3J2, J{4 = 0. \n(11.19)\nThe important commutation relations for these new angular momentum ladder operators are \n(Problem 11.1)\n \n3Jz, J+4 = +UJ+\n \n3Jz, J-4 = -UJ- , \n(11.20)\n\n360 \nHyperﬁne Structure and the Addition of Angular Momenta\nwhich are analogous to the relations\n \n3H, a-4 = +U va- \n \n3H, a4 = -U va  \n(11.21)\nfrom the harmonic oscillator problem [Eqs. (9.24) and (9.25)].\nIn the harmonic oscillator problem, we used the commutation relations in Eq. (11.21) to show \nthat a- and a raise and lower, respectively, the energy by one energy quantum U v and hence change \nthe label n. The physical requirement that the energy of the harmonic oscillator cannot be negative \nyielded the termination condition for the ladder, a0 09 = 0, which resulted in the energy spectrum \nEn = 1n + 1\n22U v. In the angular momentum case, the commutation relations in Eq. (11.20) simi-\nlarly imply that J+ and J- raise and lower, respectively, the angular momentum component by one \nquantum U and hence change the label mj (Problem 11.2). The physical condition that limits the \nextent of the angular momentum ladder of states is that the angular momentum component Jz can-\nnot be greater than the magnitude of J, which is the square root of J2. This physical condition leads \nto the conclusion that the top and bottom states of the angular momentum manifold for a given j are \nmj = {j, with the termination equations (Problem 11.3)\n \nJ+ 0\n j j9 = 0\n \nJ- 0\n j, -j9 = 0. \n(11.22)\nIn the angular momentum case, the operator J2 provides an additional label j, in contrast to the \nsingle label n for the harmonic oscillator states. But the ladder operators J+ and J- commute with J2 \n[Eq. (11.19)], so their action does not change the j label, only the mj label. Hence, the ladder operators \ndo not connect manifolds with different j values. That is why we commonly restrict our attention to a \nmanifold of mj states for a given j, such as in Fig. 11.2. The actions of the ladder operators are sum-\nmarized by the equation (Problem 11.2):\n \nJ{ 0  jmj9 =  U3j1 j +12 - mj1mj{124\n1>20  j, mj{19  . \n(11.23)\nSimilar to the harmonic oscillator ladder operators, the angular momentum ladder operators do not \npreserve the normalization of states. In addition, the angular momentum ladder operators have dimen-\nsions of U, as evidenced in Eq. (11.23). An example of the manifold or ladder of angular momentum \nstates for j = 2 is shown in Fig. 11.3. This view of the ladder of angular momentum states will be \nuseful when we apply perturbation theory to these states.\nJ\nJ\u000f\nJ\u000f\nJ\u000f\nJ\u000f\nJ\nJ\nJ\n\u00022,2\u0003\u0007\n\u00022,1\u0003\u0007\n\u00022,0\u0003\u0007\n\u00022,\n1\u0003\u0007\n\u00022,\n2\u0003\u0007\nFIGURE 11.3 Manifold or ladder of angular momentum states for j = 2.\n\n11.4 Diagonalization of the Hyperﬁne Perturbation \n361\n11.4 \u0002 DIAGONALIZATION OF THE HYPERFINE PERTURBATION\nWe are now ready to ﬁnd the matrix elements of the hyperﬁne Hamiltonian that we need in order to \napply perturbation theory to the ground state of hydrogen. The full quantum state vector includes the \nspatial wave function and the spin vectors for the electron and proton. For example, if the electron spin \nis up and the proton spin is down, then the atomic state vector is\n \n0 c1s9 \u0003  c1s 1r, u, f2 0  +9e 0  -9p, \n(11.24)\nwhere subscripts distinguish the electron and proton spin vectors. We have already used the spatial \nwave function to reduce the hyperﬁne Hamiltonian to a simple form in Eq. (11.9), so we limit our dis-\ncussion to the spin vectors. The electron (s = 1/2) and proton (I = 1/2) are both spin-1/2 particles, so \nthe individual spin states satisfy the eigenvalue equations:\n \nSz0{9e = { U\n2\n 0{9e , S20{9e = 3U2\n4\n 0{9e\n \nIz0{9p  = { U\n2\n 0{9p , I20{9p = 3U2\n4\n 0{9p. \n(11.25)\nDue to the ﬂexibility of Dirac notation, we can write the state 0  +9e 0  -9p many equivalent ways—what \nyou put inside the ket symbol is just a mnemonic label. We can specify all the quantum numbers in \neach ket:\n \n0  +9e 0  -9p = @ s = 1\n2 , ms = 1\n29@ I = 1\n2 , mI = -1\n29 \n(11.26)\nor we can write a single ket for the whole system, with all quantum numbers speciﬁed:\n \n 0  +9e 0  -9p = @ s = 1\n2 , ms = 1\n2 , I = 1\n2 , mI = -  1\n29\n \n = @ s = 1\n2 , I = 1\n2 , ms = 1\n2 , mI = -  1\n29 \n(11.27)\nor we can suppress the spin quantum numbers s and I because we know that they do not change:\n \n 0  +9e 0  -9p = @ ms = 1\n2 , mI = -  1\n29\n \n = 0  + -9.\n \n(11.28)\nIn the last case, we use the ﬁrst symbol for the electron and the second symbol for the proton, so that \n0  + -9 and 0  - +9 are distinct states.\nThe electron-proton spin states exist in a new vector space obtained by combining the spaces for \nthe spins of the two individual particles. There are four possible ways to combine the electron and \nproton spin states, so this new vector space is spanned by four basis states:\n \n0  + +9 , 0  + -9 , 0  - +9 , 0  - -9. \n(11.29)\nThis basis, in which the states are eigenstates of both Sz and Iz, is called the “uncoupled” basis. The \nreason for its name will become clearer in the next section when we learn about the alternative, “cou-\npled” basis. The four possible spin combinations in Eq. (11.29) imply that the zeroth-order hydrogen \nground state is not nondegenerate as we learned in Chapter 8, but rather is fourfold degenerate. Hence, \nin order to use perturbation theory to ﬁnd the effect of the hyperﬁne interaction, we must use degener-\nate perturbation theory, which requires us to diagonalize a 4*4 matrix.\n\n362 \nHyperﬁne Structure and the Addition of Angular Momenta\nLet’s ﬁrst clarify how the spin operators act in this new four-dimensional vector space. Because \neach spin operator is associated with a speciﬁc particle, each operator acts only on those aspects of the \nsystem kets that are associated with that particle. For example, Sz is associated with electron spin only:\n \nSz0  + -9 = Sz0  +9e 0  -9p = 5Sz0  +9e60  -9p = e+ U\n2\n 0  +9ef 0  -9p = U\n2\n 0  + -9. \n(11.30)\nIn general, the spin component eigenvalue equations in the four-dimensional vector space are\n \n Sz0 ms\n mI9 = ms\n U0 ms\n mI9\n \n Iz0 ms\n mI9 = mI\n U0 ms\n mI9, \n(11.31)\nwhere the allowed magnetic quantum numbers are ms = {1/2 and mI = {1/2. Because these spin \noperators act only on their own parts of the states, they commute with each other (Problem 11.5).\nIn Chapter 2, we represented spin operators as 2*2 matrices. In this example, we have four basis \nstates, so we need 4*4 matrices. Using the eigenvalue equations (11.31), the new matrices represent-\ning the spin component operators are straightforward to derive (Problem 11.6 and activity on system \nof two spin-1/2 particles):\n \n Sz \u0003 U\n2 §\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n-1\n¥  \n+ +\n+ -\n- +\n- -\n \n(11.32)\n \n Iz \u0003 U\n2 §\n1\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n-1\n¥  \n+ +\n+ -\n- +\n- -\n  ,\nwhere we have been explicit about labeling the rows, and by inference the columns, with the four basis \nstates of the two-particle system using the labeling convention in Eq. (11.29).\nThe hyperﬁne Hamiltonian is proportional to the operator \n \nS~I = Sx Ix + Sy Iy + Sz Iz. \n(11.33)\nThe matrix representing this operator is off-diagonal because the Sx , Sy , Ix , and Ix operators are off-\ndiagonal. To make the matrix elements easier to calculate, it helps to rewrite S~I using the angular \nmomentum ladder operators (Problem 11.7):\n \nS~I = 1\n2 1S+\n I- + S-\n I+2 + Sz Iz. \n(11.34)\nRecall that the ladder operators yield zero when acting on the extreme states; for example\n \n S+ 0 + +9 = 0\n \n I- 0 + -9 = 0. \n(11.35)\n\n11.4 Diagonalization of the Hyperﬁne Perturbation \n363\nThus, the action of the ladder operators on the basis states yields only a few nonzero results. For \nexample, using Eq. (11.23), we ﬁnd (Problem 11.8)\n \n S+ 0  - +9 = U3s1s + 12 - ms1ms + 124\n1>20  + +9\n \n = U31\n2 3\n2 - 1-  1\n221-  1\n2 + 124\n1>20  + +9\n \n = U33\n4 + 1\n44\n1>20  + +9\n \n = U0  + +9.\n \n(11.36)\nThe action of S~I on the basis states @ ms\n mI9 is:\n \n S~I0  + +9 = 51\n2\n 1S+\n I- + S- I+2 + Sz Iz6 0  + +9\n \n = 51\n2\n 10 + 02 + 1\n2\n U 1\n2\n U6 0  + +9\n \n = 1\n4\n U20  + +9\n \n S~I0  - -9 = 51\n2\n 10 + 02 + 1-1\n2 2U1-1\n2 2U6 0  - -9\n \n = 1\n4\n U20  - -9\n \n(11.37)\n \n S~I0  + -9 = 0 + 1\n2\n UU0  - +9 + 1\n2\n U1-1\n2 2U0  + -9\n \n = 1\n4\n U2320  - +9 - 0  + -94\n \n S~I0  - +9 = 0 + 1\n2\n UU0  + -9 + 1\n2\n U1-1\n2 2U0  - +9\n \n = 1\n4\n U2320  + -9 - 0  - +94.\n \nProjecting these results onto the basis states yields the matrix representation\n \nS~I \u0003 U2\n4  •\n1\n0\n0\n0\n0\n-1\n2\n0\n0\n2\n-1\n0\n0\n0\n0\n1\nμ  \n+ +\n+ -\n- +\n- -\n  . \n(11.38)\nThe operator S~I is not diagonal in the 0 ms\n mI9 (uncoupled) basis, so the 0 ms\n mI9 states are not eigen-\nstates of S~I. The two extreme states 0  + +9 and 0  - -9 are eigenstates of S~I, but the states 0  + -9and \n0  - +9 are not.\nUsing the result in Eq. (11.38), the hyperﬁne perturbation Hamiltonian becomes\n \nH =\nhf \u0003 A\n4 •\n1\n0\n0\n0\n0\n-1\n2\n0\n0\n2\n-1\n0\n0\n0\n0\n1\nμ  \n+ +\n+ -\n- +\n- -\n  . \n(11.39)\n\n364 \nHyperﬁne Structure and the Addition of Angular Momenta\nDegenerate perturbation theory tells us to diagonalize the perturbation Hamiltonian in the degenerate \nsubspace. The hyperﬁne Hamiltonian is block diagonal, so we know two eigenvalues and eigenstates \nby inspection:\n \nE1 = A>4,  0 E19 = 0  + +9  \n \nE2 = A>4,  0 E29 = 0  - -9.\n \n(11.40)\nThe other two eigenvalues are found by diagonalizing the submatrix indicated by the box in \nEq. (11.39):\n \n2 -A>4-l\nA>2\nA>2\n-A>4-l\n2 = 0  \n \n1-A>4 - l2\n2 - 1A>22\n2 = 0\n \n \n1-A>4 - l2 = {1A>22\n \n(11.41)\n \nl = -A>4 { A>2\n \n \nl = b\nA>4\n-3A>4.\n \nThe resultant eigenstates are superpositions of the two states 0  + -9 and 0  - +9:\n \nE3\n = A>4,     0 E39 =\n1\n12 30  + -9 + 0  - +94 \n \nE4\n = -3A>4,  0 E49 =\n1\n12 30  + -9 - 0  - +94. \n(11.42)\nThe energy level diagram of this perturbation is shown in Fig. 11.4. The degeneracy has been \npartially lifted. The two spin-aligned states  0  + +9 and 0  - -9 have the same positive energy shift, \nas you might expect because the magnetic moments are anti-aligned. Perhaps surprisingly, the spin-\nanti-aligned states  0  + -9 and 0  - +9  combine into two different superposition states with different \nenergy shifts. There must be something different about those two superposition states, 0 E39 and 0 E49, \nthat is not yet obvious. We will address this in the next section. The energy difference between the two \nhyperﬁne levels is A, so that [using Eq. (11.10)]\n \n\u0006Ehf = A = 2m0\n3\n ge\n mB\n gp\n mN 0 c1s 1020\n2. \n(11.43)\nThe square of the hydrogen 1s wave function at the origin is [Eq. (8.77)]\n \n0 c1s 1020\n2 =\n1\npa 3\n0\n , \n(11.44)\nwhere a0 is the Bohr radius. Hence, the hyperﬁne splitting of the hydrogen ground state is\n \n \u0006Ehf = 2m0\n3  \nge\n mB\n gp\n mN\npa3\n0\n= a4mec2 4\n3\n ge\n gp a\nme\nmpb\n \n \n = 5.88 * 10-6 eV = h * 1420.4057517667(9) MHz. \n(11.45)\n\n11.5 The Coupled Basis \n365\nThe energy scale in hydrogen is set by the Rydberg energy 13.6 eV = h * 3.285 * 1015 Hz, so the \nhyperﬁne splitting is about a million times smaller. The transition between the two hyperﬁne states is \na magnetic dipole transition in the microwave region of the spectrum. The electric dipole transition is \nforbidden because the matrix element is zero due to parity (both states have the same 1s spatial wave \nfunction). The transition between the two hyperﬁne states has a wavelength of 21 cm, and has played a \npivotal role in radio astronomy. Because most of the elemental matter in the universe is in the form of \nhydrogen, observation of the 21-cm line is used to map the matter distribution in our galaxy and in the \nuniverse. The hydrogen hyperﬁne transition is also observed in the laboratory, where it is used as the \nactive transition in the hydrogen maser, which permits the extremely precise frequency measurements \nindicated by the precision of the energy separation quoted above. An analogous transition in cesium is \nthe basis of the atomic clock used in national standards laboratories.\n11.5 \u0002 THE COUPLED BASIS\nLet’s return to the question posed in the last section: What distinguishes the two states \n0 E39 = 30  + -9 + 0  - +94> 12 and 0 E49 = 30  + -9 - 0  - +94> 12 from each other? How can that \nlittle minus sign play such a large role in the energy of the two states? The answer to these questions \ncomes from considering what we do in applying degenerate perturbation theory. In the new basis of \nhyperﬁne energy eigenstates, the hyperﬁne Hamiltonian is diagonal. As we discussed when we derived \ndegenerate perturbation theory, if we are clever enough to choose the right basis for  representing the \nperturbation Hamiltonian at the start of a problem, then the matrix is already diagonal and the prob-\nlem is solved. So the question we really want to answer is: What is special about the new hyperﬁne \nbasis states, and how should we have known to choose that basis to start the problem rather than the \n(uncoupled) basis we did choose?\nWhen two classical magnetic dipoles interact, they exert torques on each other that try to align the \nmagnetic moments. Because of the torque, the angular momentum of each particle is not conserved. \nThe magnitude of each particle’s angular momentum stays the same, but the direction changes. The \nquantum mechanical manifestation of this is that the electron and proton spin observables S2 and I2 \n1s\n\u0002\u000f\u000f\u0003\u0007\n\u0002\n\n\u0003\u0007\n(\u0002\u000f\n\u0003\u0005\u000f\u0007\u0002\n\u000f\u0003)\u0007\n1\n√2\n(\u0002\u000f\n\u0003\u0005\n\u0007\u0002\n\u000f\u0003)\u0007\n1\n√2\n1420MHz\n3A/4\nA/4\nFIGURE 11.4 Hyperﬁne structure of the ground state of hydrogen.\n\n366 \nHyperﬁne Structure and the Addition of Angular Momenta\ncommute with the hyperﬁne Hamiltonian and the component observables Sz and Iz do not (Problem \n11.9). The quantum numbers s and I are thus “good” quantum numbers, but ms and mI are not good \nquantum numbers, which is evident in Eq. (11.42) because the 0 E39 and 0 E49 hyperﬁne eigenstates \ninvolve superpositions of eigenstates of the original 0 ms\n mI9 basis—the uncoupled basis. To ﬁnd the \ngood quantum numbers for this problem and hence the “correct” or convenient basis, we must look for \na conserved quantity.\nThe hyperﬁne interaction between the electron and proton arises from the torques the particles \nexert on each other. This is an internal torque, so the total angular momentum of the system is con-\nserved. Hence, the basis of eigenstates of the total angular momentum of the system of the two par-\nticles is the basis in which the hyperﬁne Hamiltonian is diagonal. If we had chosen that basis to start \nthe problem, then the hyperﬁne Hamiltonian would already be diagonal and the perturbation results \nwould come by inspection. Let’s now show this.\nIn the hydrogen ground state, the total angular momentum of the system is the sum of the spin \nangular momentum of the electron and the spin angular momentum of the proton because there is no \norbital angular momentum. For historical reasons, we label the total angular momentum F:\n \nF = S + I. \n(11.46)\nThis new total spin operator behaves like an angular momentum because it is a sum (or coupling) of \ntwo angular momenta, and obeys the commutation relations of an angular momentum. For example,\n \n 3Fx , Fy4 = 3Sx + Ix , Sy + Iy4\n \n \n = 3Sx , Sy4 + 3Sx , Iy4 + 3Ix , Sy4 + 3Ix , Iy4 \n(11.47)\n \n = iUSz + 0 + 0 + iUIz\n \n \n = iUFz,\n \nwhere we have used the fact that the operators for the electron and the proton commute with each \nother. Because the total angular momentum behaves like all other angular momenta, it must have a \nset of basis kets 0 FMF9 that are eigenstates of the total angular momentum operators F2 and Fz. The \nquantum number F is the total angular momentum quantum number of the two-particle system. The \nquantum number MF is the total magnetic quantum number of the two-particle system. We refer to \nthese new states 0 FMF9 as the coupled basis because they arise from coupling or adding together two \nangular momenta. We refer to the original 0 ms\n mI9 basis as the uncoupled basis because it uses quan-\ntum numbers from the individual angular momenta before we consider their coupling or addition.\nWe are familiar with the uncoupled basis 0 ms\n mI9, which means that we know the single-particle \nquantum numbers s, I, ms, and mI. But we do not yet know the quantum numbers F and MF of the \ncoupled basis, which characterize the two-particle system. How do we ﬁnd them? Well, how do we \nﬁnd any eigenstates and eigenvalues? We solve the eigenvalue equation. This will also tell us how the \ncoupled basis eigenstates 0 FMF9 relate to the uncoupled basis eigenstates 0 ms\n mI9.\nSo our task is to ﬁnd the eigenstates and eigenvalues of the total angular momentum operators \nF2 and Fz. We do this by diagonalizing the matrices representing these physical observables, so we \nneed to ﬁnd the matrices ﬁrst. We have already written down the matrices for Sz and Iz above, in the \nuncoupled basis we started with—the eigenstates 0 ms\n mI9 of the two individual particles. We’ll start \nwith that approach and ﬁnd the matrix representations of the total angular momentum operators in this \nbasis. Let’s ﬁrst consider the total angular momentum component operator Fz, which is\n \nFz = Sz + Iz. \n(11.48)\n\n11.5 The Coupled Basis \n367\nTo ﬁnd the matrix for Fz , we add the matrices in Eq. (11.32) to get\n \nFz \u0003 U •\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\nμ  \n+ +\n+ -\n- +\n- -\n  . \n(11.49)\nThe Fz operator is already diagonal in the uncoupled 0 ms\n mI9 basis—that was easy! This immediately \ntells us that the states 0 ms\n mI9 are eigenstates of Fz . This is also clear if we operate with Fz on the \n0 ms\n mI9 states:\n \n Fz0 ms\n mI9 = 1Sz + Iz20 ms\n mI9\n \n \n = 1ms + mI2 U0 ms\n mI9. \n(11.50)\nThe eigenvalue equation for Fz in the coupled 0 FMF9 basis is\n \nFz0 FMF9 = MF U0 FMF9. \n(11.51)\nComparing Eqs. (11.50) and (11.51), we ﬁnd that the allowed magnetic quantum numbers MF are\n \nMF = ms + mI. \n(11.52)\nThe allowed single-particle magnetic quantum numbers are ms = {1>2 and mI = {1>2, so the \nfour allowed values of the total angular momentum magnetic quantum number MF are 1, 0, 0, and \n-1. These eigenvalues are also evident by inspection of the diagonal elements of the Fz matrix in \nEq. (11.49). A key point about these eigenvalues is that two of them are the same. Both the 0  + -9 and \n0  - +9 states have the magnetic quantum number MF = 0. This degeneracy of states is emphasized \nwith the box in the Fz matrix in Eq. (11.49), showing that the matrix is block diagonal. The degeneracy \nof the 0  + -9 and 0  - +9 states implies an ambiguity as to the eigenstates corresponding to MF = 0. \nThis degeneracy is an important aspect that we exploit in a moment.\nNow consider the matrix representation of the F2 operator, which is not so easy. The total angular \nmomentum operator is\n \n F2 = 1S + I2\n2 = S2 + I2 + S~I + I~S\n \n(11.53)\n \n = S2 + I2 + 2S~I,\nwhere again we have used the fact that the spin operators for the two particles commute with each \nother. Because the uncoupled states 0 ms\n mI9 are eigenstates of S2 and I2, those operators are  diagonal—\nin fact they are proportional to the identity in this subspace (Problem 11.6). We already know the \nmatrix representing S~I from the hyperﬁne Hamiltonian, so using Eq. (11.38), we ﬁnd the F2 operator \nin the 0 ms\n mI9 basis:\n \nF2 \u0003 U2 •\n2\n0\n0\n0\n0\n1\n1\n0\n0\n1\n1\n0\n0\n0\n0\n2\nμ  \n+ +\n+ -\n- +\n- -\n  . \n(11.54)\n\n368 \nHyperﬁne Structure and the Addition of Angular Momenta\nThe F2 matrix is block diagonal, as was Fz, but it is not diagonal within the 0  - +9, 0  + -9 subspace of \ndegenerate MF = 0 states.\nTo ﬁnd the eigenvalues and eigenvectors of the F2 operator, we diagonalize the matrix in \nEq. (11.54). However, because the matrix is block-diagonal, we ﬁnd the two eigenvalues 2U2 for the \n0  + +9 and 0  - -9 states by inspection. The eigenvalue equation for F2 in the coupled basis is\n \nF2@ FMF9 = F1F + 12U2@ FMF9, \n(11.55)\nso the eigenvalues 2U2 imply a quantum number F = 1 for the states 0  + +9 and 0  - -9. The Fz \neigenvalues MF of these two states are obtained by inspection of the Fz matrix in Eq. (11.49) or from\nEq. (11.52). These two eigenstates are thus\n \ncoupled basis | uncoupled basis \n \n 0 F = 1 , MF = 19 = 0  + +9  \n \n0 F = 1 , MF = -19 = 0  - -9. \n(11.56)\nTo ﬁnd the other two eigenvalues and eigenvectors of the F2 operator, we diagonalize the subma-\ntrix within the 0  + -9 and 0  - +9 states, as indicated by the box in Eq. (11.54). As you can show \nin Problem 11.10, the eigenvalues of the submatrix are 2U2 and 0, which correspond to the values \nF = 1 and F = 0. The magnetic quantum numbers are both MF = 0 for these two states, and the \neigenstates are\n \ncoupled basis | uncoupled basis \n \n0 F = 1 , MF = 09 =  1\n12 30  + -9 + 0  - +94 \n \n0 F = 0 , MF = 09 =  1\n12 30  + -9 - 0  - +94. \n(11.57)\nThe diagonalization procedure is equivalent to a rotation in Hilbert space, so it will, in general, \nundiagonalize other matrices that are diagonal in the original basis. So you might expect that the \ndiagonalization of F2 would undiagonalize the Fz matrix that we found to be diagonal in the 0 ms\n mI9 \nbasis. However, because the 0  + -9 and 0  - +9 states are degenerate with respect to Fz (i.e., they have \nthe same MF = 0 values), the Fz matrix is proportional to the identity matrix in the degenerate sub-\nspace of 0  + -9 and 0  - +9 states. The identity matrix is not altered by a rotation, so the ambiguity of \neigenstates of Fz in the degenerate subspace has the beneﬁt that the diagonalization of F2 does not \nundiagonalize Fz.\nIn summary, the four eigenstates 0 FMF9 of the coupled basis expressed in terms of the eigenstates \n0 ms\n mI9 of the uncoupled basis are \n \ncoupled basis | uncoupled basis \n \n 0 119 = 0  + +9\n \n 0 109 =\n1\n12 30  + -9 + 0  - +94  t Triplet state\n \n 0 1, -19 = 0  - -9\n \n(11.58)\n \n 0  009 =\n1\n12 3 0  + -9 - 0  - +94   r Singlet state  . \n\n11.5 The Coupled Basis \n369\nThese states are typically referred to as the triplet 1F = 12 and singlet 1F = 02 states. These are \nexactly the eigenstates we found in Eqs. (11.40) and (11.42) when we diagonalized the hyperﬁne per-\nturbation Hamiltonian in the 1s ground state.\nLet’s take a moment to reﬂect on what we have done. We started with two spin-1/2 particles and \nfound that the total angular momentum of the combined system could be 0 or 1, (i.e., F = s + I = 1\nand F = s - I = 0 were both allowed). For each allowed value of F, the allowed magnetic quantum \nnumbers run from -MF to +MF in unit steps as is the case for all angular momenta, (i.e., MF = 1, 0, -1\nfor the F = 1 case and MF = 0 for the F = 0 case). We learned how to express the new coupled \nbasis states 0 FMF9 in terms of the old uncoupled basis states 0 ms\n mI9, as shown in Eq. (11.58). The \nexpansion coefﬁcients in Eq. (11.58) that connect the two bases are called Clebsch-Gordan coef-\nﬁcients. They are commonly tabulated as in Table 11.1.\nNow we have two complete orthonormal bases to choose from—the coupled basis 0 FMF9 and \nthe uncoupled basis 0 ms\n mI9. The choice of which basis to use depends on which basis is best suited \nto the problem at hand, which typically depends on the Hamiltonian. For the hyperﬁne Hamiltonian, \nthe coupled basis is the “good” basis because the coupled basis eigenstates are the energy eigenstates, \nwhich reﬂects the fact that the total angular momentum is conserved.\nNow that we know that we could have chosen the coupled basis to solve the hyperﬁne problem \nmuch more easily, let’s do that and make sure we get the same answer that we obtained from the \nuncoupled basis analysis. Choosing the coupled basis means writing the matrix representing the per-\nturbation Hamiltonian using the 0 FMF9 states, rather than using the uncoupled 0 ms\n mI9 states as we \ndid in Section 11.4. To do this, we need to know how the S~I operator in the hyperﬁne Hamiltonian \nacts on the 0 FMF9 states. Using Eq. (11.53), we ﬁnd that the operator S~I can be expressed in terms of \nother operators whose action on the 0 FMF9 basis is known:\n \nS~I = 1\n21F2 - S2 - I22. \n(11.59)\nThis leads to the hyperﬁne Hamiltonian\n \n H =\nhf = A\nU2 S~I\n \n \n =\nA\n2U2 1F2 - S2 - I22.\n \n(11.60)\nTable 11.1 Clebsch-Gordan Coefﬁcients for System of Two Spin-1/2 Particles\ns = 1\n2\nF\n1\n1\n1\n0\nl = 1\n2\nM)\n1\n0\n-1\n0\n mV \nmO\n 1\n2 \n1\n2\n 1\n2 \n-  1\n2\n-  1\n2 \n1\n2\n -  1\n2 \n-  1\n2\n1\n0\n0\n0\n0\n1\n12\n1\n12\n0\n0\n0\n0\n1\n0\n1\n12\n-  1\n12\n0\n\n370 \nHyperﬁne Structure and the Addition of Angular Momenta\nThe matrix representing F2 is diagonal because 0 FMF9 are eigenstates of F2. The matrices represent-\ning S2 and I2 are diagonal because the 0 FMF9 states all have the same quantum numbers s = 1>2 and \nI = 1>2. The result is that the matrix representing the hyperﬁne Hamiltonian in the coupled basis is \n(Problem 11.11)\n \nH =\nhf \u0003 A\n4 §\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n-3\n¥ \n11\n10\n1, -1\n00\n  , \n(11.61)\nwhere the rows (and columns) are labeled with the F, MF quantum numbers.\nAs advertised, the hyperﬁne perturbation Hamiltonian is diagonal in the coupled basis. Degener-\nate perturbation theory calls for us to diagonalize this matrix in the degenerate 1s ground state, so we \nﬁnd the energy shifts by inspection of Eq. (11.61). The perturbation corrections are\n \nE (1)\nhf = b +A>4; F = 1\n-3A>4; F = 0 , \n(11.62)\njust as we found in Eqs. (11.40) and (11.42) by working in the uncoupled basis. We have solved the \nhyperﬁne perturbation problem in the coupled basis in a few lines instead of the several pages required \nfor the uncoupled basis approach. That is the sense in which the coupled basis is the convenient basis \nfor this problem. More important, the hyperﬁne eigenstates are the basis states of the coupled basis, so \nwe call the coupled basis the “correct” basis.\nWe can also solve for the hyperﬁne energy corrections using an operator approach because the \nperturbation Hamiltonian is diagonal in the coupled basis. Degenerate perturbation theory is equiva-\nlent to nondegenerate perturbation theory when the matrix is already diagonal, so the ﬁrst-order energy \nshifts are the expectations values of the perturbation:\n \n E (1)\nhf = 8FMF0 H =\nhf 0 FMF9\n \n =\nA\n2U2 8FMF@F2 - S2 - I2 @ FMF9.\n \n(11.63)\nThe 0 FMF9 states are eigenstates of the three operators in Eq. (11.63), so the result is\n \n E (1)\nhf =\nA\n2U2 3F1F + 12 - s1s + 12 - I1I + 124U2\n \n(11.64)\n \n = A\n2\n c F1F + 12 - 3\n2 d\n \n \n = b +A>4; F = 1\n-3A>4; F = 0 ,\n \nwhich is the same result again.\n11.6 \u0002 ADDITION OF GENERALIZED ANGULAR MOMENTA\nTo generalize from the example of adding the angular momenta of two spin-1/2 particles to the prob-\nlem of adding any two generalized angular momenta, it is instructive to consider an alternative deri-\n\n11.6 Addition of Generalized Angular Momenta \n371\nvation of the coupled states 0 FMF9 that employs the angular momentum ladder operators. The state \n0 119 = 0  + +9 is an eigenstate in both the uncoupled and coupled bases, with the largest possible val-\nues of the magnetic quantum numbers ms, mI, and MF. Such a state is called a stretched state. In this \nalternative method, we use the lowering operator F-  of the total angular momentum to generate other \ncoupled basis states, and by comparing the action of F-  in the two bases, we learn how the coupled \nand uncoupled basis states are related to each other.\nThe action of the lowering operator F-  of the total angular momentum on the stretched state \n0 119 = 0  + +9 generates the state 0 109, with a multiplicative factor 22U according to Eq. (11.23). In \nthe uncoupled basis, the lowering operator is F- = S - + I- , so we can also calculate the result of the \nlowering operation in the uncoupled basis:\n \ncoupled basis | uncoupled basis \n \n F- 0 119 = F- 0  + +9\n \n \n F- 0 119 = 1S - + I-2 0  + +9\n \n \n F- 0 119 = S - 0  + +9 + I- 0  + +9 \n \n 22U0 109 = U3 0  + -9 + 0  - +94.\n \n(11.65)\nThis lowering operation produces the same state no matter which basis we work in, so we  conclude that \n \n0 109 =\n1\n12 3 0  + -9 + 0  - +94 \n(11.66)\njust as we learned from the previous diagonalization procedures. Successive application of the lower-\ning operator then allows us to construct the ladder of states from the initial stretched state 0 119 down to \n0 109 and ﬁnally 0 1, -19 = 0  - -9, as shown in Fig. 11.5. However, the ladder operator changes only \nthe MF quantum number, not the F quantum number, so we do not generate the 0  009 state with this \nprocedure. Rather, we generate the 0  009 state by using the orthogonality condition to ﬁnd a state that \nis orthogonal to the 0 109 state and comprises the same uncoupled states 10  + -9 and 0  - +92 used in the \n0 109 state. The general linear combination of 0  + -9 and 0  - +9 state is\n \n0  009 = a0  + -9 + b0  - +9 \n(11.67)\nF\nF\northogonality\n\u000211\u0003\u0007\n\u000200\u0003\u0007\n\u000210\u0003\u0007\n\u00021,\n1\u0003\u0007\nFIGURE 11.5 Generation of the coupled state manifold using the lowering operator \nand the  orthogonality condition for the case of two spin-1/2 particles.\n\n372 \nHyperﬁne Structure and the Addition of Angular Momenta\nand the orthogonality condition is\n \n 0 = 8100 009\n \n \n =\n1\n12 18+ - 0 + 8- + 021a0  + -9 + b0  - +92 \n(11.68)\n \n =\n1\n12 1a + b2.\n \nHence, we conclude that a = -b = 1> 12 and \n \n0 009 =\n1\n12 1 0  + -9 - 0  - +92 \n(11.69)\nas we found earlier.\nThis general procedure of using ladder operators and orthogonality generates all the states in \nany coupled basis, and also yields the proper Clebsch-Gordan coefﬁcients. For the spin-1/2 case, the \nquantum numbers MF = {1\n2 never occur because the ladder starts with integer values of MF and the \nlowering operator changes MF by 1 each time. The orthogonality step preserves MF but changes F.\nNow let’s generalize to the problem of adding any two angular momenta. Consider two angular \nmomenta J1 and J2 coupled together to form a total angular momentum J:\n \nJ = J1 + J2. \n(11.70)\nThe angular momenta J1 and J2 are characterized by the quantum numbers j1, m1 and j2, m2, respec-\ntively, and the total angular momentum J is characterized by the quantum numbers J, M. Specifying \nall the eigenvalues, we write the uncoupled and coupled bases as:\n \n0  j1 j2\n m1\n m29  uncoupled basis\n \n(11.71)\n \n0  j1 j2\n JM9 \n coupled basis.  \nThe uncoupled basis vectors are eigenstates of J2\n1, J2\n2, J1z, and J2z. The coupled basis vectors are \neigenstates of J2\n1, J2\n2, J2, and Jz. In any given problem, the values of j1 and j2 are ﬁxed, so we could \nsuppress these labels. The convention we use is to suppress the j1 and j2 labels in the coupled states \n0 JM9, but not in the uncoupled states 0  j1 j2\n m1\n m29. This way, when we put in actual numbers (vs. \nalgebraic symbols) we can immediately tell whether a state is in the coupled basis (two labels) or the \nuncoupled basis (four labels). Using this notation, the relation between the coupled and uncoupled \nbases for the system of two spin-1/2 particles is\n \ncoupled basis | uncoupled basis \n \n 0119 = @1\n2 1\n2 1\n2 1\n29\n \n \n 0109 =\n1\n12 1@ 1\n2 1\n2 1\n2 -1\n2 9 + @ 1\n2 1\n2 -1\n2  1\n292  \n(11.72)\n \n 01, -19 = @ 1\n2 1\n2 -1\n2  -1\n2 9\n \n \n 0 009 =\n1\n12 1@ 1\n2 1\n2 1\n2 -1\n2 9 - @ 1\n2 1\n2 -1\n2  1\n292. \nIn terms of these generalized angular momentum labels, the spin-1/2 problem has j1 = 1/2 and \nj2 = 1>2 and the coupled J is equal to 0 or 1, which corresponds to the extreme values j1 + j2 and \n\n11.6 Addition of Generalized Angular Momenta \n373\nj1 - j2 . In the general case, the state generation procedure in Fig. 11.5 yields allowed values of J at all \nthe integer steps in between these extreme values:\n \nJ = j1 + j2,  j1 + j2 - 1,  j1 + j2 - 2, ... 0  j1 - j20   . \n(11.73)\nThe absolute value is needed because we require J Ú 0. For example, if j1 = 3 and j2 = 1, then the \nallowed values of J are 4, 3, 2. For each allowed value of J, the allowed values of M are -J to J in \ninteger steps:\n \nM = -J, -J + 1, ... , J - 1, J  , \n(11.74)\nwhich is what we expect for a generalized angular momentum. As a check, note that the total num-\nber of states is 12 j1 + 1212 j2 + 12, whether one counts in the coupled or uncoupled bases, which \nmust be the case (Problem 11.14). The state generation procedure in the general case is depicted in \nFig. 11.6, and an example of the resultant spectrum of states is shown in Fig. 11.7 for the case j1 = 1 \nand j2 = 1. The stretched state 0  J = j1 + j2, M = j1 + j29 = 0  j1 j2 , m1 = j1 , m2 = j29 is always an \neigenstate of both bases, so the coupled state generation procedure can start there in all cases. In Fig. \n11.7, the state 0 229=  0 11119 is the starting point for the generation procedure.\nThe general state generation procedure depicted in Fig. 11.6 works for any pair of j1, j2 values, \nand it tells us the Clebsch-Gordan coefﬁcients we need to express the coupled basis states in terms of \nthe coupled basis states, or vice versa. This procedure can be quite tedious for large angular momenta, \nas can the other method of diagonalizing the J2 and Jz matrices that we used in Section 11.5 for two \nspin-1/2 particles coupled together. Fortunately, the angular momentum addition problem has been \nsolved by others and the resultant Clebsch-Gordan coefﬁcients are conveniently tabulated. A general \nformula relating the coupled and the coupled states can be found by using the completeness relation. \nBecause we don’t mix j1, j2 manifolds, the completeness relation for the states in the uncoupled basis \nwithin a given j1, j2 manifold is\n \na\nj1\nm1= -j1\na\nj2\nm2= -j2\n 0  j1 j2\n m1\n m298 j1 j2\n m1\n m20 = 1. \n(11.75)\northogonality\northogonality\n\u0002j1\u000fj2,j1\u000fj2\u0003 \n\u0002j1\u000fj2,j1\u000fj2\n1\u0003 \n\u0002j1\u000fj2\n1,j1\u000fj2\n1\u0003 \n\u0002j1\u000fj2\n1,j1\u000fj2\n2\u0003 \n\u0002j1\u000fj2\n2,j1\u000fj2\n2\u0003 \n\u0002j1\u000fj2,j1\u000fj2\n2\u0003 \nJ\nJ\nJ\nJ\nJ\nJ\nFIGURE 11.6 The generation of the coupled state manifold using the lowering operator\nand the  orthogonality condition for the case of two generalized angular momenta j1 and j2.\n\n374 \nHyperﬁne Structure and the Addition of Angular Momenta\nOperate with this projection operator on a coupled state to get\n \n 0 JM9 = e\na\nj1\nm1= -j1\n  a\nj2\nm2= -j2\n 0  j1 j2\n m1\n m298 j1 j2\n m1\n m20 f 0 JM9 \n \n =\na\nj1\nm1= -j1\n  a\nj2\nm2= -j2\n58 j1 j2\n m1\n m20 JM960  j1 j2\n m1\n m29\n \n(11.76)\nwith the result\n \n0 JM9 =\na\nj1\nm1= -j1\n  a\nj2\nm2= -j2\nC j1 j2 J\nm1m2M0  j1 j2\n m1\n m29    , \n(11.77)\nwhere the scalar products 8 j1 j2\n m1\n m20 JM9 connecting the coupled and uncoupled bases are  written as\n \nC j1 j2 J\nm1m2M = 8 j1 j2\n m1m20 JM9. \n(11.78)\nThe coefﬁcients C j1 j2 J\nm1m2M are the Clebsch-Gordan coefﬁcients we introduced in Section 11.5.\nClebsch-Gordan coefﬁcients are tabulated in many books, although there are several different \nconventions on how to write the tables. A few examples of Clebsch-Gordan coefﬁcients are shown \nin Tables 11.2–11.5. Columns represent coupled states expressed in terms of uncoupled states. For \nexample, in the case of j1\n =1, j2 =1>2, the coupled state @ 3\n2 1\n29 can be read from the second column of \nTable 11.3:\n \n@ 3\n2 1\n29 = 4\n1\n3@  1 1\n2 1, -  1\n29 + 4\n2\n3@  1 1\n2 0 1\n29. \n(11.79)\nAll the Clebsch-Gordan coefﬁcients are real, so the inverse expansion uses the same coefﬁcients:\n \n 0  j1 j2\n m1\n m29 =\na\nj1+  j2\nJ= 0  j1-j20\n0 JM98JM0  j1 j2\n m1\n m29 \n \n =\na\nj1+  j2\nJ= 0  j1-j20\nC j1 j2 J\nm1m2M0 JM9.\n \n(11.80)\n\u000222\u0003\u0004\u00021111\u0003 \n\u00022\n2\u0003\u0004\u000211\n1\n1\u0003 \n\u000221\u0003\u0004\n\u00021110\u0003\u0005\u000f\u0007\u0007\u0007\u0007\u0007\u0007\u00021101\u0003\n1\n√2\n1\n√2\n\u000211\u0003\u0004\n\u00021110\u0003\u0005\n\u0007\u0007\u0007\u0007\u0007\u0007\u00021101\u0003\n1\n√2\n1\n√2\n\u00022 \n1\u0003\u0004\n\u0002110\n1\u0003\u0005\u000f\u0007\u0007\u0007\u0007\u0007\u0007\u000211\n10\u0003\n1\n√2\n1\n√2\n\u00021\n1\u0003\u0004\n\u0002110\n1\u0003\u0005\n\u0007\u0007\u0007\u0007\u0007\u0007\u000211\n10\u0003\n1\n√2\n1\n√2\n\u000210\u0003\u0004\n\u0002111\n1\u0003\u0005\n\u0007\u0007\u0007\u0007\u0007\u0007\u000211\n11\u0003\n1\n√2\n1\n√2\n\u000200\u0003\u0004\n\u0002111\n1\u0003\u0005\n\u0007\u0007\u0007\u0007\u0007\u0007\u00021100\u0003\u000f\u0007\u0007\u0007\u0007\u0007\u0007\u000211\n11\u0003\n1\n√3\n1\n√3\n1\n√3\n\u000220\u0003\u0004\n\u0002111\n1\u0003\u0005\u000f\u0007\u0007\u0007\u0007\u0007\u0007\u0007\u0007\u0007\u0007\u0007\u0007\u0007\u00021100\u0003\u000f\u0007\u0007\u0007\u0007\u0007\u0007\u0007\u000211\n11\u0003\n1\n√6\n1\n√6\n√\n2\n3\nFIGURE 11.7 The manifold of coupled angular momentum states for j1 = 1, j2 = 1, \nshowing the coupled states 0 JM9 expressed in terms of the uncoupled states 0  j1 j2\n m1\n m29.\n\n11.6 Addition of Generalized Angular Momenta \n375\nTable 11.2 Clebsch-Gordan Coefﬁcients for j1 \u0003 1\n2 and j2 \u0003 1\n2\nj1 = 1\n2\nj\n1\n1\n1\n0\nj2 = 1\n2\nm\n1\n0\n-1\n0\nm1\n1\n2\n1\n2\n-  1\n2\n-  1\n2\nm2\n1\n2\n-  1\n2\n1\n2\n-  1\n2\n1\n0\n0\n0\n0\n1\n12\n1\n12\n0\n0\n0\n0\n1\n0\n1\n12\n-  1\n12\n0\nNote that there is no sum over M in Eq. (11.80) because M = m1 + m2 is the only allowed M state. \nReading a row in the Clebsch-Gordan tables gives the inverse expansion of uncoupled states expressed \nin terms of coupled states. For example, in the case of j1=1, j2 =1/2, the uncoupled state @ 11\n2 0 1\n29 can \nbe read from the third row of Table 11.3: \n \n@ 11\n2 0 1\n29 = 4\n2\n3@ 3\n2 1\n29 - 4\n1\n3@ 1\n2 1\n29. \n(11.81)\nNote the correspondence between the Clebsch-Gordan Table 11.5 for the case of j1=1, j2 =1, and the \nmanifold of states depicted in Fig. 11.7.   \nNote the large number of zeroes in the Clebsch-Gordan tables. These zeroes are important \nbecause angular overlap integrals of wave functions that are used to ﬁnd transition probabilities can \nbe expressed in terms of Clebsch-Gordan coefﬁcients. The zeroes thus imply selection rules for transi-\ntions based upon the geometry of the states and the type of transition in question.\nTable 11.3 Clebsch-Gordan Coefﬁcients for j1 \u0003 1 and j2 \u0003 1\n2\nj1 = 1\nj\n3\n2\n3\n2\n3\n2\n3\n2\n1\n2\n1\n2\nj2 = 1\n2\nm\n3\n2\n1\n2\n-  1\n2\n-  3\n2\n1\n2\n-  1\n2\nm1\nm2\n1\n0\n0\n0\n0\n0\n0\n1\n13\n4\n2\n3\n0\n0\n0\n0\n0\n0\n4\n2\n3\n1\n13\n0\n0\n0\n0\n0\n0\n1\n0\n4\n2\n3\n-  1\n13\n0\n0\n0\n0\n0\n0\n1\n13\n-4\n2\n3\n0\n1\n1\n0\n0\n-1\n-1\n1\n2\n-  1\n2\n1\n2\n-  1\n2\n1\n2\n-  1\n2\n\n376 \nHyperﬁne Structure and the Addition of Angular Momenta\nTable 11.4 Clebsch-Gordan Coefﬁcients for j1 \u0003 3\n2 and j2 \u0003 1\n2\nj1 = 3\n2\nj\n2\n2\n2\n2\n2\n1\n1\n1\nj2 = 1\n2\nm\n2\n1\n0\n-1\n-2\n1\n0\n-1\nm1\nm2\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n2\n13\n2\n0\n0\n0\n0\n0\n0\n0\n0\n1\n12\n1\n12\n0\n0\n0\n0\n0\n0\n0\n0\n13\n2\n1\n2\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n13\n2\n-  1\n2\n0\n0\n0\n0\n0\n0\n0\n0\n1\n12\n-  1\n12\n0\n0\n0\n0\n0\n0\n0\n0\n1\n2\n-  13\n2\n0\n3\n2\n3\n2\n1\n2\n1\n2\n-  1\n2\n-  1\n2\n-  3\n2\n-  3\n2\n1\n2\n-  1\n2\n1\n2\n-  1\n2\n1\n2\n-  1\n2\n1\n2\n-  1\n2\nTable 11.5 Clebsch-Gordan Coefﬁcients for j1 \u0003 1 and j2 \u0003 1\nj1 = 1\nj\n2\n2\n2\n2\n2\n1\n1\n1\n0\nj2 = 1\nm\n2\n1\n0\n-1\n-2\n1\n0\n-1\n0\nm1\nm2\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n12\n0\n1\n12\n0\n0\n0\n0\n0\n0\n0\n1\n16\n0\n4\n2\n3\n0\n1\n16\n0\n0\n0\n0\n0\n0\n0\n1\n12\n0\n1\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n12\n0\n-  1\n12\n0\n0\n0\n0\n0\n0\n0\n1\n12\n0\n0\n0\n-  1\n12\n0\n0\n0\n0\n0\n0\n0\n1\n12\n0\n-  1\n12\n0\n0\n0\n1\n13\n0\n-  1\n13\n0\n1\n13\n0\n0\n1\n1\n1\n0\n0\n0\n-1\n-1\n-1\n1\n0\n-1\n1\n0\n-1\n1\n0\n-1\n\nSummary \n377\n11.7 \u0002 ANGULAR MOMENTUM IN ATOMS AND SPECTROSCOPIC NOTATION\nAn important application of angular momentum addition occurs in atoms where we must combine \nthe orbital angular momentum and the spin angular momentum of the electrons (we neglect the \nspin angular momentum of the nucleus here). The total angular momentum of the electrons is \ntypically denoted by J :\n \nJ = L + S . \n(11.82)\nFor a single electron atom such as hydrogen, the spin is s = 1/2, so the allowed values of the total \nangular momentum j are\n \n j = / + s, ...,0 / - s0\n \n \n = b/ + 1\n2 , / - 1\n2 ,   \n/ Ú 1\n   1\n2    ,   \n/ = 0 . \n(11.83)\nThe standard convention is to use lower-case letters (j, l, s) for the angular momenta of a single elec-\ntron, and upper-case letters (J, L, S) for the angular momenta of the complete atom.\nFor atoms with more than one electron, we must add all the electron angular momenta together. \nUsually we add all the orbital angular momenta together to get the total orbital angular momentum L, \nadd all the spin angular momenta to get the total spin S, and then couple L and S to get J. The results of \nthis angular momentum coupling are denoted with spectroscopic notation to specify the atomic state \n(also called term notation or Russell-Saunders notation)\n \n2S+1LJ, \n(11.84)\nwhere S and J are numbers and L is a letter specifying the orbital angular momentum. The letters used \nfor the orbital angular momentum states are\n \nL =    0\n1\n2\n3\n4\n5\n6\n7\n...\nletter = S\nP\nD\nF\nG\nH\nI\nK\n.... \n(11.85)\nFor example, the ground state of hydrogen has L = 0, S = 1/2, J = 1/2, and is denoted as 2S1>2. \nThis designation is the same for all the alkali atoms because they each have one electron outside a \nclosed shell. The ground state of carbon has L = 1 and S = 1, which couple to form the 3P0 state with \nJ = 0. Other values of J are possible according to the rules we have developed, but they turn out to \nhave higher energy because of internal perturbations.\nSUMMARY\nIn this chapter, we have introduced the concept of adding or coupling angular momenta. We noted that \nall angular momenta, whether spin or orbital, obey the general eigenvalue equations\n \n J2@  jmj9 = j1 j + 12U2@  jmj9 \n \n Jz@  jmj9 = mj U@  jmj9.\n \n(11.86)\n\n378 \nHyperﬁne Structure and the Addition of Angular Momenta\nWe introduced the angular momentum ladder operators\n \nJ+ = Jx + iJy \n \nJ- = Jx - iJy \n(11.87)\nthat raise and lower the magnetic quantum number according to the relation\n \nJ{ @  jmj9 = U3j1j + 12 - mj1mj { 124\n1>2@  j,mj { 19. \n(11.88)\nWe considered the general problem of coupling two angular momenta J1 and J2 together to form \nthe total angular momentum\n \nJ = J1 + J2. \n(11.89)\nWe described a system of two angular momenta using either the uncoupled basis or the coupled basis\n \n 0  j1 j2\n m1\n m29  uncoupled basis \n \n 0 JM9 \n coupled basis.  \n(11.90)\nThe allowed values of the coupled angular momentum quantum number are\n \nJ = j1 + j2 ,  j1 + j2 - 1 ,  j1 + j2 - 2 , ... 0  j1 - j20  \n(11.91)\nand the allowed coupled magnetic quantum numbers are\n \nM = -J , -J + 1, ... , J - 1, J. \n(11.92)\nWe expressed the coupled basis vectors in terms of the uncoupled basis vectors using the expansion\n \n0 JM9 =\na\nj1\nm1= -j1\na\nj2\nm2=-j2\nC j1 j2 J\nm1m2\n M0  j1 j2\n m1\n m29,  \n(11.93)\nwhere the scalar products connecting the coupled and uncoupled bases are the Clebsch-Gordan \ncoefﬁcients\n \nC j1 j2 J\nm1m2M = 8 j1 j2\n m1m2@ JM9. \n(11.94)\nWe studied the concept of angular momentum addition in the hyperﬁne structure of the ground \nstate of hydrogen, which is governed by the Hamiltonian\n \nH =\nhf = A\nU2 S~I \n(11.95)\nthat couples together the electron spin S and the proton spin I. The utility of the coupled basis was \nevidenced by the fact that the hyperﬁne Hamiltonian is not diagonal in the uncoupled basis, but it is \ndiagonal in the coupled basis, where the coupled angular momentum is\n \nF = S + I . \n(11.96)\n\nProblems   \n379\nIn this system of two spin-1/2 particles, the coupled basis in terms of the uncoupled basis is\n \ncoupled basis | uncoupled basis \n \n 0 119 = 0 1\n2 1\n2 1\n2 1\n29 = 0  + +9\n \n 0 109 =\n1\n12 10 1\n2 1\n2 1\n2 -1\n2 9 + 0 1\n2 1\n2 -1\n2  1\n292 =\n1\n12 10  + -9 + 0  - +92t Triplet\n \n 0 1, -19 = 0 1\n2 1\n2 -1\n2  -1\n2 9 = 0  - -9\n \n  @  009 =\n1\n12 1@ 1\n2 1\n2 1\n2 -1\n2 9 - @ 1\n2 1\n2 -1\n2  1\n292 =\n1\n12 1@  + -9 - @  - +92r Singlet . \n(11.97)\nIn the hydrogen ground state, the hyperﬁne interaction causes the triplet levels to be displaced from \nthe singlet level.\nPROBLEMS \n 11.1 Verify the commutation relations in Eqs. (11.18), (11.19), and (11.20).\n 11.2 Use the commutation relations in Eq. (11.20) to demonstrate that the angular momentum lad-\nder operators act as advertised. Derive Eq. (11.23) that characterizes the action of the ladder \noperators. (Hint: review the harmonic oscillator ladder operators.)\n 11.3 Show that the restriction that the angular momentum component Jz cannot be greater than the \nmagnitude of J Ai.e., the square root of J2B implies that the largest possible value of the mag-\nnetic quantum number is mj = j.\n 11.4 Consider a generic spin-3/2 system.\na) Write down the eigenstates of this system and the eigenvalue equations for S2 and Sz.\nb) Write down the matrices representing S2 and Sz by inspection.\nc) Use Eq. (11.23) that characterizes the action of the ladder operators to generate the matri-\nces representing Sx and Sy.\nd) Find the eigenvalues of Sx. \n 11.5 Show that Eq. (11.31) implies that the electron spin and proton spin operators commute with \neach other.\n 11.6 Use the eigenvalue equations (11.31) to derive the matrix representations in Eq. (11.32) for \nthe electron spin and proton spin component operators in the uncoupled basis. By similar \nmeans, ﬁnd the matrix representations of S2 and I2 in the uncoupled basis and conﬁrm that \neach is proportional to the identity matrix (see activity on system of two spin-1/2 particles).\n 11.7 Show that S~I = Sx Ix + Sy Iy + Sz Iz can be rewritten using the angular momentum ladder \noperators as S~I = 1\n21S +I- + S -I+2 + Sz Iz.\n 11.8 Calculate the action of the ladder operators S +, S -, I+, I- on each of the four uncoupled angu-\nlar momentum states 0{{9 of the ground state of hydrogen. Use your results to calculate the \nmatrix representing the hyperﬁne Hamiltonian H =\nhf = AS~I>U2 in the uncoupled basis.\n 11.9 Show that the electron and proton spin observables S2 and I2 commute with the hyperﬁne \nHamiltonian H =\nhf = AS~I>U2 and that the component observables Sz and Iz do not.\n 11.10 Diagonalize the matrix representing F2 in Eq. (11.54) and conﬁrm the eigenvalues and eigen-\nstates quoted in the text.\n\n380 \nHyperﬁne Structure and the Addition of Angular Momenta\n 11.11 Consider the ground state hyperﬁne system of the hydrogen atom. Calculate the matrices for \nS2, I2, and F2 in the coupled basis and show that the hyperﬁne Hamiltonian is diagonal in \nthis basis.\n 11.12 Consider a system of two particles. Particle #1 has spin 1 1s1 = 12 and particle #2 has spin \n1/2 (s2 = 1>2). The total spin of the system is S = S1 + S2. \na) List all the possible uncoupled basis states 0 s1s2\n m1m29.\nb) Identify the stretched state 0 s1s2s1s29.\nc) Starting with the stretched state, generate all the coupled basis states 0 SM9 using the low-\nering operator and the orthogonality condition as outlined in Section 11.6 of the text.\nd) From the results in (c), construct the Clebsch-Gordan table for this system.\n11.13 Use the scheme developed in Section 11.6 for generating coupled basis states to create the \nClebsch-Gordan coefﬁcients in Table 11.3.\n11.14 Consider a system of two angular momenta j1 and j2. Demonstrate that the total number of \nstates is 12j1 + 1212j2 + 12 whether you count states in the coupled or the uncoupled basis.\n11.15 Consider a system of two angular momenta with j1 = 1 and j2 = 1\n2.\na) Write down all the possible states of this system in the uncoupled basis 0  j1 j2\n m1m29.\nb) What are the allowed values of the coupled angular momentum quantum numbers J and M \nfor this system?\nc) Write down all the possible states of this system in the coupled basis 0 JM9.\nd) Use the Clebsch-Gordan coefﬁcients in Table 11.3 to express the coupled basis states \n0 JM9 in terms of the uncoupled basis states 0  j1 j2\n m1m29.\n11.16 Deuterium is an isotope of hydrogen with one electron bound to a nucleus (the deuteron) \ncomprising a proton and a neutron. The deuteron has spin I = 1 and has a gyromagnetic ratio \ngD = 0.857, which is the only change needed to use Eq. (11.10) for the hyperﬁne interaction \nin deuterium. Determine the hyperﬁne structure of the ground state of deuterium (i.e., ﬁnd the \neigenvalues and eigenstates). Calculate the splitting of the ground state and produce a ﬁgure \nlike Fig. 11.4 for deuterium.\n11.17 A positronium atom is a hydrogen-like atom with a positron 1m = me, q = +e, spin 1/22 as \na nucleus and a bound electron. The hyperﬁne structure in the ground state of positronium is \ndescribed by a perturbation Hamiltonian H\u0004 = AS1~S2>U2 where Si are the spins of the elec-\ntron and positron.\na) What is the Bohr energy of the ground state of positronium (ignore hyperﬁne structure \nfor now)?\nb) The electron and positron spins can be coupled to form the total spin S of the atom. Write \ndown the spin states of the coupled and uncoupled bases and how they relate to each other.\nc) Express the hyperﬁne Hamiltonian in the ground state as a matrix in both the coupled and \nuncoupled spin bases.\nd) Determine the effect of the hyperﬁne perturbation interaction on the ground state of posi-\ntronium. Draw an energy level diagram to illustrate your results.\n11.18 Consider two electrons, each with spin angular momentum si = 1>2 and orbital angular \nmomentum /i = 1. \na) What are the possible values of the quantum number L for the total orbital angular momen-\ntum L = L1 + L2? \n\nResources \n381\nb) What are the possible values of the quantum number S for the total spin angular momen-\ntum S = S1 + S2? \nc) Using the results from (a) and (b), ﬁnd the possible quantum numbers J for the total angu-\nlar momentum J = L + S.\nd) What are the possible values of the quantum number j1 of the total angular momentum \nJ1 = L1 + S1 of electron #1? Same question for electron #2.\ne) Using the results from (d), ﬁnd the possible quantum numbers J for the total angular \nmomentum J = J1 + J2 and compare to the results in (c).\n11.19 Express the angular momentum ladder operators in the position representation. Apply the \nraising operator to the spherical harmonic Y 0\n21u, f2 and verify that your result agrees with \nEq. (11.23).\n11.20 Consider a system of two particles. Particle #1 has spin 1 (s1 = 1) and particle #2 has spin 1/2 \n(s2 = 1>2). The system is in a state with total spin 1/2 and z–component -U>2. If you mea-\nsure the z–component of the spin of particle #1, what are the possible results, and what are the \nprobabilities of the measurements? Same question for particle #2.\n11.21 Consider a system comprising three electrons, each with spin angular momentum si = 1>2. \nIgnore the orbital angular momentum.\na) How many possible spin states are there in this system? Identify the states in the uncou-\npled basis.\nb) Identify the stretched state of the system and use the lowering operator and the orthogonal-\nity condition as outlined in Section 11.6 of the text to generate all the coupled basis states. \n(Hint: use Gram-Schmidt orthogonalization.)\nc) From the results in (b), construct a “Clebsch-Gordan table” for this system. Your answer is \nnot unique.\nRESOURCES\nActivities\nThis activity is available at\nwww.physics.oregonstate.edu/qmactivities\nSystem of two spin-1/2 particles: Students review the spin eigenvalue equations, determine the \npossible states of a two-spin system, and ﬁnd the matrix representations of operators in the two-\nspin system.\nFurther Reading\nPedagogical articles on the hyperﬁne interaction:\nD. J. Grifﬁths, “Hyperﬁne splitting in the ground state of hydrogen,” Am. J. Phys. 50, 698–703 \n(1982).\nG. W. Parker, “Spin current density and the hyperﬁne interaction in hydrogen,” Am. J. Phys. 52, \n36–39 (1984).\n\nC H A P T E R \n12\nPerturbation of Hydrogen\nSpectroscopy of the hydrogen atom has played a central role in the development of quantum mechan-\nics itself and of perturbation theory in particular. Nobel Laureates Arthur Schawlow and Theodor \nHänsch made important advances in hydrogen atom spectroscopy and noted that, “The spectrum of \nthe hydrogen atom has proved to be the Rosetta stone of modern physics: once this pattern of lines \nhad been deciphered much else could also be understood.” The scientiﬁc process whereby advances in \nexperimental precision of spectroscopic measurements have led to advances in theoretical understand-\ning of the hydrogen atom has been repeated throughout the last century. State of the art techniques \nnow permit the energy levels of the hydrogen atom to be measured with 15 digits of precision, provid-\ning one of the best testing grounds for quantum theory.\nIn Chapter 10, we studied the Stark effect—the perturbation of hydrogen energy levels by an \nexternal electric ﬁeld. In Chapter 11, we studied the hyperﬁne interaction—the perturbation of hydro-\ngen energy levels by the magnetic interaction between the electron spin and the nuclear spin. In this \nchapter we study further magnetic perturbations—due to both external and internal magnetic ﬁelds. \nThe internal ﬁelds give rise to the ﬁne structure of the hydrogen energy levels and to the hyperﬁne \nstructure that we have already studied. The external ﬁelds give rise to the Zeeman effect—the mag-\nnetic analog to the Stark effect. We also study internal perturbations due to relativistic effects, which \nare part of the ﬁne structure. We treat all of these effects as small perturbations to the hydrogen energy \nlevels we found in Chapter 8. It is possible to treat some of the internal perturbations exactly using the \nrelativistic Dirac equation, but that is beyond the scope of this text.\n12.1 \u0002 HYDROGEN ENERGY LEVELS\nLet’s review what we know about the energy levels of hydrogen. The zeroth-order hydrogen atom \nHamiltonian is a combination of kinetic and potential energy terms:\n \nH0 = p 2\n2m -\ne2\n4pe0r . \n(12.1)\nThe zeroth-order energy eigenvalue equation is\n \nH00 n/m9 = E (0)\nn 0 n/m9, \n(12.2)\nwhere the zeroth-order eigenstate wave functions are\n \n0 n/m9 \u0003 Rn/1r2Y m\n/ 1u,f2 \n(12.3)\n\n12.1 Hydrogen Energy Levels \n383\nand the zeroth-order energy eigenvalues are\n \nE (0)\nn\n= -  1\nn2 m\n2U2 a e2\n4pe0\nb\n2\n. \n(12.4)\nWe refer to these energy levels as the Bohr energies, even though Bohr found them using an incomplete \nquantum mechanical analysis. The Bohr energies are independent of the quantum numbers / and m, \nwhich implies a degeneracy of n2 for each n level. A diagram of the zeroth-order energy levels is shown \nin Fig. 12.1, where we have identiﬁed the separate / states of each n level. One important result of this \nchapter is that some of the degeneracy of the n levels is lifted. In Eq. (12.2), we have suppressed the \nsuperscript on the zeroth-order eigenstates 0 n/m9 but not the energies E (0)\nn  because we are focused on \nﬁnding energy corrections and will not be concerned with eigenstate corrections, so all references to \neigenstates in this chapter are to zeroth-order eigenstates.\nThe zeroth-order hydrogen energies are often expressed in terms of the Rydberg energy\n \nRyd = m\n2U2 a e 2\n4pe0\nb\n2\n\u0002 13.6 eV. \n(12.5)\n1s\n2s\n2p\n3s\n3p\n3d\n4s\n4p\n4d\n4f\n\r\nEnergy (eV)\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\nFIGURE 12.1 Energy level diagram of hydrogen showing the n \u0003 1 through n \u0003 4 \nstates. Arrows indicate the allowed optical transitions between the levels shown.\n\n384 \nPerturbation of Hydrogen\nIt is convenient to express the Rydberg energy in terms of another characteristic energy multiplied by \na dimensionless constant:\n \nRyd = 1\n2\n mc2 a\ne2\n4pe0\n Ucb\n2\n. \n(12.6)\nThe characteristic energy is the electron rest mass energy Erest = mc2, and the dimensionless constant \nis the ﬁne-structure constant [Eq. (8.41)]\n \na =\ne2\n4pe0\n Uc . \n(12.7)\nWith these choices, the hydrogen energy levels take on the simple form:\n \nE (0)\nn\n= -  1\nn2 1\n2\n a2mc2  . \n(12.8)\nGiven all these different formulae, it may not be clear which numbers are most important. Most \nwould agree that there are three numbers from these formulae that you should have ingrained into your \nmemory as well as your own name. With these in your memory banks, you will be able to do quantum \nmechanical calculations when stranded on a desert island. They are: (1) the hydrogen ground state energy:\n \nE (0)\n1s = -13.6 eV  , \n(12.9)\n(2) the electron rest mass energy:\n \nmc2 = 511 keV  , \n(12.10)\nand (3) the ﬁne-structure constant:\n \na =\n1\n137  . \n(12.11)\nBecause these three constants are related by Eq. (12.8),\n \n E (0)\n1s = -  1\n2\n a2mc2\n \n \n -13.6 eV \u0002 -  1\n2 \n1\n(137)2 511 keV \n \n(12.12)\nyou need remember only two of them if you know the hydrogen energy level equation (12.8).\nOur plan in this chapter is to discuss the real hydrogen atom by looking at various perturbations \nthat shift the energy levels from the Bohr energies shown in Fig. 12.1. In order to provide a road \nmap for our journey, we show these perturbation corrections in Fig. 12.2 for the ﬁrst two states of \nhydrogen, ordered from left to right by decreasing magnitude of the correction. The ﬁrst correction \nis the ﬁne structure, which includes several terms arising from the electron spin and from relativity. \nThe Lamb shift is a quantum electrodynamic effect that we discuss only qualitatively. The hyperﬁne \nstructure is caused by the interaction of the electron and nuclear magnetic moments, as we saw in the \n\n12.1 Hydrogen Energy Levels \n385\nprevious chapter. The ﬁne-structure constant a sets the scale for these perturbations, as outlined in \nTable 12.1. The reason is that the ﬁne-structure constant is a dimensionless measure of the strength \nof the electromagnetic interaction; it is also called the electromagnetic coupling constant. Because \na V 1, the perturbation approach to quantum electrodynamics is valid.\nThe ﬁne-structure constant in hydrogen perturbations also sets the scale of the electron velocity. \nThe total energy of the hydrogen atom is roughly equal parts kinetic and potential energies:\n \nE \u0007 T \u0007 V. \n(12.13)\nRelating the Bohr energy to the kinetic term\n \n1\n2\n a2mc 2 \u0007 1\n2\n mv 2   1    a2 \u0007 v 2\nc 2 \n(12.14)\ntells us that\n \na \u0007 v\nc . \n(12.15)\nn \u0004\u00072\nn \u0004\u00071\n,\nF \u0004\u00070\nF \u0004\u00071\nF \u0004\u00070\nF \u0004\u00071\nF \u0004\u00070\nF \u0004\u00071\nF \u0004\u00071\nF \u0004\u00072\nBohr Energies\nFine Structure\nLamb Shift\nHyperfine Structure\n2p 3\n2\n2p 3\n2\n2p 1\n2\n2p 1\n2\n2s 1\n2\n2s 1\n2\n1s 1\n2\n1s 1\n2\nFIGURE 12.2 Corrections to the n \u0003 1 and n \u0003 2 Bohr energy levels ordered by magnitude (large to \nsmall, left to right). The shifts are not drawn to scale and are increasingly magniﬁed from left to right.\nTable 12.1 Hydrogen Energy Scales\nTerm\nScale\nBohr energy\na2mc 2\nFine structure\na4mc 2\nLamb shift\na5mc 2\nHyperﬁne structure\n1me>mp2a4mc 2\n\n386 \nPerturbation of Hydrogen\nHence, the electron has a speed roughly 1% of the speed of light 1a = 1>137 \u0005 1%2. This velocity is \nlarge enough to make relativity important to the bound state energies, but it is small enough that the \nperturbative approach is valid.\n12.2 \u0002 FINE STRUCTURE OF HYDROGEN\nThe ﬁne structure of hydrogen has two primary contributions: (1) the relativistic correction caused by the \nelectron velocity, and (2) the spin-orbit correction caused by the magnetic interaction between the elec-\ntron spin magnetic moment and the magnetic ﬁeld generated by the electron orbital angular momentum.\n12.2.1 \u0002 Relativistic Correction\nThe relativistic energy of a particle includes kinetic energy and rest mass energy, but not potential energy:\n \nE = 1m2c4 + p2c22\n1>2.\n \n(12.16)\nWe would like to expand E in terms of a small parameter. From our discussion above, we expect that \nsmall parameter to be the ratio v>c. In quantum mechanics, we use momentum instead of velocity, so \nthe relevant small parameter is the ratio p>mc. We create a perturbative expansion of the relativistic \nenergy by factoring out mc2 from Eq. (12.16) to isolate the ratio p>mc\n \nE = mc2\nC1 + a p\nmcb\n2\n, \n(12.17)\nusing the binomial expansion\n \nE = mc2£ 1 + 1\n2\n a p\nmcb\n2\n- 1\n8\n a p\nmcb\n4\n+ ...§ , \n(12.18)\nand keeping the three leading terms:\n \nE \u0002 mc2 + p2\n2m -\np4\n8m3c2 .  \n(12.19)\nThe leading term in Eq. (12.19) is the rest energy of the electron, which we did not include in the \nzeroth-order Hamiltonian in Eq. (12.1). There is no need to include it now because it only shifts the \nzero level of energies. The second term in Eq. (12.19) is the classical expression for the kinetic energy \nthat we used in the zeroth-order Hamiltonian. The third term is the new relativistic kinetic energy cor-\nrection, and it becomes the Hamiltonian for the relativistic perturbation:\n \nH =\nrel = -  \np4\n8m3c2 . \n(12.20)\nNote that this perturbation is negative, which means it increases the binding energy. This perturba-\ntion Hamiltonian is two orders of p>mc smaller than the zeroth-order Hamiltonian, so we expect the \nresultant energy correction to be smaller than the zeroth-order energy differences by two orders of the \nﬁne-structure constant a, as indicated in Table 12.1.\n\n12.2 Fine Structure of Hydrogen \n387\nNow that we have the Hamiltonian, we apply perturbation theory to ﬁnd the energy level correc-\ntions due to this relativistic term. We ﬁrst ask whether we should use nondegenerate or degenerate \nperturbation theory. We know that the hydrogen energy levels are degenerate with respect to the quan-\ntum numbers / and m, so we expect to require degenerate perturbation theory. However, the operator \np4 commutes with the operators L2 and Lz, so the perturbation Hamiltonian is diagonal within each \ndegenerate subspace (Problem 12.1). Degenerate perturbation theory requires us to diagonalize the \nperturbation matrix within each degenerate subspace, so we simply identify the energy corrections \nfrom the diagonal elements. This perturbation has nothing to do with the spin, so it commutes with the \nspin operator. This means we can ignore the spin for now and use the states 0 n/m9 for the unperturbed \nbasis. (Note that we use m for mass and for the magnetic quantum number, but it is clear from the \ncontext which is which.)\nThe ﬁrst-order relativistic energy correction is\n \n E (1)\nrel = 8H =\nrel9 = 8n/m0 H =\nrel0 n/m9 \n \n = -  \n1\n8m3c2 8n/m0 p40 n/m9.\n \n(12.21)\nThe matrix element of the operator p4 requires integrals of fourth-order derivatives of the wave func-\ntions. There is an easier way. Use the zeroth-order Hamiltonian to express the operator p2 in terms of \nother operators:\n \n H0 = p2\n2m -\ne2\n4pe0r\n \n \n p2 = 2maH0 +\ne2\n4pe0rb. \n(12.22)\nThe matrix elements we need are\n \n 8n/m@p4@ n/m9 = 8n/m@p2p2@ n/m9\n \n \n = 8n/m0 2maH0 +\ne2\n4pe0rb 2maH0 +\ne2\n4pe0rb 0 n/m9. \n(12.23)\nThe zeroth-order eigenvalue equation H00 n/m9 = E (0)\nn 0 n/m9 tells us the action of the  Hamiltonian H0 \non the eigenstates, so we get\n \n 8n/m@p4@ n/m9 = 4m2 8n/m@ aE (0)\nn\n+\ne2\n4pe0rbaE (0)\nn\n+\ne2\n4pe0rb@ n/m9\n \n \n = 4m2 aAE (0)\nn B\n2\n+ 2E (0)\nn  e2\n4pe0\n h 1\nr i\nn/\n+ a e2\n4pe0\nb\n2\nh 1\nr 2 i\nn/\nb, \n(12.24)\nwhere we have used shorthand for the matrix elements (and have dropped the m label because the \nmatrix elements do not depend on m):\n \n8 f 1r29n/ = 8n/m0  f 1r2 0 n/m9. \n(12.25)\n\n388 \nPerturbation of Hydrogen\nThe resultant relativistic energy correction is\n \n8H =\nrel9 = -  \n1\n2mc2 cAE (0)\nn B\n2\n+ 2 e2\n4pe0\n E (0)\nn h 1\nr i\nn/\n+\ne4\n14pe022 h 1\nr 2 i\nn/\nd . \n(12.26)\nNow we are left with two spatial integrals that are much simpler than the ones you would have \nobtained from the matrix elements of p4 directly. These integrals are quite common because they \ntell us the expectation values of different powers of the radial position. The radial operator does not \ninvolve the angular variables, so the angular parts of the integrals are unity Athe Rn/1r2 and Y m\n/ 1u, f2 \nwave functions are separately normalizedB and the matrix elements reduce to radial integrals that we \npresented in Chapter 8 [Eq. (8.89)]:\n \n h 1\nr i\nn\u0002\n=\nL\n\u0005\n0\n1\nr\n R 2\nn\u00021r2r 2 dr =\n1\nn2a0\n \n(12.27)\n \n h 1\nr 2 i\nn\u0002\n=\nL\n\u0005\n0\n1\nr 2 R 2\nn\u00021r2r 2 dr =\n1\n1/ + 1\n22n3a 2\n0\n. \nTo make the three terms in Eq. (12.26) have the same form and to collect constants, use the hydrogen \nenergy formula\n \nE (0)\nn\n= -  1\nn2 \ne2\n214pe02a0\n= -  1\n2n2 a2mc2  \n(12.28)\nto rewrite e2>4pe0\n \ne2\n4pe0\n= a0a2mc2,  \n(12.29)\nwith the ﬁnal result that\n \nE (1)\nrel = -  1\n2\n a4mc2 c\n1\nn31/ + 1\n22\n-\n3\n4n4d . \n(12.30)\nAs we expected, the relativistic correction is a2 times smaller than the Bohr energy and is negative.\n12.2.2 \u0002 Spin-Orbit Coupling\nSpin-orbit coupling is the second part of the hydrogen ﬁne structure. The orbital motion of the elec-\ntron causes the electron to experience a magnetic ﬁeld, which interacts with the spin magnetic moment \nof the electron, hence the name. This internal magnetic ﬁeld effect is distinct from effects due to exter-\nnal magnetic ﬁelds—the Zeeman effect—that we study later in this chapter. It is also distinct from the \nmagnetic ﬁeld generated by the nuclear spin, which causes the hyperﬁne interaction. The origin of \nthe internal orbital magnetic ﬁeld can be understood in either of two ways. In one view, the electron \nmoves in the electric ﬁeld of the proton, which gives rise to a motional magnetic ﬁeld. Or we can view \nthe problem from the electron’s point of view: the electron sees a proton orbiting it, which creates a \nmagnetic ﬁeld at the electron, with the same resultant interaction. Let’s use this second viewpoint to \ncalculate the effect classically and then extend it to quantum mechanics.\n\n12.2 Fine Structure of Hydrogen \n389\nWe treat the proton orbiting the electron as a current loop, as shown in Fig. 12.3, and use the \nBiot-Savart law to calculate the magnetic ﬁeld from that loop of current. The magnetic ﬁeld at the \ncenter of the loop is\n \nB = m0\n I\n2r , \n(12.31)\nwhere the current I is that of the proton with charge + e orbiting in a period T = 2pr>v, with v being \nthe speed. The speed of the proton in the electron frame is the same as the speed of the electron in the \nproton frame, so we relate the ﬁeld experienced by the electron to the electron angular momentum \nthrough its relation with the velocity:\n \nL = mvr \n \nv = L\nmr . \n(12.32)\nThe resulting magnetic ﬁeld is\n \nB = m0\n2r \neL\n2pmr 2 =\nm0eL\n4pmr 3 =\neL\n4pe0\n mc2r 3 .  \n(12.33)\nWe make this into a vector equation because B and L point in the same direction:\n \nB =\ne\n4pe0\n mc2r 3 L. \n(12.34)\nThe energy of interaction between a magnetic dipole and a magnetic ﬁeld is\n \nE = -M~B. \n(12.35)\nThe intrinsic (spin) magnetic moment of the electron is\n \nM = -  e\nm\n S , \n(12.36)\ne\np\u000f\nr\nB\nv\nFIGURE 12.3 A proton rotating about an electron generates a magnetic \nﬁeld at the electron position.\n\n390 \nPerturbation of Hydrogen\nusing the electron gyromagnetic ratio ge = 2. The resultant spin-orbit interaction energy is\n \n ESO = - a-  e\nm\n Sb~\ne\n4pe0mc2r 3 L \n(12.37)\n \n =\ne2\n4pe0\n m2c2r 3 L~S.\n \nBy substituting the quantum mechanical operators for spin and orbital angular momentum, we arrive \nat the Hamiltonian for the spin-orbit perturbation. However, the classical result in Eq. (12.37) is incor-\nrect by a factor of 1>2 due to Thomas precession—a relativistic effect due to the acceleration of the \nelectron. We include this correction in our quantum mechanical Hamiltonian:\n \nH =\nSO =\ne2\n8pe0\n m2c2r 3 L~S. \n(12.38)\nWe are now in a position to use the addition of angular momentum tools that we learned in \nChapter 11. We are including spin in the hydrogen atom now, so we must incorporate spin into the \neigenstates. Previously, we speciﬁed the hydrogen energy eigenstates as 0 n/m9. We now specify the \nzeroth-order eigenstates as 0 n/sm/ms9, where the subscript on the orbital magnetic quantum num-\nber m/ distinguishes it from the spin magnetic quantum number ms. The states 0 n/sm/ms9 are still \nzeroth-order energy eigenstates because spin does not play a role in the zeroth-order Hamiltonian. The \nstates 0 n/sm/ms9 are the “uncoupled states” we introduced in Chapter 11, but we learned there that we \ncould also use the “coupled basis” 0 n/sjmj9, which is characterized by the total angular momentum \nJ = L + S. In Chapter 11, we found that the coupled basis was preferred for the hyperﬁne interac-\ntion problem because the total angular momentum F = S + I is conserved, but the individual angular \nmomenta S and I are not due to the interaction S~I. Similarly, the L~S interaction in the spin-orbit \nperturbation Hamiltonian in Eq. (12.38) causes the individual angular momenta L and S to not be \nconserved and the total angular momentum J = L + S to be conserved. Hence, the spin-orbit Hamil-\ntonian is not diagonal in the uncoupled 0 n/sm/ms9basis, but it is diagonal in the coupled 0 n/sjmj9basis. \nOnce again, the “smart” choice is the coupled basis because it makes the diagonalization procedure \nrequired by degenerate perturbation theory much easier—trivial, in fact.\nAs we did with the hyperﬁne interaction in Chapter 11, we use the deﬁnition of the total angular \nmomentum to ﬁnd a convenient expression for the scalar product term L~S in the spin-orbit interac-\ntion in terms of coupled basis operators:\n \n J = L + S\n \n \n J2 = L2 + S2 + 2L~S\n \n(12.39)\n \n L~S = 1\n2\n 1J2 - L2 - S22. \nThe operators J 2, L2, and S2 are diagonal in the coupled basis, so the spin-orbit Hamiltonian is diago-\nnal. If we had chosen the uncoupled basis, then we would have expressed the scalar product L~S using \nuncoupled basis operators:\n \n L~S = Lx\n Sx + Ly\n Sy + Lz\n Sz\n \n(12.40)\n \n = 1\n2\n 1L +\n S - + L -\n S +2 + Lz\n Sz. \n\n12.2 Fine Structure of Hydrogen \n391\nL~S is not diagonal in the uncoupled basis because the angular momentum ladder operators connect \nadjacent states, just as we found for the S~I matrix in Eq. (11.38) for the hyperﬁne structure calcula-\ntion (Problem 12.2).\nAs we have noted several times, when the perturbing Hamiltonian is already diagonal within the \ndegenerate subspace, then ﬁrst-order nondegenerate and degenerate perturbation theory are equivalent. \nHence, the energy correction due to spin-orbit coupling is obtained by ﬁnding the expectation values\n \nE (1)\nSO = 8H =\nSO9 = 8n/sjmj@H =\nSO@ n/sjmj9, \n(12.41)\nwhich are the diagonal matrix elements of the perturbation Hamiltonian in the degenerate subspace in \nthe coupled basis. Substituting the spin-orbit Hamiltonian from Eq. (12.38) yields\n \nE (1)\nSO =\ne2\n8pe0m2c2 8n/sjmj@ 1\nr 3 L~S@ n/sjmj9, \n(12.42)\nand using Eq. (12.39), we ﬁnd\n \nE (1)\nSO =\ne2\n16pe0m2c2 h 1\nr 3 i\nn/\n8/sjmj0 J2 - L2 - S20 /sjmj9.  \n(12.43)\nThe angular momentum matrix element is\n \n8/sjmj 0 J2 - L2 - S20 /sjmj9 = 3 j1 j + 12 - /1/ + 12 - s1s + 124 U2, \n(12.44)\nand the radial matrix element is [from Chapter 8, Eq. (8.89)]\n \nh 1\nr 3 i\nn/\n=\n1\na 3\n0 n3/1/ + 1\n221/ + 12\n.  \n(12.45)\nCollecting constants, and writing the Bohr radius as a0 = U>amc, we ﬁnd the spin-orbit energy \ncorrection\n \nE (1)\nSO = 1\n4\n a4mc2  \nj1 j + 12 - /1/ + 12 - 3\n4\nn3/1/ + 1\n221/ + 12\n. \n(12.46)\nThe spin-orbit shift is a2 times smaller than the Bohr energy, as is the relativistic correction. For an s \nstate, with / = 0, the expression in Eq. (12.46) is problematic because the denominator is zero, but \nthe numerator is also zero because j = 1>2 is the only possibility for the total angular momentum \nquantum number when / = 0. This problem with the / = 0 case is not really a problem, because we \ndo not expect any spin-orbit coupling for s states with no orbital angular momentum to create a mag-\nnetic ﬁeld. Hence, the spin-orbit correction in Eq. (12.46) applies only to the cases / \u0002 0.\nThe total ﬁne-structure correction is the sum of the relativistic and spin-orbit corrections. If we \nignore the problem with the / = 0 spin-orbit term for the moment and add together the results in \nEqs. (12.30) and (12.46), we obtain (Problem 12.3):\n \nE (1)\nfs\n= E (1)\nrel + E (1)\nSO = -  1\n2\n a4mc2 1\nn3 c\n1\nj + 1\n2\n- 3\n4n d . \n(12.47)\n\n392 \nPerturbation of Hydrogen\nThis result depends on j, but not on /, so the problem of the / = 0 singularity is gone (see below) and \nmiraculously we can use Eq. (12.47) for all / levels. The j dependence in the ﬁne-structure correction \nlifts some of the degeneracy of the nonrelativistic E (0)\nn  levels in hydrogen, which are 2n2 degener-\nate when spin is included. The lifting of the degeneracy is depicted in Fig. 12.4 for the energy levels \nn = 1, 2, 3. States with the same quantum numbers n and j have the same ﬁne-structure energy, even \nthough they may have different values of the orbital angular momentum quantum number /. This \ndegeneracy is exact to all orders in the relativistic Dirac theory (but not in the real atom—see below).\nThe miraculous resolution of the / = 0 problem where the relativistic and spin-orbit terms add \nto give the total ﬁne-structure correction, masks a subtle physical effect. By ignoring the restriction of \nthe spin-orbit correction to / \u0002 0 as we did, the sum of the spin-orbit and relativistic corrections just \nhappens to include a new term for the / = 0 case. This new term is known as the Darwin term. The \nphysical explanation of the Darwin term requires relativistic quantum mechanics in the form of the \nDirac equation, which predicts that the electron wave function includes some components at relativistic \nenergies that lead to high frequency oscillation of the electron motion. This trembling or jittering motion \nof the electron—zitterbewegung in German—smears out the electron, making it appear bigger than an \nideal point particle. A larger electron is bound less strongly to a point nucleus if the electron-nucleus \nseparation is less than the effective size of the trembling electron. The zitterbewegung effect is still \nmuch smaller than the Bohr radius a0, so the Darwin term is limited to s-states because they are the only \nstates with a ﬁnite probability of being near the origin [Eq. (8.77)].\nThe separate contributions of the spin-orbit, relativistic, and Darwin terms to the ﬁne structure are \nshown in Fig. 12.5 for the n = 2 states. The /-dependent spin-orbit interaction splits the degeneracy \nof the n = 2 levels, but the relativistic effect plus the Darwin term brings the two j = 1>2 levels, \n2s1>2 and 2p1>2, back together. The sign difference between the spin-orbit corrections of the 2p1>2 and \n2p3>2 states arises from the differing relative orientations of the spin and orbital angular momenta in \nthese states, which is also evident later when we study the Zeeman effect.\nn \u0004\u00071\nn \u0004\u00072\nn \u0004\u00073\n,\n43.8 GHz\n2.74 GHz\n4.87 GHz\n1.62 GHz\n0.54 GHz\nBohr Energies\nFine Structure\n3d 5\n2\n3p 3\n2 3d 3\n2\n,\n3s 1\n2 3p 1\n2\n,\n2s 1\n2\n1s 1\n2\n2p 1\n2\n2p 3\n2\n13.7 GHz\nFIGURE 12.4 Fine structure of the n = 1, 2, 3 states of hydrogen. The vertical scale \nis different for each n level and the separation between n levels is not to scale.\n\n12.3 Zeeman Effect \n393\nThe degeneracy of j states in the Dirac model is ﬁnally lifted when we consider quantum electro-\ndynamics (QED). In QED, the electromagnetic ﬁeld is quantized in a manner similar to the harmonic \noscillator problem in Chapter 9. This approach works because the electromagnetic ﬁeld energy is the \nsum of squares 1E2 + B22 and the same concept of ladder operators is applicable. The ladder opera-\ntors of the electromagnetic ﬁeld correspond to the creation and annihilation of photons, with the state \n0  n9 representing n photons in one mode of the light ﬁeld. The ground state 0  09 represents the vacuum \nstate, where no photons are present. However, the ground state energy Uv>2 implies that there is some \nresidual electromagnetic ﬁeld in the vacuum, even when no photons are present. This residual ﬁeld acts \non the electron and causes it to move about and become smeared out—similar to the zitterbewegung of \nthe Darwin term (the same effect but a different cause). The result of this perturbation is that s-states \nare bound less tightly and are shifted up slightly in energy, as shown in Fig. 12.2. This shift is known as \nthe Lamb shift, named after Willis Lamb who discovered this effect in 1947. Lamb and his graduate \nstudent Robert Retherford measured the energy difference between the 2s1>2 and 2p1>2 states by induc-\ning transitions between these two states using microwave equipment that had been developed during \nWorld War II. Soon after the experiment, Hans Bethe made a theoretical estimate of the effect, which \nlaid the groundwork for the development of QED. Willis Lamb won the Nobel Prize in physics in 1955 \nfor this groundbreaking work.\n12.3 \u0002 ZEEMAN EFFECT\nThe Zeeman effect is the shift of atomic energy levels caused by an external applied magnetic ﬁeld. \nThe applied ﬁeld couples to the magnetic moments in the atom associated with the orbital and spin \nangular momenta of the electron and the proton spin angular momentum. The Zeeman effect occurs in \nall atoms, but we limit this presentation to the hydrogen atom.\nThe energy of interaction between the applied magnetic ﬁeld and the magnetic moments in the \natom is\n \nE = -M~B. \n(12.48)\nThe magnetic moments of the electron are proportional to the Bohr magneton\n \nmB = e U\n2me\n\u0002 h * 1.4 MHz\nGauss . \n(12.49)\nn \u0007 2\nBohr\nSpin-Orbit\nRelativistic\nDarwin\n2p 3\n2\n2p 1\n2\n2p 1\n2\n2p 1\n2\n2s 1\n2\n2s 1\n2\n2s 1\n2\n2p 3\n2\n2p 3\n2\nFIGURE 12.5 Hydrogen ﬁne structure in the n \u0003 2 level.\n\n394 \nPerturbation of Hydrogen\nThe magnetic moment of the proton is approximately 1000 times smaller because of the large proton \nmass, so we neglect the proton contribution to the Zeeman effect.\n12.3.1 \u0002 Zeeman Effect without Spin\nLet’s begin by considering a spin-independent Zeeman effect, where we ignore the spin of the elec-\ntron. This effect is called the “normal Zeeman effect” in the literature, but that name is misleading \nbecause the spin and orbital magnetic moments contribute equally. Nevertheless, we use this unrealis-\ntic model because it is an easy calculation and it introduces us to the essential features of the Zeeman \neffect. Because we are ignoring the electron spin, we also ignore the ﬁne structure and consider the \nmodel of the hydrogen atom we used in Chapter 8 that includes only the kinetic energy and the Coulomb \ninteraction energy, with eigenstates 0 n/m/9 and Bohr energies E (0)\nn\n= -  Ryd>n2. We perturb the system \nby applying a magnetic ﬁeld B aligned along the z-axis. The electron magnetic moment associated with \nthe orbital motion\n \nML = -  e\n2me\n L = -  g/\n mB\nL\nU  \n(12.50)\ninteracts with the applied ﬁeld B, giving an energy\n \n E = -ML~B\n \n(12.51)\n \n = g/\n mB 1\nU\n L~B, \nwhere g/ = 1 is the orbital gyromagnetic ratio (we keep the g/ so we can distinguish orbital and spin \ncontributions later). The resultant Zeeman perturbation Hamiltonian is\n \n H =\nZ = g/\n mB 1\nU\n L~B \n(12.52)\n \n = g/\n mB B\nU\n Lz.  \nThe zeroth-order energy eigenstates 0 n/m/9 are degenerate, so we must use degenerate perturba-\ntion theory to ﬁnd the energy corrections caused by the Zeeman perturbation in Eq. (12.52). However, \nthe states 0 n/m/9 are eigenstates of Lz, so once again we have made the “smart” choice of basis. The \nmatrix representing H =\nZ is diagonal, so the ﬁrst-order energy corrections are the diagonal elements of \nthe perturbation Hamiltonian\n \n E (1)\nZ\n= 8H =\nZ9 = 8n/m/@H =\nZ@ n/m/9 \n \n = g/mB\nB\nU8n/m/0 Lz0 n/m/9.\n \n(12.53)\nEvaluating the matrix elements yields\n \nE (1)\nZ\n= g/\n mBBm/  . \n(12.54)\nThe normal Zeeman effect is proportional to the orbital gyromagnetic ratio g/, the Bohr magneton mB, \nthe magnetic ﬁeld strength B, and the magnetic quantum number m/. This general form is repeated \n\n12.3 Zeeman Effect \n395\nwhen we study the Zeeman effect in more realistic models. The gyromagnetic ratio and the particular \nmagnetic quantum number change as we include other magnetic moments in the model. A typical \nZeeman energy level diagram without spin is shown in Fig. 12.6 for the 2p state of hydrogen, with \nthe energy shifts proportional to m/ and the applied ﬁeld strength B. The m/ = 1 state has the orbital \nangular momentum aligned with the ﬁeld, which means that the magnetic moment is anti-aligned, and \nthe magnetic interaction energy is therefore positive. The degeneracy present in the zeroth-order state \nis lifted by the perturbation.\nThe Zeeman energy structure displayed in Fig. 12.6 provides a better understanding of the Stern-\nGerlach experiment we studied in Chapter 1. The experiments in Chapter 1 measured spin magnetic \nmoments, but the Stern-Gerlach effect applies equally well to the measurement of magnetic moments \narising from orbital angular momentum as in the normal Zeeman effect. In a Stern-Gerlach device, the \nexternal magnetic ﬁeld varies spatially, which implies a spatially varying Zeeman energy perturba-\ntion. A spatial dependence of the energy (strictly speaking a potential energy, which is the case here) \ngives rise to a force\n \n Fz = -  0\n0z E (1)\nz\n \n \n = -  \n0E (1)\nz\n0B  0B\n0z\n \n(12.55)\n \n \n = -g/\n mB\n m/ 0B\n0z . \nEach value of the magnetic quantum number m/ leads to a different value of the force and hence to a \ndifferent deﬂection of the beam in a Stern-Gerlach device. For example, a p state has three m/ values \n11, 0, -12 corresponding to the three energy levels in the Zeeman structure of Fig. 12.6, and results in \nthree beams exiting a Stern-Gerlach analyzer, as depicted in Fig. 12.7. The effective magnetic moment \nof the atom is given by the slope -0E (1)\nz >0B of the Zeeman energy plot, which is -g/\n mB\n m/ for the \nZeeman effect without spin.\nBext\nE2p\nE\nml \u0004\u00071\nml \u0004\u00070\nml \u0004\u0007\n1\nFIGURE 12.6 Zeeman level structure of the hydrogen 2p state, ignoring spin.\n\n396 \nPerturbation of Hydrogen\n12.3.2 \u0002 Zeeman Effect with Spin\nA more realistic model of the atom includes the electron spin. In this case, the Zeeman effect is referred \nto as the “anomalous Zeeman effect” in the literature because it was discovered experimentally before \nspin was known and it did not agree with the expected “normal” Zeeman effect predictions. In the \nZeeman effect with spin, we include the interaction of the spin magnetic moment with the external \napplied ﬁeld, in addition to the interaction of the orbital magnetic moment with the applied ﬁeld as we \ndid in the previous section. Thus, the total atomic magnetic moment is\n \nM = MS + ML = -ge e\n2me\n S - g/ e\n2me\n L = -gemB S\nU - g/mB\nL\nU ,  \n(12.56)\nwhere the spin gyromagnetic ratio ge is approximately 2 Awith its own correction of order a2B. The \ninteraction Hamiltonian is\n \n H =\nz = -M~B = mB\nU\n 1g/L + geS2~B \n \n = mBB\nU\n 1g/\n Lz + ge\n Sz2.\n \n(12.57)\nIn this more realistic model of the atom, we must include the ﬁne structure that we calculated \nin Section 12.2. The relative roles of the ﬁne structure and the Zeeman effect are determined by \nthe magnitudes of the energy corrections caused by each effect. The magnitude of the ﬁne-structure \ncorrections are displayed in Fig. 12.4 for a few states. The magnitude of the Zeeman corrections scale \nwith the magnitude of the applied ﬁeld, as illustrated in Fig. 12.6. The Zeeman effect applied to the \nﬁne-structure states of Fig. 12.4 lifts some of the degeneracy and splits levels into Zeeman structures \nlike Fig. 12.6. For small enough magnetic ﬁelds, the Zeeman corrections are much smaller than \nthe ﬁne-structure splittings, in which case we treat the ﬁne structure as part of the zeroth-order \nHamiltonian and treat the Zeeman effect as a small perturbation. For large magnetic ﬁelds, we include \nthe Zeeman effect in the zeroth-order Hamiltonian, and treat the ﬁne structure as a small perturbation. \nFor magnetic ﬁelds where the ﬁne structure and Zeeman corrections are comparable, we must treat \nboth effects as one perturbation.\n12.3.2.1 \u0002 Weak magnetic ﬁeld\nLet’s start with the weak magnetic ﬁeld case, and treat the ﬁne structure as part of the zeroth-order \nHamiltonian. In this case, the zeroth-order states are the coupled basis states @ n/sjmj9 in which the ﬁne \nstructure is diagonal, and the zeroth-order energies are the Bohr energies E (0)\nn  plus the ﬁne-structure \n?\n?\nLz\n0\n?\n1\n\n1\n\u00021\u0003\u0007\n\u00020\u0003\u0007\n\u0002\n1\u0003\nFIGURE 12.7 Stern-Gerlach measurement of the orbital \nangular momentum component Lz.\n\n12.3 Zeeman Effect \n397\nshifts in Eq. (12.47). Our task is to ﬁnd the effect of the Zeeman perturbation in Eq. (12.57) on these \nzeroth-order energies.\nInspection of the Zeeman interaction Hamiltonian in Eq. (12.57) shows that it is diagonal in \nthe uncoupled basis @ n/sm/ms9, but it is not diagonal in the coupled basis @ n/sjmj9. Unfortunately, \nwe must use the coupled basis because that is the basis we used to diagonalize the zeroth-order \nHamiltonian, which now includes the ﬁne structure. We no longer have the freedom to search for a \n“good” basis where the perturbation is diagonal. There doesn’t appear to be a straightforward way \nto ﬁnd a general expression for the Zeeman energy shifts in this case. Of course, we can solve the \nproblem by brute force by ﬁnding the matrix representations in the coupled basis of the non-diagonal \nLz and Sz matrices that comprise the Zeeman Hamiltonian, and then diagonalizing the perturbation \nHamiltonian in a degenerate subspace, which is given by a speciﬁc n and j in this case. It turns out \nthat there is a way to solve this problem in general, but it requires some advanced concepts from \nangular momentum theory that we do not have yet. So, to motivate why these advanced concepts \nwork, let’s do one problem by brute force and then quote without proof the angular momentum con-\ncepts we need to derive a general result.\nThe angular momentum operators Lz and Sz are both diagonal in the uncoupled basis 0 n/sm/ms9, \nand we know that the uncoupled and coupled bases are connected through the Clebsch-Gordan coef-\nﬁcients. Hence, it is straightforward (but tedious) using the Clebsch-Gordan expansion\n \n@  jmj9 = a\nm/\n ms\n@ /sm/\n ms98/sm/\n ms@ jmj9 \n(12.58)\nto ﬁnd the matrix representations of Lz and Sz in the coupled basis. We’ll calculate one matrix element \nto demonstrate the method and leave the others for you to do (Problem 12.6 and activity on Zeeman \nperturbation matrices in the coupled basis).\nConsider the hydrogen 2p states with n = 2, / = 1, and s = 1>2. The allowed values of the \ncoupled basis quantum number j 1J = L + S2 are j = 3>2 and j = 1>2. Table 11.3 of Clebsch-\nGordan coefﬁcients tells us that two particular coupled states are\n \n @ 3\n2 1\n29 =\n1\n13 @1 1\n2 1 -1\n2 9 + 4\n2\n3 @ 1 1\n2 0 1\n29  \n \n @ 1\n2 1\n29 = 4\n2\n3 @1 1\n2 1 -1\n2 9 -\n1\n13 @1 1\n2 0 1\n29. \n(12.59)\nUsing these expansions, we calculate the matrix element of Lz in the coupled basis in terms of uncoupled-\nbasis matrix elements:\n \n81\n2 1\n2 @Lz@ 3\n2 1\n29 = 14\n2\n3 81 1\n2 1 -1\n2 @ -\n1\n1381 1\n2 0 1\n2 @2Lz1 1\n13 @1 1\n2 1 -1\n2 9 + 4\n2\n3 @1 1\n2 0 1\n292.  \n(12.60)\nThe uncoupled-basis matrix elements are diagonal, leaving\n \n81\n2 1\n2 @Lz@ 3\n2 1\n29 = 12\n3 81 1\n2 1 -1\n2 0 Lz@ 1 1\n2 1 -1\n2 9 - 12\n3 81 1\n2 0 1\n2 @Lz@ 1 1\n2 0 1\n29.  \n(12.61)\nThe diagonal uncoupled-basis matrix elements are m/\n U, yielding the result\n \n 81\n2 1\n2 @Lz@ 3\n2 1\n29 = 12\n3\n 11U2 - 12\n3\n 10U2 \n \n = 12\n3  U.\n \n \n(12.62)\n\n398 \nPerturbation of Hydrogen\nRepeating this calculation for all possible values of j, mj in the hydrogen 2p level, we ﬁnd the com-\nplete Lz and Sz matrices in the coupled basis (Problem 12.6):\n \n Lz \u0003 U ß  \n1\n0\n0\n0\n0\n0\n0\n1\n3\n0\n0\n12\n3\n0\n0\n0\n1\n3\n0\n0\n12\n3\n0\n0\n0\n-1\n0\n0\n0\n12\n3\n0\n0\n2\n3\n0\n0\n0\n12\n3\n0\n0\n2\n3\n ∑ \n3\n2 , 3\n2 \n3\n2 , 1\n2 \n3\n2 , -1\n2  \n3\n2 , -3\n2  \n1\n2 , 1\n2 \n1\n2 , -1\n2  \n  \n(12.63)\n \n Sz \u0003 U ß  \n1\n2\n0\n0\n0\n0\n0\n0\n1\n6\n0\n0\n-12\n3\n0\n0\n0\n-1\n6\n0\n0\n-12\n3\n0\n0\n0\n-1\n2\n0\n0\n0\n-12\n3\n0\n0\n-1\n6\n0\n0\n0\n-12\n3\n0\n0\n1\n6\n∑ \n3\n2 , 3\n2 \n3\n2 , 1\n2 \n3\n2 , -1\n2  \n3\n2 , -3\n2  \n1\n2 , 1\n2 \n1\n2 , -1\n2  \n, \n(12.64)\nwhere we have labeled the rows and columns with the j, mj quantum numbers and boxed subspaces as \ndiscussed below. Degenerate perturbation theory requires us to construct the perturbation Hamiltonian \nEq. (12.57) using these matrices and then diagonalize the resultant. However, we need do that only \nwithin each degenerate subspace. The ﬁne structure lifts the degeneracy of the j = 3>2 and j = 1>2 \nstates, so we treat these subspaces separately. Inspection of the Lz and Sz matrices in Eqs. (12.63) and \n(12.64) shows that the only off-diagonal matrix elements in Lz and Sz are between states with different \nvalues of j, which have different ﬁne-structure shifts. Lz and Sz are each diagonal within the separate \nj = 3>2 and j = 1>2 subspaces boxed in Eqs. (12.63) and (12.64) and we can neglect the off-diagonal \nelements in applying degenerate perturbation theory. We got lucky! We did not have the freedom to \nchoose a basis to make the perturbation diagonal, but in the basis of ﬁne-structure eigenstates, the \nZeeman perturbation is already diagonal within each degenerate subspace. For the j = 3>2 subspace, \nthe Zeeman Hamiltonian is represented by the matrix\n \nH =\nZ \u0003 mBB •\n2\n0\n0\n0\n0\n2\n3\n0\n0\n0\n0\n-2\n3\n0\n0\n0\n0\n-2\nμ \n3\n2 , 3\n2 \n3\n2 , 1\n2\n3\n2 , -1\n2  \n3\n2 , -3\n2  \n, \n(12.65)\nwhere we have substituted the gyromagnetic ratios: g/ = 1, ge = 2. From the matrix in Eq. (12.65), \nwe ﬁnd the Zeeman energy shifts by inspection to be 2mBB, 2mBB>3, -2mBB>3, -2mBB. For the \nj = 1>2 subspace, the Zeeman Hamiltonian must also include the 2s1>2 states in addition to the 2p1>2 \nstates (Problem 12.7).\n\n12.3 Zeeman Effect \n399\nTo ﬁnd a general expression for the Zeeman energy shift rather than solving each case by matrix \nconstruction, we invoke a result from advanced angular momentum theory. To motivate this new idea, \nconsider the total angular momentum component operator Jz = Lz + Sz. For the hydrogen 2p state, \nthe matrix representing Jz is\n \n Jz \u0003 U ß\n   \n3\n2\n0\n0\n0\n0\n0\n0\n1\n2\n0\n0\n0\n0\n0\n0\n-1\n2\n0\n0\n0\n0\n0\n0\n-3\n2\n0\n0\n0\n0\n0\n0\n1\n2\n0\n0\n0\n0\n0\n0\n-1\n2\n    ∑  \n3\n2 , 3\n2 \n3\n2 , 1\n2 \n3\n2 , -1\n2  \n3\n2 , -3\n2  \n1\n2 , 1\n2 \n1\n2 , -1\n2  \n. \n(12.66)\nCompare this Jz matrix to the Lz and Sz matrices in Eqs. (12.63) and (12.64), concentrating on the \nseparate j = 3>2 and j = 1>2 boxed subspaces and ignoring the off-diagonal elements in Lz and Sz. \nThe submatrices for Lz and Sz within a given j subspace are proportional to the Jz submatrix in that \nsame subspace, with proportionality factors that are j-dependent. For Lz, the proportionality factors \nare 2>3 for the j = 3>2 case and 4>3 for the j = 1>2 case. For Sz, the proportionality factors are \n1>3 for the j = 3>2 case and -1>3 for the j = 1>2 case. These relations between the matrices in \nthe coupled basis that represent Lz and Sz and the matrix that represents the total angular momen-\ntum Jz are speciﬁc examples of the Wigner-Eckhart theorem, which is a fundamental part of the \ntheory of angular momentum.\nThe speciﬁc formula we need from the Wigner-Eckhart theorem relates the matrix element of \nany general vector component Vz to the matrix element of the total angular momentum component Jz:\n \n8 jmj@ Vz@ jm=\nj9 =\n8 jmj@V~J@  jmj9\nU2j1 j +12\n 8 jmj@  Jz@ jm=\nj9. \n(12.67)\nFor the Zeeman calculation, L or S play the role of the vector V. Equation (12.67) is called the projec-\ntion theorem because of the role of the projection V~J in determining the constant of proportional-\nity between the matrix elements of Vz and Jz. Note that the matrix element of the projection V~J is a \ndiagonal element, but the Vz and Jz matrix elements are general matrix elements between different mj \nstates within a given j subspace.\nTo use the projection theorem, we need to know the diagonal matrix elements 8 jmj0 V~J0  jmj9, \nwhich depend on the vector V we are using. For the orbital angular momentum L, the required projec-\ntion is obtained from the relation\n \nJ = L + S \n(12.68)\nby rearranging and squaring:\n \n S = J - L\n \n \n S2 = J2 + L2 - 2L~J\n \n(12.69)\n \n L~J = 1\n21J2 + L2 - S22. \n\n400 \nPerturbation of Hydrogen\nSimilarly, for the spin angular momentum:\n \nS~J = 1\n21J2 + S2 - L22. \n(12.70)\nHence, the diagonal matrix elements required in the projection theorem are\n \n 8 jmj@S~J@  jmj9 = U2\n2\n 3 j1 j +12 + s1s +12 - /1/ +124  \n \n 8 jmj@L~J@  jmj9 = U2\n2\n 3 j1 j +12 + /1/ +12 - s1s +124. \n(12.71)\nUsing these coefﬁcients in the projection theorem yields the Lz and Sz matrix elements in the coupled \nbasis within a given j subspace\n \n 8 jmj@Sz@  jm =\nj9 =\nj1 j +12 + s1s +12 - /1/ +12\n2 j1 j +12\n 8 jmj@ Jz@ jm =\nj9  \n \n 8 jmj@Lz@  jm =\nj9 =\nj1 j +12 + /1/ +12 - s1s +12\n2 j1 j +12\n 8 jmj@ Jz@ jm =\nj9. \n(12.72)\nThus, the projection theorem has allowed us to ﬁnd the matrix representations, within a speciﬁc sub-\nspace, of operators that are not diagonal in the coupled basis. Because Jz is diagonal in the coupled \nbasis, Eq. (12.72) tells us that Lz and Sz are also diagonal within a given j subspace, as we saw in \nEqs. (12.63) and (12.64). For the hydrogen 2p example, the proportionality constants in Eq. (12.72) \nare exactly those we found by inspection above.\nWe put all these results together to ﬁnd the ﬁrst-order Zeeman energy correction:\n \n E (1)\nZ\n= 8H =\nZ9 = 8n/sjmj@ H =\nZ@n/sjmj9\n \n = 8n/sjmj@ mBB\nU\n 1g/\n Lz + ge\n Sz2@ n/sjmj9\n \n = mBB\nU\n 1g/8n/sjmj@Lz@ n/sjmj9 + ge8n/sjmj@Sz@ n/sjmj92\n  (12.73)\n \n = mBB\nU\n ag/ \nj1 j +12+/1/ +12-s1s +12\n2 j1 j +12\n+ ge \nj1 j +12+s1s +12-/1/ +12\n2 j1 j +12\nb8n/sjmj@ Jz@n/sjmj9\n \n = mBB\nU\n ag/ \nj1 j +12+/1/ +12-s1s +12\n2 j1 j +12\n+ ge \nj1 j +12+s1s +12-/1/ +12\n2 j1 j +12\nb mj\n U.\nThis result can be written in the standard Zeeman form\n \nE (1)\nZ\n= gjmBBmj \n(12.74)\n\n12.3 Zeeman Effect \n401\nif we deﬁne a new gyromagnetic ratio gj :\n \ngj = g/ \nj1 j +12 + /1/ +12 - s1s +12\n2 j1 j  +12\n+ ge \nj1 j +12 + s1s +12 - /1/ +12\n2 j1 j +12\n, \n(12.75)\nwhich we refer to as the Landé g factor. This gyromagnetic ratio accounts for the relative contributions \nof the magnetic moments due to the spin and orbital angular momenta caused by their differing magni-\ntudes (gyromagnetic ratios: g/ = 1, ge = 2) and differing alignments (projection theorem). Substituting \nthe gyromagnetic ratios into Eq. (12.75), we obtain the Landé g factor\n \ngj = 1 +\nj1 j +12 + s1s +12 - /1/ +12\n2 j1 j +12\n. \n(12.76)\nFor the hydrogen 2p example, the Landé factors are g3>2 = 4>3 and g1>2 = 2>3. For the 2s1>2 state, \nthe Landé g factor is 2, i.e. gj = ge because the only magnetic moment comes from the electron spin in \nthat state. For hydrogen, with only one electron, s = 1>2, so j = /{1>2, and we can write the Landé \ng factor in general as\n \ngj = 1{ \n1\n2/ + 1. \n(12.77)\nWe thus get a Zeeman energy correction that is dependent on j and /:\n \nE (1)\nZ\n= mBBmj a\n 1{ \n1\n2/ + 1b, \n(12.78)\nwith the { sign for j = /{1>2. Figure 12.8 shows the weak-ﬁeld Zeeman level splittings in the 2p \nlevels of hydrogen.\nBext\nE\n3/2 3/2\n3/2 1/2\n1/2 1/2\n3/2 \n1/2\n1/2 \n1/2\n3/2 \n3/2\nj\nmj\n2p 1\n2\n2p 3\n2\n FIGURE 12.8 Weak-ﬁeld Zeeman structure of the hydrogen 2p ﬁne-structure levels \nlabeled with the quantum numbers of the coupled basis states.\n\n402 \nPerturbation of Hydrogen\n12.3.2.2 \u0002 Strong magnetic ﬁeld\nNow consider the case where the magnetic ﬁeld is strong enough that the Zeeman shifts are much \nlarger than the ﬁne-structure shifts. The perturbation assumption regarding the Zeeman effect is no \nlonger valid and it is more appropriate to include the Zeeman Hamiltonian\n \nH =\nZ = mBB\nU\n 1g/\n Lz + ge\n Sz2 \n(12.79)\nin zeroth-order and treat the ﬁne structure as a perturbation. In that case, the uncoupled basis 0 n/sm/ms9 \nis the preferred basis because the Zeeman Hamiltonian is diagonal in that basis. With this new choice, \nthe zeroth-order energies are the Bohr energies plus the the Zeeman corrections:\n \nE (0)\nn\n= -  Ryd\nn2\n+ 8n/sm/\n ms@ H =\nZ@n/sm/\n ms9. \n(12.80)\nThe additional Zeeman energies are the expectation values\n \n \u0006EZeeman = 8n/sm/\n ms@H =\nZ@ n/sm/\n ms9\n \n \n = 8n/sm/\n ms@  mBB\nU\n 1g/\n Lz + ge\n Sz2@ n/sm/\n ms9 \n \n(12.81)\n \n = mBB\nU\n 1g/\n m/\n U + ge\n ms\n U2.\n \nSubstituting the values g/ = 1 and ge = 2 yields\n \n\u0006EZeeman = mBB1m/ + 2ms2 \n(12.82)\nfor the strong-ﬁeld Zeeman effect. These zeroth-order energies for the 2p state as a function of mag-\nnetic ﬁeld are shown as dashed lines in Fig. 12.9, keeping in mind that these are valid only at high \nﬁelds. The Zeeman effect lifts most of the degeneracy of the Bohr energies.\nNow we treat the ﬁne structure as a perturbation to the zeroth-order states that include the \nZeeman interaction. Because we are using the uncoupled basis, we have to revisit our calculations \nof the ﬁne-structure corrections. The relativistic Hamiltonian is diagonal in both the uncoupled and \ncoupled bases, but the spin-orbit Hamiltonian is diagonal only in the coupled basis. However, the \noff-diagonal matrix elements of the spin-orbit Hamiltonian do not couple any of the states that are \ndegenerate with respect to the Zeeman Hamiltonian (Problem 12.8). Hence we can use nondegenerate \nperturbation theory to ﬁnd the ﬁrst-order ﬁne-structure corrections. In the uncoupled basis, the spin-\norbit energy corrections are the expectation values\n \n E (1)\nSO = 8H =\nSO9 = 8n/sm/\n ms@ H =\nSO@n/sm/\n ms9\n \n \n =\ne2\n8pe0\n m2c2 8n/sm/\n ms@ 1\nr 3 L~S@ n/sm/\n ms9\n \n \n =\ne2\n8pe0\n m2c2 h 1\nr 3 i\nn/8/sm/\n ms@  1\n2 1L +\n S - + L -\n S +2 + Lz\n Sz@/sm/\n ms9. \n(12.83)\n\n12.3 Zeeman Effect \n403\nBext\n2p\nE\nml ms\n1\n1\n2\n1\n\n1\n2\n\n1\n1\n2\n\n1 \n1\n2\n0\n\n1\n2\n0\n1\n2\n FIGURE 12.9 Strong-ﬁeld Zeeman structure of the 2p states of hydrogen. Solid lines \nshow the ﬁne-structure corrections to the Zeeman levels (dashed lines). The quantum  \nnumbers indicate the uncoupled basis states.\nBecause the ladder operators connect adjacent states, they produce no diagonal matrix elements and \nEq. (12.83) reduces to\n \n E (1)\nSO =\ne2\n8pe0\n m2c2 h 1\nr 3 i\nn/8/sm/\n ms\n @Lz\n Sz@ /sm/\n ms9 \n \n =\ne2\n8pe0\n m2c2 h 1\nr 3 i\nn/\nm/\n ms\n U2.\n \n(12.84)\nUsing Eq. (12.45) for the radial matrix element, we obtain\n \nE (1)\nSO = 1\n2\n a4mc2 \nm/\n ms\nn3/1/ + 1\n221/ + 12\n. \n(12.85)\nAdding together the relativistic [Eq. (12.30)] and spin-orbit corrections yields the ﬁne-structure shifts\n \n E (1)\nfs\n= 1\n2\n a4mc2 J 3\n4n3 -\n/1/ + 12 - m/\n ms\nn3/1/ + 1\n221/ + 12 R,  \n(12.86)\nwhich are shown in Fig. 12.9 for the 2p states.\n12.3.2.3 \u0002 Intermediate magnetic ﬁeld\nIn the intermediate magnetic ﬁeld regime where the Zeeman corrections and the ﬁne-structure correc-\ntions are comparable, we have to treat both effects as perturbations to the zeroth-order Bohr energy \nlevels. We then have to diagonalize the perturbation Hamiltonian H\u0004 = H =\nfs + H =\nZ in each degener-\nate E (0)\nn  energy subspace. In the uncoupled basis @ n/sm/ms9, H =\nZ is diagonal but H =\nfs is not diagonal, \nwhile in the coupled basis @ n/sjmj9, H =\nfs is diagonal but H =\nZ is not diagonal. Hence, there is no obvious \n\n404 \nPerturbation of Hydrogen\npreferred basis; we have to do some work to diagonalize the perturbation Hamiltonian matrix in either \ncase. For example, in the n \u0003 2 subspace the matrix representing the perturbation Hamiltonian in the \ncoupled basis is [we have left out the 2s states here—they do not couple to the 2p states so they can be \ndiagonalized separately (Problem 12.9)]\n \nH\u0004 \u0003 ß\n-a + 2b\n0\n0\n0\n0\n0\n0\n-a + 2\n3b\n0\n0\n- 12\n3 b\n0\n0\n0\n-a - 2\n3b\n0\n0\n- 12\n3 b\n0\n0\n0\n-a - 2b\n0\n0\n0\n- 12\n3 b\n0\n0\n-5a + 1\n3b\n0\n0\n0\n- 12\n3 b\n0\n0\n-5a - 1\n3b\n∑ \n3\n2 , 3\n2 \n3\n2 , 1\n2 \n3\n2 , -1\n2  \n3\n2 , -3\n2  \n1\n2 , 1\n2 \n1\n2 , -1\n2  \n, \n(12.87)\nwhere we have deﬁned\n \n a =\n1\n128 a4mc2 \n \n b = mBB.\n \n(12.88)\nThe off-diagonal terms in Eq. (12.87) come from the off-diagonal matrix elements of the Lz and Sz \nmatrices in Eqs. (12.63) and (12.64). We were able to ignore those terms in the weak-ﬁeld Zeeman \ncalculation because they represented coupling between different zeroth-order degenerate subspaces. \nIn the intermediate ﬁeld case, these terms represent coupling between states within the same degen-\nerate subspace, so they must be included in the degenerate perturbation theory calculation. Diago-\nnalization of the matrix in Eq. (12.87) yields the energy shifts shown in Fig. 12.10 (Problem 12.9). \nBext\nE\nWeak Field\nIntermediate Field\nStrong Field\n1\nml ms\nmJ\n1\n2\n\n1\n1\n2\n1 \n1\n2\n\n1 \n1\n2\n0 \n1\n2\n0\n1\n2\n3\n2\n1\n2\n1\n2\n\n1\n2\n\n1\n2\n\n3\n2\n2p 1\n2\n2p 3\n2\n FIGURE 12.10 Perturbation of the hydrogen 2p states caused by the Zeeman effect and the \nﬁne structure as a function of the applied magnetic ﬁeld.\n\n12.3 Zeeman Effect \n405\nNote the linear-to-quadratic-to-linear behavior of some of the energy shifts as the applied ﬁeld goes \nfrom weak to intermediate to strong. For the weak ﬁeld case, the coupled basis is the “good” basis with \nstates deﬁned by the quantum numbers j and mj as in Fig. 12.8, while for the strong ﬁeld, the uncou-\npled basis is the “good” basis, with states deﬁned by the quantum numbers m/ and ms as in Fig. 12.9. \nThe results of the intermediate-ﬁeld calculation are valid at all ﬁelds and give the previous results in \nthe appropriate limits (Problem 12.9).\n12.3.3 \u0002 Zeeman Perturbation of the 1s Hyperﬁne Structure\nLet’s study the Zeeman effect in the ground state of hydrogen. The only energy level structure in the \nhydrogen ground state is the hyperﬁne structure we studied in Chapter 11. The ﬁne-structure effects \nand the QED Lamb shift do not lift any of the degeneracy in the ground state—they only shift the \nlevel, as shown in Fig. 12.2. Hence we include the ﬁne structure and Lamb shifts as part of the zeroth-\norder energy and treat the Zeeman effect and the hyperﬁne interaction as the two relevant perturba-\ntions for this problem.\nIn Chapter 11, we found the energy eigenvalues and eigenstates of the hyperﬁne perturbation in \nthe 1s1>2 ground state of hydrogen. We found that the hyperﬁne Hamiltonian\n \nH =\nhf = A\nU2 S~I \n(12.89)\nwas diagonal in the coupled basis of 0 FMF9 states. For the 1s1>2 state of hydrogen, the orbital angular \nmomentum is zero and the Zeeman Hamiltonian of Eq. (12.57) reduces to\n \nH =\nZ = 2mBB\nU\n Sz, \n(12.90)\nwhere we have set ge = 2.\nThe solution of this problem is analogous to the anomalous Zeeman effect where we had two per-\nturbations: the ﬁne structure and the Zeeman effect. In this problem, the hyperﬁne interaction takes the \nplace of the ﬁne-structure interactions, and we also consider three regimes of magnetic ﬁeld strength \ncorresponding to the relative magnitudes of the Zeeman and hyperﬁne shifts. For weak ﬁelds, we (1) \ninclude the hyperﬁne structure in the zeroth-order Hamiltonian, (2) use the coupled states 0 FMF9 as \nthe “good” basis because the hyperﬁne Hamiltonian is diagonal in that basis, and (3) treat the Zeeman \neffect as a perturbation. For strong ﬁelds, we (1) include the Zeeman effect in the zeroth-order Hamil-\ntonian, (2) use the uncoupled states 0 sIms\n mI9 as the “good” basis because the Zeeman Hamiltonian is \ndiagonal in that basis, and (3) treat the hyperﬁne interaction as a perturbation. In intermediate ﬁelds, \nwe treat both the hyperﬁne interaction and the Zeeman effect as perturbations and diagonalize the full \nperturbation Hamiltonian in whichever basis we like because neither is preferred. We’ll do the inter-\nmediate case and leave the weak and strong cases for homework (Problem 12.12).\nIn the coupled basis, the hyperﬁne interaction is diagonal, as given in Eq. (11.61):\n \nH =\nhfs \u0003 A\n4 §\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n-3\n¥ \n11\n10\n1, -1\n00\n , \n(12.91)\n\n406 \nPerturbation of Hydrogen\nwhere A is the hyperﬁne splitting between the degenerate F = 1 triplet states and the F = 0 singlet \nstate and the matrix rows are labeled with the coupled basis quantum numbers F and MF. In this same \nbasis, the Zeeman Hamiltonian is (Problem 12.12)\n \nH =\nZ \u0003 mBB §\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n-1\n0\n0\n1\n0\n0\n¥ \n11\n10\n1, -1\n00\n . \n(12.92)\nDiagonalization of the sum of these two matrices yields the energies plotted in Fig. 12.11 (Problem \n12.13). In weak ﬁelds, the Zeeman shift is linear in the ﬁeld and proportional to the coupled basis \nmagnetic quantum number MF , analogous to the weak ﬁeld result in Eq. (12.78) and in Fig. 12.8. In \nstrong ﬁelds, the Zeeman shift is linear in the ﬁeld and proportional to the uncoupled basis magnetic \nquantum number ms, analogous to the strong ﬁeld result in Eq. (12.82) and in Fig. 12.9. In intermedi-\nate ﬁelds, neither basis is a “good” basis and some of the energies exhibit quadratic dependence on the \nﬁeld strength, analogous to Fig. 12.10.\nThe deﬂection of an atom in a Stern-Gerlach device is proportional to the slope -0E (1)\nz >0B of the \nZeeman energy plot [Eq. (12.55)]. In the linear case of Fig. 12.6, this results in three beams output \nfrom the Stern-Gerlach analyzer, as shown in Fig. 12.7. The Zeeman structure of the ground state \nof hydrogen displayed in Fig. 12.11 tells us that the slope and therefore the number of output beams \ndepends on the magnitude of the magnetic ﬁeld. Hence, the effective magnetic moment of the atom \nas measured by a Stern-Gerlach analyzer depends on the strength of the magnetic ﬁeld. The hydrogen \nground state atom produces three beams from a Stern-Gerlach analyzer in weak ﬁelds, two beams in \nstrong ﬁelds, and four beams in intermediate ﬁelds (Problem 12.16). I. I. Rabi used this concept to \nstudy nuclear magnetic moments, for which he was awarded the Nobel Prize in physics in 1944.\nBext\nF\u00041\nF\u00040\nE\n1\n\n /2 \n1/2\n\n1/2\n1/2\n1/2 \n1/2\n1/2\n1/2\nms\nmI\nWeak Field\nIntermediate Field\nStrong Field\nMF\n1\n0\n0\n\n1\nE(0)1s\nFIGURE 12.11  The Zeeman structure of the hydrogen ground state hyperﬁne levels.\n\nProblems \n407\nSUMMARY\nThe Bohr energy levels of hydrogen are perturbed by the ﬁne-structure effects, which include the \nrelativistic correction and the spin-orbit interaction. These effects partially lift the degeneracy of the \nn levels, splitting each into its possible j states. The ﬁne-structure energy corrections are a2 smaller \nthan the Bohr energies, where a \u0002 1>137 is the ﬁne-structure constant.\nAn applied magnetic ﬁeld causes Zeeman splitting of each degenerate ﬁne-structure or hyperﬁne-\nstructure level into the states labeled with the magnetic quantum number m. For weak or strong ﬁelds, \nthe Zeeman corrections are linear in the applied magnetic ﬁeld, with the splitting given by the general \nexpression\n \nE (1)\nZ\n= gmBBm,  \n(12.93)\nwhere the appropriate g-factor and the magnetic quantum number m depend on which level is being \nsplit. For intermediate ﬁelds, the Zeeman perturbation and the ﬁne structure and/or hyperﬁne structure \nmust be simultaneously diagonalized, leading to energy corrections that are quadratic in the applied \nmagnetic ﬁeld.\nPROBLEMS\n 12.1 Show that the operator p4 commutes with the operators L2 and Lz, so that the relativistic per-\nturbation Hamiltonian is diagonal within each degenerate subspace.\n 12.2 Find the matrix representation of L~S in the coupled basis and in the uncoupled basis for the \nn = 2 state of hydrogen.\n 12.3 Derive the ﬁne-structure result in Eq. (12.47) to demonstrate the miraculous resolution of the \n/ = 0 problem.\n 12.4 Explain in words why the Zeeman interaction Hamiltonian in Eq. (12.57) is diagonal in the \nuncoupled basis 0 n/sm/ms9 but not diagonal in the coupled basis @ n/sjmj9.\n 12.5 Find the matrix representations of Lz and Sz in the uncoupled basis 0 n/sm/ms9 for the 2p states \nof hydrogen.\n 12.6 Use the Clebsch-Gordan expansion method demonstrated in Section 12.3.2 to calculate the \nmatrix representations of Lz and Sz in the coupled basis @ n/sjmj9 for the 2p states of hydrogen.\n 12.7 Find the matrix representation of the Zeeman Hamiltonian in the coupled basis @ n/sjmj9 for the \nn = 2, j = 1>2 subspace. Make sure to include the 2s1>2 states in addition to the 2p1>2. Find \nthe weak-ﬁeld Zeeman energy shifts and plot them as a function of the magnetic ﬁeld.\n 12.8 Consider the strong-ﬁeld Zeeman effect perturbation calculation, where we found it best to \nwork in the uncoupled basis. Show that the off-diagonal matrix elements of the spin-orbit \nHamiltonian do not couple any of the states that are degenerate with respect to the Zeeman \nHamiltonian.\n 12.9 Diagonalize the intermediate-ﬁeld perturbation matrix for the 2p states in Eq. (12.87) and \nproduce the plot in Fig. 12.10. Show that the energy shifts approach the weak- and strong-ﬁeld \nresults in the appropriate limits. Calculate the energy shifts of the 2s levels and add them to \nthe plot.\n\n408 \nPerturbation of Hydrogen\n 12.10 Find the matrix representation of the perturbation Hamiltonian H\u0004 = H =\nfs + H =\nZ in the \nuncoupled basis for the 2p states of hydrogen. Diagonalize the perturbation Hamiltonian and \nproduce the plot in Fig. 12.10.\n 12.11 Rearrange the rows and columns of the intermediate-ﬁeld perturbation matrix for the 2p states \nin Eq. (12.87) in order to make it appear block diagonal. Explain how the block diagonal \nnature of the matrix is manifested in Fig. 12.10.\n 12.12 Calculate the perturbed energies of the hydrogen 1s ground state caused by the Zeeman effect \nand the hyperﬁne interaction in (a) the weak ﬁeld limit, and (b) the strong ﬁeld limit. Estimate \nthe magnitude of the applied magnetic ﬁeld that separates these two limits.\n 12.13 Diagonalize the intermediate-ﬁeld perturbation Hamiltonian representing the Zeeman effect \nand the hyperﬁne interaction in the hydrogen 1s ground state and produce the energy diagram \nin Fig. 12.11, with energy and magnetic ﬁeld scales added.\n 12.14 Calculate the size of the following energy terms and spin orbit and relativistic corrections for \nthe hydrogen atom Afor (a)-(d), tabulate your results and give answers in three forms: theoreti-\ncal in terms of anmc2, numerical in eV or meV, and in GHzB.\na) The energy difference between the n = 1 and n = 2 states BEFORE any perturbations \nwere considered.\nb) The correction to the n = 1 and n = 2 states due to spin-orbit coupling. Note that the for-\nmula we derived in class is problematic for / = 0. Show that if you set j = / + 1\n2 and then \nuse j = 1\n2, the problem goes away. (This is the Darwin term we talked about, but go ahead \nand call it spin-orbit here.)\nc) The correction to the n = 1 and n = 2 states due to the relativistic term.\nd) The total correction to these states, (i.e., the ﬁne-structure correction).\ne) What wavelength resolution must your detector have to be able to resolve the two lines \nin the n = 2 to n = 1 transition? Be careful here. When you include the correction, you \nwill ﬁnd that it is very small compared to the unperturbed value. Be sensible about how to \ninclude the effects.\nf) Is it important to use the reduced mass of the electron in your calculations or is it OK to \nuse the free mass?\n 12.15 Deuterium is an isotope of hydrogen with one electron bound to a nucleus (the deuteron) \ncomprising a proton and a neutron. The deuteron has spin I = 1 and has a gyromagnetic ratio \ngD = 0.857, which is the only change needed to use Eq. (11.10) for the hyperﬁne interaction \nin deuterium.\na) Find the hyperﬁne structure of the ground state of deuterium. Calculate the splitting of the \nground state (in MHz) and produce a ﬁgure like Fig. 11.4. (This part is Problem 11.16.)\nb) Solve for the Zeeman splitting of the ground state of deuterium in intermediate ﬁelds and \nproduce a ﬁgure like Fig. 12.11.\n 12.16 Calculate the effective magnetic moment of the hydrogen atom in its ground state and conﬁrm \nthat the hydrogen ground state atom produces three beams from a Stern-Gerlach analyzer in \nweak ﬁelds, two beams in strong ﬁelds, and four beams in intermediate ﬁelds.\n\nResources \n409\nRESOURCES\nActivities\nThis activity is available at\nwww.physics.oregonstate.edu/qmactivities\nZeeman perturbation matrices in the coupled basis: Students write down angular momentum \nmatrices in the uncoupled basis by inspection, and use Clebsch-Gordan coefﬁcients to calculate angu-\nlar momentum matrices in the coupled basis.\nFurther Reading\nThe history of hydrogen atom spectroscopy and advances afforded by laser techniques are detailed in \nT. W. Hänsch, A. L. Schawlow, and G. W. Series, “The spectrum of atomic hydrogen,” \nScientiﬁc American 94–110 (March 1979).\nArthur L. Schawlow, Nobel lecture. \nhttp://nobelprize.org/nobel_prizes/physics/laureates/1981/schawlow-lecture.html\nTheodor W. Hänsch, Nobel Lecture. \nhttp://nobelprize.org/nobel_prizes/physics/laureates/2005/hansch-lecture.html\nT. W. Hänsch, “Nobel Lecture: Passion for Precision,” Rev. Mod. Phys. 78, 1297 (2006).\nRabi’s technique for studying nuclear magnetic moments:\nG. Breit and I. I. Rabi, “Measurement of Nuclear Spin,” Phys. Rev. 38, 2082 (1931).\nG. H. Fuller, “Nuclear Spins and Moments,” J. Phys. Chem. Ref. Data 5, 835 (1976).\n\nC H A P T E R  \n13\nIdentical Particles\nTo study systems like multielectron atoms, we need to properly account for the fact that all  fundamental \nparticles like electrons and protons are identical. In classical physics, particles are not identical—we \ncan always ﬁnd a way to uniquely identify a particular particle. Even if we make two classical particles \n“the same” to the utmost level of precision, we can still ﬁnd a way to identify the two particles without \naffecting their classical motion. For example, billiard balls behave identically, but can be identiﬁed \nby their numbers. In quantum mechanics, there is no way to identify two different electrons—they \nare indistinguishable. Two hydrogen atoms are identical no matter where they are in the universe. \nResearchers rely on this fact when they compare their experimental results on the spectra of hydrogen \natoms in different laboratories. To account for the indistinguishability of fundamental particles, we \nintroduce a new postulate in quantum mechanics, which leads to the Pauli exclusion principle that \nis responsible for the periodic table and all of chemistry. We apply this new postulate to the helium \natom to learn how the indistinguishability of the two electrons in the atom affects the energies and the \nallowed states.\n 13.1 \u0002  TWO SPIN-1/2 PARTICLES\nTo start our discussion of identical particles, let’s return to the system of two spin-1/2 particles that we \nstudied in Chapter 11. We found that we could describe the system using either of two bases:\n \n 0  + +9,  0  + -9,  0  - +9,  0  - -9    uncoupled basis   0  s1s2m1m29 \n \n 0\n 119,  0\n 109,  0\n 1, -19,  0\n 009     coupled basis    0  SMS9. \n(13.1)\nThe coupled basis is preferred when the two particles or systems interact, such as in the hyperﬁne \ninteraction or the spin-orbit interaction, because the Hamiltonian is diagonal in that basis. In general, \nour choice of basis depends on the problem at hand, but that choice is one we make based solely on \nconvenience in ﬁnding the energy eigenvalues. If we choose the other basis, we still ﬁnd the correct \neigenvalues—it just takes a little more work. However, if the two particles are identical, then that free-\ndom of choice of basis is no longer available. Let’s see why.\nConsider the ket 0  + -9 in the uncoupled basis that represents a quantum state in which particle 1 \nhas spin up and particle 2 has spin down. If the two particles in question are a proton and an electron, \nthen this representation is clear and unambiguous. However, if the two particles are electrons, then \nthis representation is more problematic. How can we possibly know that the particle that we measure \nto have spin up is electron 1 and not electron 2? We cannot. In quantum mechanics there is no way to \ndistinguish the two particles, and we cannot tell which particle has spin up and which has spin down. \n\n13.1 Two Spin-1/2 Particles \n411\nOr as Dr. Seuss said, we cannot know “Whether this one was that one . . . or that one was this one Or \nwhich one was what one . . . or what one was who. ” There is no experiment we can perform on the \nsystem of two electrons that would distinguish the state 0  + -9 from the state 0  - +9. This leads us \nto conclude that the uncoupled basis is inappropriate for representing this system of two identical \nspin-1/2 particles. So how do we mathematically represent the state with one particle having spin up \nand one particle having spin down?\nThe best way would be to start over and abandon the attempt at labeling the quantum numbers of \nindividual particles and instead specify how many particles have particular sets of quantum numbers. \nThis approach is the basis of more advanced treatments, but is too much of a change for us to make \nat this stage. So we adapt our labeling scheme to this new problem. A reasonable guess for represent-\ning the state with one particle having spin up and one particle having spin down would be to use a \nsuperposition of the two states 0  + -9 and 0  - +9 in a way that does not favor one over the other. From \nChapter 11 on the addition of angular momenta, we know that such superpositions already exist in the \ncoupled basis representation:\n \n 0 109 =\n1\n12 30  + -9 + 0  - +94 \n \n 0\n 009 =\n1\n12 30  + -9 - 0  - +94.\n \n(13.2)\nThese two states differ only by the minus sign coefﬁcient. Let’s see what the importance of that minus \nsign is, and in doing so, learn why the coupled basis is appropriate for describing systems of identical \nparticles.\nImagine that, unbeknownst to us, someone exchanged the two identical particles in the system, \nso that what we originally thought was particle 1 is now particle 2 and vice versa. The two states in \nEq. (13.2) would then become\n \n 1\n12 30  + -9 + 0  - +94    exchange   \n>  1\n12 30  - +9 + 0  + -94 =\n1\n12 30  + -9 + 0  - +94 \n \n 1\n12 30  + -9 - 0  - +94    exchange   \n>  1\n12 30  - +9 - 0  + -94 = - 1\n12 30  + -9 - 0  - +94.\n \n(13.3)\nThe ﬁrst state is unchanged and the second state acquires a minus sign, which is an overall phase shift \nof 180°. An overall phase shift causes no measurable change (Problem 1.3), so the physical states are \nunchanged by this exchange operation. We denote the exchange operation by the exchange operator \nP12, whose action on uncoupled basis states is\n \nP120 s1s2m1m29 = 0 s2s1m2m19. \n(13.4)\nIn terms of the coupled basis representation, the action of the exchange operator P12 on the two \nstates in Eq. (13.2) is\n \n P120 109 = + 0 109  \n \n P120 009 = - 0 009.\n \n(13.5)\nThese results tell us that the coupled basis states 0 109 and 0 009 are eigenstates of the exchange  operator \nwith eigenvalues +1 and -1, respectively. We call these states symmetric 1 0 1092 and  antisymmetric \n1 0 0092 states, by which we mean that they are symmetric and antisymmetric with respect to the \nexchange of the two particles, as opposed to symmetric and antisymmetric with respect to space or \n\n412 \nIdentical Particles\nsomething else. Note that the other two coupled basis states 0 119 and 0 1,-19 are both symmetric with \nrespect to exchange, (i.e., have eigenvalues of  +1):\n \n P120 119 = P12\n 0  + +9 = 0  + +9 = + 0 119 \n \n P12\n 0 1, -19 = P12\n 0  - -9 = 0  - -9 = + 0 1, -19.\n \n(13.6)\nNow consider a measurement upon an eigenstate of the exchange operator, which we denote by \n0 c{9, where the {  indicates the eigenvalue. The probability of recording some ﬁnal result is\n \n Pc{S cf = @8cf @ c{9@\n2. \n(13.7)\nIf we exchange the two particles before the measurement, then the probability is\n \n PP\n 12\n c{S cf = @8cf @5P12\n @ c{96@\n2 = @8cf @  P12\n @ c{9@\n2 \n \n = @8 cf @1{12@ c{9@\n2 = @8cf  @  c{9@\n2\n \n(13.8)\n \n  = Pc{S cf . \nHence, the calculated probability for a measurement made upon a state  0 c{9 is not changed if the \nparticles are exchanged. This agrees with our statement above that experiments cannot distinguish \nbetween systems with the identical particles exchanged. Thus the coupled basis superpositions are \npromising representations of the physical system of identical particles.\nAt this point, both coupled states 0 109 and 0 009 plausibly represent the physical state of a sys-\ntem of two identical spin-1/2 particles with one particle having spin up and one having spin down. \nHowever, we know from Chapter 11 on angular momentum addition that these two states have an \nimportant difference. The state 0 109 has total spin angular momentum S = 1, while the state 0 009 has \ntotal spin angular momentum S = 0. Thus, they are clearly not describing the same system. So how \ndo we know which of these two states to use to describe our system of two electrons with one having \nspin up and the other having spin down?\nNature chooses for us. You probably already know about the Pauli exclusion principle that for-\nbids having two electrons in the same quantum state. The Pauli exclusion principle is a speciﬁc exam-\nple of a broader quantum mechanical principle that we call the symmetrization postulate.\nThe symmetrization postulate stipulates that a system of identical particles is required to have a \nquantum state vector that is either symmetric or antisymmetric with respect to exchange of any pair \nof particles. Nature has sorted particles into two classes depending on whether they obey the sym-\nmetric or antisymmetric version of this principle. Particles that are required to have symmetric states \nare called bosons and particles that are required to have antisymmetric states are called fermions. \nFurthermore, this symmetry property is correlated with the spin angular momentum of the particles \ncomprising the system. Bosons are particles with integer spin (0, 1, 2, …) and are required to have \nsymmetric quantum states. Fermions are particles with half-integer spin (1/2, 3/2, 5/2, …) and are \nrequired to have antisymmetric quantum states. This connection between the spins of particles and \ntheir exchange symmetry can be proved using relativistic quantum mechanics, so we take it as a pos-\ntulate. It has been conﬁrmed in many experiments. Because the exchange symmetry determines the \nstatistical behavior of these particles, this concept is often called the spin-statistics theorem.\nElectrons have spin 1/2, so they are fermions, as are protons and neutrons. Photons and mesons are \nexamples of bosons. Composite particles, like atoms, are fermions or bosons, depending on the total \n\n13.1 Two Spin-1/2 Particles \n413\nFor the system of two identical spin-1/2 particles we discussed above, you would then conclude that \nthe two particles must be in the antisymmetric state 0 009, and the symmetric states 0 119, 0 109, and \n0 1, -19 are not allowed. However, we cannot yet reach that conclusion because the symmetrization \npostulate applies to the complete state vector, and we have not yet included the spatial part of the state \nvector.\nWe assume that we can separate the spin and spatial aspects of the state vectors. This is not \nalways possible, but it works for all the systems we study. The complete quantum state vector then has \nthe form\n \n@ c9 = @ cspatial9@ cspin9. \n(13.9)\nThe symmetrization postulate must be applied to this complete quantum state. For bosons, the com-\nplete state vector must be symmetric under exchange of particles. Thus, the spatial part must be \nsymmetric if the spin part is symmetric, or the spatial part must be antisymmetric if the spin part is \nantisymmetric:\n \n @  c SS\nboson I = @  c S\nspatial I  @  c S\nspin I  \n \n @  c AA\nboson I = @  c A\nspatial I  @  c A\nspin I. \n(13.10)\nThe two parts must have the same exchange symmetry, or else the full eigenstate would not be sym-\nmetric. For fermions, the complete state vector must be antisymmetric under exchange. Thus, the spa-\ntial part must be symmetric if the spin part is antisymmetric, or the spatial part must be antisymmetric \nif the spin part is symmetric:\n \n 0\n c SA\nfermion I = 0\n c S\nspatial I 0\n c A\nspin I \n \n 0\n c AS\nfermion I = 0\n c A\nspatial I 0 c  S\nspin I. \n(13.11)\nThe two parts cannot have the same exchange symmetry, or else the full eigenstate would not be anti-\nsymmetric. In Eqs. (13.10) and (13.11) we use superscripts to denote the exchange symmetry of the \nstate vectors.\nBosons (integer spin: 0, 1, 2, …) must have symmetric states. \nFermions (half-integer spin: 1/2, 3/2, 5/2, … ) must have antisymmetric states.\nspin of the system. For example, hydrogen has one electron and one proton—two spin-1/2 fermions— \nso it has integer total spin (1 or 0) and is a boson. Deuterium has one electron and a nucleus (the deu-\nteron) comprising one proton and one neutron, so it has half-integer spin and is a fermion. Thus, dif-\nferent isotopes of the same atom can behave differently when one considers the collective behavior of \natoms. For example, samples of liquid 3He (fermion) and liquid 4He (boson) behave quite differently \nat very low temperatures.\nTo summarize, the symmetrization postulate tells us that the quantum state vector of a system of \ntwo (or more) identical particles must be either symmetric or antisymmetric with respect to exchange \nof the two (or any two) particles. All particles are divided into either fermions or bosons:\n\n414 \nIdentical Particles\n13.2 \u0002 TWO IDENTICAL PARTICLES IN ONE DIMENSION\nWe take the example of a system of two identical particles bound within a one-dimensional potential \nenergy well to study the application of the symmetrization postulate to the complete state vector. Lim-\niting the discussion to one dimension is sufﬁcient to illustrate the most important ramiﬁcations of the \nsymmetrization principle. Let’s ﬁrst look at the spatial part of the state vector. The Hamiltonian for a \nsingle particle in one dimension is\n \nHsingle = p2\n2 m + V 1x2. \n(13.12)\nAssume we know the wave function solutions to the energy eigenvalue equation:\n \nHsingle w n 1x2 = En w n 1x2. \n(13.13)\nThe Hamiltonian for two particles in this potential energy well is\n \nH =\np2\n1\n2 m + V 1x12 +\np2\n2\n2 m + V 1x22, \n(13.14)\nwhere x1 labels the position of particle 1 along the x-axis, and p1 is the momentum of particle 1; x2 and \np2 are the equivalent for particle 2. We assume for the moment that the two particles do not interact \nwith each other and that the Hamiltonian has no spin dependence. The two-particle energy eigenvalue \nequation is\n \nHc 1x1, x22 = Ec 1x1, x22. \n(13.15)\nThe wave function c1x1, x22 of the system is a function of the two coordinates x1 and x2 locating the \ntwo particles. This two-particle wave function is a new concept, so a few comments about it are in \norder.\nThe complex square of the two-particle wave function yields the probability density\n \nP1x1, x22 = @\n c 1x1, x22@\n2, \n(13.16)\nbut because we have two particles, we must be clear what this density means. We interpret the prob-\nability density as a two-particle probability density, and so\n \n@ c 1x1, x22@\n2\n dx1 dx\n 2 \n(13.17)\nis the probability of ﬁnding particle 1 at position x1 within a volume dx1 and ﬁnding particle 2 at posi-\ntion x2 within a volume dx2. (“Volume” in this case is a length, but in a three-dimensional problem, it \nwould be a true volume.) We normalize the system wave function by integrating the two-particle prob-\nability density over both coordinates:\n \n \nO\n@ c 1x1, x22@\n2 dx1 dx2 = 1, \n(13.18)\nwhich means that the probability of ﬁnding both particles within the whole volume available to the \nsystem is unity.\n\n13.2 Two Identical Particles in One Dimension \n415\nIn this noninteracting example, the two-particle Hamiltonian is the sum of two single-particle \nHamiltonians, so the product function wna\n 1x12wnb\n 1x22 describing the system with particle 1 in energy \nstate na and particle 2 in energy state nb satisﬁes the energy eigenvalue equation (13.15) with energy \nEna + Enb. That would be the end of the story if the two particles were distinguishable (like an elec-\ntron and a proton, for example), but for indistinguishable or identical particles we must ﬁnd spatial \neigenstates that are either symmetric or antisymmetric with respect to exchange of the two particles. \nAs we did with the spin state vectors for the spin-1/2 states in the last section, we form the symmetric \nor antisymmetric superpositions\n \n @ c S\nspace9 \u0003 c S\nna\n nb1x1, x22 = NS 3wna1x12wnb1x22 + wna1x22wnb1x124 \n \n  @ c A\nspace9 \u0003 c A\nna\n nb1x1, x22 = NA 3wna1x12wnb1x22 - wna1x22wnb1x124, \n(13.19)\nwhere NS, A are the normalization constants. The wave function c S\nna\n nb1x1, x22 is symmetric with respect \nto exchange of the two particles, and the wave function c A\nna\n nb1x1, x22 is antisymmetric. Each of these \nsolutions has the two-particle energy\n \nEna\n nb = Ena + Enb. \n(13.20)\n13.2.1 \u0002 Two-Particle Ground State\nThe ground state of this system has both particles in the single-particle ground state, so na = 1 and \nnb = 1 and the energy of the state is E11 = 2E1. The symmetric two-particle ground-state wave\nfunction is\n \nc\n S\n111x1, x22 = w11x12w11x22. \n(13.21)\nHowever, the antisymmetric two-particle wave function is identically equal to zero:\n \nc\n A\n111x1, x22 = NA  3w11x12w11x22 - w11x22w11x124 = 0, \n(13.22)\nso there is no possibility of having an antisymmetric spatial wave function in the ground state, regard-\nless of whether the system comprises bosons or fermions.\nTo properly apply the symmetrization postulate to the complete state vector of this system, we \nmust include the spin in the state vector. Let’s assume that the system is either composed of two spin-0 \nbosons, or two spin-1/2 fermions. For two spin-0 bosons, the total spin must be zero and the only possible \nsystem spin state is 0 SM9 = 0 009, which is equal to the uncoupled basis state 0 s1s2\n m1\n m29 = 0 00009. \nThis spin state is symmetric under exchange of the two particles ( Problem 13.2). The complete state \nvector for bosons must be symmetric, so the spatial wave function must always be symmetric in this \nspin-0 example. Hence, the ground state of the two spin-0 bosons in a one-dimensional system is\n \n @ c\n SS\n11\n 9 \u0003 c\n S\n111x1, x22@  009 = w11x12w11x22@  009. \n(13.23)\nThe notation in Eq. (13.23) is a mixture of wave function language and abstract ket notation, but it \nmakes the space-spin distinction clear.\n\n416 \nIdentical Particles\nFor two spin-1/2 fermions, the total spin is 0 or 1, with the coupled basis states\n \n 0 119 = 0  + +9 \n \n 0 109 =\n1\n12 30  + -9 + 0  - +94 t Symmetric Triplet states \n \n 0 1,-19 = 0  - -9 \n \n 0 009 =\n1\n12 30  + -9 - 0  - +94 6  Antisymmetric Singlet state \n(13.24)\nbeing the eigenstates of the exchange operator we need to construct the complete state vectors. For the \nground state, the symmetric spin triplet states are excluded because the required antisymmetric spatial \nstate is identically zero [Eq. (13.22)]. The ground state of two spin-1/2 fermions is therefore\n \n@\n c\n SA\n11 9 \u0003 c\n S\n11 1x1, x22@\n 009 = w11x12w11x22@\n 009. \n(13.25)\nThis result exposes a problem with our notation. The spin state 0 009 in Eq. (13.23) is not \nthe same as the spin state 0 009 in Eq. (13.25). For two spin-0 bosons, the state 0 009 is really \n@ s1 = 0,  s2 = 0,  S = 0,  M = 09 and is symmetric under particle exchange, whereas for two spin-1/2\nfermions, the state 0 009 is @ s1 = 1\n2,  s2 = 1\n2,  S = 0,  M = 09 and is antisymmetric under particle \nexchange. We will continue with the notation 0 SM9 for coupled basis states, but note this limitation.\n13.2.2 \u0002 Two-Particle Excited State\nThe ﬁrst excited state of the two-particle system has one particle in the single-particle ground state \nand one particle in the ﬁrst single-particle excited state, so na = 1, nb = 2 and the energy of the \nstate is E12 = E1 + E2 . In this case, both the symmetric and antisymmetric spatial wave functions in\nEq. (13.19) are nonzero.\nFor the spin-0 boson case, the spatial wave function must be symmetric because there is only a \nsymmetric spin state, so the state vector is\n \n@\n c\n SS\n129 \u0003 c\n S\n12 1x1, x22@\n 009 =\n1\n12 3w1 1x12w2 1x22 + w21x12w1 1x224@\n 009. \n(13.26)\nFor the spin-1/2 fermion case, the total state vector must be antisymmetric, so the symmetric spa-\ntial wave function must combine with the antisymmetric singlet spin state\n \n@\n c\n SA\n12 9 \u0003 c\n S\n12 1x1, x22@\n 009 =\n1\n12 3w1 1x12w2 1x22 + w11x22w2 1x124@\n 009, \n(13.27)\nand the antisymmetric spatial wave function must combine with the symmetric triplet spin states\n \n@\n c\n AS\n12  9 \u0003 c\n A\n12 1x1, x22@ 1M 9 =\n1\n12 3w1 1x12w2 1x22 - w11x22w2 1x124@ 1M 9, \n(13.28)\nwith M = 1, 0, -1. The ﬁrst excited state of the fermion system is four-fold degenerate, while the \nboson state is nondegenerate.\nIf the two particles were distinguishable, then the ﬁrst excited state would be two-fold degener-\nate (assuming no spin), with states c12 1\n x1, x2 2 = w1 1\n x1 2w2 1\n x2 2 and c21 1\n x1, x2 2 = w2 1\n  x12w1 1\n x2 2. A \nschematic of the ground and ﬁrst excited states for all three cases is shown in Fig. 13.1. For the spin-1/2 \nfermions in Fig. 13.1(c), we use arrows to indicate the spin combinations. The states 0 119 and 0 1, -19 \nhave the two spins aligned, in which case the notation is clear. However, the states 0 109 and 0 009 are \ndifferent superpositions of spin up and down states, so the notation is somewhat unclear. You must \nremember that 0 109 =\n1\n12 10  + -9 + 0  - +92 and 0 009 =\n1\n12 10  + -9 - 0  - +92.\n\n13.2 Two Identical Particles in One Dimension \n417\n13.2.3 \u0002 Visualization of States\nTo visualize the spatial aspect of these states, assume the two particles are bound in an inﬁnite square \npotential energy well. The one-particle energy eigenstate wave functions for the inﬁnite square well are\n \n0 n9 \u0003 wn1x2 = A\n2\nL sin anpx\nL b. \n(13.29)\nThe spatial wave function for the ground state is the same [Eq. (13.21)] for all three cases of distin-\nguishable particles, identical bosons, and identical fermions. The two-particle probability density for \nthe ground state is thus\n \n P1x1, x22 = 0  c1x1, x22 0\n2\n  \n \n = 0 w11x12w11x22 0\n2\n \n \n = 4\nL2 sin2 apx1\nL b sin2 apx2\nL b , \n(13.30)\nas shown in Fig. 13.2. The system probability density is two-dimensional because there are two \nparticles, each with a one-dimensional probability density.\ndistinguishable particles\nbosons\nfermions\nE11\u00042E1\n1\n2\n1\n2\n2\n1\n(a)\n(b)\n(c)\nE12\u0004E1\u000fE2\n\u00101(x1)\u00101(x2)\nΨs11(x1,x2)\u000200\u0003\u0007\nΨs11(x1,x2)\u000200\u0003\nΨA12(x1,x2)\u000211\u0003\nΨA12(x1,x2)\u00021,\n1\u0003\nΨA12(x1,x2)\u000210\u0003\nΨS12(x1,x2)\u000200\u0003\nΨs12(x1,x2)\u000200\u0003\n\u00101(x1)\u00102(x2)\n\u00102(x1)\u00101(x2)\n FIGURE 13.1 Schematic diagrams of ground and ﬁrst excited states of two \nparticles in a one-dimensional well for (a) distinguishable particles, (b) identical \nspin-0 bosons,and (c) identical spin-1/2 fermions.\n\n418 \nIdentical Particles\nFor the ﬁrst excited state of the system with one particle in the single-particle ground state and \none particle in the ﬁrst single-particle excited state, the wave function does depend on the type of par-\nticle, as shown in Fig. 13.1. For the distinguishable particle case, there are two possible states, shown \nin Fig. 13.3(a) and (b). For the spin-0 boson case, there is only one possible state, which is the symmetric \nwave function c\n S\n12 1x1, x22 given in Eq. (13.26) with the probability density shown in Fig. 13.3(c). \nFor the case of two spin-1/2 fermions, the excited state can either be in the symmetric [Fig. 13.3(c)] or \nantisymmetric [Fig. 13.3(d)] spatial wave function, depending on the spin state as given in Eqs. (13.27) \nand (13.28), respectively. For bosons and fermions, the probability density is symmetric with respect \nto particle exchange, which is evident in the symmetry about the diagonal line x1 = x2 in Figs. 13.3(c) \nand (d). The wave function c\n A\n12 1x1, x22 that underlies the probability density in Fig. 13.3(d), is antisy-\nmetric about the line x1 = x2 , but its square—the probability density—is symmetric. The probability \ndensity of the asymmetric spatial state c\n A\n12 1x1, x22 is identically zero along the line x1 = x2:\n \nc\n A\n12 1x1, x12 =\n1\n12 3w1 1x12w2 1x12 - w1 1x12w2 1x124 = 0, \n(13.31)\nillustrating that two fermions in a symmetric spin state cannot be in the same location. This is the \nPauli exclusion principle that two electrons with the same spin orientation 1 0 119 = 0  + +9 or \n0 1, -19 = 0  - -9 statesB cannot be in the same spatial state. The symmetrization postulate tells us that \nthis also applies to two electrons with opposite spin but combined in a symmetric manner 1 0 109 stateB. \nThe spatial probability density shown in Fig. 13.3(d) illustrates the idea that fermions in a symmetric \nspin state appear to “repel” each other. In contrast, two fermions with opposite spins combined in \nan antisymmetric manner 1 0 009 stateB to make a spin-singlet state have a symmetric spatial wave \nfunction and the probability density shown in Fig. 13.3(c), the same as two bosons. In this case, the \nprobability density is peaked along the line x1 = x2 , illustrating that two bosons or two spin-singlet \nfermions have a tendency to “attract” each other.\nFIGURE 13.2 Two-particle spatial probability density for the ground state of a system of two particles \nin an inﬁnite square well, displayed using height (left) or grayscale with contours (right). This probability \ndensity is the same for distinguishable particles, identical bosons, and identical fermions.\n\n13.2 Two Identical Particles in One Dimension \n419\nFIGURE 13.3 Two particle probability \ndensities for the ﬁrst excited state of a sys-\ntem of two particles in an inﬁnite square \nwell. (a, b) Distinguishable particles, \n(c) Symmetric spatial state for spin-0 \nbosons or spin-singlet fermions, (d) Anti-\nsymmetric spatial state for spin-triplet \nfermions.\n\n420 \nIdentical Particles\n13.2.4 \u0002 Exchange Interaction\nThis apparent spatial attraction between bosons and spin-singlet fermions and repulsion between spin-\ntriplet fermions does not reﬂect any potential energy of interaction between the two particles—we \nhave assumed that the particles do not interact in this simple model. Rather, this apparent interaction \nis a consequence of the symmetrization requirement imposed on the wave functions. This effect is \ncalled the exchange force or the exchange interaction. One way to quantify the exchange interaction \nand demonstrate the difference between particles in symmetric and antisymmetric spatial states is to \ncalculate the expectation value of the square of the separation between the two particles\n \n H1x1 - x22\n2 I = H  x2\n1 - 2x1x2 + x2\n2\n I\n \n \n = H  x2\n1\n I + H  x2\n2\n I - 2 H  x1x2\n I. \n(13.32)\nWe’ll leave the bulk of this calculation to you, but let’s demonstrate how to calculate one of these two-\nparticle expectation values.\nConsider the expectation value 8x2\n19 in the fermionic state @ c\n AS\n12 9 = @ c\n A\n129@ 1M9, where \n@ c\n A\n129 \u0003 c\n A\n121x1, x22 is the spatial part. Using Dirac bra-ket notation to begin, we have\n \n H x2\n1I = H c\n AS\n12\n @ x2\n1@ c\n AS\n12  I\n \n \n = H c\n A\n12 @ H1M @ x2\n1@ c\n A\n12 I @1M I. \n(13.33)\nWe separate the space and spin parts of the matrix element and recall that the position x1 does not act \non the spin states, so\n \nHx 2\n1I = H c\n A\n12 @ x2\n1@ c\n A\n12 I H1M @1MI. \n(13.34)\nThe spin state projection is unity: 81M0 1M 9 = 1. Let’s keep the spatial matrix element in Dirac nota-\ntion by using the notation 0 n91 \u0003 wn1x12, such that 0 191 \u0003 w11x12 and 0 192 \u0003 w11x22. The Dirac ket \nrepresentation of the two-particle spatial state [Eq. (13.28)] in terms of the single-particle spatial states \n3@  c\n A\n129 =\n1\n12 1@ 191@ 292 - @ 291@ 19224 yields\n \n Hx2\n1I =\n1\n12  A1H1@2H2@  -2 H1@1H2@ B Ax2\n1B 1\n12  A @1I1@2I2 - @1I2@2I1B\n \n \n = 1\n2  EA1H1@ x2\n1@1I1B\n \nA2H2@2I2B - A1H1@ x2\n1@2I1B\n \nA2H2@1I2B\n \n \n        - A1H2@ x2\n1@1I1B\n \nA2H1@2I2B + A1H2@ x2\n1@2I1B\n \nA2H1@1I2BF, \n(13.35)\nwhere we have isolated the separate matrix elements and projections for particles 1 and particle 2. \nInvoking the orthonormality of the single-particle eigenstates yields\n \n Hx2\n1I = 1\n2 A1H1@x2\n1@1I1 + 1H2@x2\n1@2I1B. \n(13.36)\nThus we are left with calculating two single-particle expectation values. The subscript label indicating \nthe particle number is irrelevant for that calculation, leaving\n \n Hx2\n1I = 1\n2  AH1@ x2@1I + H2@ x2@\n 2IB. \n(13.37)\n\n13.2 Two Identical Particles in One Dimension \n421\nTo calculate the single-particle expectation values for the particle in the inﬁnite square well, we must \nuse an integral in the position representation:\n \n 8n@ x2@\n n9 =\nL\nL\n0\nw*\nn1x2 x2 wn1x2dx . \n(13.38)\nFollowing this example, you can calculate the expectation value of the interparticle spacing in \nEq. (13.32). The difference between the results for different spatial states resides in the cross-term \n8x1x29. The ﬁnal result for the state with one particle in the n = 1 state and one particle in the n = 2 \nstate of the inﬁnite square well is (Problem 13.6)\n \n 481x1 - x2229S = 0.20L \n \n 481x1 - x2229D = 0.32L \n \n 481x1 - x2229A = 0.41L \n(13.39)\nfor the three cases of distinguishable particles (D), identical particles in symmetric spatial states (S) \n(bosons or spin-singlet fermions), and identical particles in antisymmetric spatial states (A) (spin-\ntriplet fermions). These results indicate that particles in symmetric spatial states (typically bosons) are \ncloser to each other, and particles in antisymmetric spatial states (typically fermions) are farther apart \nfrom each other, compared to the distinguishable particle case.\nThe relation between the symmetry/antisymmetry of the spatial wave function and the interpar-\nticle spacing is also evident if we measure the particle separation probability density P1x1 - x22. This \none-dimensional probability density is measured by recording the positions of each particle and ﬁnd-\ning the interparticle separation 1x1 - x22. To calculate this one-dimensional probability density, we \nintegrate the two-particle probability density P1x1, x22 parallel to the x1 = x2 line (i.e., project the \ntwo-particle probability densities of Fig. 13.3 onto the diagonal line x2 = L - x1). The result of this \ncalculation for the state with one particle in the n = 1 state and one particle in the n = 2 state of the \ninﬁnite square well is shown in Fig. 13.4 (Problem 13.8). The distribution for symmetric spatial states \nis peaked at the origin, indicating that these identical particles are more likely than distinguishable \nparticles to be found close to each other. The distribution for antisymmetric spatial states is zero at the \norigin 1i.e., x1 = x22, indicating that the two identical particles cannot be found at the same location.\n13.2.5 \u0002 Consequences of the Symmetrization Postulate\nWe have seen the effect of the symmetrization postulate on a two-particle system (an effective interac-\ntion that leads to changes in the interparticle spacing). The consequences of the symmetrization pos-\ntulate for a many-particle system are much more radical, and are much different for systems of bosons \nand fermions. For example, in a three-particle system, the ground states are different because only two \nspin-1/2 fermions can be in the single-particle ground state, as illustrated in Figs. 13.5(a) and (b). For \nsystems of N particles, the boson ground state has all N particles in the single-particle ground state and \nsystem energy NE1 [Fig. 13.5(c)], while the spin-1/2 fermion ground state has energy levels occupied \nup to the N/2 single-particle state and system energy W NE1 [Fig. 13.5(d)]. The proper study of these \ntypes of systems requires statistical mechanics and thermodynamics. The states depicted in Fig. 13.5 \nrequire temperatures near absolute zero so that the thermal energy is much less than the energy spacings.\nFor a system of bosons, if the requisite low temperature is reached and the density of the particles \nis high enough, then the ground state of the system exhibits a wealth of interesting quantum mechanical \n\n422 \nIdentical Particles\neffects. When the inter-particle spacing is comparable to the de Broglie wavelength of the particles, \nthen the system of bosons begins to behave as a single macroscopic quantum object. As the critical \nvalue of low temperature and high density is reached, the quantum mechanical attraction of the bosons \narising from the symmetrization postulate takes over and the system “collapses” into the ground \nstate. This dramatic event is a phase transition in the state of the matter and is called Bose-Einstein\ncondensation. Liquid helium exhibits this phase transition at 2.18 K. The speciﬁc heat and the ther-\nmal conductivity increase discontinuously to signal the onset of the Bose-Einstein condensation. \nThe viscosity of liquid helium drops dramatically and the system behaves as a superﬂuid, easily ﬂow-\ning through small capillaries and even up and out of its container. Liquid helium is a strongly interact-\n(a)\n(b)\n(c)\n(d)\nbosons\nfermions\nbosons\nfermions\nFIGURE 13.4 Probability density of interparticle separation for the ﬁrst excited state \n(na = 1, nb = 2) of a system of two particles in the inﬁnite square well for the cases of \ndistinguishable particles (dashed line), identical particles in symmetric spatial states  \n(peaked at zero) and identical particles in antisymmetric spatial states (minimum at zero).\nFIGURE 13.5 Ground states of multiple particle systems in a one-dimensional potential \nfor a three-particle system of (a) bosons or (b) spin-1/2 fermions, and a six-particle system \nof (c) bosons or (d) spin-1/2 fermions.\n\nL\n0\nL\nP(x1\nx2)\nx1\nx2\nsymmetric\nantisymmetric\ndistinguishable\n\n13.3 Interacting Particles \n423\ning system, so the theory of its low temperature quantum behavior is quite complicated. Dilute atomic \ngases provide a better testing ground for the study of the basic quantum mechanics of Bose-Einstein \ncondensation. Recent experiments have cooled atoms to temperatures below 1 μK and achieved Bose-\nEinstein condensation. The atoms are close enough to have overlapping de Broglie wavelengths, but \nfar enough apart that the atomic interactions are small. Moreover, the strength of the interactions can \nbe adjusted through magnetic ﬁeld changes, and the quantum effects can be studied as a function of the \nstrength of the interaction. This new ﬁeld has spawned a wealth of interesting effects and garnered the \nNobel Prize in Physics in 2001.\nFor fermions, the behavior of a multiparticle system is dominated by the particles near the high-\nest occupied state. The fermions at the low energy levels are “buried” in the sea of fermions and have \nnowhere to go, because the Pauli exclusion principle forbids them from making transitions to states \nthat are already occupied. Only particles near the top of the distribution see nearby unoccupied lev-\nels to which they might make transitions. For example, in atoms, the electrons near the top are the \nvalence electrons that determine the spectroscopy and chemistry of the atom. For electrons in solids, \nthe energy at the top of the distribution of fermions is called the Fermi energy and plays a vital role in \nthe behavior of the solid.\n13.3 \u0002 INTERACTING PARTICLES\nThe apparent spatial “attraction” or “repulsion” of identical particles evident in Eq. (13.39) and \nFig. 13.4 has a profound effect when we consider a real interaction between the two particles. The \ndifferent spatial correlations of the particles lead to different energy shifts for the different spatial \nsymmetry states. Consider two particles in a one-dimensional potential energy well, with an interac-\ntion potential energy between the two particles. We assume that this new term is small enough that we \ncan treat it with perturbation theory. Assume that this interaction potential energy depends only on the \nparticle separation:\n \n H\u0004 = Vint 1x1 - x22. \n(13.40)\nWe use perturbation theory to ﬁnd the ﬁrst-order energy corrections\n \nE(1) = 8c(0)@  H\u0004@ c(0)9 \n(13.41)\nusing the states from the last section as the zeroth-order states. We assume that the interaction is spin \nindependent, so the spin does not affect this perturbation calculation. That is, in the matrix element\n \n 8  c(0)@  H\u0004@\n c(0)9 = 8cspatial\n @8  cspin\n @  H\u0004@\n cspatial\n 9@  cspin\n 9 \n \n = 8  cspatial\n @  H\u0004@ cspatial\n 9 8  cspin\n @\n cspin\n 9 \n \n = 8  cspatial\n @  H\u0004@\n cspatial\n 9,\n \n(13.42)\nthe spatial and spin states separate and only the spatial states enter into the perturbation calculation. The \nonly role of the spin is to determine the allowed spatial states through the symmetrization postulate.\nFor a system of two identical spin-0 bosons, the zeroth-order ground state is\n \n@ c\n SS\n11 9 \u0003 c\n S\n111x1, x22@  009 = w11x12w11x22@  009, \n(13.43)\n\n424 \nIdentical Particles\nso the ﬁrst-order perturbation is\n \n E (1)\n11 = H  c\n S\n11@  H\u0004@  c\n S\n11I H00 @  00I\n \n = H c\n S\n11@Vint 1x1 - x22@ c\n S\n11\n I\n \n =\nL\n\u0005\n- \u0005 L\n\u0005\n- \u0005\nw*\n11x12w*\n11x22Vint 1x1 - x22w11x12w11x22dx1 dx2\n \n =\nL\n\u0005\n- \u0005 L\n\u0005\n- \u0005\n0\n w11x12 0\n2 Vint 1x1 - x22 0 w11x22 0\n2dx1 dx2.\n \n(13.44)\nIt is convenient to deﬁne the general form of this matrix element as the direct integral\n \n Jnm =\nL\n\u0005\n- \u0005 L\n\u0005\n- \u0005\n0\n wn1x12 0\n2 Vint 1x1 - x22 0 wm1x22 0\n2dx1 dx2. \n(13.45)\nThe direct integral is the interaction energy between the two probability densities Pn1x12 = 0\n wn1x12 0\n2 \nand Pm1x22 = 0\n wm1x22 0\n2 that represent the two particles. With this deﬁnition, the perturbed ground-\nstate energy for a system of two spin-0 bosons is\n \n E 11 = 2E (0)\n1\n+ J11. \n(13.46)\nFor a system of two identical spin-1/2 fermions, the zeroth-order ground state is\n \n@ c\n SA\n11 I \u0003 c\n S\n111x1, x22@  00I = w11x12w11x22@  00I \n(13.47)\nand the ﬁrst-order perturbation is\n \n E (1)\n11 = H c\n S\n11@ H\u0004@ c\n S\n11\n I H00 @00I\n \n \n = H c\n S\n11@Vint 1x1 - x22@ c\n S\n11\n I \n \n = J11 ,\n \n(13.48)\nwhich is the same as the boson case. The interaction is spin independent, and the ground-state spatial \nwave function is the same for bosons and fermions.\nFor the ﬁrst excited state of the two-particle system, the identical spin-0 bosons must have a sym-\nmetric wave function [Eq. (13.26)], so the state vector is\n \n@\n c\n SS\n129 \u0003 c\n S\n12 1x1, x2@\n 002 =\n1\n12 3w1 1x12w2 1x22 + w11x22w2 1x124@\n 009 \n(13.49)\nand the ﬁrst-order perturbation is\n \n E(1)\n12 = Hc\n S\n12@  H\u0004@  c\n S\n12 I H  00@00I\n \n \n = Hc\n S\n12@Vint 1x1 - x22@c\n S\n12\n I \n \n = 1\n2 L\n\u0005\n- \u0005 L\n\u0005\n- \u0005\n3w*\n11x12w*\n21x22 + w*\n11x22w*\n21x124Vint 1x1 -  x22 \n(13.50)\n \n3w11x12w21x22 + w11x22w21x124dx1\n  dx2. \n\n13.3 Interacting Particles \n425\nThis gives four terms, but they are equal in pairs if we swap the integration dummy variables x1 and x2,\nyielding (Problem 13.10)\n \n E (1)\n12 =\nL\n\u0005\n- \u0005L\n\u0005\n- \u0005\n0 w11x12 0\n2Vint 1x1 - x22 0 w21x22 0\n2dx1 dx2\n \n +\nL\n\u0005\n- \u0005L\n\u0005\n- \u0005\nw*\n11x12w*\n21x22Vint 1x1 - x22w11x22w21x12dx1 dx2. \n(13.51)\nThe ﬁrst term in Eq. (13.51) is the direct integral J12 deﬁned in Eq. (13.45). The second term is a new \nterm, which we call the exchange integral and deﬁne, in general, as\n \nKnm =\nL\n\u0005\n- \u0005L\n\u0005\n- \u0005\nw*\nn1x12w*\nm1x22Vint 1x1 - x22wn1x22wm1x12dx1 dx2. \n(13.52)\nWith this deﬁnition, the energy of the ﬁrst excited state of the system of two identical spin-0 bosons is\n \nE12 = E (0)\n1\n+ E (0)\n2\n+ J12 + K12. \n(13.53)\nThe exchange integral has no classical explanation. It is a manifestation of the symmetrization require-\nment. It is not caused by spin, but it is intimately related to spin because of the role of spin in the sym-\nmetrization postulate.\nFor two identical spin-1/2 fermions, the excited state spatial wave function can be either symmet-\nric or antisymmetric depending on the spin state. For the antisymmetric singlet spin state, the spatial \nwave function must be symmetric\n \n@\n c\n SA\n12 9 \u0003 c\n S\n12 1x1, x22@\n 009 =\n1\n12 3w1 1x12w2 1x22 + w11x22w2 1x124@\n 009. \n(13.54)\nThe ﬁrst-order energy shift is\n \n E (1)\n12 = H c\n S\n12\n @  H\u0004@  c\n S\n12\n I  H00@00I\n \n \n = H c\n S\n12\n @Vint Ax1 - x2B @ c\n S\n12 I \n \n = J12 + K12.\n \n(13.55)\nThis is the same shift as the boson excited state because the spatial wave function is the same and the \nspin does not affect the expectation value.\nFor the symmetric triplet spin state, the spatial wave function is antisymmetric\n \n@\n c\n AS\n12 9 \u0003 c\n A\n12 1x1, x22@1M 9 =\n1\n12 3w1 1x12w2 1x22 - w21x12w1 1x224@1M 9. \n(13.56)\nThe resultant ﬁrst-order energy correction\n \n E(1)\n12 = Hc\n A\n12\n @  H\u0004@c\n A\n12\n I H1M @1M I  \n \n = Hc\n A\n12\n @Vint 1x1 - x22@\n c\n A\n12 I \n \n = J12 - K12\n \n(13.57)\n\n426 \nIdentical Particles\nhas a negative exchange integral contribution because of the minus sign in the asymmetric spatial \nwave function (Problem 13.12). We combine the results in Eqs. (13.55) and (13.57) to express the \nenergy of the ﬁrst excited state of the two spin-1/2 fermion system as\n \n E12 = E (0)\n1\n+ E (0)\n2\n+ J12 { K12, \n(13.58)\nwhere the +1-2 sign refers to the symmetric (antisymmetric) spatial state and the respective singlet \n(triplet) state.\nThe energies of the ground and excited states are shown in Fig. 13.6, where we assume that \nthe direct and exchange integrals J and K are positive, which is typical for the Coulomb interaction \nbetween identical charged particles. The direct integral raises all energy states because of the posi-\ntive repulsive interaction expected classically for charged particles of the same sign. The exchange \nintegral reﬂects the additional repulsive interaction caused by the spatial correlation or anticorrelation \nof the particles arising from the symmetrization postulate. For bosons and spin-singlet fermions, the \nspatial “attraction” that arises from the symmetrization postulate [Fig. 13.3(c)] increases the positive \nrepulsive interaction energy because they are closer together in space. For spin-triplet fermions, the \nspatial “repulsion” that arises from the symmetrization postulate [Fig. 13.3(d)] decreases the interac-\ntion energy. The degeneracy of the excited state in the fermion case is partially lifted by the exchange \n(a)  bosons\n(b)  fermions\nJ12\nE1 E2\nE1 E2\n2E1\n2E1\nJ12\nJ11\nJ11\nK12\n2K12\nΨS12 00\nΨS11 00\nΨS11 00\nΨS12 00\nΨA12 1M\nFIGURE 13.6 Energies and state vectors for the ground and ﬁrst excited states of two identical \n(a) spin-0 bosons or (b) spin-1/2 fermions in a one-dimensional potential energy well.\n\n13.4 Example: The Helium Atom \n427\nintegral term. The resultant energies depend on the spin of the system even though spin is not part of \nthe interaction Hamiltonian. The spin plays its role by determining which spatial states are allowed.\n13.4 \u0002 EXAMPLE: THE HELIUM ATOM\nThe symmetrization postulate and the resultant Pauli exclusion principle are key elements in under-\nstanding atomic structure and the periodic table. The ramiﬁcations of the symmetrization postulate are \nﬁrst evident in the case of the helium atom with two electrons. The helium Hamiltonian is similar to \nhydrogen but with added potential energy terms due to the second electron interacting with the doubly \ncharged nucleus and the two electrons interacting with each other. The helium Hamiltonian is\n \n  H = ¢\np2\n1\n2m\n -\n 2e2\n4pe0\n r1\n≤+ ¢\np2\n2\n2m\n -\n 2e2\n4pe0\n r2\n≤+\ne2\n4pe0\n r12\n , \n(13.59)\nwhere r12 is the separation of the two electrons, as shown in Fig. 13.7. The Coulomb repulsion term \nbetween the two electrons is clearly of the same order of magnitude as the Coulomb terms represent-\ning the interaction of each electron with the nucleus, but we treat it as a perturbation so that we can \nwrite the Hamiltonian as a zeroth-order term whose solutions we know, plus a perturbation:\n \n  H = H0 + H\u0004  \n \n  H\u0004 =\ne2\n4pe0\n r12\n. \n(13.60)\nThe zeroth-order Hamiltonian is the sum of two hydrogen atom Hamiltonians, each with a nuclear \ncharge Z = 2. The eigenstates and eigenenergies of a hydrogenic atom with nuclear charge Z are \nobtained from the hydrogen atom solutions from Chapter 8 by the substitution e2 S Ze2, which scales \nthe energies by a factor Z\n 2 and the size of the radial wave function by 1>Z. For example, the ground-\nstate wave function of a hydrogenic atom with nucleus +Ze is\n \n  c100 1r, u, f2 =\nC\nZ  3\npa3\n0\n  e -Zr>a0 , \n(13.61)\nr2\nr1\nr12\nHe\u0002\u0002\n\u0003e\n\u0003e\nFIGURE 13.7 Helium atom coordinates.\n\n428 \nIdentical Particles\nwhere a0 = 4pe0\n  U2>me2 is the Bohr radius. The zeroth-order energy of the helium atom with one \nelectron in state na and one electron in state n b is the sum of the hydrogenic energies:\n \n E (0)\nnanb = -Z\n 2 Ryd ¢  1\nn2\na\n+ 1\nn2\nb\n ≤ \n \n = -4 Ryd ¢  1\nn2\na\n+ 1\nn2\nb\n ≤ . \n(13.62)\nThough we don’t expect this perturbation approach to yield very precise results, it is a useful ﬁrst \nattempt and illustrates many of the important new aspects that arise from the symmetrization postulate.\nAs we did in the previous section, we separate the spin and spatial aspects of the state vectors \nbecause spin and position are not coupled. The complete eigenstates have the form\n \n@ c9 = @ cspatial9@ cspin9 \n(13.63)\nand must be antisymmetric under exchange of the two fermions. Thus the spatial part must be symmetric \nif the spin part is antisymmetric, or the spatial part must be antisymmetric if the spin part is symmetric.\nThe spatial part of the state vector represents the state with one particle in the hydrogenic state \nna /a ma and one particle in the state nb /b  mb . The properly symmetrized spatial wave functions are\n \nc S\nna\n /a\n  ma\n , nb\n /b\n  mb 1r1 , r22 =\n1\n12 3cna\n /a\n  ma 1r12 cnb\n /b\n  mb 1r22 + cna\n /a\n  ma 1r22 cnb\n /b\n  mb 1r124 \n \nc A\nna\n /a\n  ma\n , nb\n /b\n  mb 1r1 , r22 =\n1\n12 3cna\n /a\n  ma 1r12 cnb\n /b\n  mb 1r22 - cna\n /a\n  ma 1r22 cnb\n /b\n  mb 1r124. \n(13.64)\nThe spin part of the state vector is obtained by properly symmetrizing the spin states of two spin-1/2 \n fermions, which yields the eigenstates 0 SM9 of the total spin with S = 0 or 1. The four states are\n \n 0 119 = 0  + +9 \n \n 0 109 =\n1\n12 30  + -9 + 0  - +94   t Triplet state \n \n 0 1, -19 = 0  - -9 \n \n 0\n 009 =\n1\n12 30  + -9 - 0  - +94   6 Singlet state. \n(13.65)\nThe complete antisymmetric quantum state vector of the helium atom is obtained by combining \nthe antisymmetric singlet state with symmetric spatial wave function or by combining the symmetric \ntriplet state with antisymmetric spatial wave function. Thus, the only possible states are:\n \n @  c SA\nna\n /a\n ma, nb\n /b\n mb\n I = @  c S\nna\n /a\n ma, nb\n /b\n mb\n I  @ 00I  \n \n @  c AS\nna\n /a\n ma, nb\n /b\n mb\n I = @  c A\nna\n /a\n ma, nb\n /b\n mb\n I  @1M I . \n(13.66)\nThe other combinations are not possible states for this system.\n13.4.1 \u0002  Helium Ground State\nThe ground state of helium has both electrons in hydrogenic ground states, so the antisymmetric spa-\ntial state is identically zero [Eq. (13.22)] and only the symmetric spatial state is allowed. To ensure \n\n13.4 Example: The Helium Atom \n429\n0eV\n\u000310eV\n\u000320eV\n\u000330eV\n\u000340eV\n\u000350eV\n\u000360eV\n\u000370eV\n\u000380eV\n\u000390eV\n\u0003100eV\n\u0003110eV\n0eV\n\u000310eV\n\u000320eV\n\u000330eV\n\u000340eV\n\u000350eV\n(na, nb)\n(4, 4)\n(1, 4)\n(1, 3)\n(1, 2)\n(1, 1)\n(1, \u0004)\n(3, 4)\n(3, 3)\n(2, 4)\n(2, 3)\n(2, 2)\n(\u0004, \u0004)\nthat the total state vector is antisymmetric, Eq. (13.66) tells us that the spin part of the ground state \nmust be the antisymmetric singlet state 0 009. The triplet state 0 1M9 is not permitted in the helium \nground state. Thus the ground state of helium is\n \n @  cground\n I = @  c\n SA\n1s,1s I = @ c\n S\n1s,1s I@  00I, \n(13.67)\nwith a zeroth-order energy determined by the sum of two hydrogenic ground-state energies:\n \n E (0)\n1s,1s = -4 Ryd a 1\n12 + 1\n12b = -8 Ryd = -108.8 eV. \n(13.68)\nThe zeroth-order helium energy states are shown in Fig. 13.8, obtained using Eq. (13.62).\nAn aside about energy levels is in order here. In hydrogen, the ground-state energy is -13.6 eV,\nwhere zero energy corresponds to the electron and proton inﬁnitely far apart and at rest. We refer to \nthis zero energy level as the ionization level. For the calculation we have just done for the helium \nground state, the zero of energy corresponds to both electrons removed to inﬁnity, and so is referred to \nas the double ionization level. However, it is more common in the literature to quote atomic energy \nFIGURE 13.8 Helium atom energies in zeroth order, where hydrogenic Bohr energies are \nassumed. The energy scale on the left is referenced to the double ionization level and the \nenergy scale on the right is referenced to the single ionization level, which is at -54.4 eV \nwith respect to the double ionization level.\n\n430 \nIdentical Particles\nlevels with respect to the single ionization level corresponding to one electron removed from the \natom. If we remove one electron from helium, we are left with a hydrogenic ion with an energy\n \n E (0)\n1s,\u0005 = -4 Ryd a 1\n12 -\n1\n\u00052b = -4 Ryd = -54.4 eV, \n(13.69)\nwhich is half of the energy in Eq. (13.68). To quote energies referenced to the single ionization level, \nwe must subtract this energy (as shown in Fig. 13.8), in which case we get a helium ground-state \nenergy of -54.4 eV.\nThe experimental value for the helium ground-state energy is -25 eV (referenced to the single \nionization level), which is quite different from our zeroth-order estimate. This is not unexpected, as we \nsaid above that the electron-electron interaction, which is neglected in zeroth-order, is the same order \nof magnitude as the electron-nucleus interactions that are responsible for the binding. Even though they \nare the same order, our approach is to treat the electron-electron repulsion as a perturbation and ﬁnd the \nperturbed energies of this system. The helium ground state is nondegenerate, so we ﬁnd the shift caused \nby the perturbation by ﬁnding the expectation value of the perturbation in the zeroth-order state:\n \n E (1)\n1s,1s = H  c\n SA\n1s,1s\n @  H\u0004@  c\n SA\n1s,1s\n I\n \n \n = H  c\n S\n1s,1s @\n \nH00 @ \ne2\n4pe0\n r12\n @  cS\n1s,1s I  @  00 I\n \n \n = H  cS\n1s,1s @ \ne2\n4pe0\n r12\n @  cS\n1s,1s  I  H00 @  00 I\n \n \n =\nO\nc*\n100 1r12  c*\n100 1r22 \ne2\n4pe00 r1 - r20\n c100 1r22c100 1r12d 3r1  d 3r2. \n(13.70)\nThis integral is the direct integral we deﬁned in the one-dimensional example in Eq. (13.45). In this \nthree-dimensional Coulomb interaction problem, we deﬁne the direct integral as\n \nJn/,n\u0004/\u0004 =\nO\n0 cn/m1r12 0 2 \ne2\n4pe00 r1 - r20 0 cn\u0004/\u0004m\u00041r22 0 2  d 3r1  d 3r2. \n(13.71)\nThese integrals are independent of m, but not /, which is why we drop the m subscript on the \nenergies. To calculate the direct integrals, it is useful to use the spherical harmonic addition theorem\n \n1\n0 r1 - r20 = a\n\u0005\n/=0\n a\n/\nm=-/\n \n4p\n2/ + 1 r/\n6\nr/+1\n7\n Y*\n/m1u1, f12Y/m 1u2, f22, \n(13.72)\nwhere r7 stands for the larger of the two distances r1 and r2, and r6 the smaller.\nThe ground-state direct integral in Eq. (13.70) can be done and the result is (Problem 13.14):\n \n E (1)\n1s,1s = 5\n8 \nZe2\n4pe0\n a0\n= 5\n4 \ne2\n4pe0\n a0\n= 10\n4  Ryd = 34 eV. \n(13.73)\nThe shift is positive because the electrons repel each other, yielding a positive Coulomb potential \nenergy. The new estimate of the ground-state energy (relative to the single ionization level) is\n \nE1s,1s \u0002 E (0)\n1s,1s + E (1)\n1s,1s = -54.4 eV + 34 eV = -20.4 eV, \n(13.74)\nwhich is now much closer to the experimental value of -25 eV. To make a better estimate, we would \nhave to account for the shielding of the nuclear charge by the presence of the second electron.\n\n13.4 Example: The Helium Atom \n431\n 13.4.2 \u0002 Helium Excited States\nNow let’s turn our attention to the excited states of helium. The zeroth-order energy level diagram in \nFig. 13.8 makes it clear that all states with both electrons excited have a zeroth-order energy above \nthe single ionization level E(0)\n1s,\u0005. For example, the doubly excited state na = 2, nb = 2 has an energy\n \nE (0)\n2,2 = -4 Ryd a 1\n22 + 1\n22b = -2 Ryd = -27.2 eV, \n(13.75)\nwhich is 27.2 eV above the single ionization level. Such doubly excited states are not stable. They decay \nto a lower energy state with one electron in the hydrogenic ground state and the second electron travel-\ning to inﬁnity with the excess energy. This decay is very likely and so the lifetime of the doubly excited \nstates is very short. The likelihood of this process leads to its name: auto-ionization. For this reason, it is \ncommon to limit the discussion of excited atomic states (in this helium example as well as other atomic \nsystems) to those where only one electron is excited and the others remain in the atomic ground state.\nBecause the excited electron is in a different spatial state than the remaining ground-state elec-\ntron, both the symmetric and antisymmetric spatial states are allowed. We also expect additional \ndegeneracy because the hydrogen excited states are degenerate with respect to the angular momentum \nquantum numbers / and m.\nThe ﬁrst excited state of helium has na = 1, nb = 2. The two possible states are:\n \n@  c\n SA\n1s, 2/I = @  c\n S\n1s, 2/I 0 009 \u0003\n1\n12 3 c100 1r12  c2/m 1r22 + c100 1r22  c2/m 1r124 0 009    \n \n@  c\n AS\n1s, 2/9 = @  c\n A\n1s, 2/I 0 1M9 \u0003\n1\n12 3 c100 1r12  c2/m 1r22 - c100 1r22  c2/m 1r124 0 1M 9. \n(13.76)\nIn both the symmetric and antisymmetric spatial cases, there are four possible states corresponding to \nthe single 2s 1/ = 0, m = 02 and the three 2p 1/ = 1, m = 0, {12 states. When we combine these \nstates with the single spin singlet state and the three spin triplet states, we ﬁnd that there are 16 possible \nstates overall. All these states are degenerate in the unperturbed system with Hamiltonian H0.\nThe unperturbed energy of these states is the hydrogenic energy shown in Fig. 13.8. We apply \ndegenerate perturbation theory to ﬁnd the effect of the electron-electron repulsion term H\u0004 on these\n16 degenerate states. The perturbation Hamiltonian is diagonal, so the energy corrections are the \ndiagonal elements\n \nE (1)\n1s, 2/ = Hc\n SA\n1s, 2/\n @  H\u0004@  c\n SA\n1s, 2/\n I \n(13.77)\nfor the symmetric spatial state, and\n \nE\n (1)\n1s, 2/ = Hc\n AS\n1s, 2/\n @  H\u0004@  c\n AS\n1s, 2/\n I \n(13.78)\nfor the antisymmetric spatial state. In both cases, the spin states are unaffected by the perturbation \n[see Eq. (13.70)] so we are left with a spatial integral:\n \nE 112\n1s,2/ =\nO\n1\n12\n 3c*\n100 1r12c*\n2/m 1r22 { c100 1r22c2/m 1r124\ne2\n4pe00 r1 - r20  \n \n 1\n12 3  c*\n100 1r12  c*\n2/m 1r22 { c100 1r22  c2/m 1r124d 3r1 d 3r2, \n(13.79)\n\n432 \nIdentical Particles\nwhere the { distinguishes the two states in Eq. (13.76). This gives four terms, but they are equal in pairs \nif we swap the integration dummies r1 and r2. Hence, we cancel the factor of 1/2 and get two terms:\n \n E (1)\n1s, 2/ =\nO\n0\n c100 1r12  0\n2\n \ne2\n4pe0 0 r1 - r20\n 0\n c 2/m1r22  0\n2 d 3r1 \n d 3r2 \n(13.80)\n \n {\nO\nc*\n100 1r12  c*\n2/m 1r22 \ne2\n4pe0\n 0 r1 - r20\n  c100 1r22  c2/m 1r12d 3r1 d 3r2. \nThe ﬁrst term is the direct integral and the second term is the exchange integral, which in general is\n \nKn/, n\u0004/\u0004 =\nO\nc *\nn/m1r12c *\nn\u0004/\u0004m\u0004 1r22 \ne2\n4pe0 0 r1 - r20\n c n/m 1r22  c n\u0004/\u0004m\u0004 1r12  d 3r1  d 3r2. \n(13.81)\nSo we write the energy perturbation as\n \n E (1)\n1s, 2/ = J1s, 2/ { K1s, 2/, \n(13.82)\nwhere the +1-2 sign refers to the symmetric (antisymmetric) spatial state and the respective singlet (trip-\nlet) state. The direct integral is also called the Coulomb interaction energy because it is the electrostatic \ninteraction potential energy of the two electrons: one in the 1s state and the other in the 2s or 2p state.\n1s2s\n1s2p\n2 1P\n2 3P\n2 1S\n2 3S\n1 1S\n2K1s,2p\n1s2s\n2K1s,2s\nJ1s,2s\nJ1s,1s\nJ1s,2p\n1s2p\n1s2\nFIGURE 13.9 Shifts and splittings of the helium ground state and ﬁrst excited state \ncaused by the direct and exchange interactions.\n\n13.4 Example: The Helium Atom \n433\n41S\n41P\n41D\n31S\n21S\n11S\n21P\n23S\n23P\n31P\n31D\n43S\n43P\n43D\n33S\n33P\n33D\nOrthohelium\nParahelium\nHe\nFIGURE 13.10 Helium energy spectrum.\nBoth the direct and exchange integrals in helium are positive, so the singlet states are higher in \nenergy than the corresponding triplet states, as shown in Fig. 13.9. The energy levels are labeled with \na modiﬁed spectroscopic notation n 2S+1L [see Eq. (11.84)], with n being the state of the excited elec-\ntron. The J label is suppressed because it has no bearing on the energy at this order of approximation. \nWe understand the singlet-triplet ordering of the energy levels by noting that in the singlet state, the \nspin state is antisymmetric and the spatial state is symmetric, implying that the two electrons get closer \nto each other and therefore increase the repulsive Coulomb potential energy. In the triplet state, the \nspin state is symmetric, the spatial state is antisymmetric, and the two electrons are farther apart, thus \nlowering the Coulomb potential energy.\nThis ordering of the energy levels, with the singlet state above the triplet state, is evident through-\nout the excited states of helium. Another important feature of the singlet and triplet states is that opti-\ncal transitions between these states are forbidden. The electromagnetic light ﬁeld does not couple to \nthe spin, so the selection rules for optical transitions require there to be no change in the spin quantum \nnumber between two states. Hence, transitions between the singlet and triplet states of helium are \nforbidden, and it was originally believed that there were two types of helium: parahelium 1S = 02 \nand orthohelium 1S = 12. Thus, energy diagrams of helium often show the singlet and triplet levels \nseparately, as in Fig. 13.10. We now know that transitions between parahelium and orthohelium do \noccur, with small probability, due to higher-order effects. Note that the 23S state of orthohelium is \nthe lowest state on the triplet side. Due to the spin selection rule, it is metastable against decay to the \nground state 11S. The lifetime of this metastable state is 8000 seconds, which is generally much longer \nthan the time it takes a helium atom to travel through an experimental system, so the state effectively \nhas an inﬁnite lifetime in laboratory experiments.\n\n434 \nIdentical Particles\n13.5  \u0002   THE PERIODIC TABLE\nThe helium atom illustrates the importance of the symmetrization postulate in determining the spec-\ntrum of energy levels of a multielectron atom. Let’s now qualitatively explain how the symmetrization \npostulate, in the guise of the Pauli exclusion principle, determines the structure of the periodic table \nas the atomic number Z increases. To a zeroth approximation, the states of multielectron atoms are \nthe hydrogenic states labeled with n, /, and m. However, we must also include spin, so there are four \nquantum numbers n, /, m/ and ms labeling each electron (the ﬁfth number s = 1/2 is the same for all \nelectrons so we suppress it). The zeroth-order hydrogen energy states En depend only on the quantum \nnumber n and are n2 degenerate with respect to / and m/. The additional ms degree of freedom doubles \nthe degeneracy, so that each hydrogenic energy level is 2n2 degenerate. We refer to each n energy \nlevel as a shell and to each n/ orbital as a subshell. Each subshell has 212/ + 12 possible states.\nIf electron were bosons, any number could occupy the hydrogenic ground state n = 1, similar \nto Fig. 13.5(c), and chemistry would be boring. But because electrons are fermions, only one electron \ncan occupy each state speciﬁed by the four quantum numbers n, /, m/, and ms. Hence, as the atomic \nnumber Z increases through the periodic table, we expect that each additional electron occupies the \nlowest available hydrogenic energy state, ﬁlling each subshell with 212/ + 12 electrons and each \nshell with 2n2 electrons, analogous to Fig. 13.5(d). The resulting electronic conﬁgurations are denoted \nby listing the subshells with the number of electrons in each subshell as a superscript, Ae.g., 1s2B. We \nthus expect the periodic table to reﬂect the pyramidal structure shown in Table 13.1. But that would \nmean that the seven rows of the periodic table would have 280 atoms, whereas we know there are just \nover 100 atoms. The periodic table does have a pyramidal structure, but not one that reﬂects the num-\nbers in Table 13.1. Why not?\nThe primary reason is that the nuclear charge is shielded by inner shell electrons in a way that lifts \nthe / degeneracy we expect from the simple hydrogen case, giving rise to energy levels speciﬁed by \nthe n and / quantum numbers. For a given n, higher values of / correspond to orbits farther from the \nnucleus because the increased angular momentum leads to a larger centrifugal barrier. Hence, the elec-\ntrons in high angular momentum orbitals are shielded from the nuclear attraction by the electrons in \nlower orbits and are bound less tightly. This screening effect explains why electrons ﬁll the hydrogenic \norbitals in the sequence 1s, 2s, 2p, 3s, etc. However, the screening effect is so large that it exceeds the \nhydrogenic n S n + 1 level separation in some cases, which disturbs the expected shell ﬁlling struc-\nture of Table 13.1. A schematic of the ordering of the energy levels of multielectron atoms is shown \nTable 13.1  Electronic Conﬁgurations in a Periodic Table Based Upon \nPurely Hydrogenic Energy Levels\nShell (n)\nSubshell Conﬁguration\nDegeneracy (2n2)\n1\n1s 2\n2\n2\n2s 2 2p6\n8\n3\n3s 2 3p6 3d 10\n18\n4\n4s 2 4p6 4d 10 4f  14\n32\n5\n5s 2 5p6 5d 10 5f\n  \n 14 5g 18\n50\n6\n6s 2 6p6 6d 10 6f  14 6g18 6h 22\n72\n7\n7s 2 7p6 7d\n 10 7f  14 7g18 7h 22 7i  26\n98\n\n13.5 The Periodic Table \n435\nin Fig. 13.11. The screening effect results in four major differences from the unshielded hydrogenic \nmodel: (1) the / degeneracy is lifted, (2) the nd levels are shifted up to lie above the 1n + 12s levels, \n(3) the nf levels are shifted up to lie above the 1n + 22s levels, and (4) the np levels are the high-\nest levels within their “group” of levels. Hence the energy ﬁlling proceeds in the manner shown in \nTable 13.2, with the number of atoms per row shown at right.\n1s\n2s\n2p\n3s\n3p\n4s\n3d\n4p\n5s\u0005\n4d\n5p\n6s\n4f\u0005\n5d\n6p\u0005\nEnergy\nFIGURE 13.11 Approximate ordering of the energies of subshells after \naccounting for the shielding of the nuclear charge. The energies are not to scale.\nTable 13.2 Electronic Conﬁgurations in the Periodic Table\nRow\nSubshell Conﬁguration\nNumber of Atoms\n1\n 1s 2\n2\n2\n 2s 2 \n \n \n2p6\n8\n3\n 3s 2 \n \n \n3p6\n8\n4\n 4s 2 \n \n3d  10 \n4p6\n18\n5\n 5s 2 \n \n4d  10 \n5p6\n18\n6\n 6s  2 \n4 f   14 \n5d  10 \n6p6\n32\n7\n 7s  2 \n5 f   14 \n6d  10 \n7p6\n32\n\n436 \nIdentical Particles\nH\n1\nLi\n3\nNa\n11\nK\n19\nRb\n37\nCs\n55\nFr\n87\nBe\n4\nMg\n12\nCa\n20\nSr\n38\nBa\n56\nRa\n88\nB\n2p\n3p\n4p\n5p\n6p\n7p\n3d\n4d\n5d\n6d\n7s\n6s\n5s\n4s\n3s\n2s\n1s\n5\nAl\n13\nGa\n31\nIn\n49\nTl\n81\nC\n6\nSi\n14\nGe\n32\nSn\n50\nPb\n82\nN\n7\nP\n15\nAs\n33\nSb\n51\nBi\n83\nO\n8\nS\n16\nSe\n34\nTe\n52\nPo\n84\nF\n9\nCl\n17\nBr\n35\nI\n53\nAt\n85\nNe\n10\nHe\n2\nAr\n18\nKr\n36\nXe\n54\nRn\n86\nUut\n113\nUuq\n114\nUup\n115\nUuh\n116\nUus\n117\nUuo\n118\nSc\n21\nY\n39\nLu\n71\nLr\n103\nTi\n22\nZr\n40\nHf\n72\nRf\n104\nV\n23\nNb\n41\nTa\n73\nDb\n105\nCr\n24\nMo\n42\nW\n74\nSg\n106\nMn\n25\nTc\n43\nRe\n75\nBh\n107\nFe\n26\nRu\n44\nOs\n76\nHs\n108\nMt\n109\nDs\n110\nRg\n111\nCn\n112\nCo\n27\nRh\n45\nIr\n77\nNi\n28\nPd\n46\nPt\n78\nCu\n29\nAg\n47\nAu\n79\nZn\n30\nCd\n48\nHg\n80\n4f\n5f\nLa\n57\nAc\n89\nCe\n58\nTh\n90\nPr\n59\nPa\n91\nNd\n60\nU\n92\nPm\n61\nNp\n93\nSm\n62\nPu\n94\nEu\n63\nAm\n95\nGd\n64\nCm\n96\nTb\n65\nBk\n97\nDy\n66\nCf\n98\nHo\n67\nEs\n99\nEr\n68\nFm\n100\nTm\n69\nMd\n101\nYb\n70\nNo\n102\nAlkalis\nNoble gases\nHalogens\nAlkaline earths\nTransition metals\nLanthanides (rare earths)\nActinides\nFIGURE 13.12 Periodic table of the elements.\nTable 13.3 Electronic Conﬁgurations of Some Elements\n1\nH\n1s 2\n25\nMn\n[Ar] 4s 2 3d 5\n2\nHe\n1s 2\n28\nNi\n[Ar] 4s 2 3d 8\n3\nLi\n[He] 2s1\n29\nCu\n[Ar] 4s1 3d 10\n4\nBe\n[He] 2s 2\n30\nZn\n[Ar] 4s 2 3d 10\n5\nB\n[He] 2s 2 2p1\n36\nKr\n[Ar] 4s 2 3d 10 4p6\n6\nC\n[He] 2s 2 2p 2\n37\nR b\n[Kr] 5s1\n7\nN\n[He] 2s 2 2p 3\n46\nPd\n[Kr] 4d 10\n8\nO\n[He] 2s 2 2p 4\n54\nXe\n[Kr] 5s 2 4d 10 5p6\n9\nF\n[He] 2s 2 2p 5\n55\nCs\n[Xe] 6s1\n10\nNe\n[He] 2s 2 2p6\n57\nLa\n[Xe] 6s 2 5d 1\n11\nNa\n[Ne] 3s1\n58\nCe\n[Xe] 6s 2 4f 1 5d 1\n18\nAr\n[Ne] 3s 2 3p6\n59\nPr\n[Xe] 6s 2 4f  3\n19\nK\n[Ar] 4s1\n86\nRn\n[Xe] 6s 2 4f 14 5d 10 6p6\n21\nSc\n[Ar] 4s 2 3d 1\n87\nFr\n[Rn] 7s1\n23\nV\n[Ar] 4s 2 3d 3\n92\nU\n[Rn] 7s 2 5f 3 6d 1\n24\nCr\n[Ar] 4s1 3d 5\n94\nPt\n[Rn] 7s 2 5f  6\nThe full periodic table is shown in Fig. 13.12 and reﬂects the pyramidal structure of Table 13.2 \nrather than Table 13.1. Electrons ﬁll up the 1s subshell in the ﬁrst row and the 2s and 2p subshells \nin the second row, as shown in Table 13.3. So far, this follows the purely hydrogenic case. But the \n\n13.6 Example: The Hydrogen Molecule \n437\nscreening effect pushes the 3d energy level up near the 4s energy level, so the third row has only \nthe 3s and 3p subshells. The 3d states are not ﬁlled until the fourth row of the periodic table. The 4s \nstates are ﬁlled ﬁrst for potassium and calcium, then the 3d states are ﬁlled for scandium through \nzinc, and ﬁnally the 4p states are ﬁlled for gallium through krypton. The 4s and 3d levels are so \nclose that there are some anomalies in the transition metals in the fourth row, as indicated in Table \n13.3. Chromium and copper each have only one 4s electron and one more 3d electron than you \nmight expect. The ﬁfth row ﬁlls in the order 5s, 4d, and 5p, analogous to the fourth row because \nthe f subshells are pushed up two groups. The ﬁfth row transition metals also exhibit anomalies in \nthe 5s and 4d ordering, with palladium being the most extreme in having no 5s electrons. The sixth \nand seventh rows both include f subshells and also have anomalous ﬁlling among the s, f, and d \nsubshells.\n13.6 \u0002  EXAMPLE: THE HYDROGEN MOLECULE\nNow let’s take a look at another two-electron system that will introduce us to some molecular phys-\nics and prepare us for the periodic systems in Chapter 15. Consider the hydrogen molecule with two \nnuclei (protons), each with a bound electron, with the two atoms bound to each other to make a four-\nparticle system, as shown in Fig. 13.13. We label the electrons 1 and 2 and the protons A and B. The \nHamiltonian for the molecule includes hydrogen Hamiltonians for each electron-proton pair; addi-\ntional Coulomb potential energy terms for the electron-electron, proton-proton, and electron-other-\nproton pairs; and kinetic energy for the nuclei:\n \nH = Hatom,1A + Hatom,2B + Vee + Vpp + Vep + TN, \n(13.83)\nwhere\n \n Hatom,1A = ¢\np 2\n1\n2m -\ne2\n4pe0\n r1A\n≤\n \n Hatom,2B = ¢\np 2\n2\n2m -\ne2\n4pe0\n r2B\n≤\n \n Vee =\ne2\n4pe0\n r12\n \n(13.84)\n \n Vpp =\ne2\n4pe0\n RAB\n \n Vep = - \ne2\n4pe0\n r1B\n-\ne2\n4pe0\n r2A\n \n Tnuc =\np 2\nA\n2MA\n+\np 2\nB\n2MB\n.\n \nWe treat this two-electron system as we did the helium atom in the sense that we put the electrons into \nthe lowest energy states of the one-electron system and then account for the required symmetrization \nof the two identical electrons. So we must ﬁrst discuss the one-electron molecule.\n\n438 \nIdentical Particles\n13.6.1 \u0002  The Hydrogen Molecular Ion H2\n+\nThe hydrogen molecular ion H +\n2  has one electron and has a Hamiltonian\n \n Hion =\np2\n1\n2m -\ne2\n4pe0\n r1A\n-\ne2\n4pe0\n r1B\n+\ne2\n4pe0\n RAB\n+\np2\nA\n2MA\n+\np2\nB\n2MB\n. \n(13.85)\nWe do not need the subscript labeling the electron as #1, but we keep it to connect with the H2 case. \nTo construct approximate energy eigenstates, we use the method of linear combination of atomic \norbitals (LCAO), which assumes that we can use the atomic energy eigenstates as basis functions. If \nthe two protons are far apart, then we expect that in the ground state of the ion, the electron is attached \nto one proton and is in the hydrogen atomic ground state. The electronic wave function in this case is\n \n0 cseparated9 \u0003 c1s 1r1A2, \n(13.86)\nassuming the electron is on proton A. However, the ion Hamiltonian in Eq. (13.85) is spatially sym-\nmetric about the center of the molecule located at the midpoint of the internuclear separation RAB and \nthe eigenstates should reﬂect this spatial symmetry. Hence we construct two possible ground states of \nthe ion that are symmetric and antisymmetric spatially:\n \n 0 c g\n1s91 \u0003\n1\n12 3c1s1r1A2 + c1s1r1B24 \n \n 0 c u\n1s91 \u0003\n1\n12 3c1s1r1A2 - c1s1r1B24. \n(13.87)\nThese states are even (g) and odd (u), respectively, under reﬂection about the midpoint of RAB , and \nare labeled as gerade and ungerade states (German for even and odd). We use this labeling notation to \ndistinguish the spatial symmetry (g, u) from the exchange symmetry (S, A) that we’ll need for the H2 \ntwo-electron molecule.\nTo estimate the ground-state energy of the ion, we calculate the expectation value of the energy \n8E9 = 8Hion9 using the wave functions in Eq. (13.87). We ignore the motion of the nuclei by assum-\ning that the internuclear separation RAB is ﬁxed. In calculating the energy expectation value, we inte-\ngrate over the electron position, but the result is still dependent on the choice for the ﬁxed value of \nRAB. This dependence is evident in the results shown in Fig. 13.14. For both the gerade and ungerade \nstates, the energy at large internuclear separation is simply the hydrogen energy -13.6 eV expected \ne1\n\u0003\ne2\n\u0003\npB\n\u0002\npA\n\u0002\nr1A\nr2A\nr1B\nr2B\nRAB\nr12\nFIGURE 13.13 Hydrogen molecule.\n\n13.6 Example: The Hydrogen Molecule \n439\nfor the system of one ground state atom and one distant proton. At very small internuclear separation \n(RAB V a0), the energy of both states becomes positive and very large due to the strong proton-\nproton Coulomb repulsion. For intermediate internuclear separation, the gerade and ungerade states \nhave different energies. The minimum in the gerade state energy indicates an attraction that leads \nto a stable molecule with an internuclear separation given by the bond length R0. The energy of the \nungerade state has no minimum and is repulsive at all distances, implying that a system in this state \nwill dissociate into a bound hydrogen atom and an isolated proton. Hence, we refer to the gerade state \nas a bonding orbital and the ungerade state as an antibonding orbital. Note that the energy of the \nbonding orbital shown in Fig. 13.14 is the potential energy function we used in Chapter 9 (Fig. 9.14) \nto ﬁnd the motion of the nuclei in a diatomic molecule. This approximate method of treating the elec-\ntron motion ﬁrst and then the nuclear motion is the Born-Oppenheimer approximation. It relies on \nthe assumption that the nuclear motion is much slower than the electron motion because of the large \nmass difference.\nTo gain a qualitative understanding of the differences between the gerade and ungerade states, \nconsider a one-dimensional view of the wave functions and probability densities of the two states. \nAlong the line of the internuclear separation, the gerade and ungerade states are\n \n @ c g\n1s9 \u0003\n1\n12 cc1s ar + RAB\n2 b + c1s ar - RAB\n2 b d  \n \n @ c u\n1s9 \u0003\n1\n12 cc1s ar + RAB\n2 b - c1s ar - RAB\n2 b d . \n(13.88)\nSubstituting the hydrogen atomic ground-state wave function c1s1r2 = e-r>a0\u00062pa3\n0 into \nEq. (13.88) yields the plots shown in Fig. 13.15. For the gerade state, the wave functions add\n[Fig. 13.15(a)] and the resulting electron probability density [Fig. 13.15(b)] is large between the two \nprotons. This excess negative charge increases the attractive Coulomb interaction of the electron \nand protons enough to overcome the proton-proton Coulomb repulsion and permit a stable bound \nmolecule. In contrast, the wave functions of the ungerade state subtract [Fig. 13.15(c)] and produce a \n1\n2\n3\nR0\nR(Å)\n\u000314\n\u000312\n\u000310\nE(eV)\nΨ1s\ng\nΨ1s\nu\nFIGURE 13.14 Energies of the bonding and antibonding orbitals of the \nhydrogen molecular ion, as a function of the internuclear separation.\n\n440 \nIdentical Particles\nzero point in the electron density [Fig. 13.15(d)] between the two protons. This deﬁciency of negative \ncharge between the protons causes the proton-proton Coulomb repulsion to dominate and leads to the \nantibonding behavior of the ungerade state.\n13.6.2  \u0002 The Hydrogen Molecule H2\nWe now return to the hydrogen molecule with two electrons. In the atomic case, our ﬁrst guess for the \nground state of the two-electron helium atom was to put both electrons in the 1s ground hydrogenic \natomic state (with Z = 2) in a symmetric spatial state and to form an antisymmetric spin-singlet state \nto satisfy the symmetrization postulate. By analogy, our ﬁrst guess for the ground state of the two-\nelectron hydrogen molecule puts each electron in the @\n c g\n1s9 ground hydrogen molecular ion state to \nmake a spatial state that is symmetric with respect to exchange and puts the two electrons in an anti-\nsymmetric spin-singlet state to satisfy the symmetrization postulate:\n \n@ c\n SA\n1s,1s9 = @ c\n g\n1s91 @ c\n g\n1s92 @\n 009 \u0003 1\n2 3c1s 1r1A2 + c1s 1r1B24 3c1s 1r2A2 + c1s1r2B24@\n 009. (13.89)\nJust as we found for helium, there is no possible way to make a spatial state that is antisymmetric \nwith respect to electron exchange when both electrons are in the @  c g\n1s9 one-electron ground state, so \n@  c\n SA\n1s,1s9 is the only possible state in the ground state of the molecule. We conclude that the ground state \nof the hydrogen molecule is a spin singlet state.\nWe can gain more insight into the molecular ground state by looking at the state @  c\n SA\n1s,1s9 more \nclosely. If we expand Eq. (13.89), we obtain\n \n @  c\n SA\n1s,1s9 \u0003 1\n2 3c1s 1r1A2 c1s 1r2 A2 + c1s 1r1B2 c1s1r2B2\n \n \n + c1s 1r1A2 c1s 1r2B2 + c1s 1r2 A2 c1s 1r1B24@ 009. \n(13.90)\n(a)\n(c)\n(b)\n(d)\nA\nB\nA\nB\nA\nB\nA\nB\nΨ1s\ng\n\u0002Ψ1s\u00022\ng\n\u0002Ψ1s\u00022\nu\nΨ1s\nu\nFIGURE 13.15 Hydrogen molecular ion wave functions (a, c) and probability densi-\nties (b, d) for gerade (a, b) and ungerade (c, d) states.\n\n13.6 Example: The Hydrogen Molecule \n441\nWe can divide this into two terms, labeled “covalent” and “ionic”\n \n @\n c\n S\ncov\n 9 \u0003 1\n2 3c1s 1r1A2 c1s 1r2B2 + c1s 1r2 A2 c1s 1r1B24  \n \n @\n c S\nion\n 9 \u0003 1\n2 3c1s 1r1A2 c1s 1r2 A2 + c1s 1r1B2 c1s 1r2B24, \n(13.91)\nso that\n \n@\n c\n SA\n1s,1s\n 9 = 1@\n c S\ncov\n 9 + @\n c S\nion\n 92@ 009. \n(13.92)\nThe state @\n c S\ncov9 corresponds to the situation with one electron associated with each nucleus, whereas the \nstate @ c S\nion9 corresponds to the situation with both electrons associated with one nucleus. When the nuclei \nare well separated, @\n c S\ncov9 corresponds to two isolated hydrogen atoms and @\n c S\nion9 corresponds to a proton \nand a negative hydrogen ion, which has an energy larger than the two isolated hydrogen atoms. Hence, we \nexpect that @\n c S\ncov9 would be a better guess for the ground state of the molecule. The state @\n c S\ncov9 represents \ncovalent bonding and the state @\n c S\nion9 represents ionic bonding.\nFor the covalent bond, we can also form an antisymmetric state\n \n@\n c A\ncov9 \u0003 1\n2 3c1s 1r1A2 c1s 1r2B2 - c1s 1r2 A2 c1s 1r1B24, \n(13.93)\nwhich must be associated with the symmetric spin-triplet state:\n \n@\n c AS\ncov9 = @\n c A\ncov9@ 1M9. \n(13.94)\nIf we use the two states @\n c SA\ncov9 = @\n c S\ncov9@ 009 and @\n c AS\ncov9 = @\n c A\ncov9@ 1M9 to ﬁnd the energy expectation \nvalues, then we are using the valence bond method. The results of this calculation for the case with \nboth electrons in the 1s atomic states are shown in Fig. 13.16. The results are qualitatively similar to\n0.5\n1.0\n1.5\n2.0\nR0\nR(Å)\n\u00033\n\u00032\n\u00031\n1\n2\nE(eV)\nΨA11 \u00021M\u0003\nΨS11 \u000200\u0003\nFIGURE 13.16 Energies of the bonding and antibonding orbitals of the \nhydrogen molecule obtained with the valence bond method, with the zero of \nenergy referenced to the dissociation limit.\n\n442 \nIdentical Particles\nFig. 13.14 in that there is a bonding orbital and antibonding orbital. This is to be expected because the state \n@\n c S\ncov9 is a gerade state and the state @\n c A\ncov9 is an ungerade state. But now for the H2 molecule, these states \nare also linked to the exchange symmetry and hence the spin. Following the argument for the hydrogen \nion, we conclude that the symmetric spatial state (singlet spin state) has a lower energy than the antisym-\nmetric spatial state (triplet spin state) because of the increased electron-proton Coulomb attraction in the \ngerade state. Note that this ordering of the singlet and triplet states is opposite the case for the excited \nstates of helium. In that case, the increased overlap of the electrons in the symmetric spatial state led to an \nincreased Coulomb repulsion of the two electrons and a higher energy for the spin singlet state.\nSUMMARY\nFor a proper quantum mechanical description of multiple-particle systems, we must account for the indis-\ntinguishability of fundamental particles. The symmetrization postulate requires that the quantum state \nvector of a system of identical particles be either symmetric or antisymmetric with respect to exchange of \nany pair of identical particles within the system. Nature dictates that integer spin particles—bosons—have \nsymmetric states, while half-integer spin particles—fermions—have antisymmetric states. The symme-\ntrization postulate applies to the complete quantum state vector, including both the spin and space \nparts of the system. As a consequence of the symmetrization postulate, some states are not allowed. \nThe best known manifestation of this is the Pauli exclusion principle, which limits the number of \nelectrons in given atomic levels and leads to the structure of the periodic table.\nPROBLEMS\n 13.1 Show that the eigenvalues of the exchange operator P12 are {1.\n 13.2 For a system of two identical spin-0 bosons, the total spin must be zero and the only possible \nsystem spin state is 0 SM9 = 0 009. Express this state in the uncoupled basis and show that it is \nsymmetric with respect to exchange of the two particles.\n 13.3 Consider a system of two identical spin-1 particles. Find the spin states for this system that are \nsymmetric or antisymmetric with respect to exchange of the two particles.\n 13.4 Specify the exchange symmetry of the following wave functions:\n \nca1x1, x22 =\n1\n1x1 + x22\n \ncb1x1, x22 =\na1x1 - x22\n1x1 - x222 + b\n \ncc1x1, x22 =\na1x1 - 3x22\n1x1 + x222 + b\n \ncd 1x1, x2, x32 =\nx1 x2 x3\nx 2\n1 + x 2\n2 + x 2\n3 + b\n.\n 13.5 Use your favorite software tool to plot the two-particle probability density for two non-\ninteracting particles in a one-dimensional harmonic oscillator potential for the case where one \nof the particles is in the single-particle ground state and the other is in the single-particle ﬁrst \nexcited state. Do this for (a) distinguishable particles (of the same mass), (b) identical spin-0 \n\nProblems \n443\nbosons, and (c) identical spin-1/ 2 fermions in a spin triplet state. In each case, write the system \nwave function and discuss the important features of your plots.\n 13.6 Consider two noninteracting particles of mass m in an inﬁnite square well. For the case with \none particle in the single-particle state 0 n9 and the other in the state 0 k91n \u0002 k2, calculate the \nexpectation value of the squared interparticle spacing 81x1 - x2229, assuming (a) the particles \nare distinguishable, (b) the particles are identical spin-0 bosons, and (c) the particles are identi-\ncal spin-1/2 fermions in a spin triplet state. Use bra-ket the notation as far as you can, but you \nwill have to do some integrals. Verify the results in Eq. (13.39).\n 13.7 Consider two noninteracting particles of mass m in the harmonic oscillator potential well. For \nthe case with one particle in the single-particle state 0 n9 and the other in state 0 k9 1n \u0002 k2,\ncalculate the expectation value of the squared interparticle spacing 81x1 - x2229, assuming (a) \nthe particles are distinguishable, (b) the particles are identical spin-0 bosons, and (c) the par-\nticles are identical spin-1/ 2 fermions in a spin triplet state. Use bra-ket notation as far as you \ncan, but you will have to do some integrals.\n 13.8 Calculate the one-dimensional particle separation probability density P1x1 - x22 for a \nsystem of two identical particles in an infinite square well with one particle in the single-\nparticle ground state 0 19 \u0003 w11x2 and the other in the state 0 29 \u0003 w21x2. Do this for \nthe three cases of (a) distinguishable particles (of the same mass), (b) identical particles \nin a symmetric spatial state, and (c) identical particles in an antisymmetric spatial state. \nReproduce Fig. 13.4.\n 13.9 Calculate the one-particle probability density P1x12 by integrating the two-particle probability \ndensity P1x1, x22 over the position x2 of particle 2 (i.e., projecting the two-particle probability \ndensity onto the x1 axis). Do this for the three cases of (a) distinguishable particles (of the same \nmass), (b) identical particles in a symmetric spatial state, and (c) identical particles in an anti-\nsymmetric spatial state. Demonstrate that measuring the position of one particle independent of \nthe location of the other particle is the same for all three cases.\n 13.10 Show that Eq. (13.51) follows from Eq. (13.50).\n 13.11 Consider two indistinguishable, uncharged spin-1/2 fermions in the one-dimensional harmonic \noscillator potential V1x2 = 1\n2 mv2x2. The two particles interact with each other through a perturb-\ning potential H\u0004 = 1\n2 a1x1 - x222, where the positive constant a is considered small (a V mv2).\na) For the unperturbed two-particle system, ﬁnd the energy eigenvalues and eigenstates \nof the ground state and the ﬁrst excited state (you need not determine the spatial  \nwave functions, bra-ket notation is sufﬁcient). Specify and discuss the degeneracy  \nof each level.\nb) Discuss qualitatively how the energies in (a) are perturbed by the interaction of the par-\nticles. Draw an energy level diagram showing the unperturbed and perturbed energy levels.\n 13.12 Show that the sign of the exchange contribution K12 is negative for the spin-triplet state in the \nﬁrst excited state of a system of two identical spin-1/ 2 particles [see Eq. (13.57)].\n 13.13 Consider the ﬁrst excited state of helium where one electron is in the n = 1 hydrogenic state \nand the other electron is in the n = 2 hydrogenic state.\na) Using term or spectroscopic notation, list all the allowed states of this system.\nb) How many total states are there?\nc) What is the energy of this level, ignoring the interactions of the electrons with each other?\nd) Describe qualitatively the shifts of this energy level that result from considering the interac-\ntions of the electrons with each other.\n\n444 \nIdentical Particles\n 13.14 Find the ﬁrst-order perturbed energy of the helium ground state by calculating the direct \nintegral J in Eq. (13.70). Find the numerical value of your result (in eV ) and conﬁrm \nEq. (13.73).\n 13.15 Find the ﬁrst-order perturbed energies of the helium excited states 1s 2 s and 1s 2 p by calculat-\ning the direct and exchange integrals J, K in Eqs. (13.71) and (13.81). Find the numerical val-\nues of your results (in eV ) and make a diagram similar to Fig. 13.9.\n 13.16 Show that the state of the hydrogen molecule that is antisymmetric with respect to electron \nexchange when both electrons are in the @ c g\n1S9 state is identically zero.\n 13.17 Consider two indistinguishable, noninteracting spin-1/2 fermions in a one-dimensional inﬁnite \nsquare well potential of length L.\na) What is the ground-state energy of the two-particle system?\nb) What is the ground-state wave function?\nc) What is the ﬁrst excited state energy of the two-particle system?\nd) What are the wave functions of the ﬁrst excited state?\ne) What is the degeneracy of the ﬁrst excited state?\nf) Discuss qualitatively how the excited-state energies change if we consider the particles to \nbe interacting through the Coulomb potential.\nRESOURCES\nFurther Reading\nThe work on Bose-Einstein Condensation that was awarded the 2001 Nobel Prize in Physics is \ndescribed at:\nnobelprize.org/nobel_prizes/physics/laureates/2001/\nFurther details on molecular energy calculations are presented in\nB. H. Bransden and C. J. Joachain, Physics of Atoms and Molecules, 2nd ed., Harlow, \nEngland:Prentice Hall, 2003.\nDr. Seuss’s take on indistinguishability can be found in\nDr. Seuss, The Sneetches and Other Stories, New York: Random House, 1961. \n\n \n445\nC H A P T E R \n14\nTime-Dependent  \nPerturbation Theory\nIn Chapters 10 and 12, we studied time-independent perturbation theory and found that changes in the \nHamiltonian lead to changes in the energy eigenstates of a system. In those examples, the Hamiltonian \nwas not time dependent, so the perturbed energy levels were still stationary states of the system. Now \nwe turn to the problem of understanding how a system responds to changes in the Hamiltonian that are \na function of time. We will ﬁnd that the new perturbed energy states are no longer stationary states and \nthat changes or transitions between states can occur. In Chapter 3, we solved the time-dependent case \nexactly for a sinusoidal perturbation of the two-level spin system. We found that spin ﬂips or transi-\ntions between spin up and down states occur when the frequency of the time dependence is close to the \nBohr frequency characterizing the energy splitting of the two states. This resonance condition is also \nan important idea in this chapter.\nThe transitions between energy states that arise from a time-dependent Hamiltonian play a major \nrole in experimental studies of quantum mechanical systems. We have referred many times to spec-\ntroscopic experiments that provide evidence of the energies of quantum systems. These spectroscopic \nexperiments rely on the interaction between the oscillating electromagnetic ﬁelds of laser beams and \natoms or molecules that respond to these time-dependent ﬁelds. The examples in this chapter will help \nus better understand these light-matter interactions.\n14.1 \u0002 TRANSITION PROBABILITY\nThe typical experiment that we wish to model with time-dependent perturbation theory is the fol-\nlowing: we start with a system in a particular initial quantum state 0 i9, we turn on a perturbing Ham-\niltonian H\u00041t2 at time t = 0, and then we measure the probability that the system is in a new ﬁnal \nquantum state 0  f 9 at a later time. For example, a hydrogen atom in its ground state 0 1s9 is perturbed \nby an incident laser beam, and we wish to know the probability of the atom making a transition to \nthe 0 3p9 excited state. The Hamiltonian is assumed to be H0 before the perturbation, and as in time-\nindependent perturbation theory, we assume that we know the solutions to the unperturbed energy \neigenvalue equation:\n \nH00 n9 = En0 n9. \n(14.1)\nIn the hydrogen example, H0 is the hydrogen atom Hamiltonian and En and 0 n9 Ashorthand for 0 n/m9B \nare the eigenenergies and eigenstates we solved for in Chapter 8.\n\n446 \nTime-Dependent Perturbation Theory\nThe Schrödinger equation that governs the time evolution of a quantum system is\n \nH0 c9 = iU d\ndt 0 c9. \n(14.2)\nThe full Hamiltonian\n \nH = H0 + H\u00041t2 \n(14.3)\nis now time dependent, so we cannot follow the standard recipe we developed in Chapter 3 for deter-\nmining the time evolution of the quantum state vector. In principle, we have to rediagonalize the \nHamiltonian and ﬁnd the new energy eigenstates, and then do that each time the Hamiltonian changes. \nBecause the Hamiltonian is continuously changing, that is nearly impossible to do.\nRather, we take an approach that is similar to that taken in time-independent perturbation \ntheory: we assume the perturbation is small enough that the zeroth-order energy eigenstates are \na good approximation for starting the solution. But now we are more interested in solving the \nSchrödinger equation than in solving the energy eigenvalue equation. We are not so interested in \nhow the perturbation changes the energies of the states; rather, we want to ﬁnd how the perturba-\ntion changes the time evolution of the system. We use the original energy basis for expanding gen-\neral states of the system, even though these states may not be energy eigenstates of the perturbed \nsystem.\nUsing the zeroth-order energy basis, the initial state of the system, before the perturbation is \nturned on, is\n \n0 c1t = 029 = a\nn\ncn0 n9. \n(14.4)\nWe know from the Schrödinger recipe of Chapter 3 that the time evolution of this initial state without \nany perturbation would be\n \n0 cH\u0004=01t29 = a\nn\ncne-iEnt\u0006U0 n9, \n (14.5)\nwhere each term acquires a time-dependent phase evolution factor dependent on the energy of that \nterm. The application of the perturbation H\u00041t2 gives rise to new energy eigenstates and hence new \ntime evolution phase factors. However, if the perturbation is small, then we expect that the new \nsolution will be close to the zeroth-order solution of Eq. (14.5). Hence, we assume that we can \nmodify the zeroth-order solution by including another factor that reﬂects the additional time depen-\ndence caused by the perturbation. We do this by allowing the expansion coefﬁcients to be time \ndependent:\n \n0 cH\u0004\u000201t29 = a\nn\ncn1t2e-iEnt\u0006U0 n9. \n(14.6)\nNow our task is to determine how the coefﬁcients cn1t2 depend on time, with the obvious restric-\ntion that they equal their original values cn102 at t \u0003 0. Substituting Eq. (14.6) for the time evolved \nstate into the Schrödinger equation (14.2), we ﬁnd\n \n 1H0 + H\u00041t220 c1t29 = iU d\ndt 0 c1t29\n \n(14.7)\n \n 1H0 + H\u00041t22a\nn\ncn1t2e-iEnt\u0006U0 n9 = iU d\ndt a\nn\ncn1t2e-iEnt\u0006U0 n9, \n\n14.1  Transition Probability \n447\nand using the zeroth-order energy eigenvalue equation (14.1) to cancel some terms yields\n a\nn\nc Encn1t2e-iEnt\u0006U0 n9 + H\u00041t2cn1t2e-iEnt\u0006U0 n9 d = iUa\nn\nc dcn1t2\ndt\n e-iEnt\u0006U0 n9 - iEn\nU\n cn1t2e-iEnt\u0006U0 n9 d\n \n a\nn\nH\u00041t2cn1t2e-iEnt\u0006U0 n9 = iUa\nn\ndcn1t2\ndt\n  e-iEnt\u0006U0 n9. \n(14.8)\nTo simplify this differential equation, we isolate one coefﬁcient in the sum on the right side by project-\ning the whole equation onto a particular energy state, say 0 k9, and use orthogonality to ﬁnd\n \n 8k0 a\nn\nH\u00041t2cn1t2e-iEnt\u0006U0 n9 = 8k0 iUa\nn\ndcn1t2\ndt\n e-iEnt\u0006U0 n9 \n \n a\nn\ncn1t2e-iEnt\u0006U8k0 H\u00041t2 0 n9 = iU dck1t2\ndt\n e-iEkt\u0006U.\n \n \n(14.9)\nRearranging terms yields a differential equation\n \niU \ndck1t2\ndt\n= a\nn\ncn1t2ei(Ek-En)t\u0006U8k 0 H\u00041t2 0 n9 \n(14.10)\nfor each coefﬁcient ck of the expansion. This result is still exact, but it gives us a set of coupled dif-\nferential equations that is difﬁcult to solve. We seek a perturbative solution by using an iterative \napproach. We expand the coefﬁcient cn in a perturbation series\n \ncn = c(0)\nn\n+ c(1)\nn\n+ c(2)\nn\n+ ..., \n(14.11)\nwhere the superscript denotes the order of the perturbation. The right side of Eq. (14.10) already has one \norder of the perturbation in H\u00041t2, so when we equate the two sides of the equation to the same order of \nthe perturbation, we end up with the order of cn on the right side being one less than the order on the left \nside (Problem 14.1). The zeroth-order term of Eq. (14.10) is\n \niU \ndc(0)\nk 1t2\ndt\n= 0. \n(14.12)\nThis says that the coefﬁcients cn have no time dependence when there is no perturbation, which is \nconsistent with Eq. (14.5) where all the Schrödinger evolution time dependence is already speciﬁed. \nThe ﬁrst-order term of Eq. (14.10) is\n \niU \ndc(1)\nk 1t2\ndt\n= a\nn\nc(0)\nn 1t2ei(Ek-En)t\u0006U8k0 H\u00041t2 0 n9. \n(14.13)\nWe can continue in this manner to all orders if we wish, but we will not go beyond the ﬁrst-order \nsolution. To collapse the sum on the right side of Eq. (14.13), we make the assumption mentioned \nabove that the system starts in one particular eigenstate 0 i9 of the zeroth-order Hamiltonian, that is \n0 c1029 = 0 i9. Thus the initial coefﬁcients obey:\n \ncn102 = dni. \n(14.14)\n\n448 \nTime-Dependent Perturbation Theory\nIn zeroth-order there is no time dependence, according to Eq. (14.12), so we obtain\n \nc(0)\nn 1t2 = dni. \n(14.15)\nSubstituting Eq. (14.15) into Eq. (14.13) collapses the sum to just one term and yields the ﬁrst-order \ndifferential equation for the coefﬁcient ck1t2:\n \niU \ndc(1)\nk 1t2\ndt\n= ei(Ek-En)t\u0006U8k0 H\u00041t2 0 i9. \n(14.16)\nWe solve Eq. (14.16) by integrating directly to give\n \nck1t2 = 1\niU L\nt\n0\n8k0 H\u00041t\u00042 0 i9ei(Ek-Ei)t\u0004\u0006Udt\u0004  . \n(14.17)\nWe have dropped the superscript on ck1t2 because c(0)\nk 1t2 = 0 for k \u0002 i and we will not solve for \nhigher-order terms, so Eq. (14.17) gives us the complete coefﬁcient to our desired order. Equation (14.17) \ntells us how the expansion coefﬁcient ck1t2 for the energy eigenstate 0 k9 evolves with time subject to the \nperturbation H\u00041t2, given that the system started in the state 0 i9. Equation (14.17) has the familiar form \nof a Fourier transform, so we interpret the result as the Fourier coefﬁcient (in frequency space) of the \nperturbation H\u00041t2 at the Bohr frequency\n \nvki = Ek - Ei\nU\n. \n(14.18)\nIf the perturbation H\u00041t2 has an appreciable component at the Bohr frequency vki associated with the \nenergy difference between the initial state 0 i9 and some other state 0 k9, then the probability of the sys-\ntem making a transition from the initial state 0 i9 to the state 0 k9 is large.\nTo ﬁnd the probability that the system is measured to be in a particular ﬁnal state 0  f 9 at a later \ntime, we project the time-evolved state Eq. (14.6) onto the ﬁnal state\n \n PiSf 1t2 = 08 f 0 c1t290\n2\n \n \n = ` 8 f 0 a\nn\ncn1t2e-iEnt\u0006U0 n9 `\n2\n \n(14.19)\n \n = 0 cf 1t20\n2\n \nand substitute Eq. (14.17) to obtain\n \nPiSf 1t2 = 1\nU2 `\nL\nt\n0\n8 f 0\n H\u00041t\u00042 0 i9ei(Ef-Ei)t\u0004\u0006Udt\u0004 `\n2\n    .  \n(14.20)\nOf course, to actually do the integral and ﬁnd the probability, we need to know the form of the \n perturbation H\u00041t2.\n\n14.1  Transition Probability \n449\nExample 14.1: Constant perturbation The simplest example of a time-dependent perturba-\ntion is one that is turned on at t = 0 and then turned off at a later time, but that is constant dur-\ning the time it is on. The integral in Eq. (14.17) to ﬁnd the coefﬁcient cf 1t2 of the ﬁnal state is \nstraightforward:\n \n cf 1t2 = 1\niU\n 8 f 0 H\u00040 i9\nL\nt\n0\neivfi\n t\u0004dt\u0004\n \n \n = 1\niU\n 8 f 0 H\u0004 0 i9 eivfi\n t - 1\nivfi\n \n \n = 1\niU\n 8 f 0 H\u0004 0 i9eivfit\u00062  eivfit\u00062 - e-ivfit\u00062\nivfi\n \n \n(14.21)\n \n = 2\niU\n 8 f 0 H\u0004 0 i9eivfit\u00062  \nsin1vfi\n t>22\nvfi\n. \nThe probability that the system is measured in the ﬁnal state 0  f 9 is\n \nPiSf 1t2 = @cf 1t2 @\n2 = 408 f 0 H\u00040 i9 0\n2\nU2v2\nfi\n sin21vfi\n t>22. \n(14.22)\nThis agrees with the Rabi formula found in Chapter 3 for the probability of a spin ﬂip caused by \na small perturbing constant magnetic ﬁeld [Eq. (3.63)]. Equation (14.22) tells us that to make the \ntransition from the state 0 i9 to the state 0  f 9, there are two essential requirements: (1) the matrix ele-\nment 8 f 0 H\u00040 i9 that determines whether the perturbation connects the two levels must be nonzero, \nand (2) to get appreciable probability, there must be frequency components in the time-dependent \nHamiltonian that include the Bohr frequency vfi for the transition. The ﬁrst requirement is related to \nthe selection rules that we have mentioned previously and that we discuss more fully in Section 14.4. \nThe second requirement is the resonance condition inherent in the Fourier integral of Eq. (14.17). \nEven though the perturbation H\u00041t2 in this example is constant during its application, there is a fre-\nquency component at the Bohr frequency vfi arising from the off-on-off time dependence.\nExample 14.2: Gaussian perturbation Now let’s add some more interesting time dependence \nby assuming that a perturbation is turned on and then turned off with a Gaussian time dependence as \nshown in Fig. 14.1. The form of the perturbation is\n \nH\u00041t2 = V0e-  t2\u0006t2, \n(14.23)\nwhere t is the characteristic time constant of the perturbation. This perturbation is peaked at t = 0 \nand becomes minimal a few time constants away from that. It differs mathematically from the \nsituation we had above where the perturbation started at t = 0. We accommodate this change by \nshifting the starting time of the integral in Eq. (14.17). The major contribution to the integral comes \nfrom times that are a few time constants before and after the peak at t = 0, but mathematically it \n\n450 \nTime-Dependent Perturbation Theory\nis simpler to integrate between {\u0005. The coefﬁcient cf 1\u00052 after the perturbation has been applied \nis therefore\n \n cf 1\u00052 = 1\niU L\n\u0005\n- \u0005\n8 f 0 H\u00041t\u00042 0 i9ei(Ef-Ei)t\u0004\u0006U dt\u0004 \n \n \n = 1\niU L\n\u0005\n- \u0005\n8 f 0 V00 i9e-t\u00042\u0006t2eivfit\u0004 dt\u0004. \n(14.24)\nThe sin1vfit\u00042 part of the complex exponential eivfit\u0004 is odd with respect to t\u0004 = 0, so that part of the \nintegral is zero, giving\n \n cf 1\u00052 = 1\niU\n 8 f 0 V00 i9\nL\n\u0005\n- \u0005\ne-t\u00042\u0006t2 cos1vfit\u00042dt\u0004 \n \n = 1\niU\n 8 f 0 V00 i91pte-v2\nfi t2\u00064.\n \n(14.25)\nThe probability that after the perturbation the system is measured in the ﬁnal state 0  f 9 is\n \nPiSf = 0 cf 1\u000520\n2 = pt2\nU2 08 f 0 V00 i90\n2e-v2\nfi t2\u00062. \n(14.26)\nThis result tells us that to have appreciable probability for the transition from the state 0 i9 to \nthe state 0  f 9, the time constant t must be of order 1>vfi , so that there are frequency components in \nthe time-dependent Hamiltonian that include the Bohr frequency vfi for the transition.\nAn important lesson from this example concerns a perturbation that is turned on and off very \nslowly (i.e., the time constant is very long compared with other times relevant to the system). As \nthe time constant t becomes large enough that the product vfi t approaches inﬁnity, the probability \nPiSf in Eq. (14.26) approaches zero, meaning that the system does not change states. This is an \nexample of the adiabatic theorem in quantum mechanics.\n14.2 \u0002 HARMONIC PERTURBATION\nThe previous examples have illustrated the importance of frequency components that match the Bohr \nfrequency of the transition. Frequency components in the time dependence of the Hamiltonian that are \n0\nt\nH'(t)\n2Τ\nFIGURE 14.1 Gaussian time dependence of perturbation.\n\n14.2 Harmonic Perturbation \n451\nfar from the Bohr frequency of a particular transition do not produce appreciable probability for that \ntransition. Hence, the most efﬁcient way to make a transition is to impose a sinusoidal perturbation \nat the transition frequency. The study of such resonant interactions is the most important example of \ntime-dependent perturbation theory.\nAt t = 0 we turn on a time-dependent perturbation Hamiltonian that has separate space and \ntime parts:\n \n H\u00041t2 = 2V1r\nu2cos vt\n \n \n = V1r\nu21eivt + e-ivt2. \n(14.27)\nThere are different conventions for including the factor of 2 in Eq. (14.27) or not; without it, one needs \na factor of 1/2 for each complex exponential. Substituting this harmonic perturbation into Eq. (14.17) \nyields the probability amplitude for making a transition from an initial state 0 i9 to a ﬁnal state 0  f 9:\n \n cf 1t2 = 1\niU L\nt\n0\n8 f 0 V1r\nu21eivt\u0004 + e-ivt\u00042 0 i9ei(Ef-Ei)t\u0004\u0006U dt\u0004\n \n= 1\niU\n 8 f 0 V 0 i9\nL\nt\n0\nc ei(vfi+v)t\u0004 + ei(vfi-v)t\u0004 d dt\u0004 \n(14.28)\n \n= 1\niU\n 8 f 0 V 0 i9£ ei1vfi +\n v2t - 1\ni1vfi + v2\n+ ei1vfi - v2t - 1\ni1vfi - v2 §  \n \n= 1\niU\n 8 f 0 V 0 i9£ ei(vfi+v)t\u00062 \nsin \nvfi +  v\n2\n t\nvfi +  v\n2\n+ ei(vfi-v)t\u00062 \nsin \nvfi -  v\n2\n t\nvfi -  v\n2\n§ . \nTo ﬁnd the probability, we square this amplitude, which leads to cross terms and a complicated \nexpression. This is what we have to do if the two terms inside the square brackets are of comparable \nsize, which happens if the frequency is far from a resonance. However, if the resonance condition is \nsatisﬁed or nearly satisﬁed, then one of the two terms inside the square brackets dominates because \nthe denominator approaches zero. Which term dominates depends on the sign of the energy difference \nEf - Ei = U vfi.\n1) If the initial state is lower in energy than the ﬁnal state, then the energy difference \nEf - Ei = U vfi is positive and the second term in Eq. (14.28) is large for an excitation frequency \nthat matches the Bohr frequency: v = vfi . In this case, the dominant probability amplitude is for the \ntransition from a lower state to an upper state, which we call absorption [see Fig. 14.2(a)]. The system \nabsorbs energy from the external perturbation.\nAbsorption\nEf \bEi\nEmission\nEf \tEi\n\u0002f \u0003\n\u0002f \u0003\n\u0002i\u0003\n\u0002i\u0003\nFIGURE 14.2 (a) Absorption and (b) emission processes.\n\n452 \nTime-Dependent Perturbation Theory\n2) If the initial state is higher than the ﬁnal state, then the energy difference Ef - Ei = U vfi is \nnegative and the ﬁrst term in Eq. (14.28) is large for an excitation frequency that matches the Bohr fre-\nquency: v = -vfi. In this case, the dominant probability amplitude is for the transition from an upper \nstate to a lower state, which we call emission [Fig. 14.2(b)]. The system emits energy to the external \nperturbation. This emission is caused by the applied ﬁeld, so it is referred to as stimulated emission.\nOnly one of these two terms plays a role in any particular experiment, so we needn’t worry about \nboth together. For now, we consider the absorption case only (we just need to change the sign preced-\ning v if we change to the emission case). The probability of measuring the system in the ﬁnal state is\n \nPiSf 1t2 = 0 Vfi0\n2\nU2  \n sin2 \nvfi -v\n2 t\n1\nvfi -v\n2 2\n2 , \n(14.29)\nwhere we have adopted a shorthand notation for the matrix element of the perturbation:\n \nVfi = 8 f 0 V0 i9. \n(14.30)\nIt is useful to look at this result both as a function of time and as a function of frequency. As a func-\ntion of time, there is an oscillatory dependence as shown in Fig. 14.3, with a period of 2p>1vfi - v2. \nWe saw similar results in Chapter 3 for the Rabi oscillations in the spin case (Fig. 3.12), with a slightly \ndifferent oscillation or ﬂopping frequency. The perturbation result in Eq. (14.29) is equal to the Rabi \nﬂopping probability in Eq. (3.104) for the case of small perturbations. In practice, this oscillating \nprobability is hard to observe, which is related to the ﬁnite lifetime of excited states that we address in \nSection 14.5.\nAs a function of frequency, the transition probability is shown in Fig. 14.4 and displays the \nexpected resonance behavior, with a peak in the probability at v = vfi. The similar result for resonant \nRabi spin ﬂopping was shown in Fig. 3.11. The peak of the probability in Fig. 14.4 grows as t 2, so it \ncould become greater than one, which would violate our perturbation approximation. The resonance \ncurve in Fig. 14.4 has a ﬁnite width, which implies that the resonance condition v = vfi is not an \nexact requirement. Rather, there is a spread of frequencies \u0006v that cause appreciable transition prob-\nability. The frequency width of the probability plot in Fig. 14.4 is approximately \u0006v = 4p>t, where \nthe time t is the duration of the interaction. If we call this duration \u0006t, then we have\n \n\u0006v\u0006t \u0003 4p\n\u0006t \u0006t = 4p, \n(14.31)\nt\n2Π(Ωfi\u0004Ω)\nP(t)\nFIGURE 14.3 Oscillations of the transition probability as a function of time.\n\n14.2 Harmonic Perturbation \n453\nwhich is the Fourier frequency-time uncertainty relation. We can convert this to a Fourier energy-time \nuncertainty relation by using \u0006E = U\u0006v to obtain\n \n \u0006E\u0006t \u0003 U\u0006v\u0006t = U4p \n \n \u0003 U.\n \n(14.32)\nThe neglect of the factor of 4p is consistent with the level of this approximation. This uncertainty rela-\ntion tells us that the longer we observe a system, the better we can measure the energy.\nThe resonance peak in Fig. 14.4 resembles a Dirac delta function in the limit that the frequency \nwidth \u0006v = 4p>t S 0, which implies t S \u0005 . Mathematically, the sinc function in Eq. (14.29) \nbecomes a Dirac delta function in this limit:\n \nlim\n tS \u0005\nsin2 \nvfi -v\n2 t\n1\nvfi -v\n2 2\n2\n= 2ptd1vfi - v2. \n(14.33)\nIf we assume this long time limit in Eq. (14.29), we obtain the probability\n \nPiSf 1t S \u00052 = 2pt\nU2  @ Vfi@\n2 d1vfi - v2. \n(14.34)\nThis form makes it evident that the probability increases linearly with time. This behavior is more com-\nmon than the oscillating probability of Fig. 14.3. The linear time dependence seems reasonable because \nwe expect that the more we perturb a system, the more likely it is to undergo a change. The linear time \ndependence in Eq. (14.34) allows us to deﬁne a transition rate as the probability per unit time, which \nwe obtain by differentiating the probability:\n \nRiSf = d\ndt PiSf 1t2, \n(14.35)\nwith the result:\n \nRiSf = 2p\nU2  @ Vfi@\n2 d1vfi - v2  . \n(14.36)\nThe delta function in Eq. (14.36) is called the energy conserving delta function—it requires that \nthe quantum of energy causing the transition (e.g., the photon of a laser beam) match the energy dif-\nference between the two states. In many practical applications, there is a spread of ﬁnal energy states \nΩ\n4Π/t\nΩfi\nP(Ω)\nFIGURE 14.4 Probability of excitation as a function of frequency.\n\n454 \nTime-Dependent Perturbation Theory\nrather than a discrete quantum state. For example, in a solid, the band structure of the electronic energy \nlevels represents a continuous range of allowed states (see Chapter 15). In those cases, the relevant \ntransition rate is a sum over the rates to all accessible states. We assume that these rates are incoherent \nso that we can add the rates (i.e., probabilities) rather than the amplitudes. We assume that the spread \nin energies is larger than the width of the sinc function that we turned into a delta function but small \nenough that the rates to all states are the same. Let g1E2 be the density of states per unit energy, such \nthat g1E2dE is the number of energy levels between E and E + dE. Then the total rate is given by the \nintegral over all the rates:\n \n RiSf =\nL\nEf\n +e\nEf\n -e\n2p\nU2  @ Vfi@\n2 d1vfi - v2g1E2dE  \n \n = 2p\nU2  @ Vfi@\n2\nL\nEf\n +e\nEf\n -e\nd1vfi - v2g1E2U  dv . \n \n(14.37)\nThe range over which we integrate in Eq. (14.37) is not important because of the Dirac delta function. \nThe resultant transition rate is\n \nRiSf = 2p\nU 0 Vfi0\n2g1Ef2  . \n(14.38)\nThis result is referred to as Fermi’s golden rule. It is much more practical than Eq. (14.36) because \nthe nonphysical delta function is gone. Fermi’s golden rule is general enough that it applies to many \ntypes of interactions. In the next section we’ll study one particular application.\n14.3 \u0002 ELECTRIC DIPOLE INTERACTION\nOne of the most important applications of time-dependent perturbation theory is to the interaction \nbetween an atom and an electromagnetic ﬁeld. Studying this problem tells us how lasers and atoms \ninteract, and it also leads to an understanding of the ﬁnite lifetime of excited quantum states. The \ninteraction Hamiltonian between an atom and an applied electromagnetic ﬁeld is the same as we used \nfor the Stark effect in Chapter 10, except that the electric ﬁeld is now time dependent. We neglect the \ninteraction of the atom with the magnetic component of the electromagnetic ﬁeld because it is smaller \nby a factor of the ﬁne structure constant a .\nThe electric dipole Hamiltonian is\n \nH\u0004 = -d~E . \n(14.39)\nThe electric ﬁeld is\n \n E1t2 = 2E 0en cosvt\n \n \n = en1E 0eivt + E 0e-ivt2, \n \n(14.40)\nwith the same form of the time dependence as in the generic harmonic perturbation example in the pre-\nvious section. The polarization of the electric ﬁeld is speciﬁed by the unit vector en. We ignore the spa-\ntial variation of the ﬁeld because the size of the atom (\u00070.1 nm) is much smaller than the wavelength \nof visible light (\u0007500  nm), which is the most common case. This means that at any given instant, the \n\n14.3 Electric Dipole Interaction \n455\nwhole atom sees the same electric ﬁeld. This assumption is the electric dipole approximation. We \nuse the previous harmonic perturbation calculation and identify the perturbation in Eq. (14.27) as:\n \nV = -d~enE 0. \n(14.41)\nThe atom’s electric dipole moment is\n \nd = -er, \n(14.42)\nresulting in the perturbation\n \nV = eE 0en~r. \n(14.43)\nApplication of Fermi’s golden rule in the form of Eq. (14.36) yields the transition rate:\n \nRiSf = 2p\nU2  08 f 0 eE 0en~r0 i90\n2 d1vfi - v2. \n(14.44)\nOnly the r term in the matrix element depends on the atomic states, so we simplify the rate to\n \nRiSf =\n2pe2E 2\n0\nU2\n 0 en~8 f 0 r0 i90\n2 d1vfi - v2. \n(14.45)\nThe delta function in Eq. (14.45) is not physical, so we must see how to apply this transition rate \nexpression to a real situation. The two most common situations are depicted in Fig. 14.5: (a) the per-\nturbing ﬁeld is not a single frequency and is not coherent, in which case we sum over transition rates \ncaused by the spread of frequencies; or (b) the quantum energy states are continuous, in which case \nwe sum over transition rates to a spread of energy states, as we did in the last section. The ﬁrst case \nis necessary when a broadband light source excites a discrete atomic transition. The second case is \nnecessary when using a monochromatic laser to excite a system to a spread of excited states, or even \nto a single excited state that is broadened by its ﬁnite lifetime. We start with the ﬁrst case because it \nallows us to study the interaction between blackbody radiation and atoms, which Einstein used to \nmodel the broadening of atomic states. Once we know how the atomic states are broadened, we’ll use \nthat knowledge to study the second case of single frequency excitation.\nBroadband Excitation\n(a)\nMonochromatic Excitation\n(b)\n\u0002f \u0003\n\u0002f \u0003\n\u0002i\u0003\n\u0002i\u0003\nFIGURE 14.5 (a) Broadband excitation to a discrete level, and (b) monochromatic \nexcitation to a broadened level.\n\n456 \nTime-Dependent Perturbation Theory\n14.3.1 \u0002 Einstein Model: Broadband Excitation\nThe Einstein model assumes a gas of two-level atoms in thermal equilibrium with blackbody radiation \nat temperature T. The atoms are considered to have discrete energy levels and the blackbody radiation \nis modeled as a broadband incoherent electromagnetic ﬁeld. The goal is to reduce Eq. (14.45) to a \nsimple form for the transition rate involving the atom properties and the ﬁeld properties.\nThe electromagnetic ﬁeld of the blackbody radiation has an energy density per unit volume given by\n \nu = e0\n2\n E2 +\n1\n2m0\n B2. \n(14.46)\nThe energy density in the electric and magnetic ﬁelds is the same. Substituting Eq. (14.40) into \nEq. (14.46) gives\n \nu = e0\n E 2 = 4e0\n E 2\n0 cos21vt2. \n(14.47)\nThe time-average over one cycle gives a factor of 1/2, resulting in\n \nurms = 2e0\n E 2\n0. \n(14.48)\nFor broadband radiation, the energy density in the electromagnetic ﬁeld is\n \nurms = r1v2dv, \n(14.49)\nwhere r1v2 is the ﬁeld energy per unit volume per unit angular frequency interval. Combining \nEqs. (14.48) and (14.49) gives\n \nE 2\n0 =\nr1v2\n2e0\n dv , \n(14.50)\nwhich we substitute into Eq. (14.45) to obtain the transition rate. We integrate over all the transition \nrates due to each frequency component because the blackbody light is incoherent, which gives\n \n RiSf = pe2\ne0\n U2 0 en~8 f 0 r0 i90\n2\nL\n\u0005\n0\nr(v)  d(vfi - v)  dv \n \n = pe2\ne0\n U2 r1vfi20 en~8 f 0 r0 i90\n2.\n \n \n(14.51)\nInside the black box containing the blackbody radiation and the atoms, the radiation is isotropic \nand the polarization vector is random, so we average Eq. (14.51) over all possible directions of the \npolarization vector en. To do this average, let u be the angle between en and r . The three-dimensional \nspatial average of 0 en~rn 0\n2 is\n \n H 0 en~rn 0\n2I =\n1\n4p L\n0 en~rn 0\n2d\t\n \n \n =\n1\n4p L\n2p\n0\nL\np\n0\ncos2 u sin u du df \n \n =\n1\n4p\n 2pc -  1\n3 cos3 ud\np\n0\n \n \n(14.52)\n \n = 1\n3.\n \n\n14.3 Electric Dipole Interaction \n457\nThus we get for the transition rate:\n \nRiSf =\npe2\n3e0\n U2 r1vfi208 f 0 r0 i90\n2. \n(14.53)\nEinstein grouped the atomic factors into the now-famous Einstein B coefﬁcient\n \nBif =\npe2\n3e0\n U2 08 f 0 r0 i90\n2 \n(14.54)\nand wrote the transition rate as\n \nRiSf = Bif r1vfi2. \n(14.55)\nThe Einstein B coefﬁcient is the same if we swap initial and ﬁnal states, as is the electromagnetic \nenergy density r1vfi2 evaluated at the transition frequency, so the rates of emission and absorption are \nthe same.\nIn the Einstein model, a collection of atoms is in thermal equilibrium with blackbody radiation \nat a temperature T. The atoms are treated as having two states 0 19 and 0 29 with energies E1 and E2, \nrespectively. The radiation induces transitions from 0 19 to 0 29—absorption, and from 0 29 to 0 19—\nstimulated emission, as depicted in Fig. 14.6. Because the absorption and stimulated emission rates are \nthe same, the populations of the two levels would be the same if there were no other processes. But the \nBoltzmann thermal distribution law tells us that the populations of levels decrease as the energy level \nincreases. Einstein argued that there must be a third process—spontaneous emission—connecting the \ntwo levels in order for thermal equilibrium to be maintained. This is called the principle of detailed \nbalance. Spontaneous emission occurs spontaneously, independent of the applied ﬁeld, and therefore \nit causes the excited state 0 29 to decay to the ground state 0 19 even when no ﬁeld is present. Because of \nthis spontaneous decay, the excited state 0 29 has a ﬁnite lifetime, in contradiction to our previous dec-\nlaration that all energy eigenstates are stationary states. The transition rate for spontaneous emission \nwas deﬁned by Einstein as A21 and is called the Einstein A coefﬁcient. The spontaneous emission rate \nis independent of the applied ﬁeld. The stimulated emission rate depends on the applied ﬁeld accord-\ning to Eq. (14.55).\nLet’s now calculate the Einstein A coefﬁcient of spontaneous emission. Assume that there are N1 \natoms in the lower state 0 19 and N2 atoms in the upper state 0 29. The transition rates we have discussed \nso far are the rates for single atoms, so the rates for a collection or ensemble of atoms are obtained by \nmultiplying the single atom rates by the population of the initial state. For example, the number of \natoms per second that absorb photons and change from state 0 19 to state 0 29 is the transition rate for \nAbsorption\nStimulated Emission\nSpontaneous Emission\n\u00022 \u0003\n\u00021 \u0003\nFIGURE 14.6 Einstein model of absorption and emission of photons.\n\n458 \nTime-Dependent Perturbation Theory\na single atom B12\n r1v212 times the number of atoms N1 in state 0 19: N1B12 r 1v212. This absorption \nprocess decreases the number of atoms in state 0 19, while the two emission processes increase the \nnumber. The sum of the three rates yields the rate equation for state 0 19:\n \ndN1\ndt\n= -N1B12\n r1v212 + N2B21\n r1v212 + N2\n A21. \n(14.56)\nThe rate equation for state 0 29 is similarly\n \ndN2\ndt\n= +N1B12\n r1v212 - N2B21\n r1v212 - N2\n A21. \n(14.57)\nNote that dN1>dt = -dN2>dt because there are only two levels in this model system and all atoms \nleaving one state end up in the other state.\nIn steady state, the number of atoms in either state is constant (but not equal to each other), with \nas many atoms making upward transitions as downward transitions. Hence, the change dN1>dt equals \nzero and we can solve Eq. (14.56) for the radiation energy density:\n \nr1v212 = A21\nB12\n \n1\n1N1>N221B12>B212 - 1. \n(14.58)\nThe blackbody energy density is determined by the Planck blackbody radiation formula:\n \nr1v2 =\nU\np2c3 \nv3\ne Uv\u0006kBT -1\n, \n(14.59)\nwhich comes from the Boltzmann probability of occupation of the modes of the radiation ﬁeld. In ther-\nmal equilibrium, elementary statistical mechanics tells us that the number of atoms in an energy level \nE is proportional to the Boltzmann factor exp1-E>kBT2, where kB is Boltzmann’s constant. Hence, \nthe ratio of level populations is:\n \nN1\nN2\n= e-E1\u0006kBT\ne-E2\u0006kBT = e(E2-E1)\u0006kBT = e Uv21\u0006kBT. \n(14.60)\nCombining Eqs. (14.58), (14.59), and (14.60) leads to two conditions:\n \nB21 = B12, \n(14.61)\nwhich we already knew from Eq. (14.54), and\n \nA21 =\nU v3\n21\np2c3 B21, \n(14.62)\nwhich relates the Einstein A and B coefﬁcients. Using Eq. (14.54) for the Einstein B coefﬁcient leads \nus to the spontaneous emission rate:\n \nA21 =\ne2v3\n21\n3pe0\n Uc3 0820 r0 190\n2  . \n(14.63)\nThe decay of a state caused by spontaneous emission implies that excited states are not stationary \nstates, as we have assumed all along about quantum energy eigenstates. Rather, excited states have \n\n14.3 Electric Dipole Interaction \n459\nan inherent ﬁnite lifetime due to spontaneous emission. If we have a system of atoms in the excited \nstate, with no electromagnetic ﬁelds present, then the rate equation for the upper level is\n \ndN2\ndt\n= -N2 A21. \n(14.64)\nSolving this differential equation yields the time dependence of the upper level population\n \nN21t2 = N2102e-A21t = N2102e-t\u0006t. \n(14.65)\nThe upper level population decays exponentially, as shown in Fig. 14.7, with a lifetime t given by the \ninverse of the Einstein A coefﬁcient\n \nt =\n1\nA21\n  . \n(14.66)\nThis inherent ﬁnite lifetime of the excited state means that there is a fundamental limit to the time we \nhave to observe the system in this state. Therefore, the energy-time uncertainty relation in Eq. (14.32) \nimplies that the ﬁnite lifetime of the excited state places a fundamental limit on how well we can mea-\nsure the energy. Hence, the energy of an excited state is uncertain or broadened. The uncertainty in our \nmeasurement of the energy difference between the ground and the excited state is\n \n\u0006E = U\n\u0006t = U\nt = U A21. \n(14.67)\nNo matter how precise our measurement apparatus is, we cannot overcome this limitation. The energy \nuncertainty in Eq. (14.67) is the spread in energy of a state that was depicted in Fig. 14.5(b), which we \naddress in the next section.\nThough we now have a way to calculate the spontaneous emission rate, we have not discovered \nthe mechanism that is responsible for the decay of excited states in the absence of a perturbing radia-\ntion ﬁeld. The approach that we have taken here to atom-light interactions is known as the semiclas-\nsical method because we have treated the atoms quantum mechanically, but we have treated the light \nas a classical ﬁeld. To properly explain spontaneous emission, we must use quantum electrodynamics \n(QED). Quantum electrodynamics treats the light quantum mechanically as a harmonic oscillator, \nwith the state 0 n9 representing a light ﬁeld with n photons. The ground state 0 09 has no photons (i.e., \nno ﬁeld excitations), but has an energy U v>2 , just as the ground state of the harmonic oscillator does. \nThis vacuum state energy represents residual energy in the electromagnetic ﬁeld, which “stimulates” \nΤ\nt\nN0\nN\ne\u0003t/Τ\nN0\ne\nFIGURE 14.7 Exponential time decay of the population of an excited atomic state.\n\n460 \nTime-Dependent Perturbation Theory\nthe emission of photons from excited atoms. Hence, spontaneous emission can be considered to be \nemission that is stimulated by the vacuum. Recent experiments have made this interpretation clear by \nshowing that the spontaneous emission rate can be changed by altering the vacuum, which is possible \nif you put an atom in a specially sized box, a cavity, that alters the allowed radiation modes at the fre-\nquency of interest. The quantum mechanical interaction of atoms and quantized light ﬁelds is known \nas cavity QED.\n14.3.2 \u0002 Laser Excitation\nWe now address the problem of a monochromatic laser exciting an atom with an upper level that is \nnot sharply deﬁned, as depicted in Fig. 14.5(b). We assume that the spread in energy of the upper \natomic level is caused by spontaneous emission, though collisions or other environmental factors are \nalso possible causes. Fermi’s golden rule in Eq. (14.38) tells us that the transition rate depends on the \ndensity of energy states. We found the spread of the ﬁnal energy state in Eq. (14.67) for the case of \nspontaneous emission. The functional form of the density of energy states g1E2, or equivalently the \nfrequency density, is determined by the Fourier transform of the emitted electromagnetic ﬁeld. For the \nexponential time dependence of a spontaneously decaying upper state, this Fourier transform yields \na Lorentzian function (Problem 14.5). For a transition between states 0 19 and 0 29 with a spontaneous \ndecay rate A21 from state 0 29, the Lorentzian density of states for the upper state is\n \ng1E2 =\nU A21\u00062p\n1E - U v212\n2 + aU A21\n2 b\n2 . \n(14.68)\nThe density of states is normalized to unity, 1\n\u0005\n0 g1E2dE = 1, because there is only one state at the \nupper level; it is just spread out by its ﬁnite lifetime. Substituting the density of states into Fermi’s \ngolden rule in Eq. (14.38) yields the transition rate\n \nR1S2 = 2p\nU\n 0 V210\n2 g1Ef2 \n \n=\n2pe2E 2\n0\nU\n 0 en~820 r0 190\n2\n \nU A21\u00062p\n1E - U v212\n2 + a\nU A21\n2 b\n2 . \n \n(14.69)\nUsing the frequency Lorentzian 1 f 1v2dv = g1E2dE2\n \nf 1v2 =\nA21\u00062p\n1v - v212\n2 + aA21\n2 b\n2 , \n(14.70)\nwe express the transition rate as\n \nR1S2 =\n2pe2E 2\n0\nU2\n 0 en~820 r0 19 0 2 f 1v2 \n(14.71)\n\n14.3 Electric Dipole Interaction \n461\nIn terms of the Einstein B coefﬁcient, the transition rate is\n \nR1S2 = 6e0 E 2\n0\n B21 f 1v2. \n(14.72)\nIf we excite the transition with a monochromatic laser with intensity I = 2ce0E 2\n0 , the transition rate is\n \nR1S2 = 3 I\nc\n B12 f 1v2. \n(14.73)\nThis excitation probability rate has the Lorentzian frequency dependence shown in Fig. 14.8, with a \nfull width at half maximum (FWHM) of A21 . Once again, we see the resonance behavior of the inter-\naction, such that the laser must be tuned within this frequency window in order to have appreciable \nprobability of inducing excitation of the atom.\nAnother useful way to quantify the excitation of an atom by a laser is with a quantity known as \nthe cross section. To understand why an area is useful in this regard, consider characterizing the efﬁ-\nciency of the interaction as the ratio of what you get out to what you put in:\n \nefficiency = output\ninput . \n(14.74)\nIn this case, you put in light and get out excited atoms.\nWe usually characterize the input laser light in terms of the intensity I, measured in Watts per \nsquare meter. However, to simplify matters, let’s quantify the light in terms of the number of photons \nper unit area per unit time. Each photon in the laser beam has an energy U v , so the number of photons \nper unit area per unit time is the intensity divided by the energy per photon:\n \n# photons\narea ~ time =\nI\nU v. \n(14.75)\nWe quantify the output of excited atoms by the transition rate R, which is a probability (i.e., num-\nber) per unit time. Thus the efﬁciency we have deﬁned becomes:\n \n efficiency = output\ninput\n \n \n = R1S2\nI\nU v\n=\n# per unit time\n# per unit time per unit area . \n \n(14.76)\nΩ21\nΩ\nR1\u00022(Ω)\nA21\nFIGURE 14.8 Lorentzian frequency dependence of the excitation probability.\n\n462 \nTime-Dependent Perturbation Theory\nBy dimensional analysis, the efﬁciency we have deﬁned is really an effective area for excitation. This \neffective area is what we call the cross section \u0016. To calculate the cross section, we assume that the \nlaser is on resonance (v = v21) in order to provide the maximum efﬁciency:\n \n s = R1S2\nI>U v =\n31I>c2B12 f 1v212\nI>U v21\n \n \n =\n31I>c2B1212>pA212\nI>U v21\n \n \n(14.77)\n \n = 6 U v21\npc  B12\nA21\n. \nFrom Eq. (14.62) relating the Einstein rate coefﬁcients, we know the ratio of B12 to A21. This allows us \nto calculate the cross section for on-resonance excitation:\n \n s = 6 U v21\npc  B12\nA21\n= 6 U v21\npc  p2c3\nU v3\n21\n \n = 6p c2\nv2\n21\n= 6p 1\nk2\n21\n= 6p \n1\n12p>l212\n2 , \n(14.78)\nresulting in\n \ns = 3 \nl2\n21\n2p  . \n(14.79)\nThis result is amazingly simple. It says that the atom is effectively the size of the wavelength of light \n(\u0007100 - 1000 nm) when considering its interaction with resonant light! The physical size of the \natom (the Bohr radius \u00070.1 nm) is irrelevant in this case, although it would be more appropriate if \nwe were considering collisions between two atoms. For atom-light interactions, the atom acts as an \nefﬁcient antenna, despite its small size.\n14.4 \u0002 SELECTION RULES\nThe Einstein A and B coefﬁcients depend upon the matrix element 8 f 0 en~r0 i9 from the electric dipole \ninteraction between the two states. If this matrix element is zero for some reason, then there is no \nprobability that the transition between the states will occur. There are some general guidelines as to \nwhen such matrix elements are expected to be zero, and we call these selection rules. Transitions \nfor which the matrix element is zero are therefore not allowed and are called forbidden transitions. \nHowever, recall that we are working within the electric dipole approximation, which means that we \nhave neglected magnetic dipole, electric quadrupole, and higher multipole interactions. It may happen \nthat a transition is forbidden within the electric dipole approximation but is allowed by a higher-order \nmultipole interaction. The higher-order interactions typically have transition rates that are reduced by \nan extra order of the ﬁne structure constant \u0017.\n\n14.4 Selection Rules \n463\nThe selection rules derive from general properties of the electric dipole matrix elements, not from \nthe details of a speciﬁc atom or molecule. To see this, ﬁrst separate the radial and angular parts of the \nmatrix element:\n \n 8 f 0 en~r0 i9 = en~8 f 0 r rn 0 i9\n \n \n =\nL\n\u0005\n0\nr 2 dr\nL\nd\tR*\nnf/f 1r2 Y mf*\n/f 1u,f2 en~rn r Rni/i 1r2 Y mi\n/i \n 1u, f2 \n(14.80)\n \n = a\nL\n\u0005\n0\nR*\nnf /f 1r2 Rni /i 1r2r 3 drba\nL\nY mf*\n/f 1u,f2 en~rn Y mi\n/i  1u, f2  d\tb. \nThe radial integral does depend critically on the details of a speciﬁc atom or molecule and is not typi-\ncally zero. The angular integral, however, depends on the spherical harmonics, which are independent \nof the details of the central potential (see Section 7.4). The dot product term in Eq. (14.80) is expressed \nin terms of the angles \u0007 and \u0018 between the electric ﬁeld polarization vector en and the electron position \nunit vector rn\n \nen~rn = ex sin u cos f + ey sin u sin f + ez cos u. \n(14.81)\nIt is useful to express the trigonometric functions in Eq. (14.81) in terms of the spherical harmonics:\n \nen~rn = A\n4p\n3\n aezY 0\n1 1u, f2 +\n-ex + iey\n12\n Y 1\n1 1u, f2 +\nex + iey\n12\n Y \n -1\n1  1u, f2b. \n(14.82)\nThus, the dot product en~rn is proportional to spherical harmonics of order 1. This key point derives \nfrom making the electric dipole approximation. Higher-order multipole matrix elements involve \nhigher-order spherical harmonics and hence yield different selection rules.\nUsing Eq. (14.82), we ﬁnd that the angular integral of the electric dipole matrix element in \nEq. (14.80) becomes three integrals, each of which is an integral of the product of three spherical \n harmonics:\n \nL\nY mf*\n/f 1u, f2Y m\n1  1u, f2Y mi\n/i 1u, f2d\t, \n(14.83)\nwhere one spherical harmonic is limited to order 1 by the electric dipole approximation and the index \nm varies over 1, 0, –1 according to the three terms in Eq. (14.82). You would expect that such an \nintegral over three spherical harmonic functions would be difﬁcult to do. However, we now make use \nof the Clebsch-Gordan coefﬁcients from Chapter 11 on the addition of angular momenta. We found \nthere that a coupled angular momentum state can be expressed in terms of uncoupled states using \nthe Clebsch-Gordan coefﬁcients. For orbital angular momentum this means that one spherical har-\nmonic can be decomposed into products of pairs of other spherical harmonics. Given this knowledge \nof Clebsch-Gordan coefﬁcients, we ﬁnd the angular integral:\n \nL\nY mf*\n/f 1u, f2Y m\n1  1u, f2Y mi\n/i  1u, f2d\t = c 312/i + 12\n4p12/f + 12 d\n1\n2\n8/i1mi m0 /f mf98/i1000 /f 09. (14.84)\nThe Clebsch-Gordan coefﬁcient 8/i1mi m0 /f mf9 is the key to understanding the selection rules. \nWe know from Chapter 11 that only certain values of the coupled angular momentum quantum \n\n464 \nTime-Dependent Perturbation Theory\nnumbers are allowed for a given set of uncoupled angular momentum quantum numbers, and that \nmany entries in the tables of the Clebsch-Gordan coefﬁcients are zero. The Clebsch-Gordan coefﬁcient \n8/i1mi m0 /f mf9 characterizes the addition of the uncoupled angular momenta j1 = /i and j2 = 1 to \nform the coupled angular momentum j = /f . The rules of adding angular momenta limit the values of \nthe coupled angular momentum /f to:\n \n/f = /i + 1, /i, /i - 1. \n(14.85)\nThe magnetic quantum numbers are also limited by the Clebsch-Gordan coefﬁcient 8/i1mi m0 /f mf9 to:\n \n mi + m = mf  \n \n m = mf - mi\n . \n(14.86)\nA further restriction on the matrix elements comes from a consideration of parity. The integrand \nin Eq. (14.84) must be even with respect to spatial symmetry inversion for the integral to be nonzero. \nThe spherical harmonics have parity given by 1-12/, that is\n \nY m\nl 1u, f2 = 1-12/\n Y m\nl 1p - u, f + p2. \n(14.87)\nFor the three cases allowed by Eq. (14.85), the integrand with /f = /i has odd parity (2 /i + 1), while \nthe integrand with /f = /i { 1 has even parity (2 /i or 2 /i + 2) . We conclude that the angular inte-\ngral with /f = /i is identically zero, and ﬁnd another rule:\n \n/f \u0002 /i\n . \n(14.88)\nWe say that /f = /i is not allowed by parity. Combining the rules in Eqs. (14.85), (14.86), and (14.88), \nwe ﬁnd the selection rules for electric dipole transitions:\n \n \u0006/ = {1\n \n \n \u0006m = 0,{1  . \n \n(14.89)\nThese selection rules arise solely from the angular integral in Eq. (14.80) and they reﬂect the conserva-\ntion of angular momentum of the system of atom and photon. The photon has an angular momentum \nor spin of 1. When the atom absorbs or emits a photon, the ﬁnal atomic state must reﬂect the change in \nangular momentum of the electromagnetic ﬁeld.\nApplication of the \u0006/ = {1 selection rule to the hydrogen atom limits the possible spontaneous \nemission transitions to those shown in Fig. 14.9. For example, within the n = 2 level, the 2p state can \ndecay to the 1s state, and does so with a 1.6 ns lifetime (Problem 14.7). However, the 2s state cannot \ndecay to the ground state because \u0006/ = 0 is not allowed. The Lamb shift does displace the 2s1>2 state \nslightly above the 2p1>2 state to which it can decay, but the transition rate is very small due to the cube \nof the Bohr frequency in Eq. (14.63). Hence, the 2s1>2 state has a very long decay lifetime of 1/7 sec, \nwhich is caused by a two-photon decay mechanism to the ground state.\nApplication of the \u0006m = 0,{1 selection rule to the hydrogen 2p S 1s transition yields the \nallowed transitions shown in Fig. 14.10, for the case of emission. In the \u0006m = +1 transition, the \natom gains one unit of angular momentum projection along the z-axis, so the emitted photon must \nhave one unit of angular momentum projection in the negative z-direction. Such a photon is called \n\n14.4 Selection Rules \n465\nmf \u0005\u00060\nmi \u0005\u0006\u00031\nmi \u0005\u00060\nmi \u0005\u00061\n1s\n2p\nΣ\u0003\n\bm \u0005\u0006\t1\n\bm \u0005\u0006\u00031\nΠ\nΣ\t\n\bm \u0005\u00060\nFIGURE 14.10 Hydrogen 2p S 1s transition.\n1s\n2s\n2p\n3s\n3p\n3d\n4s\n4p\n4d\n4f\n\u000b\nEnergy (eV)\n0\n\u00031\n\u00032\n\u00033\n\u00034\n\u00035\n\u00036\n\u00037\n\u00038\n\u00039\n\u000310\n\u000311\n\u000312\n\u000313\n\u000314\nFIGURE 14.9 Decay scheme of hydrogen for allowed electric dipole transitions.\na s- polarized photon or a photon with negative helicity. The \u0006m = -1 transition produces a \ns+ photon with positive helicity. The s+ and s- photons have a polarization vector that rotates \naround the z-axis and are also called circularly polarized states. The \u0006m = 0 transition produces a \nphoton that has linear polarization along the z-axis, which is referred to as a p polarized photon. For \nthe case of absorption, the \u0006m values in Fig. 14.10 change sign, but the s+ and s- labels remain \nunchanged (Problem 14.13).\n\n466 \nTime-Dependent Perturbation Theory\nSUMMARY \nIn time-dependent perturbation theory, we focus on ﬁnding the probability that an applied perturba-\ntion causes a transition between energy levels of the unperturbed Hamiltonian. In contrast, in time-\nindependent perturbation theory, we focus on ﬁnding the changes in energy levels caused by the \nperturbing Hamiltonian (assumed static).\nThe probability amplitude for a transition from the initial state 0 i9 to the ﬁnal state 0  f 9 subject to \nthe time-dependent perturbation H\u00041t2 is\n \ncf 1t2 = 1\niU L\nt\n0\n8 f  0 H\u00041t\u00042 0 i9ei(Ef-Ei)t\u0004\u0006U dt\u0004. \n(14.90)\nThe probability of the transition is\n \nPiSf 1t2 = 1\nU2 `\nL\nt\n0\n8 f  0 H\u00041t\u00042 0 i9ei(Ef-Ei)t\u0004\u0006U dt\u0004 `\n2\n. \n(14.91)\nFor harmonic perturbation at frequency v , the transition probability for long times grows linearly \nwith time and we deﬁne the transition rate\n \nRiSf = 2p\nU2  @ Vfi@\n2\n d1vfi - v2. \n(14.92)\nFor an electric dipole interaction, the transition rate is\n \nRiSf = Bif r1vfi2 \n(14.93)\nif the excitation source is broadband where r1vfi2 is the energy density and Bif is the Einstein B coefﬁcient\n \nBif =\npe2\n3e0\n U2 08 f 0 r0 i90\n2. \n(14.94)\nIf the transition is excited with a monochromatic source with intensity I = 2ce0E 2\n0, the transition rate is\n \nR1S2 = 3 I\nc\n B12 f 1v2, \n(14.95)\nwhere f 1v2 is the frequency response function of the atom.\nAn excited state in an atom has a ﬁnite lifetime due to spontaneous emission. The lifetime is the \ninverse of the Einstein A coefﬁcient\n \nA21 =\ne2v3\n21\n3pe0\n Uc3 0820 r0 190\n2. \n(14.96)\nElectric dipole transitions have the selection rules\n \u0006/ = {1\n \n \u0006m = 0,{1. \n \n(14.97)\n\nProblems \n467\nPROBLEMS \n 14.1 Use the perturbation series expansion of the coefﬁcient cn given by Eq. (14.11) in the dif-\nferential equation (14.10) and verify the zeroth-order and ﬁrst-order equations (14.12) and \n(14.13). You may wish to use the l notation of Chapter 10 to keep track of orders.\n 14.2 A particle of mass m is initially in the ground state (E1) of an inﬁnite square well of width L. \nStarting at time t \u0003 0, the system is subject to the perturbation\nH\u00041t2 = V0 x2e-t\u0006t,\n \n where V0 and t are constants. Find the probability that the energy after time T is measured to \nbe E2. Calculate the probability in the limit T S \u0005.\n 14.3 A particle of mass m is initially in the ground state (E1) of an inﬁnite square well of width L. \nStarting at time t = 0, the system is subject to the perturbation\nH\u00041t2 = V0 xe-at\n 2,\n \n where V0 and a are constants.\na) Find the probability that the energy is measured to be E2 in the limit t S \u0005.\nb) Find the probability that the energy is measured to be E3 in the limit t S \u0005.\n 14.4 A particle of mass m is initially in the ground state (E1) of an inﬁnite square well of width L. \nFrom t = 0 to t = T, the potential is perturbed so that it becomes\nV1x2 = •\nV0,\n0 ,\n\u0005,   \n0 6 x 6 L\u00062\nL\u00062 6 x 6 L\nelsewhere,\n \n where V0 << E1. Find the probability that the energy after time T is measured to be E2 .\n 14.5 Spontaneous emission causes the population of an atom to decay with the form e -t>t . The \nradiated electromagnetic power exhibits this same time dependence, but the ﬁeld has the form \ne -t>2t because the power is proportional to the ﬁeld squared. Calculate the Fourier transform \nof the emitted ﬁeld and take its complex square to ﬁnd the frequency spectrum of the radiated \npower in spontaneous emission. Convert this frequency spectrum to an energy spectrum and \nnormalize it to unity to verify the energy density of states in Eq. (14.68).\n 14.6 A hydrogen atom in its ground state is subject to an applied electric ﬁeld\nE = E 01xn + yn + zn2e-t\u0006t.\n \n Find the probabilities that after a long time the atom is found be in each of the four n = 2 \nstates.\n 14.7 Calculate the lifetime (in seconds) of each of the four n = 2  states of hydrogen 10 n/m92. The \nlifetime is the inverse of the spontaneous emission rate (Einstein A coefﬁcient).\n 14.8 A particle in a square well potential (with walls at  x = 0 and x = L;  that is, V1x2 = 0 for \n0 6 x 6 L; V 1x2 = \u0005 otherwise) starts out in the ground state\n0 c1t = 029 = 0 19,\n\n468 \nTime-Dependent Perturbation Theory\n \n where 0 n9 are the normalized eigenstates of the unperturbed Hamiltonian. Starting at t = 0, a \ntime-dependent perturbation is applied given by\nH =1x, t2 = V0 sin px\nL\n e-gt.\na) Calculate the probability for the particle to make a transition to an excited state 0 n91n \u0002 12 \nafter a long time. Deﬁne “long time.”\nb) Are there any selection rules for this transition? If so, what are they?\n 14.9 A particle in the harmonic oscillator potential V1x2 = 1\n2 mv2x2 starts out in the ground state\n0 c1t = 029 = 0 09,\n \n where 0 n9 are the normalized eigenstates of the Hamiltonian. Starting at t = 0, a time-\ndependent perturbation is applied given by\nH =1x, t2 = Ax3e -gt.\na) Calculate the probability for the particle to make a transition to an excited state \n0 n91n \u0002 02 after a long time. Deﬁne “long time.”\nb) Are there any selection rules for this transition? If so, what are they?\n 14.10 Consider two possible types of electric dipole transitions: a p S s transition and a p S d tran-\nsition. In each case, choose one allowed set of m quantum numbers and explicitly perform the \nangular integral in Eq. (14.84), then use the Clebsch-Gordan Table 11.5 to conﬁrm your result.\n 14.11 Use the result in Eq. (14.84) and the Clebsch-Gordan Table 11.5 to identify each possible \nelectric dipole transition from an initial p state. Identify the particular Clebsch-Gordan coef-\nﬁcient in Table 11.5 that represents the parity rule /f \u0002 /i.\n 14.12 A hydrogen atom starts in the state n = 4, l = 3, m/ = 3 , where we ignore the spin. What \npossible states will the atom go through as it decays to the ground state? What are the polar-\nizations of the photons that are emitted?\n 14.13 Draw a transition diagram like Fig. 14.10, but for absorption from 1s S 2p rather than emis-\nsion. Explain why the \u0006m values change sign but the s+ and s- labels remain unchanged.\n 14.14 A particle of mass m and charge q is conﬁned in a one-dimensional harmonic oscillator poten-\ntial of natural frequency v.\na) What are the selection rules governing spontaneous emission from excited states?\nb) Which states can decay directly to the ground state?\nc) Find the spontaneous emission rate from the ﬁrst excited state to the ground state.\nd) Calculate the lifetime of the ﬁrst excited state for an electron bound in a potential with \nv = 1015 rad>s.\nRESOURCES \nFurther Reading \nMore details on atom-light interactions can be found in:\nA. Corney, Atomic and Laser Spectroscopy, Oxford: Clarendon Press, 1977.\nC. J. Foot, Atomic Physics, Oxford: Oxford University Press, 2005.\nM. Fox, Quantum Optics, Oxford: Oxford University Press, 2006.\nR. Loudon, Quantum Theory of Light, Oxford: Oxford University Press, 2000.\n\n \n469\nC H A P T E R \n15\nPeriodic Systems\nIn this chapter, we explore the energy eigenvalues and eigenstates of a periodic series of potential \nenergy wells, as shown in Fig. 15.1, with the purpose of creating a rudimentary model of a solid. In \nthis model, a single well represents an atom, and the chain of wells represents a molecule or a solid. In \nChapter 5 we explored the energy eigenstates of the ﬁnite square well potential, which is a single ele-\nment of the periodic series shown in Fig. 15.1(a). We found that there were a ﬁnite number of bound \neigenstates and a continuum of unbound eigenstates, and that the shape of the well changed only the \ndetails of the shape of the wave functions and shifted the eigenvalues slightly. The hydrogen atom, \nwhich we studied in Chapter 8, is another example, schematically depicted as an element of the series \nin Fig. 15.1(b). It also has bound states (but an inﬁnite number) and continuum states. It is a three-\ndimensional problem, rather than the one-dimensional problem we will consider here. The similarities \noutweigh the differences, and the basic features of the band structure of a solid appear when we string \nseveral such “one-dimensional atoms” together to model a solid.\nOur model uses an approximate approach that emphasizes the interaction between neighboring \natoms. We will ﬁnd out how the eigenstates of the periodic potential (or molecule or solid) can be con-\nstructed from the eigenstates of the single elements of the periodic potential (or atoms). We will also \nlearn that the eigenstates of a solid are characterized by a wavelength, and that the energies of those \neigenstates form bands centered near the atomic energy eigenvalues. The approximation presented \nhere is a powerful method that is widely used in solid state physics and chemistry, where it goes under \nthe name of tight-binding or LCAO (Linear Combination of Atomic Orbitals). The LCAO approach \nis intuitive, starting from the easily understood atomic orbitals, and building molecular orbitals by \nconsidering how the atoms interact. We introduced the LCAO method in Chapter 13 to model the H +\n2 \nmolecular ion.\nIt is also possible to ﬁnd the energy eigenvalues and eigenstates by directly solving the energy \neigenvalue equation, and we will discuss this approach at the end of the chapter. The problem \npresented here is a single-particle problem whose solution is the possible states of a single electron \nsubject to the periodic potential, so we do not concern ourselves with the identical-particle aspects \nof the problem discussed in Chapter 13. Despite this gross oversimpliﬁcation, the results are surpris-\ningly robust if we simply assume that subsequent electrons would occupy these same states, subject to \nthe Pauli exclusion principle. This independent electron approximation is sufﬁcient to explain the \npresence of energy gaps in the energy-level structure of real solids, and to provide a basis for \nunderstanding the concept of the density of levels and the rudiments of electron transport.\nOur goal is to gain a basic understanding of an energy band diagram and density of states \nplot of a solid, such as depicted in Fig. 15.2 for the semiconductor Si. The plot on the left is simply \nan energy spectrum—a plot of the allowed energies of an electron. The difference between this \nplot and the energy spectrum for atomic hydrogen, say, is that the horizontal axis represents a new \n\n470 \nPeriodic Systems\nmomentum variable associated with the eigenstate that arises because of the periodic nature of the \npotential. The graph on the right of Fig. 15.2 is a density of states plot. It represents the relative \nnumber of allowed states (per unit energy) at each energy, regardless of the momentum variable. \nSuch plots help us determine whether materials are metallic or insulating, and tell us something \nabout the optical and electronic transport properties of the solid. We begin by tackling a one-\ndimensional periodic potential, which will lead us to a simple version of one panel of the band \nstructure plot in Fig. 15.2.\n(a)\n(b)\n2a\n1a\n3a\n4a\n5a\nx\nV(x)\nV(x)\nE\n1a\n2a\n3a\n4a\n5a\nx\nE\nFIGURE 15.1 Chain of periodic wells: (a) square wells, (b) Coulomb wells.\ng(E)\nSilicon Band Structure and Density of States\nW\nL\n\f\nX W K\n\u000312\n\u000310\n\u00038\n\u00036\n\u00034\n\u00032\n0\n2\n4\n6\n8\nEnergy (eV)\nFIGURE 15.2 Band structure and density of states of Si.\n\n15.1 The Energy Eigenvalues and Eigenstates of a Periodic Chain of Wells \n471\n15.1 \u0002   THE ENERGY EIGENVALUES AND EIGENSTATES OF A PERIODIC \nCHAIN OF WELLS\nOur goal in this section is to use the LCAO method to ﬁnd approximate solutions to the energy \neigenvalue equation for a chain of periodic wells. We’ll study the one-dimensional chain of square \nwells shown in Fig. 15.1(a) as our model system. We’ll solve this problem exactly later in the chapter, \nbut the approximate LCAO method is sufﬁcient to illustrate most of the important features of a peri-\nodic system and is also more revealing. To get started, we’ll study a chain with two square wells, and \nthen we’ll solve the N-well problem.\nIn the LCAO method, we regard each individual well as an “atom.” We assume that we have \nalready solved the energy eigenvalue equation for one isolated well and so we know the energy eigen-\nvalues and the eigenstates, which we refer to as the “atomic” energies and states. For example, in \nChapter 5, we solved the energy eigenvalue equation of the ﬁnite square well. The eigenstates for two \ndifferent square wells are shown in Fig. 15.3. In our discussions, we won’t need more than the lowest \ntwo states in the well, so we’ll label them as ground (g) and excited (e) states to simplify the notation. \nWe will use kets 0 g9 and 0 e9 or wave functions w g1x2 = 8x0 g9 or we1x2 = 8x0 e9 as appropriate.\n15.1.1 \u0002   A Two-Well Chain\nThe simplest system with more than one well is the “chain” of two wells, depicted in Fig. 15.4. We’ll \nmake the problem even simpler and assume that each individual well has just one possible bound state, \nas in Fig. 15.3(a). Our goal is to solve the energy eigenvalue equation\n \nH0 c9 =\n E0 c9, \n(15.1)\nwhere the Hamiltonian H includes the usual kinetic energy of the single electron and the potential \nenergy depicted in Fig. 15.4. The two wells (atoms) are separated by a distance a (the interatomic \nspacing). The ket 0 c9 represents an eigenstate of the two-well Hamiltonian, and E is the corresponding \nenergy. We refer to 0 c9 as a “molecular” eigenstate.\nThe central idea of the LCAO or “interacting atoms” approach is to represent the system state \nvector 0 c9 in the basis of the “atomic” states that are the solutions to the energy eigenvalue problem \nfor a single isolated well. In the simpliﬁed case that we are considering, the only two atomic states in \n(a)\n(b)\n0\n\u0003b/2\nb/2\n0\n\u0003b/2\nb/2\nx\nx\nEg\nEe\nEg\n\rg(x)=\u0006\u0004x\u0002g\u0003\n\re(x)=\u0006\u0004x\u0002e\u0003\n\rg(x)=\u0006\u0004x\u0002g\u0003\nFIGURE 15.3 Finite square well and bound energy eigenstates for cases with (a) one energy \nlevel, and (b) two energy levels. These eigenstates are the basis for the eigenstates of the full \nperiodic Hamilitonian.\n\n472 \nPeriodic Systems\nthe system are the ground states of the two wells, centered on wells 1 and 2. We could label them as \n01, g9 and 0 2, g9, but we immediately simplify the notation to 019 and 0 29 because there is only one state \nper well. The wave functions representing these states are identical [as in Fig. 15.3(a)] except that they \nare displaced by a from each other:\n \n 019 \u0003 w g 1x - 1a2  \n \n 029 \u0003 w g 1x - 2a2. \n \n(15.2)\nThe LCAO method assumes that the molecular state is a linear combination of the known atomic \nstates:\n \n0 c9 =\n  c10 19 + c20 29. \n(15.3)\nThe beauty of the LCAO method is that we use the already known atomic wave functions as the \npreferred basis, so we solve the energy eigenvalue equation with the matrix approach rather than \nthe differential equation approach used to ﬁnd the atomic states. This is clearly an approximation \nbecause we expect the spatial wave functions to be altered by the new potential conﬁguration, but the \nresults are quite good in many cases.\nFor the two-atom chain, there are only two atomic states, so the matrix representing the Hamiltonian \nof the system is a 2*2 matrix. This matrix has the form\n \nH \u0003 aa\nb\nb\nab . \n(15.4)\nThe matrix elements of the Hamiltonian are\n \na =\n H11 =\n H22 = 810 H019 = 820 H0 29  \n \nb =\n H12 =\n H21 = 810 H0 29 = 820 H0 19. \n \n(15.5)\nThe two diagonal terms are equal and the two off-diagonal terms are equal because of the symmetry \nof the two-well chain. The parameters a and b are straightforward to calculate given the atomic states \nand will depend on the well depth and the spacing. We can proceed with this problem without actu-\nally calculating a and b—the important features of the band structure will be perfectly clear without \nknowing their values. However, as a physicist, you ought to be very interested in knowing how to \ncalculate them and in knowing what they mean. We’ll pursue these calculations in Section 15.8 and \nas homework problems. It turns out that the diagonal matrix elements a are approximately equal to \nthe energy of the atomic state. The off-diagonal matrix elements b are related to the probability for an \nelectron to move between the wells, and so are referred to as “hopping” matrix elements.\nWe have now reduced the two-well problem to a two-dimensional Hilbert space comprising \nthe ground atomic states, and we proceed to ﬁnd the molecular eigenstates and eigenenergies by \nx\na\n1\n2\nFIGURE 15.4 Two square wells with separation a.\n\n15.1 The Energy Eigenvalues and Eigenstates of a Periodic Chain of Wells \n473\ndiagonalizing the Hamiltonian in Eq. (15.4). You have already diagonalized many 2*2 matrices, so \nwe’ll skip the details. The energy eigenvalues are\n \nE+ = a + b\nE- = a - b\n \n(15.6)\nand the energy eigenstates are\n \n0 c+9 =\n1\n12\n 019 +\n1\n12\n 0 29 for E+ = a + b  \n \n0 c-9 =\n1\n12\n 019 -\n1\n12\n 0 29 for E- = a - b . \n \n(15.7)\nThere are two molecular states, one a symmetric and equal superposition of the two atomic states, \nand the other an equal and antisymmetric combination. The energies of the two molecular states are \ndisplaced from the energy a, by an energy b, and the sign of b determines which state has the higher \nenergy. For the two square-well chain, b 6 0 and the lower-energy state (called the bonding orbital) \nis the symmetric combination, and the higher-energy state (called the antibonding orbital) is antisym-\nmetric. Figure 15.5 depicts the level scheme and the wave functions for this two-well system. This \nsolution is reminiscent of degenerate perturbation theory in that the coupling b between the two states \nlifts the degeneracy of the atomic states. The resultant molecular states are similar to the hydrogen \nmolecular ion states we found in Eq. (13.87).\n15.1.2 \u0002 N-Well Chain\nNow consider a system with N one-dimensional wells as depicted in Fig. 15.1. As N increases beyond 2,\nthe molecule contains more and more atoms, and eventually there will be enough to think of it as a \nsolid. In this language, a solid is just a giant molecule, and we’ll continue to refer to the eigenstate of \nthe periodic system as the “molecular state” (as distinct from the “atomic state” of the isolated well). \nLet’s continue to assume that each isolated well has only one bound atomic state, so for the N-atom \nchain, there are N atomic states, which we label with their location as 0 n9. The molecular state is the \nlinear combination of atomic states\n \n0 c9 = a\nN\nn=1\ncn0 n9, \n(15.8)\nΑ\nΑ\u0003Β\nΑ\tΒ\n1a\n2a\nx\n1a\n2a\nx\nFIGURE 15.5 Two atomic states combine to form two molecular states. The bonding \nstate is symmetric and the antibonding state is antisymmetric if b 6 0.\n\n474 \nPeriodic Systems\nand the matrix representing the Hamiltonian of the system is an N*N matrix. We now make one addi-\ntional assumption. We assume that the “hopping” matrix elements b are zero unless the two wells are \nadjacent. This nearest-neighbor approximation is easily relaxed, but doing so gives little new physi-\ncal insight and increases the algebraic complexity. With this new assumption, the matrix representing \nthe Hamiltonian is an extension of the two-well Hamiltonian [Eq. (15.4)] with the b terms adjacent to \nthe main diagonal:\n \nH \u0003 ¶\na\nb\n0\n0\ng\nb\na\nb\n0\ng\n0\nb\na\nb\ng\n0\n0\nb\na\ng\nf\nf\nf\nf\nf\n∂ . \n(15.9)\nThe nonzero matrix elements are\n \n a = Hnn\n = 8n0 H0 n9\n \n b = Hn,n{1 = 8n0 H0 n{19. \n(15.10)\nFor small values of N, the Hamiltonian in Eq. (15.9) can be diagonalized either analytically or \nusing a computer to ﬁnd the energy eigenvalues and eigenstates just as we did for the N = 2 case in \nthe last section. We’ll leave that approach to the homework problems. For large N, we use a different \nsolution technique that gets at the heart of the problem. Using the matrix in Eq. (15.9), we express the \nenergy eigenvalue equation H0 c9 =\n E0 c9 as\n \n¶\na\nb\n0\n0\ng\nb\na\nb\n0\ng\n0\nb\na\nb\ng\n0\n0\nb\na\ng\nf\nf\nf\nf\nf\n∂ ¶\nc1\nc2\nc3\nc4\nf\n∂= E ¶\nc1\nc2\nc3\nc4\nf\n∂ . \n(15.11)\nThis leads to the equations\n \nac1 +  bc2 = Ec1 \nbc1 +  ac2 +  bc3 = Ec2 \nbc2 +  ac3 +  bc4 = Ec3 \nbc3 +  ac4 +  bc5 = Ec4 .\n \n \n(15.12)\n \nf \nThe ﬁrst equation and the last equation (not shown) are different, but all the other equations have the \nidentical form\n \nbcp-1 + acp + bcp+1 = Ecp . \n(15.13)\nFor now, we focus on solving this equation and ignore the different endpoint equations that we’ll \ncome back to in the next section.\n\n15.1 The Energy Eigenvalues and Eigenstates of a Periodic Chain of Wells \n475\nThe mathematical form of Eq. (15.13) is identical to the equation of motion of a collection of \nmechanical oscillators, each coupled to its two nearest neighbors, as in a beaded string. We use the \ntechnique of normal mode solutions to solve Eq. (15.13) for the coefﬁcients cp and the energy E. The \nnormal-mode approach assumes wavelike solutions of the form\n \ncp = Aeipka. \n(15.14)\nIn Eq. (15.14), p is an integer from 1 to N that labels the atomic state, and each molecular eigenstate \ncorresponds to a different set of N coefﬁcients 1c1, c2 , ... cN2. The parameter a is the separation of the \nﬁnite wells as shown in Fig. 15.1. The values of k and A are unknown for the moment; we have yet to \ndetermine them.\nSubstitute Eq. (15.14) into Eq. (15.13) to get\n \nbAei( p -1)ka + 1a - E2  Aeipka + bAei( p+1)ka = 0 , \n(15.15)\nand factor out eipka to obtain\n \nbe-ika + 1a - E2 + be+ika = 0 . \n(15.16)\nNow solve for the eigenstate energy E, making use of the Euler relation, to ﬁnd the dispersion relation\n \nE = a + 2b cos1ka2. \n(15.17)\nThe dispersion relation is plotted in Fig. 15.6 for k a continuous variable, which is the case when there \nare very many atoms, as we will discuss later.\nNotice that we assumed a form for the coefﬁcients cp in Eq. (15.14) and ended up solving for the\nenergy! Before we return to the coefﬁcients (which amounts to pinning down the values of A and k),\nwe will take some time to discuss the dispersion relation, which contains a great deal of informa-\ntion. First, notice that the energy eigenvalue of the molecular state is determined by k, so k labels \nthe molecular eigenstate. We’ll ﬁnd out in Section 15.2 what values k may have. The energy E is \nperiodic in k with period 2p>a, so clearly there is some redundancy in the information. Second, \nthe values of E are bounded above and below, as indicated by the limits a - 2b and a + 2b in\nFig. 15.6. The fact that there is a band of allowed energies is one of the most important char-\nacteristics of a solid that is replicated by our model. The progression from one atomic energy to \ntwo molecular energies to a band of energies for the one-well, two-well and N S \u0005 well cases, \n\u0003 3Π\na\n0\nk\nΑ\nΑ\u00032Β\nΑ\t2Β\nE(k)\n3Π\na\n\u0003 2Π\na\n2Π\na\n\u0003 Π\na\nΠ\na\nFIGURE 15.6 Energy eigenvalues as a function of wave vector, k, for an \ninﬁnite chain of wells, with b 6 0.\n\n476 \nPeriodic Systems\nrespectively, is shown in Fig. 15.7. Third, the band width is 4b, which indicates that the stronger \nthe interaction between neighboring states, the wider the resulting band. Recall that b is the matrix \nelement of the Hamiltonian evaluated between neighboring states and is therefore an indication of the \ninteraction strength. In real solids with the same crystal structure, those with smaller lattice parameters \nhave wider bands because atomic wave functions can overlap more efﬁciently.\nIt is now time to look more closely at k. Is it continuous or discrete, and what values does it take? \nWhat does k represent? Once we know k, we have complete knowledge of E from Eq. (15.17), and \nalmost complete knowledge of the cp [we still need A in Eq. (15.14)] and, hence, of the state vector \nfrom Eqs. (15.8) and (15.14).\n15.2 \u0002   BOUNDARY CONDITIONS AND THE ALLOWED VALUES OF k\nWe introduced the quantity k in Eq. (15.14) cp = Aeipka as an undetermined constant in the coefﬁ-\ncient of the atomic states 0\n p9 that contribute to the molecular state 0 c9. k serves to label the molecular \neigenstate under consideration. In fact, if we had anticipated the need for such a label, we might have \nwritten c k\np, with the interpretation that c k\np is the contribution of the pth atomic state to the kth molecular \neigenstate. Because k appears in the exponential function in combination with the real-space length a, \nit must have dimensions of inverse length. We often refer to the set of k values as “k space,” a recipro-\ncal space to the real space that we are used to. We must apply real-space boundary conditions to deter-\nmine which values k may assume. For solids containing a huge number of atoms 11022 cm-32, it is best \nto use periodic boundary conditions, as illustrated in Fig. 15.8. One can think of bending the linear \n1 well\n2 wells\nN wells\nFIGURE 15.7 The development of a band of energies from a discrete atomic energy as \nthe number of wells increases.\n1\n2\n3\n4\n5\n6\n7\n10\na\n9\n8\nFIGURE 15.8 Periodic boundary conditions for a 10-well chain.\n\n15.2 Boundary Conditions and the Allowed Values of k \n477\nchain of atoms into a ring, so that the 1st atom and the Nth atom are now neighbors. The next atom after \nthe Nth is the (N + 1)th, which is the same as the 1st. The physical consequence of this procedure is to \nremove the effect of the boundaries, (i.e., the surface of the solid). This effectively makes the ﬁrst and \nlast equations of the set in Eq. (15.12) identical to all the other equations and justiﬁes the neglect of the \nendpoint equations in the last section.\nThe periodic boundary condition amounts to writing\n \ncn=1 = cn=N+1 , \n(15.18)\nwhich, using Eq. (15.14), is equivalent to\n \neiNka = 1 . \n(15.19)\nThis condition is satisﬁed for\n \nNka = q 2p 1 kq = q\nN\n  2p\na\n , \n(15.20)\nwhere q is yet another integer. It is important that the integer q is not the label of the atomic states. \nIt deﬁnes the allowed values of k, which labels the molecular states. There are N physically distinct \nmolecular states that result from the N different k values corresponding to q = 1, 2, ... N. We get \nthe same set of N molecular states for the set of q values q = N + 1,  N + 2, ... 2N or for the set \nq = -N>2, -N>2 + 1, ... N>2, or indeed for any N consecutive integers. Now we write the disper-\nsion relation as\n \nEkq = a + 2b cos1kq a2, \n(15.21)\nwhich is plotted in Fig. 15.9 for N = 20 and b 6 0, for k corresponding to the set of integers \nq = -  N>2, -N>2 + 1, ... N>2. We’ll see in Section 15.6 that values of k outside this range yield \nexactly the same wave functions and energies, and hence give no new information.\nThe allowed values of k are separated by\n \n\u0006k = kq - kq-1 = 2p\nNa\n . \n(15.22)\n0\nk\nE(k)\n1st Brillouin zone\n\u0003 4Π\n5a\nΑ\nΑ\u00032Β\nΑ\t2Β\n4Π\n5a\n\u0003 Π\n5a\nΠ\n5a\n\u0003 Π\na\nΠ\na\n\u0003 2Π\n5a\n2Π\n5a\n\u0003 3Π\n5a\n3Π\n5a\nFIGURE 15.9 Dispersion relation for a chain of 20 wells. Circles represent k values that give \ndistinct  eigenstates.\n\n478 \nPeriodic Systems\nThe quantity Na, the product of the number of atoms and the interatomic spacing, is the (real-space) \nlength of the solid, which we can call L. L usually has macroscopic dimensions (mm, mm, or cm) com-\npared with the Å to nm scale for a. Because\n \n\u0006k = 2p\nL\nV 2p\na\n , \n(15.23)\nthe k spacing is very much smaller than the range of k values, k may be considered a continuous quan-\ntity for most practical purposes in a macroscopic solid, and we can consider the dispersion relation a \ncontinuous function\n \nE 1k2 = a + 2b cos1ka2  . \n(15.24)\n15.3 \u0002  THE BRILLOUIN ZONES\nAny set of N consecutive integers could be used to designate the N distinct k values in Eq. (15.20). The \nset of N integers that gives k values closest to zero deﬁnes the ﬁrst Brillouin zone in k-space. This \nset is q = -N>2, ... N>2, which means that the ﬁrst Brillouin zone extends from -p>a to +p>a in\nk space, and it is this set that is shown in Fig. 15.9. Including the state q = 0, it would seem there are \nN + 1 states, but because of the periodicity, the state at k = -p>a is the same as the one at k = +p>a, \nso there are exactly N distinct states for the N-atom chain. The width of the zone is 2p>a, which illus-\ntrates a fundamental relationship between real space and reciprocal space—if the interatomic spacings \nare large (small) in real space, then the corresponding Brillouin zones are small (large) in k space.\nOne can also deﬁne the set of N integers that give k values larger than any in the ﬁrst Brillouin \nzone, but otherwise closest to zero. This set is q = N>2, ... N and -N>2, … -N and the corresponding \nk values form the second Brillouin zone. The second Brillouin zone is the same size as the ﬁrst, but it \nis not contiguous. A similar procedure deﬁnes higher-order zones. A slightly modiﬁed process deﬁnes \nBrillouin zones in two and three dimensions, where again, higher-order zones have the same area or \nvolume as the ﬁrst zone, but are not contiguous regions of k space.\nWe have not yet speciﬁed what k actually represents, and this will become clearer in Section 15.6. \nThe quantity Uk has the dimensions of momentum, and it is often called the crystal momentum or \nquasimomentum, so called because it deﬁnes the wavelength of the envelope of the molecular wave \nfunction. In this respect, k is similar to the quantity of the same name that we studied in Chapter 5,\n \nkconventional = 22m 1E - V2>U2 , \n(15.25)\nwhich deﬁnes the (local) wavelength of the electron wave function.\nNext, we should use the allowed values of k to ﬁnd the sets of coefﬁcients that determine the speciﬁc \ncontributions of each atomic orbital to each molecular orbital, and draw real-space representations of the \nmolecular orbitals. Before we do this, let’s make a short digression to describe how the LCAO approach \ndescribed above that yielded the dispersion relation [Eq. (15.21) or (15.24)] for a single atomic state per \nwell plays out if we choose well parameters that allow two or more atomic states per well.\n15.4 \u0002  MULTIPLE BANDS FROM MULTIPLE ATOMIC LEVELS\nReal solids have multiple bands of energies, not just one. The dispersion relation Eq. (15.21) resulted \nwhen we considered the interaction between the single levels in neighboring atoms in the periodic \nsystem (these might be considered analogous to the 1s ground state of a hydrogen atom). To model the \neffects of higher-energy atomic states, we must include them in the basis set. The basis would include \nthe ground state 0 g9 and the ﬁrst excited state 0 e9 for example, if we used two atomic levels per atom, \n\n15.4 Multiple Bands from Multiple Atomic Levels \n479\nfor a total of 2 * N atomic states in the basis. We would designate them 0 n, g9 and 0 n, e9. The integer \nn labels the atom or individual well, while the second designator labels the state within that well. The \nresult would be the formation of additional energy bands with dispersion relations resembling the \none we calculated in the previous section. In the nearest-neighbor approximation, the simplest case \nwould be represented by the Hamiltonian matrix HA in Fig. 15.10 below, where there is no interaction \nbetween the ground state of one well and the excited state of the adjacent well. In that case, there are \ntwo bands formed with dispersion relations exactly like Eq. (15.24), but with different band centers \n1ag and ae2 and bandwidths 1bg and be2 for each band.\nFigure 15.11 shows the allowed energies for a periodic system with two atomic levels per atom at \nenergies ag = 2 and ae = 10, and b-values of bg = -1 and be = +2 in the same energy units. This \nchoice makes the upper band twice as wide as the lower, and puts the maximum energy of that band \nat k = 0. The allowed k values are the same as before, kq = 2pq>Na, but now there are two possible \nenergy eigenvalues for each value of kq, E (g)\nq  and E (e)\nq , with the upper index labeling the band. In this \nexample, the b-values are smaller than the spacing between the atomic states 1ae - ag2, the bands \nremain separate from one another, and the resulting band structure is a series of branches of E(k) \ncurves deﬁning bands of allowed energies, separated by gaps of forbidden energies. This model quali-\ntatively explains the band gaps we observe in real solids that are so important in semiconductors, for \nexample. Band gaps are discussed further in Section 15.12.\nWhen we discuss the energy band structure of real materials, we often label bands by the atomic \nstates from which they are primarily derived. In Fig. 15.11, we might refer to the lower band as “the \nground state band” and the upper as the “excited state band” to acknowledge that the lower (upper) \nband eigenstates are linear combinations of the ground (excited) atomic states. In real solids, we speak \nof the “1s band,” the “3d band,” and so forth. In the next section, we discuss the composition of the \nmolecular states in more detail.\nIf there is a signiﬁcant interaction 1bge2 between the ground state of one atom with the excited \nstate of its neighbor, the Hamiltonian HB in Fig 15.10 is appropriate. The dispersion relation calcula-\ntion is a nice extension of the example presented here (Problem 15.4). The two bands each contain \nmixtures of both atomic states, rather than being derived exclusively from one or the other atomic state.\nThere are several software packages that ﬁnd the energy eigenvalues and eigenstates of \none-dimensional periodic potentials (see Resources). It is very instructive to use these to examine the \nenergy spectrum and see the bands, and investigate the effect of changing the number of wells, their \nshape, and their separation. Such packages usually plot wave functions and the associated probability \ndensities, too, and this is the topic of the next section.\n1g\n1g\n2g\n2g\n1e\n1e\n2e\n2e\nHA \nΑg\nΒg\n0\n0\nΒg\nΑg\n0\n0\n0\n0\nΑe\nΒe\n0\n0\nΒe\nΑe\nHB \nΑg\nΒg\n0\nΒge\nΒg\nΑg\nΒge\n0\n0\nΒge\nΑe\nΒe\nΒge\n0\nΒe\nΑe\nFIGURE 15.10 Hamiltonian matrices in the nearest-neighbor approximation for a \nperiodic chain of two wells with two states per well. HA describes a situation where the \nstates are well separated in energy and there is no interaction between the upper state of \none well and the lower state of the adjacent well. HB relaxes that assumption.\n\n480 \nPeriodic Systems\nFigure 15.11 begins to be reminiscent of the real band structure calculation that we began with in \nFig. 15.2. In Fig. 15.11, if we label the point k = 0 as \u000f and k = p>a as X, we begin to see the simi-\nlarity to the corresponding panel in Fig. 15.2. In Fig. 15.2, \u000f, X, W, and K are simply labels of different\nk values (but in three-dimensional reciprocal space rather than one-dimensional), and the correspond-\ning values of E are plotted for these directions in reciprocal space. There are several bands, some of \nthem overlapping, and some of them with the simple shape that we have found in our rudimentary \nmodel. Nowadays, real band structure calculations are performed with powerful computers and with \nmore sophisticated methods than the LCAO method discussed here, but the LCAO method allows us \nto understand and interpret such pictures rather well.\n15.5 \u0002  BLOCH’S THEOREM AND THE MOLECULAR STATES\nHaving calculated the energy eigenvalues via the dispersion relation Eq. (15.24), we now calculate \nthe molecular eigenstates from Eqs. (15.8) and (15.14). We return to the simple example of one \natomic state per well, where the kth molecular state is represented as a superposition of all the atomic \nstates 0 n9:\n \n0 ck9 = a\nN\nn=1\nAeinka0 n9. \n(15.26)\nWe have almost all the information we need. We know the atomic states 0 n9 and the allowed values of k. \nWe don’t yet know the value of A, nor have we tested that the proposed molecular state has all the prop-\nerties expected of an eigenstate of a periodic potential.\nThe constant A is easy to ﬁnd. We built into the assumption (15.14) that every atomic state \nmakes an equal contribution (in magnitude) to the molecular state. Why? Well, every atom is identi-\ncal (assuming periodic boundary conditions), so how could the magnitude of any one atomic state’s \ncontribution be different than that of any other? The atomic state coefﬁcients, then, must differ only by \na phase factor, and that phase factor is already reﬂected in the exponential in Eq. (15.14). We require \n0\nk\nΑg\nΑe\nE(k)\nEgap\n4Βe\n4Βg\n\u0004 Π\na\nΠ\na\nFIGURE 15.11 Dispersion relation showing two energy bands derived from two atomic \nstates per well. In this example, the parameters are such that the bands do not overlap, and \nthere is an energy gap where no states are allowed for any value of k.\n\n15.5 Bloch’s Theorem and The Molecular States \n481\nthat the molecular state be normalized, and because the atomic basis states are orthogonal (approxi-\nmately), the normalization condition is\n \n8ck0 ck9 = a\nN\nn=1\n0 cn0\n2 = a\nN\nn=1\n0 A0\n2 = 1. \n(15.27)\nIt is clear from Eq. (15.27) that the appropriate constant is A = 1> 1N, so that\n \n0 ck9 = a\nN\nn=1\n 1\n2N\n einka0 n9. \n(15.28)\nNow at last we have all the information we need to construct the eigenstate, and we will proceed \nto draw some pictures of the molecular wave functions; they are in Section 15.6, to which you can \nskip immediately if you like. However, we need to check that the eigenstate has all the properties we \nexpect. (It does, of course, otherwise we wouldn’t have gone to all this trouble!)\nThe structure of the solid or molecule is periodic, so the electron probability density, a measur-\nable quantity, must also be periodic. In Dirac notation, this condition is\n \n08x0 ck9 0\n2 = 08x + ma0 ck9 0\n2, \n(15.29)\nwhere m is an integer. In wave function notation, (i.e., the position representation), this condition is\n \n0 ck 1x20\n2 = 0 ck 1x + ma20\n2. \n(15.30)\nYou might be tempted to think that the wave function itself should be periodic, too, but that is too \nstringent a requirement and has no basis in measurement. But if the wave function satisﬁes the condition\n \nck 1x + ma2 = eimkack 1x2  , \n(15.31)\nit is easy to see that Eq. (15.30) is satisﬁed. The condition in Eq. (15.31) is one expression of Bloch’s \ntheorem in one dimension. Bloch’s theorem stems from the translational symmetry of the periodic \nsystem of potential energy wells. Bloch’s theorem can be generalized to two and three dimensions \nand it is a critical part of understanding any periodic system. For our purposes, the one-dimensional \nform will sufﬁce. Now we must ask if the molecular eigenstates of the periodic potential that we have \nconstructed from atomic eigenstates of the individual wells obey Bloch’s theorem. If they do, then we \nhave been successful, and Eq. (15.26) represents the molecular eigenstates. We perform the test with \nthe wave function representation of Eq. (15.26):\n \nck1x2 =\n1\n2N\n a\nN\nn=1\neinkawn1x2, \n(15.32)\nwhere\n \nwn1x2 = 8x0 n9. \n(15.33)\nAll the atomic wave functions have the same shape, but they are displaced from one another by an \ninteger number of well spacings. This statement is represented mathematically by the equation\n \nwn1x + ma2 = wn-m1x2. \n(15.34)\nEquation (15.34) says that if we take the atomic wave function belonging to the nth well and translate \nit backwards by m lattice spacings (that’s the left-hand side), it must look the same as the atomic wave \nfunction corresponding to the (n–m)th well (that’s the right-hand side), which of course is exactly true.\n\n482 \nPeriodic Systems\nWith this in mind, start with the left-hand side of Bloch’s theorem and use Eq. (15.32) with \nx - 7 x + ma:\n \n ck (x + ma) =\n1\n2N\n a\nN\nn=1\neinkawn (x + ma)\n \n \n =\n1\n2N\n a\nN\nn=1\ne inkawn  -   m (x)    [from Eq. 15.34] \n(15.35)\n \n =\n1\n2N\n a\nN-m\nn=1-m\ne i(n  +   m) kawn (x)  (n S n + m) \n \n  =\n1\n2N\n a\nN\nn=1\ne i(n + m) kawn (x)  (can start the count anywhere). \nThe last step is possible because, with periodic boundary conditions, we are summing over atomic \nsites around an N-member ring (c0 = cN, c1 = cN+1, etc.). It doesn’t matter where we start the sum as \nlong as we include N consecutive terms. The result is [using Eq. (15.32) again]\n \nck 1x + ma2 = eimka 1\n2N\n a\nN\nn=1\neinkawn1x2 = eimkack 1x2, \n(15.36)\nwhich clearly satisﬁes Bloch’s theorem [Eq. (15.31)] and guarantees that the probability density is \nperiodic, as it must be. In the next section, we’ll draw some pictures to appreciate the patterns in the \nmolecular wave functions.\n15.6 \u0002  MOLECULAR WAVE FUNCTIONS—A GALLERY\nWe can draw the molecular wave functions of Eq. (15.32) if we know the atomic wave functions. For \na chain of ﬁnite square wells, the atomic wave functions are the eigenstates of the ﬁnite well that we \nfound in Chapter 5. The wave functions shown in Fig. 15.12 are drawn using a schematic representa-\ntion of the ground state of a ﬁnite square well. We assume for now that b 6 0; we’ll show it later. We \ncontinue to use the periodic boundary conditions we introduced in Section 15.2. The molecular wave \nfunction corresponding to the lowest energy state has k = 0 and is\n \nc0 1x2 =\n1\n2N\n a\nN\nn=1\nei0awn1x2 =\n1\n2N\n 3w11x2 + w21x2 + w31x2 + w41x2 + ...4. \n(15.37)\nThis is a simple in-phase addition of each of the basis functions, which happens to be real if the basis \nfunctions are real. The plot is shown in Fig. 15.12(a). Notice that there are no nodes in this lowest \nenergy wave function, and in this regard, it is “s-like.”\nThe molecular wave function corresponding to the highest energy state has k = p>a (or  -p>a):\n \ncp>a 1x2 =\n1\n2N\n a\nN\nn=1\neinpwn1x2 =\n1\n2N\n 3w11x2 - w21x2 + w31x2 - w41x2 + ...4. \n(15.38)\nThis antiphase addition of the basis functions, shown in Fig. 15.12(c), also has only a real compo-\nnent. Notice that the envelope (dashed line) of the molecular wave function has a wavelength equal to \n2p>k = 2a, or twice the interatomic spacing. This is the smallest possible wavelength that could have \nphysical meaning. In contrast, the “wavelength” of the k = 0 state in Fig. 15.12(a) is inﬁnite.\n\n15.6 Molecular Wave Functions—A Gallery \n483\nFor every other value of k, the system wave function is complex, with both a real and an imagi-\nnary part. One more example will sufﬁce, and further examples for different values of k within the ﬁrst \nBrillouin zone are in the homework problems. For k = p>2a, the molecular wave function is\n \n cp>2a1x2 =\n1\n2N\n a\nN\nn=1\neinp>2wn 1x2\n \n \n =\n1\n2N\n 3w11x2 + iw21x2 - w31x2 - iw41x2 + w51x2 + iw61x2...4\n \n \n =\n1\n2N\n 3w11x2 - w31x2 + w51x2...4 +\ni\n2N\n 3w21x2 - w41x2 + w61x2...4. \n(15.39)\nNotice that the patterns of the real and imaginary parts are the same, and both have a wavelength cor-\nresponding to 2p>k = 4a as evident in Fig. 15.12(b). As a further homework problem, explore the \n(b)\n(c)\nRe[ΨΠ/2a(x)]\nΨΠ/a(x)\nIm[ΨΠ/2a(x)]\n(a)\n1a\n2a\n3a\n4a\n5a\n6a\n7a\n8a\nx\nΨ0(x)\n1a\n2a\n3a\n4a\n5a\n6a\n7a\n8a\nx\n1a\n2a\n3a\n4a\n5a\n6a\n7a\n8a\nx\n1a\n2a\n3a\n4a\n5a\n6a\n7a\n8a\nx\nFIGURE 15.12 Wave functions of a system of periodic wells corresponding to the states (a) k = 0,\n(b) k = p>2a, and (c) k = p>a. The envelopes of the wave functions have wavelength l = 2p>k.\n\n484 \nPeriodic Systems\nwave functions obtained when you choose two values of k that differ by 2p>a (i.e., values of k in two \ndifferent Brillouin zones).\nBecause k determines the wavelength of the envelope of the molecular wave function, we often \ncall it the wave vector of the state. In one dimension, k is a “vector with one component,” so it’s not \nobviously distinguishable from a scalar, but in two (three) dimensions, k has two (three) components. \nIn higher dimensions, the molecular wave function is\n \n0 c9k =\n1\n2N\n a\n \nR\neik~R0 R9, \n(15.40)\nand R is the vector that locates the atom or well in real space, and also labels the atomic state associ-\nated with the atom. In one dimension, the equivalent label is na.\nWe have now solved the problem we set out to solve, namely ﬁnding the energies and wave \nfunctions of the allowed states of the periodic potential. The eigenfunctions of the periodic system \ncan be written as linear combinations of the eigenfunctions of the individual wells making up the \nperiodic chain. The different linear combinations are labeled by an index k that describes the modu-\nlation of the envelope of the wave function in terms of a wavelength l = 2p>k. k is restricted to \ndiscrete values, but these values can be so closely spaced for large N that k can be considered a \ncontinuous variable. Unique molecular wave functions result from those k values that lie within the \nﬁrst Brillouin zone. The energy of the eigenstate labeled k is given by the dispersion relation in\nEq. (15.21) or (15.24). The energies are bounded, and are so closely spaced as to form a band. If there \nare multiple states in a single well, there are multiple energy bands in the periodic well, and the bands \nmay be separated by a relatively large energy gap, depending on the strength of the coupling between \nthe states of adjacent wells. The molecular eigenstates in a band are often primarily derived from one \nof the atomic eigenstates.\n15.7 \u0002  THE DENSITY OF STATES\nSome properties of a solid do not depend on the value of the wave vector k of a particular state but \nrather on the number of states in a particular energy range, a quantity that we refer to as the density of \nstates. The density of states g(E ) is easily visualized in the case where the allowed states are discrete, \nas shown in Fig. 15.13(a). Here, the dispersion relation E(k) (see Fig. 15.9) is rotated 90° to make E the \nhorizontal axis. To ﬁnd the density of states from the dispersion plot, slice the energy axis into equal \nintervals, count the number of states (dots) in each energy interval, and plot the result as a histogram, \nas shown in Fig. 15.13(b). The solid line in Fig. 15.13(b) shows the functional form of the density of \nstates in the limit that k becomes a continuous variable.\nIn one dimension, the number of states per unit energy is calculated rather easily from the density \nof states in k space, which we can call gk(k). If there are N wells in the periodic potential, and one state \nper well, then there are N molecular states, whose corresponding k values are evenly spaced between \n-p>a and p>a [Eq. (15.20)]. The state density in k space is\n \ngk1k2 =\nN\n2p>a =\nL\n2p\n , \n(15.41)\nwhere L = Na is the length (one-dimensional volume) of the chain of wells. It makes no difference \nwhether we count states according to their energy label or their k label, so it must be true that\n \ng1E2dE = 2gk1k2dk . \n(15.42)\n\n15.7 The Density of States \n485\nThe factor of 2 accounts for the fact that for every state in an interval dk there is another state of the \nsame energy at the opposite value of k. It follows that the density of states (in one dimension) is\n \ng1E2 = 2gk 1k2 dk\ndE = L\np dk\ndE\n  . \n(15.43)\nIn the example considered here, the dispersion relation gives\n \ndE\ndk = 2ba sin 1ka2, \n(15.44)\nresulting in\n \ng1E2 =\nL\n2pba sin 1ka2\n , \n(15.45)\nwhich is plotted in Fig. 15.13(b). The density of states is proportional to the one-dimensional vol-\nume L, and it is often preferable to work with the volume-independent quantity g(E )>L. Strictly, \nbecause g(E) is a function of E rather than k, we should express k in Eq. (15.45) in terms of E via \nEq. (15.17), but the expression becomes cumbersome and is not particularly enlightening.\n(a)\n(b)\nΑ\nΑ\u00072Β\nΑ\u00042Β\nE\n0\nΑ\nΑ\u00072Β\nΑ\u00042Β\nE(k)\n0\nk\ng(E)\n\u0004 Π\na\nΠ\na\nFIGURE 15.13 (a) The dispersion relation E(k) rotated 90° to make E the horizontal axis. The dots \nrepresent the allowed states for small N; the solid line represents the continuous case. (b) The density \nof states g(E) as a function of energy. The histogram corresponds to the small N case; the solid line \nrepresents the continuous case.\n\n486 \nPeriodic Systems\nNotice that the density of states diverges at the Brillouin zone boundary. This is a quirk of the \none-dimensional geometry, but it does not cause any unphysical results. For example, if we calculate \nthe total energy of all the states, we arrive at a ﬁnite result (Problem 15.6):\n \nETOT =\nL\nEmax \nEmin \nE g 1E2  dE = Na . \n(15.46)\nIn Fig. 15.2, the real density of states for Si is plotted alongside the band structure. There are no inﬁni-\nties, but there are some sharply peaked features that correspond to local band minima or maxima.\n15.8 \u0002 CALCULATION OF THE MODEL PARAMETERS\nThe parameters a and b were introduced in Eq. (15.5) as the matrix elements of the Hamiltonian in \na periodic potential and they later appeared in the dispersion relation E(k) as the band center (a) and \nthe band width (4b). However, at that time, we did not calculate their values in terms of the atomic \nwell parameters—the width, height, and separation. In this section, we do this calculation for the case \nwhere the individual wells are square wells.\nTo evaluate a and b, we must ﬁnd matrix elements of the Hamiltonian H of the full periodic sys-\ntem in the basis of the eigenstates of the Hamiltonian H0 of an isolated well. Both H and H0 contain the \nsame kinetic energy operator\n \nT = p2\n2m\n , \n(15.47)\nbut the potential energy term in H represents the full periodic potential [Fig. 15.14(a)], while the \npotential energy in H0 represents a single well [Fig. 15.14(b)]. The parameter a is the diagonal ele-\nment of the Hamiltonian matrix\n \na = 8n0 H0 n9. \n(15.48)\nWe rewrite this as\n \na = 8n0 H0 + V\u00040 n9, \n(15.49)\nwhere V\u0004 is the difference [Fig. 15.14(c)] between the full periodic potential and the potential energy of \na single well. That is, V\u0004 is the periodic potential with one well missing—the one corresponding to H0.\nLet’s suppose there is a single eigenstate in the isolated well with energy Eg (calculated according \nto the procedures in Chapter 5). Then\n \nH00 n9 =  Eg0 n9 \n(15.50)\nand\n \na = 8n0 H00 n9 + 8n0 V\u00040 n9 =  Eg + 8n0 V\u00040 n9. \n(15.51)\nThis calculation shows that a is equal to the energy of the isolated well eigenstate plus a term that \nis the matrix element of V\u0004 in the basis of the atomic states:\n \n8n0 V\u00040 n9 =\nL\n\u0005\n- \u0005\nw*\nn1x2V\u00041x2wn1x2dx . \n(15.52)\nThis matrix element is very small because it is the integral of a potential energy that is zero (i.e., missing \na well) exactly where the wave function wn1x2 is nonzero! Where V\u00041x2 is nonzero, the wave func-\ntion is very small. Figure 15.15 shows in graphical form what Eq. (15.52) says in symbolic form. For \nexample, for an electron bound in a well that is V0 = 1 eV deep and b = 0.35  nm wide, the single \nbound energy is 0.6 eV and the difference between a and Eg is -3.6 meV for a well spacing of a = 3b.\n\n15.8 Calculation of the Model Parameters \n487\nV\n(a)\n(b)\n(c)\n1a\n2a\n3a\n4a\n5a\n1a\n2a\n3a\n4a\n5a\nx\n0\nV0\nV\n0\nV0\nV\n0\n\u0004V0\nx\n1a\n2a\n3a\n4a\n5a\nx\nFull periodic potential\nSingle well potential\nDifference potential\nFIGURE 15.14 (a) The full periodic potential energy; (b) the atomic potential energy of an \nisolated well located at the position of atom 2; (c) V\u0004, the potential energy difference between \n(a) and (b).\n1a\n2a\n3a\n4a\n5a\nx\n0\n8x039\n8x039\n8x049\nb\u00078n0V'0m9\na\u0006Eg\u00078n0V'0n9\nV'\n\u0006V0\nFIGURE 15.15 Schematic representation of the terms in Eqs. (15.52) and (15.53). The wave \nfunction widths are exaggerated to show the overlap.\n\n488 \nPeriodic Systems\nThe evaluation of b is similar, except that n and m correspond to adjacent wells:\n \nb = 8n0 H0 m9 = 8n0 H0 + V\u00040 n { 19. \n(15.53)\nWe assume that the atomic states on adjacent atoms are nearly orthogonal because the wave function \noverlap is small, so the matrix element 8n0 H00 n { 19 is neglected and we ﬁnd\n \n b = 8n0 V\u00040 n { 19\n \n \n =\nL\n\u0005\n- \u0005\nw*\nn1x2V\u00041x2wn{11x2dx . \n \n(15.54)\nThis matrix element of V\u00041x2 is much larger than the one in Eq. (15.52) because where V\u00041x2 is non-\nzero, one of the atomic wave functions is large, and only one is very small. For the same parameters \ngiven above, b is -32 meV. This square-well example is the simplest integral to calculate analytically \nand you should do this for practice. Find the form of wn1x2 from Chapter 5, and perform the calcula-\ntion. A simple Mathematica or Maple program will allow you to generalize to more bands by using \ndeeper wells with more states.\n15.8.1 \u0002 LCAO Summary\n• The LCAO approach to ﬁnding the molecular wave functions ck1x2 and corresponding energies \nof a one-dimensional periodic potential of period a is to begin with the (atomic) wave functions \nwn1x2 of a single element of the potential. If there is one atomic state per well, there are N atomic \nstates in the basis and there are N molecular wave functions, each a different superposition of the \nN atomic states:\n \nck 1x2 =\n1\n2N\n a\nN\nn=1\neinkawn1x2 , \nwhere k labels the molecular state.\n• There are N values of k ranging from -p>a to p>a in steps of 2p>Na, where N is the number of \natoms/elements. This set of k values forms the ﬁrst Brillouin zone.\n• k has dimensions of inverse length and is called the wave vector. The associated wavelength, \nl = 2p>k, is the wavelength of the envelope of the molecular wave function.\n• The periodicity of the potential introduces translational symmetry into the problem. The result \nis that the molecular wave function obeys Bloch’s theorem ck1x + ma2  = eimka c1x2, which \nguarantees that the electron probability distribution is periodic, but does not require that the wave \nfunction itself is periodic.\n• The dispersion relation gives the energy of a molecular state k. In the nearest-neighbor approxi-\nmation, and when there is only one state per well,\n \nEkq = a + 2 b cos1kq a2. \nThese energies are effectively continuous if N is large.\n• The parameters a and b, matrix elements of the Hamiltonian, are\n \na = 8n0 H0 n9\nb = 8n0 H0 n { 19.\n \n\n15.9 The Kronig-Penney Model \n489\n• a is the band center and is approximately equal to the atomic state energy. b measures the \nstrength of the interaction between adjacent wells and 4b is the width of the band. Negative b \nputs the energy minimum at k = 0 and the maxima at the Brillouin zone boundaries, and vice \nversa for positive b.\n15.9 \u0002 THE KRONIG-PENNEY MODEL\nThe ﬁnal piece of the picture is to connect the LCAO approximation to the analytical, exact solution, \nwhich is possible for the simple case of a periodic chain of square wells. This example usually goes \nunder the name of the Kronig-Penney model.\nThe LCAO approximation lets us see the progression from the atomic wave functions and the \nenergy spectrum of isolated atoms to the band structure of a solid as the number of atoms becomes \nlarger and the interaction between the atoms becomes stronger. The Kronig-Penney model, on the \nother hand, simply solves the eigenvalue equation for the exact periodic potential. It is a more “cor-\nrect” approach, but lacks the intuitive connections to the atomic system. Moreover, in a real solid, the \nexact periodic potential is unknown, but the electronic energy levels and wave functions of atoms are \nnot too difﬁcult to calculate, so the LCAO model can be a good starting point. Figure 15.16 presents \nthe LCAO dispersion relation, Eq. (15.24), and the exact dispersion relation that we are about to ﬁnd \n[Eq. (15.61)], on the same plot. We see that the LCAO is a good approximation for the energies in the \nperiodic system, especially when the coupling between states in adjacent wells is not too large.\nSeveral excellent texts treat the Kronig-Penney example in great detail, and it is a good example \nto practice solving the energy eigenvalue equation. Here, we’ll present a very broad overview, and \nconcentrate on the energy spectrum rather than the eigenstates. The periodic potential V is sketched \nin Fig. 15.17 and all the relevant lengths and energies are deﬁned. The width of the well is b, the well \nspacing is a, and the well depth is V0. The bottom of the well is located at the zero of energy.\nThe eigenvalue equation is best solved in wave function notation (position representation), just as \nit was in Chapter 5 for the single ﬁnite well. The energy eigenvalue equation is the differential equation\n \n Hc 1x2 = Ec 1x2\n \n -  U2\n2m\n d 2\ndx2  c 1x2 + V 1x2c 1x2 = Ec 1x2. \n(15.55)\n0\nk\nΑ\nΑ\u00042Β\nΑ\u00072Β\nE(k)\n\u0004 Π\na\nΠ\na\nFIGURE 15.16 The dispersion relations for an N-well periodic system as \ncalculated by the LCAO model (solid) and by the Kronig-Penney model (dashed).\n\n490 \nPeriodic Systems\nE is the eigenvalue corresponding to the eigenfunction c1x2. The solution that follows is valid for all \nvalues of E 7 0, but in the end, we’ll be interested in the bound states, E 6 V0.\nWe need look only at a single element of the periodic potential, namely that for which \n-b 6 x 6 a -b, because Bloch’s theorem, Eq. (15.31), assures us that once we have found the solu-\ntion ck 1x2 in one element, then we can ﬁnd the solution ck 1x + ma2 for any other element. The \nsolutions to the energy eigenvalue equation in regions I and II are:\n \n cI 1x2 = Aeiqx + Be-iqx; q = 22mE\nU\n \n \n cII 1x2 = Ceikx + De-ikx; k =\n22m1E - V02\nU\n , \n \n(15.56)\nwhere A, B, C, and D are constants. A quick glance at Section 5.5 will refresh your memory if you’ve \nforgotten the procedure.\nThe wave function and its derivative must be continuous, and in particular at x = 0, the boundary \nbetween regions I and II:\n \n cI 102 = cII 102 1\n A + B = C + D\n \n \n c=\nI 102 = c=\nII 102 1\n q 3A - B4 = k 3C - D4. \n \n(15.57)\nThe wave function and its derivative at the edges of the well (one lattice spacing apart) are connected \nby Bloch’s theorem:\n \n eikacI 1-b2 = cII 1-b + a2 1\n Ae-iqb + Beiqb = e-ika  3Ce-ik(a-b) + Deik(a-b)4\n \n(15.58)\n \n eikacI \u00041-b2 = cII \u00041-b + a2 1\n q 3Ae-iqb - Beiqb4 = ke-ika  3Ce-ik(a-b) - Deik(a-b)4\n . \n \nEquations (15.57) and (15.58) are written succinctly in matrix form:\n \n•\n1\n1\n-1\n-1\ne-iqb\neiqb\n-e-ikae-ik1a-b2\n-e-ikaeik1a-b2\nq\n-q\n-k\nk\nqe-iqb\n-qeiqb\n-ke-ikae-ik1a-b2\nke-ikaeik1a-b2\nμ •\nA\nB\nC\nD\nμ = 0 . \n(15.59)\n\u0004b\n0\na\u0004b\nx\nV0\n0\nI\nII\nV\nFIGURE 15.17 Periodic potential parameters for the Kronig-Penney model.\n\n15.10 Practical Applications: Metals, Insulators, and Semiconductors \n491\nThere are nontrivial solutions to the set of Eqs. (15.59) only if the determinant of the 4*4 matrix is \nzero. It is an uncomplicated but rather long process to show that the solution to the secular equation is\n \n cos 1qb2cos 1k1a - b22 - q2 + k2\n2qk\n sin 1qb2sin 1k1a - b2 2 = cos 1ka2. \n(15.60)\nEquation (15.60) is valid for any E, but if E 6 V0, then k is imaginary, and it is common to recast it \nexplicitly in terms of real quantities:\n \ncos 1qb2cosh 10 k01a - b22 - q2 - k2\n2qk\n sin 1qb2sinh 10 k01a - b2 2 = cos1ka2.  (15.61)\nThe quantities q and k contain the energy E, so if we pick a value for k, we can invert Eq. (15.61) to \nﬁnd E(k). This task is best assigned to a computer! Figure 15.18 shows a graph of the allowed energies \nfor one particular choice of well parameters, and you can see the gaps in the energy spectrum, just as \nwe found previously, when we employed the LCAO approach. In Fig. 15.16, the lowest band is plotted \ntogether with the LCAO-derived band, to show the good agreement when the bands are not too broad.\nThis example illustrates that it is possible to solve the eigenvalue equation for the one-dimensional \nperiodic chain of potential energy wells without resorting to approximate methods like LCAO. In more \ncomplicated cases in many dimensions with many electrons, exact methods are impossible for practi-\ncal purposes and approximate methods are needed. This example gives a means to assess the degree of \nsuccess of the approximation method in a simple case. The main features are similar in both methods, \nbut the exact shapes of the dispersion relations differ in their details.\n15.10 \u0002  PRACTICAL APPLICATIONS: METALS, INSULATORS, AND SEMICONDUCTORS\nThe purpose of much of the work in this chapter was to produce a rudimentary model of a solid or \nmolecule. Remember though, that the problem that we have solved is for a single electron in a periodic \npotential, while real molecules and solids have very large numbers of electrons! For example, take \nthe case of just two wells—this might be a model of a diatomic molecule, say H2. However, we have \n0\nk\nE(k)\n\u0004 Π\na\nΠ\na\nFIGURE 15.18 Energy spectrum for the Kronig-Penney \nmodel of a periodic system.\n\n492 \nPeriodic Systems\nreally modeled H2 \n+, the hydrogen molecule ion, and neglected the effect that the other electron would \nhave had on the energy spectrum. We saw in Chapter 13 how to tackle aspects of this issue, but with-\nout resorting to such detail, a reasonable approximation is to assume that the states of the two-electron \nsystem would be about the same as the simple one-electron system, and that the ground state of the \ntwo-electron system would have both electrons occupying the ground state of the one-electron system, \nbut with opposite spin, so as not to violate the Pauli exclusion principle. If there are many electrons, \nwe would say that the ground state of the system is the conﬁguration where electrons occupy the \nlowest-possible-energy one-electron states, subject to the Pauli exclusion principle, [i.e., two electrons \nwith opposite spin per state (see Fig. 13.5)]. This simple assumption leads to a qualitative explanation \nof the occurrence of metals and insulators. It must be abandoned, though, to explain many interesting \nand important phenomena, like magnetism and superconductivity, where the effects of electron cor-\nrelation are too important to be neglected.\nFigure 15.19 schematically depicts two bands in a one-dimensional 20-atom “solid”. Circles rep-\nresent allowed states and the circles are ﬁlled if electrons occupy the state. Take sodium as an example, \nwhere, in Fig. 15.19(a), the lower band might represent the 3s band, while the upper might represent \nthe 3p band. Because there are 20 3s valence electrons and each state accommodates 2 electrons, only \nthe lowest 10 states in the 3s band are ﬁlled, and the band is half-full. (Don’t worry about the slight \ndifference in ﬁlling that results for the cases of even and odd numbers of wells—it’s not important \nin a large solid.) A half-ﬁlled band is the hallmark of a metal, as we discuss below. Figure 15.19(b) \nmight represent a “solid” of 20 He atoms, where we would need to accommodate 40 electrons in the \n1s band, and all states in the lower band are ﬁlled. A ﬁlled band is characteristic of an insulator. The \nsimple model correctly predicts that solid Na (along with any alkali metal) is metallic and solid He \n(or any solidiﬁed noble gas) is an insulator. This might seem like a trivial conclusion that we could \nhave reached much more simply just by considering the valence shell of the individual atom, but real \nsystems are far more complex.\nAn example of the complexity is given by solid hydrogen, which you might expect to be metal-\nlic similar to the (effectively) one-electron solids Na, K, etc. Normal solid hydrogen is insulating, \nbecause there is a structural distortion of the lattice that causes the 1s band to split in the middle, and \nthe H electrons completely ﬁll the lower band. Another important case where our model is too sim-\nplistic is that of the group IV elements, typiﬁed by silicon and represented in Fig. 15.19(c). A simple \n“valence shell” argument would predict that solid Si consists of ﬁlled 1s, 2s, 2p and 3s bands, and a \none-third ﬁlled 3p band, and hence is metallic. Wrong! If you worked out the a and b parameters for \nthe Si 3s and 3p states (in three dimensions of course), and included all these states in the calculation,\n(a)\n(b)\n(c)\n0\nk\n0\nk\nE(k)\nE(k)\nE(k)\n0\nk\n\u0004 Π\na\n\u0004 Π\na\n\u0004 Π\na\nΠ\na\nΠ\na\nΠ\na\nFIGURE 15.19 Schematic band diagrams: (a) metal, (b) insulator, (c) semiconductor. Circles represent allowed \nstates; they are ﬁlled if the state is occupied by an electron.\n\n15.10 Practical Applications: Metals, Insulators, and Semiconductors \n493\nyou would discover that, in fact, the 3s and 3p atomic states of all the atoms combine to form two \ndistinct hybrid bands separated by a small energy gap of about 1 eV. The lower band is completely \noccupied by the Si electrons (we call it the valence band). The upper band is empty (we call it the \nconduction band). In Fig. 15.2, the highest energy of the valence band is (arbitrarily) labeled zero. \nSo Si is an insulator at very low temperatures where electrons ﬁll the states strictly in energy order. At \nroom temperature, the thermal energy of about 0.025 eV is sufﬁcient to deplete the valence band of a \nsmall number of electrons and populate the conduction band. In that case, Si has two partially ﬁlled \nbands, so it is “metallic” (i.e., conducting), but very weakly so, because there are so few current carri-\ners compared to a metal. Si is therefore a semiconductor.\nWhy is it that a partially ﬁlled band is considered the signature of a metal and a ﬁlled band that of \nan insulator? To answer, we have to think about how to represent the motion of an electron in a solid \nunder the inﬂuence of an electric ﬁeld. The eigenstates of energy E(k) that we have derived have the \nproperty that an electron in such a state has an equal probability of being found on any atom in the \ncrystal (see Fig. 15.12). For consideration of the effects of electric ﬁelds on electrons, it is useful to \ntake a more “particle-like” point of view and represent the electron by a wave packet or superposition \nof eigenstates that concentrates the probability of ﬁnding the electron in a more restricted region of \nspace. The Heisenberg uncertainty principle is important here: in “localizing” the electron in a wave \npacket of extent \u0006x, we are conceding an uncertainty in the momentum \u0006p = h>\u0006x. This uncertainty \nis expressed by the range of k values of the Bloch states used to construct the wave packet.\nThe motion of an electron’s wave packet is characterized by a group velocity (see Chapter 6 for a \nreview). This is the velocity of the group of superimposed waves (i.e., the velocity of the envelope of \na pattern of interfering waves). The crests and troughs of individual waves travel at the phase veloc-\nity, which is not necessarily the same as the group velocity. For waves with a dispersion relation v(k), \nthe phase velocity is v>k while the group velocity is dv>dk. These are the same only if the dispersion \nrelation is linear in k, as is the case, for example, for long-wavelength sound waves in a solid.\nFor an electron in a Bloch state 0\n ck9, the electron velocity is the expectation value of p>m \n(momentum/mass), that is,\n \nve = 1\nm\n 8ck 0 p0\n ck9 = 1\nm\n \nL\n\u0005\n- \u0005\nc*\nk  1x2 a-iU d\ndx\n b ck1x2dx \n(15.62)\nin one dimension. If the electron energy dispersion relation is E(k), then the electron’s (group) velocity \nis (because E = U v)\n \nvg = 1\nU \ndE1k2\ndk\n. \n(15.63)\nWe will not carry this out, but it is possible to show that ve and vg are the same if 0\n ck9 are Bloch \nstates.\nNow consider what happens when an electric ﬁeld E = E xn is applied to the solid, for example, \nby attaching electrical leads to opposite ends of the crystal and connecting them to a battery. The elec-\ntrons experience a force F = qE = -eExn. During a short time interval dt in which the force acts, an \nelectron moves a distance vgdt and the work done by the force is\n \n dw = Fdx\n \n \n = -eEvgdt\n \n \n = - aeE\nU b adE1k2\ndk\nb dt . \n \n(15.64)\n\n494 \nPeriodic Systems\nAt the same time, that electron’s energy changes by an amount\n \ndE = dE\ndk\n dk . \n(15.65)\nSetting dw = dE, we ﬁnd\n \ndk = -  eE\nU\n dt . \n(15.66)\nIntegrating to get k(t), we have\n \nk 1t2 =\n k 102 -\n eE\nU\n t . \n(15.67)\nThe message here is that application of the electric ﬁeld tends to shift the k values, and hence \nthe energies E(k) of all the electrons in the material. But can this actually happen? It depends on the \noccupation of the states in the band. If the band is full, any change of state of an electron must result in \nanother electron moving into the vacated state, leaving the electron energy and momentum distribu-\ntion unaltered. Under these conditions, no current can ﬂow and the material is an electrical insulator. \nIf the band is partially ﬁlled, plenty of unoccupied states exist within a small energy range (i.e., within \nthe same band) for these electrons to move into to change their k vectors and energies. The net electron \nenergy and momentum distribution changes and a current ﬂows under the inﬂuence of the electric \nﬁeld. This is the signature of electrical conductivity. In the case of a semiconductor, the number of \nthermally excited electrons in the upper band or holes in the lower band (see Section 15.11) is very \nsmall compared to the number in the metal, and the conductivity is weak.\n15.11 \u0002   EFFECTIVE MASS\nThe dispersion relation for a nonrelativistic free particle, one that moves in a region of constant potential, \nis given by\n \nE 1k2 = U2\n2m\n k2. \n(15.68)\nThe free electron dispersion relation simply states mathematically that the energy of a free particle \ncomes entirely from its momentum and that there is no potential energy contribution (except perhaps \nfor a constant). This parabolic or quadratic relation between energy and wave vector is characterized \nby the mass of the particle. Particles with large mass (like protons) are characterized by a parabola \nwith smaller curvature than particles with small mass (like electrons). Now, take another look at the \ndispersion for the one-dimensional chain of atoms, that is, E1k2 = a + 2b cos1ka2, which is plotted \nin Fig. 15.20 for two different values of b. Notice that near k = 0, the band function looks parabolic. \nIndeed, expand the dispersion relation E(k) for small k to ﬁnd\n \n E 1k2 = a + 2b cos1ka2\n \n \n \u0002 a + 2b C1 - 1\n2 1ka2\n2D \n \n \u0002 a + 2b - ba2k2 .\n \n \n(15.69)\nIf b 6 0, we see that near the bottom of the band, the energy is parabolic in k and varies according to\n \nE - Emin = 0 b0 a2k2. \n(15.70)\n\n15.11 Effective Mass \n495\nIf we compare Eq. (15.70) to the free particle dispersion relation, Eq. (15.68), we see that the electrons \nin states near the bottom of the band behave like free particles except that U2>2m has been replaced by \n0 b0 a2.  In other words, the electron behaves as if it had an effective mass\n \nm* =\nU2\n20 b0 a2 . \n(15.71)\nThe denominator of this expression is just the curvature of the band function for small k and the effective\nmass can be deﬁned more generally for states anywhere in the band according to\n \nm* = U2  c d 2E\ndk2 d\n-1\n. \n(15.72)\nBy this means, all the effects of the electron’s complicated interactions with the crystal lattice have \nbeen swept into one parameter, the effective mass. Figure 15.20 has the free particle dispersion rela-\ntion with the same curvature at k = 0 superimposed on the exact dispersion relation. We see that the \nupper band has the larger curvature, and hence the smaller effective mass at k = 0.\nNote the inverse dependence of m* on b or d2E>dk2. This means that the weaker the interac-\ntions between atoms (smaller beta), the “heavier” the electron is. Narrow bands (small b) are associ-\nated with high effective masses and wide bands (large b) correspond to relatively “light” electrons. \nThis makes sense intuitively: if b is small, the weak interaction or small overlap between atomic \nwave functions makes it difﬁcult for an electron to move from atom to atom under the inﬂuence of an \napplied electric ﬁeld, and it behaves as if it has a large mass.\nIn general, the effective mass changes at different positions in the band, because for any band \nshape except parabolic, the second derivative of E(k) changes. For states near the top of the band, the \neffective mass is negative! This means that the acceleration of a particle in an electric ﬁeld, a = F>m, \nis in the opposite direction to the force. While a negative mass might seem strange, it is perfectly con-\nsistent. More detailed texts on semiconductors show that when a band is almost completely full, it is \noften easier to think in terms of a small number of empty negative-mass electron states that behave like \nparticles with positive electric charges and positive masses, which we call holes. So in Fig. 15.19(c), \napplication of an applied ﬁeld would cause electrons in the conduction band to move against the ﬁeld \n0\nk\nΑ1\nΑ2\n\u0004 Π\na\nΠ\na\nE(k)\nFIGURE 15.20 The N-square-well E(k) for two values of b 6 0 represented \nby solid lines, and the parabolic free- particle E(k) represented by dashed lines. At \nk = 0, the effective mass is smaller for the more disperse (wider) upper band.\n\n496 \nPeriodic Systems\nand holes in the valence band to move in the direction of the ﬁeld. They both result in a current in the \nsame direction, so we add the contributions from the two bands. The number of carriers in each band is \nthe same because the electrons in the upper band originated in the lower band, leaving behind the same \nnumber of holes. But in the example of Fig. 15.20, the response of the holes in the lower band is more \nsluggish because of the larger effective mass. Therefore, the contribution of the electron current to the \ntotal current is larger than the hole current.\n15.12 \u0002  DIRECT AND INDIRECT BAND GAPS\nSemiconductors, particularly Si, are so important in modern technology that it is worthwhile to say a \nlittle more about them, although we will leave details to other texts dedicated to the topic. Semicon-\nductors are characterized by an (almost) full valence band and an (almost) empty conduction band. \nThe difference in energy between the highest energy state in the valence band and the lowest energy \nstate in the conduction band is called “the band gap.” The band gap is labeled in Fig. 15.21. The band \ngap of Si is 1.11 eV, and that of GaAs, another important semiconductor, is 1.43 eV. Of course, there \nare always “gaps” between the energies in different bands associated with a particular allowed value \nof k, but this is not what is meant by “the” band gap.\nAnother important characteristic of the band gap is whether it is a direct band gap or an indirect \nband gap, as illustrated in Fig. 15.21. The band gap is termed direct when the highest energy state \nin the valence band and the lowest energy state in the conduction band occur at the same value of k, \nand indirect when they occur at different values of k. The distinction is signiﬁcant because direct-gap \nsemiconductors absorb and emit light with much higher probability than indirect-gap semiconductors, \nand this is critical for materials selection in optoelectronic devices like light-emitting diodes (LEDs), \nlight sensors (LEDs operating in reverse), and solar cells.\nThe reason that direct-gap semiconductors interact more strongly with light is not hard to \nunderstand. We learned in Chapter 14 how to calculate the probability that an electron makes a tran-\nsition from one quantum state to another, and that this involves both energy and momentum con-\nservation (see Chapter 16 for the momentum aspect). The band gaps in semiconductors are of order \nDirect band gap\nIndirect band gap\n(a)\n(b)\n0\nk\nΑ2\nΑ1\nΑ2\nΑ1\nE(k)\nE(k)\nEgap\nEgap\n0\nk\n\u0004 Π\na\n\u0004 Π\na\nΠ\na\nΠ\na\nFIGURE 15.21 Transitions in (a) a direct-gap and (b) an indirect-gap semiconductor. The vertical arrows \nrepresent photons and the horizontal arrow in (b) represents a phonon.\n\n15.13 New Directions—Low-Dimensional Carbon \n497\n1–3 eV, a range that spans the energies of visible photons. Such photons then, have sufﬁcient energy to \ncause electron transitions between bands. In solids, we must also consider the conservation of crystal \nmomentum, represented by Uk:\n \nUke,init + Ukphoton = Uke,fin . \n(15.73)\nIn a direct transition, the electron’s initial and ﬁnal states have the same value of k. How is this pos-\nsible if the photon that induces the transition also has momentum? The momentum of an infrared, \nvisible or even ultraviolet photon is extremely small compared with typical electron momenta, so \nthe photon momentum does not change the electron momentum by any signiﬁcant fraction of the \nBrillouin zone width. Therefore, the transition is extremely close to being direct (a homework problem \nquantiﬁes this). It means that only a photon and an electron are necessary for a direct transition to take \nplace. On the other hand, if the transition is indirect, the electron’s momentum changes by a signiﬁcant \nfraction of the Brillouin zone width, and the photon cannot supply the needed momentum. The neces-\nsary momentum comes from another lattice denizen, the phonon, or lattice vibration. In other words, \nthe lattice changes its mode of vibration to accommodate the electron transition. In probabilistic terms, \nit means that three entities must be present at the same place and time (the electron, the photon and the \nphonon), and this is a far less likely occurrence than a coincidence of just two particles, an electron \nand a photon. The upshot is that direct transitions are far more likely than indirect transitions.\nNow the phonon supplies the necessary momentum for an indirect transition, but it also brings \nalong some energy. However, the phonon energies are rather small compared to the gap energy, so one \nphonon alone is not sufﬁcient to allow an electron to make an interband transition. As a ﬁrst approxi-\nmation, it is the photon that provides the energy and the phonon that provides the momentum for an \nelectron transition across an indirect band gap.\nSi is an example of an indirect-gap semiconductor. You can see in Fig. 15.2 that the valence band \nmaximum occurs at the k-point labeled \u000f, while the conduction band minimum occurs at the k point on \nthe line between \u000f and X. A phonon and a photon are necessary to facilitate this transition, making it \nless probable than if the gap were direct. It might seem strange then that Si is the most widely used \nsemiconductor in solar cells! As it happens, Si is the best material we have, despite the indirect-gap \nproblem. Although Si is not as efﬁcient at absorbing photons close to the band gap energy as a direct-\ngap semiconductor with the same band gap, there is sufﬁcient absorption of photons if the Si is thick \nenough. Its band gap is the perfect size to capture the photon distribution that comprises the solar \nspectrum—it is abundant, it is environmentally benign, and we have huge investments in Si-processing \ntechnology. All this makes Si the best material currently available for large-scale, economic produc-\ntion of photovoltaic cells. Intense efforts are underway to ﬁnd other materials that will do the same job \nmore efﬁciently and more economically. There are some competitors, but Si is still the most widely \nused photovoltaic material. GaAs is a direct-gap semiconductor. Photovoltaic cells made with GaAs \nare more efﬁcient than those made with Si and are used for some high-end applications, such as pow-\nering equipment in space. They are technologically more difﬁcult to produce than Si, and there are \nserious concerns about the abundance of Ga and As and the toxicity of the latter.\n15.13 \u0002 NEW DIRECTIONS—LOW-DIMENSIONAL CARBON\nOne of the most exciting “new materials” under active research at the present time is an old \nmaterial—carbon! Carbon, as diamond, has the same structure as silicon, but its wide band gap makes \nit insulating rather than semiconducting. Carbon, as graphite, has long been used as a lubricant, a \nreasonable conductor and a handy pencil. Graphite consists of weakly bonded layers of graphene, \nand graphene is a one-atom-thick sheet of C atoms strongly bonded to one another in a honeycomb \n\n498 \nPeriodic Systems\npattern. It is carbon in this two-dimensional form, as isolated graphene sheets or carbon nanotubes, \nwhich are rolled-up graphene sheets, that is the topic of intense interest. The band structure of gra-\nphene is easy to calculate with the LCAO method, because the interesting part derives from just the \nC 2pz states that are perpendicular to the graphene plane. The dispersion relation reveals that graphene \nis a gapless semiconductor—the top of the valence band and the bottom of the conduction band touch \nat several k points. Moreover, the dispersion relation features a linear dependence of E(k) on k at these \npoints. This linear dispersion relation is just like that of a photon (for which E(k) = Uck), so graphene \nis a playground to study relativity! Carbon nanotubes are particularly interesting from the perspective \nof the material presented in this chapter: nanotubes can be semiconducting or metallic, depending on \nexactly how the graphene sheet is rolled up. Graphene “ribbons” can also be made semiconducting. \nThe nanometer scale of these fascinating forms of carbon make them textbook examples of quantum \nphenomena, such as the fractional quantum Hall effect. On the applications front, graphene and car-\nbon nanotubes show promise as high performance transistors, transparent conductors, super-strong \nﬁbers, biosensors in cells, cages to store atoms, or nano-pipettes to deliver cellular cargo.\nSUMMARY\n• The model of a solid as a periodic array of potential energy wells predicts the existence of \nbands of allowed energies for electrons. This model qualitatively explains solid metals as \nmaterials whose electrons partially ﬁll the state of a band, and insulators and semiconductors \nas materials whose electrons completely ﬁll the band states and have a relatively large band \ngap between the ﬁlled states and the next available empty states. Larger band gaps are char-\nacteristic of insulators and smaller band gaps are characteristic of semiconductors.\n• Metals are good conductors because electron wave packets under the inﬂuence of an electric \nﬁeld may access nearby-energy states and change their momentum. Insulators are poor con-\nductors, because nearby-energy states are occupied by other electrons and no net momentum \nchange can occur. Semiconductors in this model are simply metals (partially ﬁlled bands) \nwith very few charge carriers that are generated thermally.\n• Electron motion in solids is modeled with the use of a wave packet, a superposition of delo-\ncalized Bloch states of different k that peaks at a speciﬁc location. This packet moves with a \nvelocity given by the group velocity (velocity of the envelope of the packet), while individual \nstates that comprise the packet move with a different velocity called the phase velocity.\n• The interactions of an electron in a solid with the lattice cause its response to external forces \nto be different than the response of a free electron. This difference is parameterized by the \neffective mass, which describes the curvature of the E(k) relation. It is especially useful near \nthe maxima and minima of bands, where the dispersion relation is often parabolic, similar to \nthe dispersion relation of a free electron.\n• The density of states g(E) is the number of states per unit energy interval. It is useful when it \nis necessary to quantify the total number of electrons involved in a process, such as optical \nabsorption, or electron transport.\n• The band gap in solids may be termed direct or indirect. A direct (indirect) gap occurs when \nthe highest occupied state in an occupied band is at the same (different) k value as (than) the \nlowest energy state in an empty band. Electrons can absorb photons or emit photons to make \na transition across the gap. Such transitions are more efﬁcient in direct-gap semiconductors.\n\nProblems \n499\nPROBLEMS\n 15.1 Write down the matrix representation of the Hamiltonian within the nearest-neighbor approxima-\ntion in terms of a and b for a linear chain of three wells, assuming only one atomic state per well. \nFind the normalized eigenfunctions and eigenvalues. This problem is quite tractable analytically.\n 15.2 Write down the matrix representation of the Hamiltonian within the nearest-neighbor approxi-\nmation in terms of a and b for a linear chain of N wells, assuming only one atomic state per \nwell. Use a computer to ﬁnd the normalized eigenfunctions and eigenvalues. Start with N = 3 \nto repeat the result from the previous problem, and then increase N. Aﬁcionado-code-writers \nmight like to make N much larger.\n 15.3 How would you alter the example presented in Problem 15.1 to ﬁnd the molecular states and ener-\ngies of a linear molecular like carbon dioxide, O=C=O, in the nearest neighbor approximation?\n 15.4 Derive the dispersion relation E(k) for the Hamiltonian HB in Fig. 15.10, which corresponds \nto the case where there are two states per well, and there is an interaction between the upper \nstate of one well and the lower state of the adjacent well in addition to the interactions between \nstates of the same energy. Assume an N-well chain as in Section 15.1.2.\n 15.5 a)  Find the LCAO state that corresponds to k = p>4a, similar to Eq. (15.39). Sketch the real \nand imaginary parts of the wave function, and illustrate that the wavelength is 8a. What is \nthe energy of this state?\nb) Pick another allowed value of k within the ﬁrst Brillouin zone, and repeat.\nc)  Pick a value of k that differs by 2p>a from one you have already chosen, and repeat.\nDiscuss your results.\n 15.6 Explain why the integral 1\nEmax\nEmin  E g1E2dE in Eq. (15.46) does indeed represent the total \n \n energy. Use the density of states expression in Eq. (15.45) to show that the integral  \nevaluates to Na, despite the inﬁnity in g(E ).\n 15.7 a) Find the density of states g(E ) for the case of the free particle in one dimension.\n \n b)  Show that the density of states g(E ) for the free particle dispersion relation in two dimen-\nsions is a constant (challenge problem).\n 15.8 Find the single bound state energy for an electron in an isolated well of depth V0 = 1 eV and \nwidth b = 0.35 nm, as discussed in Section 15.8. Find the matrix elements a and b for a \nperiodic system with well spacing a = 3b and conﬁrm the results given in the text.\n 15.9 a)  Show that the Kronig-Penney dispersion relation, Eq. (15.60), results from Eq. (15.59). \nThis is a straightforward but long calculation, and it’s easy to make mistakes. Be careful, \nand check each step.\n \n b) Show that Eq. (15.61) results from Eq. (15.60) if k is imaginary.\n 15.10 a)  Explore the band structures of C, Si, and Ge, which are all tetrahedrally-bonded solids with \nthe same crystal structure. What trends are evident and how can you explain them?\n \n   b)  In a given solid, effective masses at the extrema of higher bands tend to be lower than effec-\ntive masses at the extrema of lower bands. Is there a plausible physical interpretation of this?\n 15.11  Explain how a simplistic argument that energy bands in solids are entirely derived from the \ncorresponding atomic states might lead to the false conclusion that Mg (or any alkali earth \nelement) is in an electrical insulator. How do you rationalize the observed metallic behavior \nwithin the LCAO model?\n\n500 \nPeriodic Systems\n 15.12 a)  What is the energy of a visible photon? What are the band gaps of important semiconduc-\ntors? Are visible photon energies in the right range to facilitate electron transitions across \nthe band gap of a typical semiconductor?\n \n b)  Show that the momentum of a visible photon is insufﬁcient to facilitate electron indirect \ntransitions across the band gap of a typical semiconductor.\n \n c)  Phonons are quantized lattice vibrations. Like photons, they are massless entities, with a \ncharacteristic wavelength that determines the momentum, and a characteristic frequency \nthat determines the energy. If the characteristic wavelength of a phonon is roughly the  \nlattice spacing in a solid, and the characteristic frequency is roughly 1013 Hz, show that the \nmomentum of a phonon is in the right range to facilitate indirect electron transitions across \nthe band gap of a typical semiconductor, but that the energy is too small.\nRESOURCES\nActivities\nPeriodic Systems is a course based on Chapter 15 taught at Oregon State University. The course treats \nboth classical and quantum mechanical periodic systems. The website has a description and activities \nassociated with this course:\nwww.physics.oregonstate.edu/portfolioswiki/courses:home:pphome\nBand Structure: Explore wave functions and probability densities of chains of up to 10 square wells \nor Coulomb potential energy wells. The wells can be adjusted and an electric ﬁeld can be applied:\nhttp://phet.colorado.edu/en/simulation/band-structure \nQuantum Crystal: Explore wave functions and the dispersion relation of several different shapes of \npotential energy wells:\nhttp://www.falstad.com/qm1dcrystal/\nSolid State Physics Simulations (ISBN 0-471-54885-5), by Graham Keeler, Roger Rollins, Steven \nSpicklemire, and Ian Johnston, is one of nine parts of the Consortium for Upper-Level Physics Soft-\nware (CUPS) published by Wiley, edited by Maria Dworzecka, Robert Ehrlich, and William Mac-\nDonald. Solid State Physics Simulations has several useful programs that allow you to explore a \none-dimensional chain of atoms, band structure, dispersion relations, and the LCAO method applied \nto small clusters. There is an accompanying text. The series is out of print, but used copies are listed \nat Amazon.com.\nhttp://physics.gmu.edu/~cups/ss.html\nFurther Reading\nThe Kronig-Penney model is discussed in more detail in several well-known Quantum Mechanics texts:\nD. J. Grifﬁths, Introduction to Quantum Mechanics, 2nd ed., Upper Saddle River, NJ: Prentice \nHall, 2005.\nR. L. Liboff, Introductory Quantum Mechanics, 4th ed., San Francisco: Addison Wesley, 2003.\nA. Goswami, Quantum Mechanics, 2nd ed., Dubuque, IA: William C. Brown, 1996.\n\nResources \n501\nMore advanced references:\nC. Kittel, Introduction to Solid State Physics, 8th ed., New York: John Wiley & Sons, Inc., 2005. \nAn introductory text that treats metals, semiconductors, and insulators, and many of the \nconcepts mentioned in this chapter.\nR. F. Pierret, Semiconductor Device Fundamentals, Reading, MA: Addison Wesley, 1996. Dis-\ncusses the details of carrier transport in semiconductors and modern devices.\nA. K. Geim and A. H. MacDonald, “Graphene: Exploring Carbon Flatland,” Phys. Today 60(8), \n35–41 (2007), http://dx.doi.org/10.1063/1.2774096. Gives a nice introduction to graphene, \nand explains the linear dispersion relation and the fractional quantum Hall effect.\nC. Dekker, “Carbon Nanotubes as Molecular Quantum Wires,” Phys. Today 52(5), 22–28 (1999), \nhttp://dx.doi.org/10.1063/1.882658. Talks about measurements to distinguish the difference \nbetween semiconducting and metallic carbon nanotubes, and discusses some potential uses.\n\nC H A P T E R  \n16\nModern Applications  \nof Quantum Mechanics\nTime for some fun! (Not that we weren’t having fun before.) You have now acquired a tool set for \nunderstanding how the microscopic world works. Let’s spend this last chapter using that tool set to \nexamine two current research topics that are extensions of some of the examples of quantum mechan-\nics that you have studied in this text. Quantum mechanical forces on atoms and quantum information \nprocessing both have important connections to Stern-Gerlach spin-1/2 experiments and to resonant \natom-light interactions. These new research ﬁelds can be considered to be quantum engineering in that \nwe understand the quantum mechanics so well that we are now using it for practical applications. The \nresearch is expanding so rapidly that we cannot provide a complete overview in just one chapter. We \nwill focus on a few aspects of these ﬁelds that are directly connected to what you have learned here. \nThe resources at the end of the chapter provide references for you to learn more.\n 16.1 \u0002 MANIPULATING ATOMS WITH QUANTUM MECHANICAL FORCES\nIn the last 30 years, physicists have developed a broad collection of quantum mechanical tools to \nexert forces on atoms. These forces allow us to manipulate the positions and velocities of atoms so \nwell that we can stop atoms and hold them in place for an extended time. We can, therefore, measure \nthem for longer and improve spectroscopic energy measurements that are limited by the energy-time \nuncertainty principle. For example, the standard of time is based upon a microwave transition between \ntwo hyperﬁne states in the cesium atom and the longer the atom can be observed, the better we can \ndeﬁne the second—the basic unit of time. Along the way, researchers have uncovered a host of other \nfun things to do with mechanical forces, and they have even discovered a new form of matter—the \nBose-Einstein condensate discussed in Chapter 13. In the following two subsections, we will discuss \ntwo examples of quantum mechanical forces.\n 16.1.1 \u0002 Magnetic Trapping\nThe ﬁrst example of quantum mechanical forces on atoms is magnetic trapping, where we use mag-\nnetic ﬁelds to conﬁne atoms to a small region of space. Magnetic traps are used to conﬁne atoms at \nvery low temperatures, and have played an important role in Bose-Einstein condensation experiments. \nTo explain how a magnetic trap works, we return to the Stern-Gerlach experiment that we know and \n\n16.1 Manipulating Atoms with Quantum Mechanical Forces \n503\nlove. In fact, the ﬁrst equation in this text, Eq. (1.1), told us that a magnetic moment experiences \na force in a magnetic ﬁeld gradient. That introduction was a classical argument, but we discovered \nthe quantum mechanical underpinnings of the Stern-Gerlach experiment in Chapter 12 when we dis-\ncussed the Zeeman effect. The Stern-Gerlach force derives from the potential energy of interaction \nbetween the magnetic moment of the atom and the magnetic ﬁeld:\n \nV = -M~B . \n(16.1)\nThe force on the atom is the negative gradient of this potential energy: F = -\u0002V. The potential \nenergy of the magnetic moment in the magnetic ﬁeld is the Zeeman energy we found from perturba-\ntion theory, which has the general form [see Eq. (12.93)]\n \nVZ = E (1)\nZ\n= g m mB B . \n(16.2)\nFor this discussion, we won’t worry about whether the magnetic moment is associated with a spin (S), \norbital (L), or total angular momentum (J or F), so we leave the subscripts off the Landé g-factor and \nthe magnetic quantum number m.\nIn a typical Stern-Gerlach experiment, the deﬂection angle of the atom is small (Problem 16.1). \nBut what if the Stern-Gerlach force were large enough to signiﬁcantly deﬂect the atom, say by 90°, \nor even 180°, and the magnetic ﬁeld were shaped so that the atom kept on being deﬂected? Then \nyou could imagine constructing a system that contained the atom and didn’t let it escape. That is the \nessence of a magnetic trap.\nTo discuss the mechanics of how a magnetic trap works, it is more instructive to use the energy \napproach rather than the force approach. To trap a particle in general, the potential energy must have \na spatial minimum to form a conﬁning well. For example, the generic potential energy well shown in \nFig. 16.1 has a minimum at x = 0 and will conﬁne or trap particles that have kinetic energies less than \nVmax. As the particles move, they exchange kinetic for potential energy. Such a potential energy well \nis no different in principle from the potential energy wells you have already studied—square well, \nharmonic oscillator, hydrogen atom. We call it a trap when we control the potential energy to conﬁne \nparticles that are otherwise free to move.\nx\nVmax\nV(x)\nFIGURE 16.1 Generic potential energy for a particle trap in one dimension. \nParticles with kinetic energy less than Vmax are trapped in the vicinity of the \norigin, where the potential energy is a minimum.\n\n504 \nModern Applications of Quantum Mechanics\nFor a magnetic trap, the potential energy that determines the particle motion is the Zeeman energy \nVZ1r2 = g m mB B1r2. A generic Zeeman energy level diagram is shown in Fig. 16.2. The force on \nthe atom is the negative gradient of the Zeeman energy, so atomic states with positive magnetic quan-\ntum number m are attracted toward regions of low magnetic ﬁeld and are called weak-ﬁeld seeking \nstates. Atom states with negative m are attracted toward regions of high magnetic ﬁeld and are called \n strong-ﬁeld seeking states. A local spatial maximum in the magnetic ﬁeld is not allowed by  Maxwell’s \nequations in free space, so a magnetic trap must rely on a local minimum in the magnetic ﬁeld along \nwith a positive magnetic quantum number m. Hence, a magnetic trap conﬁnes atoms in weak-ﬁeld \nseeking states and ejects atoms in strong-ﬁeld seeking states. An atom in a weak-ﬁeld seeking state has \nits angular momentum aligned with the ﬁeld (positive m), so the magnetic moment is aligned against \nthe ﬁeld.\nIn a three-dimensional magnetic ﬁeld, the magnetic ﬁeld direction is not uniform, especially \naround the local minimum that forms the trap. The changing ﬁeld direction would seem to be problem-\natic because the potential energy VZ1r2 = g m mB B1r2 assumes a given quantization axis along which \nto measure the angular momentum component characterized by the magnetic quantum number m. \nHowever, if the magnetic ﬁeld direction does not change too quickly, then the atom’s Larmor preces-\nsion about the ﬁeld adiabatically follows the changing ﬁeld direction and the atom remains in a weak-\nﬁeld seeking state that is forced toward the origin. This condition holds in most magnetic trapping \nsituations (Problem 16.2). There are some important exceptions, but that is more detail than we need \nfor our brief introduction.\nThe simplest magnetic ﬁeld conﬁguration that produces a magnetic trap is a pair of circular coils \nwith opposing currents. This conﬁguration of anti-Helmholtz coils is shown in Fig. 16.3 with its \nresultant quadrupole magnetic ﬁeld (normal Helmholtz coils have parallel current directions and pro-\nduce a nearly uniform ﬁeld at the center). The magnitude of the magnetic ﬁeld of anti-Helmholtz coils \nis zero of the center of the trap and has a spatial dependence\n \nB1r2 = A2x2 + y2 + 4z2 . \n(16.3)\nThis ﬁeld magnitude increases linearly along any direction from the trap center, but the gradient has \ndifferent values in different directions because of the factor of 4 in Eq. (16.3). The ﬁeld magnitude \nB\nE0\nE\nm \b\t\u00042\nm \b\t\u00041\nm \b\t0\nm \b\t1\nm \b\t2\nweak-field\nseeking states\nstrong-field\nseeking states\nFIGURE 16.2 Zeeman energy levels. States with positive m are attracted to low magnetic ﬁeld \nregions and states with negative m are attracted to high magnetic ﬁeld regions.\n\n16.1 Manipulating Atoms with Quantum Mechanical Forces \n505\nalong the x-axis is shown in Fig. 16.4. As noted above, the magnetic ﬁeld direction shown in Fig. 16.3 \nis continuously changing.\nFor the magnetic trap to be useful, it should have enough potential energy depth to conﬁne atoms \nwith a range of kinetic energies, which is determined by the temperature of the ensemble of atoms. The \nthermal spread of energies is Ethermal = kBT, where we ignore factors of order unity (like p, 1>2, etc.). \nThe Landé g-factor and the magnetic quantum number are of order unity, so the potential energy well \ndepth of a magnetic trap is approximately\n \n\u0006Vtrap = mBB max . \n(16.4)\nA typical magnetic trap has a gradient of 100 Gauss>cm and a trapping region of order 1 cm, giving \na maximum ﬁeld of 100 Gauss (atom trappers use Gauss and cm as their standard units, so we follow \nI\nI\nFIGURE 16.3 The opposing currents in a pair of anti-Helmholtz coils produce a \nquadrupole magnetic ﬁeld that traps weak-ﬁeld seeking states at the center of the coils.\n\u00042\n\u00041\n0\n1\n2\nx(cm)\n100\n200\n\u0002B\u0002\u0005(Gauss)\nFIGURE 16.4 The magnitude of the magnetic ﬁeld in a quadrupole \nmagnetic trap increases linearly from the origin of the trap.\n\n506 \nModern Applications of Quantum Mechanics\ntheir lead; recall that 1 Gauss = 10-4 Tesla). Equating the trap depth and the thermal energy, we esti-\nmate the temperature of atoms that can be trapped:\n \n T =\n\u0006Vtrap\nkB\n= mB Bmax \nkB\n \n \n =\n1h 1.4 MHz>Gauss21100 Gauss2\n8.62 * 10-5 eV>K\n \n \n =\n0.58 * 10-6 eV\n8.62 * 10-5 eV>K\n \n \n(16.5)\n \n = 7mK .\n \nThat is pretty cold! We could use superconducting coils to provide much higher current. That has been \ndone, but the well depth is still only a few Kelvin. So to trap atoms with magnetic ﬁelds, we must ﬁnd \na way to reduce the temperature (i.e., the translational motion) of the atoms. The force of the magnetic \ntrap itself cannot cool the atoms because it is a conservative force; atoms in the trap speed up and \nslow down (only slightly compared to room temperature motion), but the temperature of the ensemble \nis not reduced. We could use liquid helium to cool the atoms, but that requires expensive cryogenic \ntechniques and cools only into the Kelvin range. A simpler technique, that also allows cooling to the \nmilliKelvin level required for typical magnetic traps, is laser cooling of atoms, which we will discuss \nin the next section.\nThe magnetic trap has become an important research tool in atomic physics. A variety of different \nmagnetic ﬁeld geometries have been designed to optimize the conﬁnement of the atoms, to allow opti-\ncal access of laser beams to the atoms, or to build an array of traps for quantum computing. The best \nknown application is in experiments to achieve Bose-Einstein condensation. The magnetic trap collects \nand conﬁnes atoms that have been cooled with laser cooling (more below). The atoms are then cooled \nfurther by evaporation (like coffee in a mug) in the trap. This slow process takes several seconds, so the \nability to trap the atoms is vital. These experiments are done at very low pressure (high vacuum) so that \nbackground gas atoms do not collide with trapped atoms and knock them out of the trap.\nFinally, it is interesting to note that there are two macroscopic systems that also use magnetic \nﬁelds to trap objects. There is a toy called a Levitron where a spinning magnet is suspended in air \nabove a magnetic base plate. The magnetic ﬁeld is similar to the quadrupole ﬁeld in that there is a \nregion where the ﬁeld is a minimum. The spinning magnet has its magnetic moment aligned against \nthe magnetic ﬁeld of the base plate, much like the weak-ﬁeld seeking states of the atom in the mag-\nnetic trap. The strong magnetic ﬁeld of the base plate tries to ﬂip the spinning magnet over to be \naligned with the ﬁeld, but the torque causes the spinning top to precess about the ﬁeld, like the  Larmor \nprecession of an atom’s magnetic moment. The second macroscopic system is the use of strong \nsuperconducting magnetic ﬁeld gradients to ﬂoat diamagnetic objects, for example, frogs (this was \nannounced in April 1997, but it was not an April Fool’s joke). In a diamagnetic material, an applied \nmagnetic ﬁeld induces a magnetic moment in the material that opposes the applied ﬁeld, again analo-\ngous to the weak-ﬁeld seeking states above.\n 16.1.2 \u0002 Laser Cooling\nOur second example of a quantum mechanical force is the use of lasers to slow down and cool atoms. \nLaser cooling allows us to cool atoms from room temperature or higher down to temperatures below \n1 mK—low enough to be easily conﬁned in a magnetic trap. \n\n16.1 Manipulating Atoms with Quantum Mechanical Forces \n507\nThe force that light exerts on matter is known as radiation pressure and comes about because \nlight carries momentum as well as energy. Photons have momentum given by p = h>l = Uk, where \nk = 2p>l is the wave vector. In Chapter 14, we discussed the absorption of a photon by an atom, \nwhereby the energy of the photon causes the electron to be excited to a higher level, and the angular \nmomentum of the photon is taken up by the atom according to the selection rules on the atom’s angular \nmomentum quantum number. We ignored the role of the linear momentum because it is usually quite \nsmall. However, the force of a laser on an atom can be quite large if the right conditions are satisﬁed. \nTo illustrate the conditions required for efﬁcient laser cooling, we use the rubidium atom (Rb) as an \nexample. The relevant parameters for rubidium are shown in Table 16.1.\nWhen an atom of mass M absorbs a photon, the transfer of momentum from the photon to the \natom is\n \n\u0006p = M\u0006v = Uk . \n(16.6)\nThis momentum transfer causes the atom to recoil with a change in velocity of\n \n\u0006v = vr = Uk\nM =\nh\nMl\n . \n(16.7)\nFor a rubidium atom absorbing a 780 nm resonance photon, the recoil velocity vr is 0.6 cm>s, which is \nmuch less than the typical thermal velocity of vT = 280 m>s. So one photon does not impact a rubid-\nium atom signiﬁcantly, just as one mosquito hitting the windshield does not slow down your car. But \nif the atom repeatedly absorbs photons, then the net impact can be large. For a thermal rubidium atom \nto come to rest requires approximately vT>vr \u0005 50,000 recoil kicks. For the atom to absorb this many \nphotons, the atom must return to the same state after each absorption so that it is ready to absorb another \nlaser photon. The best way to achieve this cycle is to start with the atom in the ground state and excite it \nto the ﬁrst excited state so that spontaneous emission returns it to the ground state. Hence, laser cooling \nrequires an atom that behaves like a two-level system and a laser wavelength tuned close to resonance \nwith the primary transition in the atom from the ground state of the atom 0 g9 = 0 19 to the ﬁrst excited \nstate 0 e9 = 0 29. Though no atom is truly a two-level system, there are straightforward laser techniques \nthat allow the two-level model to be applicable in laser cooling experiments, and the atom can be cycled \nthrough the absorption-emission process enough times for radiation pressure to be effective.\nThe cycle of laser absorption and subsequent spontaneous emission that is required for laser cool-\ning of an atom is depicted in Fig. 16.5. The three steps illustrated are: (1) A resonant laser beam is \nincident on an atom in the ground state of the two-level system. (2) The atom absorbs a photon, which \npromotes the electron to the excited state and causes the atom to recoil in the direction of the inci-\ndent laser with momentum change \u0006p = U k. (3) The excited atom decays back down to the ground \nstate via spontaneous emission of another photon. The spontaneous photon is emitted in a random \n direction, so the recoil kick due to the spontaneously emitted photons averages to zero over many \nTable 16.1 Rubidium Laser Cooling Parameters\nResonance Wavelength\nl = 2pc>v21\n780 nm\nResonance Linewidth\n\u0006v = A21\n2p * 6 MHz\nLifetime\nt = 1>A21\n27 ns\nMass\nM\n85 amu = 1.4 * 10-25 kg\nThermal Velocity\nvT = 22kBT>M\n280 m>s\nRecoil Velocity\nvr = Uk>M\n0.6 cm>s\n\n508 \nModern Applications of Quantum Mechanics\nabsorption-emission cycles and the average momentum change per complete absorption-emission \ncycle is 8\u0006p9cycle = U\n  k , due only to the momenta of the absorbed photons. Once the atom returns to \nthe ground state, it is ready to absorb another photon and begin the cycle anew. The average absorption-\nemission cycle time is at least as long as the spontaneous emission lifetime of the atom, but that is typi-\ncally nanoseconds, so this process can ﬁnish in much less than one second. Assuming that the minimum \ncycle time is twice the atomic lifetime (e.g., t to absorb a laser photon and t to emit a spontaneous \nphoton), the maximum force on the atom is\n \nFmax = d p\ndt =\n8\u0006p9cycle\n8\u0006t9min \n= U k\n2t\n . \n(16.8)\nThe complete process of photon absorption and emission is called scattering. We refer to the force \ndepicted in Fig. 16.5 as the scattering force to distinguish it from other radiation forces. This force \nis not conservative because the spontaneous emission is an irreversible process. Hence the scattering \nforce differs in a critical way from the magnetic force used to trap atoms described earlier. The good \naspect of this is that the non-conservative nature of the scattering force permits cooling, which is not \npossible with a conservative force. It is important to distinguish slowing from cooling. Individual \natoms are slowed by the scattering force. Cooling requires that we reduce the velocity spread of the \nensemble of atoms, which we’ll explain below.\nThe typical geometry for laser cooling is a laser beam counterpropagating against an atomic beam, \nas shown in Fig. 16.6. The scattering force decelerates the atoms with a maximum acceleration of\n \na\n max = Fmax \nM\n=\nUk\n2 Mt =\nh\n2 Mlt\n . \n(16.9)\n1\n2\n3\n\u0004\np\u0003\u0005\b\t\u0002k\n\np\u0005\b\t\u0002k\nFIGURE 16.5 The laser cooling cycle: (1) A resonant laser beam is incident on a two-\nlevel atom in its ground state. (2) The atom absorbs a photon with the energy going to \nexcite the electron and the momentum causing the atom to recoil. (3) Spontaneous emission \nproduces a photon in a random direction and the atom returns to the ground state.\n\n16.1 Manipulating Atoms with Quantum Mechanical Forces \n509\nFor example, the deceleration of a rubidium atom is\n \n a\n max =\nh\n2Mlt\n \n \n =\n16.626 * 10-34  Js2\n2185 amu * 1.66 * 10-27 kg>amu21780 * 10-9 m2127 * 10-9 s2\n \n(16.10)\n \n = 1.11 * 105  m>s2 = 1.14 * 10 4 g . \nEach absorbed photon produces a small momentum change of the atom, but the process is repeated \nso rapidly that the resulting acceleration dominates gravity 1g = 9.8 m>s22 and is sufﬁcient to stop a \nthermal atom within 1 meter (Problem 16.3).\nSo far our description explains only laser slowing or deceleration. Laser cooling requires one \nadditional aspect of the scattering force that we have neglected. The scattering force is velocity depen-\ndent because of the Doppler effect that causes the frequency experienced by a moving atom to be \nshifted from the laser frequency by an amount proportional to the atomic velocity. The Doppler-\nshifted angular frequency of a laser beam with wave vector k as observed by an atom with velocity v is\n \nvAtom = vLaser - k~v . \n(16.11)\nAn atom moving toward the laser source experiences a blue-shifted beam (higher frequency, shorter \nwavelength) and an atom moving away from the laser source experiences a red-shifted beam (lower \nfrequency, longer wavelength), as shown in Fig. 16.7. Because the scattering force relies on the \nOven\nScattered\nphotons\nLaser\nphotons\nv\nk\nFIGURE 16.6 An oven with a small opening produces an atomic beam. The photons \nfrom a counterpropagating resonant laser beam are scattered and the atoms are slowed.\nAtom\nΩ2 \b Ω \u0007 kv\nΩ1 \b Ω \u0004 kv\nv\nFIGURE 16.7 Doppler shifts of copropagating and counterpropagating laser beams. The \nlaser photons are produced in the laboratory with angular frequency v. The moving atom \nobserves these photons shifted up (counterpropagating) or down (copropagating) by kv.\n\n510 \nModern Applications of Quantum Mechanics\n resonance of the laser beam with the atomic transition, the motion of the atom has a strong effect on \nthe strength of the scattering force.\nWe quantify the velocity dependence of the scattering force by expressing the force as the \nmomentum change per scattering cycle (absorption-emission cycle) divided by the time for each \ncycle. The cycle time is the inverse of the scattering rate, which is the excitation rate R1S2 we calcu-\nlated in Chapter 14. This results in\n \n Fscatt = d p\ndt =\n8\u0006p9cycle\n8\u0006t9cycle\n \n \n = 1momentum per scattered photon2 * 1scattered photons per second2 \n(16.12)\n \n = U k R1S2 .\n \nSubstituting Eq. (16.11) into the scattering rate from Eq. (14.73), we ﬁnd\n \n R1S2 = 3 I\nc\n B12   f 1vAtom2\n \n \n = 3 I\nc\n B12 \nA21\n2p\n1vLaser - v21 - k~v2\n2 + aA21\n2 b\n2 . \n \n(16.13)\nThe scattering force is then\n \nFscatt1v2 = U k A21\n2\n  I\nI0\n \naA21\n2 b\n2\n1vLaser - v21 - k~v2\n2 + aA21\n2 b\n2 , \n(16.14)\nwhere the characteristic intensity is I0 = 1U v3A21>12pc22. This expression for the scattering force is \nvalid only for incident laser intensities that satisfy I V I0. The valid expression for all intensities is \nthe subject of Problem 16.4.\nThe Doppler shift of the laser beam has two main effects: (1) the laser frequency must be tuned \naway from the resonance frequency v21 to excite moving atoms, and (2) only atoms in a small velocity \nrange experience the radiation pressure. Both of these effects are illustrated in Fig. 16.8, which shows \nthe Maxwellian velocity distribution of rubidium atoms in a thermal beam 1N1v2\f  v3e-v2>v\n 2\nT 2 and the \nvelocity-dependent scattering force for a counterpropagating laser that is tuned 450 MHz below \nthe resonance frequency f21 = v21>2p = c>l21. For this detuning, the laser beam excites rubidium \natoms that are moving toward the laser source at v = 350 m>s (Problem 16.5). The scattering force in \nEq. (16.14) has the same Lorentzian resonance behavior of the excitation rate, with an inherent line-\nwidth \u0006v = A21 = 1>t caused by spontaneous emission. Hence, only atoms in the velocity range \n\u0006v = \u0006v>k about the resonant velocity of 350 m>s experience an appreciable scattering force. For \nrubidium, the spontaneous emission linewidth is \u0006f = \u0006v>2p = 1>2pt = 6 MHz in frequency \nspace, yielding a velocity width\n \n\u0006v = \u0006v\nk\n=\nl\n2pt =\n780 nm\n2p127 ns2 = 4.6 m>s , \n(16.15)\nas indicated in Fig. 16.8. This width is much smaller than the thermal spread of the atomic beam, \nso only a small fraction of the atoms in the beam are decelerated by the scattering force (the force is \nopposite the atomic velocity for a counterpropagating laser beam). For the laser frequency detuning \n\n16.1 Manipulating Atoms with Quantum Mechanical Forces \n511\ndepicted in Fig. 16.8, the scattering force decelerates atoms with velocities in the approximate range \n345–355 m>s. These atoms subsequently move at lower velocities and no longer experience the scat-\ntering force, because their new Doppler shift makes the laser photons appear to be off resonance. The \nscattering force thus alters the velocity distribution as shown in Fig. 16.9. The number of atoms in the \nrange 345–355 m>s is depleted and the number of atoms in the range below that is augmented.\nIf our goal is to stop the rubidium atoms in this beam, then we have failed, because the deceleration \ncaused by the scattering force has changed the Doppler shift and taken the atoms away from the initial \nresonance condition. The solution to this problem is straightforward: we change the laser frequency \n0\n200\n400\n600\nv(m/s)\n0\n\u0004200\n\u0004400\n\u0004600\n\u0004800\n\nv\nN(v), Fscatt (v)\nfLaser\u0004f21(MHz)\nN(v)\nFscatt (v)\nFIGURE 16.8 Maxwellian velocity distribution of a rubidium atomic beam at 400°C and the \nmagnitude of the scattering force for a laser tuned 450 MHz below resonance. The narrow width of \nthe scattering force arises from the spontaneous emission line width of the resonance transition.\nfLaser\u0004f21(MHz)\n0\n200\n400\n600\nv(m/s\n0\n\u0004200\n\u0004400\n\u0004600\n\u0004800\nN(v)\nFIGURE 16.9 Atoms in resonance with the detuned laser beam are slowed, depleting the \nnumber of atoms at that velocity and augmenting the number at a slightly lower velocity.\n\n512 \nModern Applications of Quantum Mechanics\nto be in resonance with the previously slowed group of atoms (e.g., from fLaser - f21 = -450 MHz \nto fLaser - f21 = -444 MHz). After this group is slowed and falls out of resonance again, we \nrepeat the laser frequency change. In practice, the laser frequency is continuously swept from the \nstarting point toward the resonance frequency f21 to keep the slowing atoms in resonance with the \nlaser beam throughout their journey. This method of compensating for the changing Doppler shift \nis called chirped cooling, in analogy with the changing pitch of a bird’s chirp. From the expression \nfor the scattering force in Eq. (16.14), we see that chirped cooling increases vLaser to keep the term \n1vLaser - v21 + kv2 = 0 as the velocity decreases. The resultant velocity distribution after the fre-\nquency chirp is ﬁnished is shown in Fig. 16.10. Atoms from the initial resonant velocity downward \nare slowed and accumulate near the ﬁnal resonant velocity of the chirp. The ﬁnal velocity distribution \n(at least the part below the initial resonant velocity) is much narrower than the initial distribution, so \nthe atoms have been cooled, not merely decelerated. It is also possible to compensate for the Doppler \nshift and keep slowing atoms in resonance by altering the atomic frequency v21 by applying either a \nspatially varying magnetic or electric ﬁeld that perturbs the atomic energy levels through the Zeeman \neffect or Stark effect, respectively.\nThe laser cooling of an atomic beam illustrated in Fig. 16.10 affects only one of the velocity \ncomponents. Cooling the complete three-dimensional velocity distribution requires scattering forces \nin all three directions. This is achieved with a conﬁguration of six laser beams along the positive and \nnegative Cartesian axes, as shown in Fig. 16.11. This arrangement of laser beams is called optical \nmolasses because it strongly damps the atomic motion, just as molasses damps the motion of a spoon \ndropped into it. At ﬁrst glance, it might appear that the counterpropagating beams of optical molasses \nwould cancel each other out to give no net force. This is true for an atom at rest, but once again the \nDoppler shift of moving atoms plays a key role.\nIn optical molasses, the six laser beams come from the same laser and have the same frequency. \nThe laser is tuned about one line width \u0006v = A21 below the resonance v21 (red detuning). For a mov-\ning atom, the laser beam propagating in the same direction as the atomic velocity is Doppler shifted to \nlower frequencies, taking it farther from resonance, while the laser beam propagating in the opposite \ndirection is Doppler shifted to higher frequencies, bringing it closer to resonance. Hence, the scat-\ntering force from the laser beam counterpropagating to the atom dominates and the atom is slowed \n0\n200\n400\n600\nv(m/s)\n0\n\u0004200\n\u0004400\n\u0004600\n\u0004800\nN(v)\nfLaser\u0004f21(MHz)\nFIGURE 16.10 In chirped laser cooling, the laser frequency is swept from the original detuning \n(–450 MHz) toward the resonance frequency and a wide range of atoms are slowed and accumulate \nnear zero velocity.\n\n16.1 Manipulating Atoms with Quantum Mechanical Forces \n513\ndown. The resultant force F+knx1v2 + F-knx1v2 along one of the axes is shown in Fig. 16.12. For the \nlaser frequency detuning shown 1vLaser - v21 \u0005 -A212, the scattering force is approximately a linear \nfunction of velocity for small velocities. The resultant atomic motion in optical molasses is similar to \nthe motion of a particle in a viscous liquid.\nFIGURE 16.11 Optical molasses comprises six laser beams along the Cartesian axes. \nAtoms at the intersection of the six laser beams are strongly cooled in all three dimensions.\n\u00046\n\u00044\n\u00042\n2\n4\n6\nv(m/s)\nFx(v)\nF +x beam\nF\u0004x beam\nFIGURE 16.12 Scattering force as a function of velocity in optical molasses.\n\n514 \nModern Applications of Quantum Mechanics\nComparing the scattering force in Fig. 16.12 with the Maxwellian velocity distribution in \nFig. 16.8, we note that the range of velocities that are affected by optical molasses is very small. \nIn a typical experiment, laser cooling of an atomic beam is ﬁrst performed to produce a sample of \natoms with low velocity, as in Fig. 16.10, and then the atoms are further cooled in all three dimen-\nsions in optical molasses. Atoms in optical molasses can be cooled to a temperature of approximately\n100 mK, which provides a sample of atoms that is easily conﬁned in a magnetic trap. This temperature \nlimit, called the Doppler cooling limit, arises from the balance between the cooling force and heat-\ning caused by the random nature of spontaneous emission. The development of these laser cooling \ntechniques resulted in the Nobel Prize for physics in 1997. Laser cooling has been used to improve the \nprecision of atomic clocks, to make precision measurements of gravity, and to create sources of atoms \nthat behave as quantum mechanical waves rather than classical particles. Laser cooling and magnetic \ntrapping were combined in the discovery of Bose-Einstein condensation, which was recognized by the \nNobel Prize for physics in 2001.\n16.2 \u0002 QUANTUM INFORMATION PROCESSING\nOur second example of a modern application of quantum mechanics is quantum information \n processing. We live in the information age. Computers, smart phones, personal digital assistants, GPS \ndevices, and more surround us, whether we want them or not. The explosion of information process-\ning systems has been enabled by the continuing miniaturization of electronic circuits. Every year, \nengineers are able to put more circuits on computer chips. Now that we have entered the nanotechnol-\nogy phase of the information revolution, we are approaching the physical limitation presented by the \natoms that make up the devices. Extrapolation of the miniaturization march would soon have us using \nindividual atoms as memory devices and circuit elements. As we approach the physical size limita-\ntion of the atoms themselves, quantum mechanics must play a role in building and using information \nprocessing devices. This shift is sure to be a disruptive inﬂuence in computing, but it also represents \nan opportunity to take advantage of unique quantum mechanical aspects of information processing.\nThe idea that quantum mechanics could be useful in computing stems in part from a talk and a \npaper by Richard Feynman in the early 1980’s. Feynman asked the question: Can a classical computer \nreliably model a quantum mechanical system? Imagine that we want to model the quantum mechani-\ncal time evolution of a system of 50 spin-1/2 particles. The Hilbert space of this 50-particle system has \n250 states, so the quantum state vector of the system requires 250 \u0005 1015 coefﬁcients to describe a gen-\neral state in the Hilbert space of this system (more details on the numerics later). A 100-particle system \nwould require 2100 \u0005 1030 coefﬁcients and a 300-particle system would require 2300 \u0005 1090 coef-\nﬁcients, which is more than the number of protons in the universe! A computer would have to keep \ntrack of all these coefﬁcients in order to properly account for the particle-particle interactions and their \neffect on the system’s Schrödinger time evolution. So it appears impossible to model the dynamics of \na modestly-sized multiparticle quantum mechanical system because the Hilbert space is so exponen-\ntially large. On the other hand, nature has no trouble managing this large Hilbert space and producing \nthose same dynamics that we are not able to model! This suggests that we let nature, in the form of a \nquantum mechanical system of 50, 100, or 300 particles, be the computer. We let this quantum com-\nputer use its own Schrödinger time evolution to calculate what our classical computer cannot.\nThis conjecture has led to an explosion in the ﬁeld of quantum information processing with \nresearch to uncover the theory of quantum information and to implement some basic experiments to \ndemonstrate the principles. The ﬁeld is too broad and too deep for us to cover thoroughly here, but \nhere is a taste of some of the possibilities, especially as they relate to the ideas you have learned in \nthis text. We’ll introduce the idea of quantum bits to store data and quantum gates to manipulate data. \n\n16.2 Quantum Information Processing \n515\nThese elements are required to make a quantum computer, so we’ll brieﬂy discuss some of the quan-\ntum algorithms that make a quantum computer attractive. Then ﬁnally, we’ll discuss how quantum \nteleportation works.\n 16.2.1 \u0002 Quantum Bits—Qubits\nClassical computing relies on binary digits—bits—to store information. Each bit has the value 0 or 1, \nand individual bits are strung together to represent larger binary numbers (for example, 001100101). \nEach binary number represents an actual number or, through coding, some other piece of information \nlike the letter “A.” The job of a classical computer is to store and process bits. Because there are only \ntwo possible states for each bit, many of the tasks required in a classical computer are implemented \nwith simple on-off switches.\nIn quantum information processing, information is stored in quantum bits, or qubits. A qubit is \na quantum system with two possible states, analogous to the 0 and 1 of a classical bit. The canonical \nqubit system is the spin-1/2 system we know and love, with the spin up state 0  +9 and the spin down \nstate 0  -9 playing the roles of the two binary states. But any two-state quantum system can be used as \na qubit. Other common qubit systems include hyperﬁne levels in atoms and polarization states of pho-\ntons. To address all of these diverse systems with the same formalism, we refer to the qubit states as \n0 09 and 0 19, whether the actual states are spin states, atomic states, or photon polarization states. But \nwe will make our discussion concrete when needed by reference to the spin-1/2 system, with the spin \nup state 0  +9 representing 0 09 and the spin down state 0  -9 representing 0 19:\n \n0 09 = 0  +9\n0 19 = 0  -9 . \n(16.16)\nSuperposition states\nThe key difference between bits and qubits is that qubits can exist in superposition states. A general \nqubit superposition is\n \n0 c9 = c00 09 + c10 19 \u0003 ac0\nc1\nb . \n(16.17)\nFor this superposition state, the probability that we measure the system to be in the 0 09 state is\n \nP0 = 0800 c90\n2 = 0 c00\n2, \n(16.18)\nand the probability that we measure the system to be in the 0 19 state is\n \nP1 = 0810 c90\n2 = 0 c10\n2. \n(16.19)\nThis is in stark contrast to a classical bit, which is either 0 or 1 with 100% probability. If that weren’t \nthe case, then our classical computers would not function very well!\nThe probabilistic nature of quantum mechanics doesn’t seem to bode well for the promise of a \nquantum computer. You would not buy a computer if the salesman told you that it would “probably” \nget the right answer. But quantum superposition states are more than simple probability mixtures of \ndifferent possibilities. A quantum superposition state is a coherent combination of states that does \ncontain an aspect of certainty that would be lacking in a classical bit that was only “probably” in the \none state. For example, the spin state\n \n0 c9 = 0  +9x =\n1\n12 0  +9 +\n1\n12 0  -9 \n(16.20)\n\n516 \nModern Applications of Quantum Mechanics\nhas 100% probability of being measured to be spin up along the x-axis, even though the probabilities \nof measuring the spin component on the z-axis are 50>50. So whether we view this state as lacking or \nhaving the certainty we expect from our computer depends on our point of view.\nSuperposition states are at the heart of the power of quantum information processing because the \namount of information contained in a quantum system grows exponentially with the number of qubits \nin the system. For example, if we build a system with 2 qubits, labeled A and B, then the basis states of \nthis system are the uncoupled basis states we used in Chapter 11:\n \n 0 009 = 0 09A0 09B\n \n 0 019 = 0 09A0 19B\n \n 0109 = 0 19A0 09B\n \n 0119 = 0 19A0 19B. \n(16.21)\nIn this 2-qubit system, a general superposition state is \n \n0 c9 = c000 009 + c010\n 019 + c100109 + c110119. \n(16.22)\nThis single 2-qubit state contains 22 = 4 pieces of information—the cij coefﬁcients. A classical 2-bit \nstate, such as 01, contains just two pieces of information. For an N-qubit system, a single superposition \nstate contains 2N pieces of information. The classical N-bit system does have 2N possible states, but \nany single state contains just N pieces of information.\nThough the N-qubit superposition state contains 2N pieces of information, it is not possible to mea-\nsure it all. When we measure the state of the system, we destroy much of the information by collapsing \nthe system state vector onto the measured state. For example, if we measure the spin components of \nthe two particles described by Eq. (16.22), we learn which one of the four basis states the system is \nin, just as we would for a classical 2-bit system. Even though there are 2N pieces of information in an \nN-qubit system, it turns out that we can extract only N pieces of classical information through our mea-\nsurements. You might ask whether we can call it information if we cannot know it! This question has \nspawned research into quantum information and how it differs from classical information. The trick of \nquantum computing is to harness the vast store of information that resides in the superposition state, \nbut is hidden from direct measurement. A number of algorithms have been discovered that access the \nhidden quantum information by performing operations that affect many or all of the qubits at once. By \nperforming these multiple operations simultaneously, we achieve quantum parallelism. You can also \nperform parallel computing with classical computers, but you do so by buying more computers!\nEntangled states\nThe power of quantum parallelism relies on the phenomenon of quantum entanglement that we intro-\nduced in Chapter 4. We learned there that entangled quantum states are responsible for the “spooki-\nness” of the Einstein-Podolsky-Rosen paradox. The EPR state 0 c9 =\n1\n12 1 0  +910  -92 - 0  -910  +922 of \nEq. (4.1) is entangled because measurements on one spin are perfectly anti-correlated with measure-\nments on the other spin. The EPR state is a speciﬁc example of the set of 2-qubit entangled states \nknown as Bell states. In terms of the basis states 0\n 009, 0\n 019, 0 109, and 0\n 119 of a 2-qubit system, the \nfour Bell states are\n \n 0\n b009 =\n1\n12 1 0 009 + 01192\n \n 0\n b019 =\n1\n12 1 0 019 + 01092\n \n 0\n b109 =\n1\n12 1 0 009 - 01192\n \n 0\n b119 =\n1\n12 1 0 019 - 01092. \n(16.23)\n\n16.2 Quantum Information Processing \n517\nThe EPR state of Chapter 4 is the Bell state 0\n b119. The Bell states comprise an alternate basis to the \nuncoupled and coupled bases we learned in Chapter 11. In quantum computing, we typically use either \nthe Bell basis or the uncoupled basis, which is called the computational basis.\nThe correlations of measurements on the EPR state, and the Bell states in general, show us that \nquantum mechanics is a nonlocal theory. Measuring one of the qubits affects the other, possibly dis-\ntant, qubit instantaneously. Rather than regarding these nonlocal correlations as spooky, we can use \nthem as a resource in quantum information processing. The nonlocal aspect of entangled states is \nuseful because we can act on one part of a system and control another part of the system, and we can \nmeasure one part to learn about another part or about the system as a whole. This is how quantum \nalgorithms are able to process the 2N pieces of information hidden in an N-qubit system. To be useful, \nthe quantum algorithms must be cleverly designed so that the answer we want is contained within the \nN pieces of classical information available through measurements on the system. It is no use having \nmore information available if we cannot access it after the calculation.\nThe importance of entangled states is also evident in our argument about the exponential increase \nin information content of a quantum superposition state. We said that the 2-qubit superposition state \n0\n c9 in Eq. (16.22) contains 22 = 4 pieces of information and that an N-qubit superposition contains \n2N pieces of information. However, there is a caveat to that statement. It turns out that there are some \nsuperposition states that have less information content because they can be expressed as a product of \n1-qubit states. An example of such a 2-qubit product state is\n \n0 c9 = 1a00 09A + a10 19A21b00 09B + b10 19B2. \n(16.24)\nProduct states do not exhibit correlations in measurement and, therefore, they are not entangled states; \nthey behave more like classical states. The 2-qubit state in Eq. (16.24) contains 2 * 2 = 4 pieces \nof information—the ai and bi coefﬁcients. For a general N-qubit system, a superposition state that is \na product state and so is not entangled contains 2 * N pieces of information. Unfortunately, for the \n2-qubit examples we have chosen, 22 = 4 and 2 * 2 = 4 are the same, so the difference between the \n2N exponential information content of general superposition states (which includes entangled states) \nand the 2 * N linear information content of non-entangled states is not immediately evident. We’ll \nleave it to you to explore the N = 3 case in Problem 16.8 and distinguish the difference. The take-\nhome message is that access to the power of quantum parallelism requires the use of entangled states.\nQuantum computing algorithms are designed to process the hidden information in the large Hilbert \nspace in a way that the desired result is brought out in the measured qubits. Two of the most impres-\nsive quantum algorithms are Shor’s factorization algorithm and Grover’s search algorithm. Factoring \na large number into its two prime factors is a difﬁcult task for a classical computer. In 1994, Peter Shor \ndeveloped a quantum algorithm that ﬁnds the prime factors of an integer in a time that is faster than a \nclassical computer by a factor that is exponential in the number of digits of the number being factored. \nBecause of the importance of factoring in encryption, Shor’s algorithm has inspired many to try to build \na quantum computer. Grover’s search algorithm allows a quantum computer to search an unsorted data-\nbase of N entries in a time proportional to 1N, compared to a classical computer that requires a time that \nis proportional to N. Details of these algorithms are available in the resources at the end of the chapter.\nQuantum algorithms are not immune to the probabilistic nature of quantum mechanics. If we run \nthe same program twice on a quantum computer, then we might get two different answers. The power \nof quantum computing is that it can produce answers in many fewer steps than a classical computer. As \nlong as we can easily conﬁrm the answers on a classical computer, then the time advantage overcomes the \nneed to run the program many times. For example, as hard as it is to ﬁnd prime factors of a large number, \nit is trivial to check whether the product of the two proposed factors do in fact yield the original number. \nLikewise, as hard as it is to ﬁnd a needle in a haystack, it is simple to determine if the object you ﬁnd is a \nneedle, so conﬁrming the result of a quantum search algorithm is straightforward on a classical computer.\n\n518 \nModern Applications of Quantum Mechanics\n 16.2.2 \u0002 Quantum Gates\nTo process information, a classical computer uses gates that operate on bits. A few typical classical \ngates are shown in Fig. 16.13 along with the truth tables that describe their operation. The NOT gate is \na 1-bit gate with one input bit and one output bit. The AND and OR gates are 2-bit gates with two input \nbits and one output bit. Using a small set of such binary logic gates, albeit a large number of them, \nclassical computers perform a wide range of tasks.\nQuantum computers likewise rely on a small set of 1- and 2-qubit gates to perform their tasks. \nThe measurement devices we have encountered throughout this text, like Stern-Gerlach devices, are \nnot quantum gates. Rather, quantum gates are devices that alter the relative coefﬁcients in a qubit \nsuperposition without destroying the coherence. A 1-qubit gate has an input state 0 cin9 and an output \nstate 0 cout9, which we write as\n \n0 cin9 = c00 09 + c10 19 \n(16.25)\nand\n \n0 cout9 = c =\n00 09 + c =\n10 19. \n(16.26)\nFor any general 1-qubit quantum gate, we represent the transformation from input to output states in \nmatrix notation as\n \n¢\nc =\n0\nc =\n1\n≤= ¢\nU11\nU12\nU21\nU22\n≤¢\nc0\nc1\n≤ . \n(16.27)\nThe transformation matrix U must be a unitary matrix (UU- = 1) to preserve the coherence of the \nqubit. The matrix elements of the transformation tell us how the qubit is changed by the gate. For \nexample, a quantum NOT gate changes 0 09 S 0 19 and also 0 19 S 0 09. The quantum NOT gate is a \nlinear operator, so it also changes a superposition a0 09 + b0 19 S b0 09 + a0 19. The quantum NOT \ngate unitary transformation matrix is\n \nUNOT \u0003 a0\n1\n1\n0b . \n(16.28)\nx    z\n0    1\n1    0\nx    y    z\n0    0    0\n0    1    0\n1    0    0\n1    1    1\nx    y    z\n0    0    0\n0    1    1\n1    0    1\n1    1    1\nx\nz\nNOT gate\nx\ny\nz\nAND gate\nx\ny\nz\nOR gate\nFIGURE 16.13 Classical logic gates.\n\n16.2 Quantum Information Processing \n519\nThis unitary operator looks similar to the Sx operator for a spin-1/2 system. That is not a coinci-\ndence. It turns out that all unitary operators for a spin-1/2 system can be expressed as a linear combina-\ntion of the four operators comprising the identity matrix 1 and the three spin-1/2 angular momentum \ncomponent operators, with the factor of U>2 removed. These dimensionless matrices are called the \nPauli matrices and are\n \nsx = a0\n1\n1\n0b     sy = a0\n-i\ni\n0 b     sz = a1\n0\n0\n-1b . \n(16.29)\nThe unitary transformation of a spin-1/2 system also has a convenient geometric interpretation \nas a rotation or a series of rotations of the spin, as affected by the spin precession we discussed in \nChapter 3. For example, the quantum NOT gate is performed by a p rotation about the x-axis, as \ndepicted in Fig. 3.8 for a state that is initially spin up. Let’s now show that this is also true for a \ngeneral initial state.\nExample 16.1 Quantum NOT gate Show that the spin precession transformation of a general \nspin state for a p rotation about the x-axis is equivalent to a quantum NOT gate.\nFor the spin to precess about the x-axis, we apply a magnetic ﬁeld B0 in the x-direction (see \nSection 3.2). The energy states in this applied ﬁeld are 0 {9x and the energies are E{ = {U v0>2, \nwhere v0 = eB0>me is the Larmor precession frequency. To ﬁnd how the state vector is changed by \nthe applied magnetic ﬁeld, we use the Schrödinger time-evolution recipe. The initial general state is\n \n0\n c1029 = c+ 0  +9 + c- 0  -9. \n(16.30)\nWe must write this state in the energy basis, which is the Sx basis in this case:\n \n 0\n c1029 = 1 0  +9x  x8+\n 0 + 0  -9x  x8-  0 20 c1029\n \n \n = c+ 1x8+\n 0  +9 0  +9x + x8-\n 0  +9 0  -9x2 + c-1x8+\n 0  -9 0  +9x + x8-\n 0  -9 0  -9x2 \n \n =\n1\n12 1c+ + c-20  +9x +\n1\n12 1c+ - c-20  -9x .\n \n(16.31)\nTo ﬁnd the time-evolved state, we insert the time-dependent phase factor for each energy basis \nstate:\n \n 0\n c1t29 =\n1\n12 1c+ + c-2e-iE+\n t>U0  +9x +\n1\n12 1c+ - c-2e-iE-\n t>U0  -9x  \n \n =\n1\n12 1c+ + c-2e-iv 0t>20  +9x +\n1\n12 1c+ - c-2e+iv 0t>20  -9x . \n(16.32)\nAs we saw in Eq. (3.35) and Fig. 3.3, the angle of spin precession is v0t, so to have a p rotation \nabout the x-axis requires that the ﬁeld be applied long enough to have v0t = p . Thus the state vec-\ntor after the time evolution is\n \n 0\n c1t29 =\n1\n12 1c+ + c-2e-ip>20  +9x +\n1\n12 1c+ - c-2e+ip>20  -9x\n \n \n =\n-i\n12 1c+ + c-2 1\n12 10  +9 + 0  -92 +\ni\n12 1c+ - c-2 1\n12 10  +9 - 0  -92 \n(16.33)\n \n = -i 1c- 0  +9 + c+ 0  -92,\n \n \nor in matrix notation:\n \nac =\n+\nc =\n-\nb = -i a0\n1\n1\n0bac+\nc-\nb . \n(16.34)\n\n520 \nModern Applications of Quantum Mechanics\nThe overall phase e-ip>2 = -i does not produce any measurable effects, so we ignore it in deﬁning \nthe quantum NOT gate transformation matrix:\n \nUNOT \u0003 a0\n1\n1\n0b . \n(16.35)\nA schematic diagram of this spin-precession experiment is shown in Fig. 16.14. The unitary spin \nprecession is performed by the magnet (box with “X ”), while the Stern-Gerlach devices perform \nmeasurements, which are nonunitary transformations. (Recall from SPINS Lab 4 that the number \n“18” in the magnet box rotates the spin by 180°.)\nRotations due to spin precession about the other Cartesian axes produce two more 1-qubit gates. \nThe quantum Z gate is a p rotation around the z-axis, with a transformation matrix (Problem 16.10)\n \nUZ \u0003 a1\n0\n0\n-1b \n(16.36)\nthat is equal to the Pauli sz matrix. The quantum Y gate is a p rotation around the y-axis, with a trans-\nformation matrix (Problem 16.11)\n \nUY \u0003 a0\n-i\ni\n0 b \n(16.37)\nthat is equal to the Pauli sy matrix.\n?\n?\n18\n^n\nX\nZ\ny\nx\nz\nΩ0t\na)\nb)\n\u0004S(0)\u0003\n\u0004S(t)\u0003\nFIGURE 16.14 (a) A Stern-Gerlach spin precession experiment and (b) the resulting precession \nof the spin vector around the x-axis for the case of a p rotation.\n\n16.2 Quantum Information Processing \n521\nOne other important 1-qubit gate is the Hadamard gate, with a transformation matrix\n \nUH \u0003\n1\n22\n a1\n1\n1\n-1b . \n(16.38)\nThe Hadamard gate can be made with a p rotation around the z-axis followed by a p>2 rotation around \nthe y-axis (Problem 16.12). The Hadamard gate transforms basis states into superposition states:\n \n UH0\n 09 =\n1\n12 10\n 09 + 0 192  \n \n UH0 19 =\n1\n12 10\n 09 - 0 192 . \n \n(16.39)\nGiven the importance of superposition states in quantum information processing, this is a useful \ngate. Note that the symbol “H ” is used for the Hadamard gate, and it must not be confused with the \nHamiltonian.\nThough we have explained the unitary transformations of 1-qubit gates in terms of the precession \nof a spin-1/2 particle in a magnetic ﬁeld, these same transformations apply to any two-level system. \nThe physical mechanisms for effecting the transformations are different, but the matrices describ-\ning them are the same. For example, pulses of light can transform an atom into a superposition of \nstates to effect a Hadamard gate. Figure 16.15 depicts a general Stern-Gerlach spin precession experi-\nment (a) using our schematic diagram from the SPINS program and (b) using a simpliﬁed schematic \nused for describing quantum information processing in general. The quantum Z gate performs the UZ \ntransformation and the quantum X gate (NOT gate) performs the UNOT = UX transformation. The \nStern-Gerlach measurement devices are not quantum gates because they do not perform a unitary \ntransformation, so we do not depict them in Fig. 16.15(b).\nThe quantum gates we have described so far are all 1-qubit gates, but the power of quantum \ninformation processing resides in entangled superposition states, so multiqubit gates are required. It \n?\n?\n18\n^n\nX\nSpin precession\nState\npreparation\nState\nmeasurement\nLoad\nqubit\nQuantum gates\nRead\nqubit\na) Stern-Gerlach schematic\nb) Quantum computing schematic\n18\nZ\nZ\nX\nq\nin\nq\nout\nY\nFIGURE 16.15 (a) A Stern-Gerlach spin precession experiment and (b) the equivalent experiment \ndepicted with quantum gates.\n\n522 \nModern Applications of Quantum Mechanics\nturns out that we can perform all the quantum tasks we need with 1-qubit gates and one type of 2-qubit \ngate. The 2-qubit gate we need is a Controlled-NOT gate (CNOT gate). A CNOT gate has two input \nqubits, referred to as the control and target qubits, and two output qubits. The target qubit is negated \n(by a 1-qubit NOT gate) if the control qubit is in state 0 19C . If the control qubit is in state 0\n 09C , then \nthe target qubit is unchanged. In both cases, the control qubit is unaltered by the gate. We denote the \ntwo-qubit states as 0 i j9 = 0 i9C 0   j9T and the transformations of the CNOT gate are\n \nUCNOT 0 009 = 0 009\nUCNOT 0 019 = 0 019\nUCNOT 0 109 = 0 119\nUCNOT 0 119 = 0 109.\n \n(16.40)\nThe transformation matrix of a CNOT gate is (Problem 16.13)\n \nUCNOT \u0003 ±\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n≤, \n(16.41)\nand the transformation of a general 2-qubit state is\n \nUCNOT 0\n c9 \u0003 §\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n¥ §\nc00\nc01\nc10\nc11\n¥ = §\nc =\n00\nc =\n01\nc =\n10\nc =\n11\n¥ . \n(16.42)\nA schematic diagram of a CNOT gate is shown in Fig. 16.16. The 1-qubit NOT (X) gate acts on the \ntarget bit based upon the condition of the control bit. The conditional connection is depicted by the \nvertical line and node connecting the control qubit with the NOT gate.\nThe physical implementation of a CNOT gate is more complicated than the 1-qubit gates described \nabove. The conditional connection between the two qubits requires an interaction between the two \nphysical qubits. For example, two spin-1/2 particles can interact through their magnetic moments, \ncausing a coupling of the Larmor precession frequencies. \nOne of the most important applications of a CNOT gate is to make entangled states. To make an \nentangled 2-qubit state, like an EPR state, we combine a 1-qubit Hadamard gate and a 2-qubit CNOT \ngate, as shown in Fig. 16.17. The Hadamard gate acts on the input control qubit to place it into a super-\nposition state, then the CNOT gate couples the two qubits together to make an entangled state.\nX\n\u0002j'\u0003Target\n\u0002j \u0003Target\n\u0002i \u0003Control\n\u0002i \u0003Control\nFIGURE 16.16 A 2-qubit controlled-NOT gate has a 1-qubit NOT gate (X ) on \nthe target qubit, which is conditionally activated based upon the control qubit.\n\n16.2 Quantum Information Processing \n523\nExample 16.2 Entangled state preparation Show that the combination of a Hadamard gate \nand a CNOT gate (Fig. 16.17) acting on the input state 0 119 produces an entangled Bell state.\nThe input state of the system is\n \n0\n c19 = 0 119 = 0 19C0\n 19T . \n(16.43)\nThe Hadamard gate acts only on the control qubit, with the result\n \n0\n c29 = UHad,C 0\n c19 = 1UHad,C 0 19C2 0 19T . \n(16.44)\nThe transformation of the single control qubit is\n \n UHad,C0\n 19C \u0003\n1\n12\n a1\n1\n1\n-1ba0\n1b  \n \n \u0003\n1\n12\n a 1\n-1b\n \n(16.45)\n \n =\n1\n12\n 1 0\n 09C - 0\n 19C2 . \nThe resultant state of the 2-qubit system before the CNOT gate is\n \n0\n c29 =\n1\n12\n 1 0\n 09C - 0 19C20\n 19T =\n1\n12\n 1 0\n 019 - 0 1192 \u0003\n1\n22\n ±\n0\n1\n0\n-1\n≤ . \n(16.46)\nThe transformation of the CNOT gate is\n \n0\n c39 = UCNOT 0\n c29 \u0003\n1\n22\n ±\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n≤±\n0\n1\n0\n-1\n≤= ±\n0\n1\n-1\n0\n≤ . \n(16.47)\nThe output state is thus\n \n0\n c39 =\n1\n12\n 1 0\n 019 - 0 1092. \n(16.48)\nThis is the entangled Bell state 0 b119 from Eq. (16.23).\nH\nX\n\u0002Ψ1\u0003\n\u0002Ψ2\u0003\n\u0002Ψ3\u0003\n\u0002i \u0003\n\u0002Βij\u0003\n\u0002j \u0003\u0005\nFIGURE 16.17 Preparation of an entangled Bell state by \napplication of a Hadamard gate (H ) and a controlled-NOT gate.\n\n524 \nModern Applications of Quantum Mechanics\nNow we see why we labeled the Bell states as we did. The particular Bell state produced in \nExample 16.2 is labeled 0\n b119 because we started with the input state 0 119. The four Bell states are pro-\nduced by using one of the basis states 0\n 009, 0\n 019, 0\n 109, or 0\n 119 as the input into the combined Hadamard \nand CNOT gates (Problem 16.14).\nThe beauty of the 2-qubit CNOT gate is that it lets us transform between the computational basis \nand the Bell basis, when combined with the 1-qubit Hadamard gates. If we reverse the order of the \nHadamard and CNOT gates, as shown in Fig. 16.18, then Bell states are transformed into computa-\ntional basis states (Problem 16.15). Hence, to determine which Bell state a system is in, we perform the \ntransformation in Fig. 16.18 and then measure the single qubits (e.g., the z-components of the spins). \nThe four possible results ij = 00, 01, 10, 11 then correspond to the four Bell states of Eq. (16.23). \nThis is called a Bell-state measurement.\n 16.2.3 \u0002 Quantum Teleportation\nUsing the tools we have described above, we now illustrate the use of entangled states (quantum \nspookiness) as a resource. The problem we want to solve is how to transmit information about an \nunknown quantum state. Imagine that Carol has given Alice a “secret” message in the form of a single \nqubit that she wants Alice to transmit to Bob. Without giving the qubit directly to Bob, how can \nAlice convey the information with the highest probability of success? The answer lies in utilizing \nentangled states, as depicted in Fig. 16.19. In a nutshell, Alice and Bob share an entangled state of \ntwo qubits that was previously prepared and is independent of Carol’s secret message qubit. Alice \nperforms a  Bell-state measurement on the two-state system comprising Carol’s qubit and Alice’s half \nof the entangled state she shares with Bob. Alice than transmits the results of her measurement to Bob \nwho performs a unitary transformation on his half of the entangled state, and voilà, his qubit is in the \nsame state as Carol’s secret message. Let’s see how this works in detail.\nAlice and Bob have previously met and share an entangled state, meaning that each has one of \nthe two qubits of a Bell state, which we assume to be the 0\n b009 state. Using explicit subscripts to dis-\ntinguish the different qubits held by Alice (A), Bob (B), and Carol (C), we denote the entangled state \nshared by Alice and Bob as\n \n0\n b009AB =\n1\n12 10\n 009AB + 0 119AB2 =\n1\n12 0\n 09A0\n 09B +\n1\n12 0\n 19A0\n 19B . \n(16.49)\nThe secret qubit that Carol wants Alice to convey to Bob is in a general, unknown superposition state\n \n0\n csecret9C = a00\n 09C + a10\n 19C . \n(16.50)\nH\nX\n\u0002Βij\u0003\n\u0002Ψ1\u0003\n\u0002Ψ2\u0003\n\u0002Ψ3\u0003\n\u0002i \u0003\n\u0002j\u0003\nFIGURE 16.18 Transformation of a Bell state to the computational \nbasis with a CNOT gate and a Hadamard gate.\n\n16.2 Quantum Information Processing \n525\nIf Alice had many copies of this state, she could make repeated measurements and determine the coef-\nﬁcients a0 and a1 with a statistical uncertainty based on the number of copies (as you did in SPINS \nlab 1). But with only one copy of the state, Alice is hard pressed to make a meaningful measurement of \nthe state and send the secret message to Bob.\nAlice’s solution is to make a joint measurement on the system comprising the secret qubit C and \nthe single qubit A of the entangled 0\n b009AB state that she shares with Bob. By a joint measurement, we \nmean that she performs a Bell-state measurement by applying a CNOT gate and a Hadamard gate to \nthe A and C qubits to transform to the computational basis (Fig. 16.18) and then measuring the single \nqubits. To see why Alice’s Bell-state measurement is useful, consider the state vector for the complete \nthree-qubit system\n \n 0\n cABC9 = 0\n  b009AB0\n  csecret9C\n \n \n = A 1\n12 0\n 09A0\n 09B +\n1\n12 0 19A0 19BBAa00\n 09C + a10 19CB\n \n \n =\na0\n12 0\n 09A0\n 09B0\n 09C +\na0\n12 0 19A0 19B0\n 09C +\na1\n12 0\n 09A0\n 09B0 19C +\na1\n12 0 19A0 19B0 19C . \n(16.51)\nThe qubits A and C are not entangled (they have never interacted), but we are free to write the system \nstate vector in terms of the basis of entangled Bell states 0 bij9AC of those two qubits. Some algebra \nreveals that the state vector of the system expressed in this way is (Problem 16.16)\n \n 0\n cABC9 = 1\n2\n 5 0\n b009AC  1a00\n 09B + a10 19B2 \n \n + 0\n b019AC  1a10\n 09B + a00 19B2\n \n \n + 0\n b109AC  1a00\n 09B - a10 19B2\n \n \n + 0\n b119AC  1a10\n 09B - a00 19B26. \n \n(16.52)\n00\n01\n10\n11\n00\n01\n10\n11\nBOB: Unitary Transformation\nALICE:  Bell State Measurement\nCAROL\nC\nA\nB\nClassical\nInformation\n\u0002Ψsecret\u0003B\n\u0002Ψsecret\u0003C\n\u0002Β00\u0003AB\nFIGURE 16.19 Quantum teleportation of a secret qubit from Alice to Bob. Alice and Bob share \nthe entangled qubit pair AB. Alice makes a Bell-state measurement upon the AC qubit pair. Alice \ntransmits the result, 10 for example, to Bob, who applies the appropriate unitary transformation  \n(see Table 16.2) to his qubit B, which is then in the same state as the original secret qubit C.\n\n526 \nModern Applications of Quantum Mechanics\nBy expressing the state vector in this Bell basis, we identify a correlation between each Bell state of \nthe qubits A and C and the state of Bob’s qubit B, which turns out to be a superposition state with the \nsecret coefﬁcients from Carol! For example, if Alice’s Bell-state measurement indicates that the A and \nC qubits are in the state 0\n b009AC , then Bob’s qubit B is in the state\n \n0\n c9B = a00\n 09B + a10 19B. \n(16.53)\nThis is exactly the secret state that Carol gave to Alice. There are four possible results of Alice’s \nBell-state measurement, each with a probability of 25%, indicating that the qubits A and C are indeed \nnot entangled. If Alice measures one of the other Bell states, then she communicates her results to Bob \nover a classical channel (she calls him on the phone) and tells him to perform a unitary transformation \non his qubit to change it to the secret state. The transformations that Bob must perform are indicated in \nTable 16.2 (Problem 16.17).\nWith this quantum teleportation scheme, Alice has conveyed Carol’s secret message to Bob using \nonly a classical information channel, and the prearranged Bell state 0\n b009AB. Note that neither Alice \nnor Bob know what the secret state is. Alice has destroyed all her qubits by measuring them, and her \nresults reveal no information to her about the secret message. Bob has not measured anything yet, but \nhas the secret qubit in his possession as long as he does what Alice tells him to do. More precisely, he \nhas a qubit that is in the same state as Carol’s original qubit. The actual physical qubit representing the \nsecret message (e.g., a particle with spin) is still with Alice, or destroyed in detection. Only the quan-\ntum information about the state of the secret qubit has been teleported to Bob.\nThis scheme is made possible by the Bell state that Alice and Bob have set up previously. The \ncorrelations inherent in that entangled state allow Alice to tell Bob what quantum gates he must use to \ntransform his half of their Bell state into the secret message. This is one of many examples that demon-\nstrate the utility of entangled quantum states for information processing.\nSUMMARY\nThese two examples have provided a mere taste of the fun you can have with quantum mechanics. \nMagnetic trapping, laser cooling, and quantum information processing are just a few of the current \nresearch topics that employ the quantum mechanics you have learned in this text. If our brief overview \nhas raised more questions than we have answered, then we have at least planted the seed for you to \ndelve deeper into these subjects. As with any research ﬁeld, there are still more questions to be raised \nand answers to be discovered. Enjoy!\nTable 16.2 Quantum Teleportation of a Secret State from Alice to Bob\nAlice measures\nAlice transmits\nBob applies\nBob transforms\n0b009\n00\n1\n11a0009B + a1019B2 = 0csecret9B\n0b019\n01\nUNOT\nUNOT 1a1009B + a0019B2 = 0csecret9B\n0b109\n10\nUZ\nUZ 1a0009B - a1019B2 = 0csecret9B\n0b119\n11\nUZUNOT\nUZ\n UNOT 1a1009B - a0019B2 = 0csecret9B\n\nProblems \n527\nPROBLEMS\n 16.1 Calculate the angular deﬂection of a room temperature rubidium atom traveling through the \nmagnetic ﬁeld gradient of a Stern-Gerlach device. Assume the gradient is 100 G>cm = 1 T>m \nand that the magnetic moment is one Bohr magneton.\n 16.2 Show that for an atom in a typical magnetic trap, the magnetic moment adiabatically follows \nthe changing magnetic ﬁeld direction. That is, show that the Larmor precession frequency \nis much larger than the frequency of motion in the trap. Estimate the motional frequency by \nconsidering the circular motion (radius 1 cm) of a rubidium atom in the trapping potential \nshown in Fig. 16.4.\n 16.3 Find the distance required to stop a room-temperature rubidium atom with the resonant scat-\ntering force. Do the same for a sodium atom.\n 16.4 The general expression for the scattering force that is valid for all intensities is\nFscatt = Uk A21\n2  I\nI0\n \naA21\n2 b\n2\n1vLaser - v21 + kv2\n2 + aA21\n2 b\n2\na1 + I\nI0\nb\n ,\n \n where I0 is a characteristic intensity. Show that this force has the same maximum value given \nby Eq. (16.8) and state the conditions required to achieve that maximum force. Plot the force \nas a function of intensity and suggest a name for I0.\n 16.5 Show that rubidium atoms with velocity v = 350 m>s are resonant with a counterpropagat-\ning laser with a frequency detuning fLaser - f21 = -450 MHz.\n 16.6 Calculate the maximum chirp rate (frequency change per unit time interval) of a laser used in \nchirped laser cooling of rubidium atoms.\n 16.7 Calculate the linear friction coefﬁcient of optical molasses, (i.e., ﬁnd the slope of the force \ncurve in Fig. 16.12 for low velocities).\n 16.8 For an N-qubit system, a general superposition state (which includes entangled states) \n contains 2N pieces of information [see Eq. (16.22)] and a product superposition state \n (nonentangled) contains 2 * N pieces of information [see Eq. (16.24)]. Demonstrate this \nfor N = 3 and N = 4.\n 16.9 Verify the operation of a quantum NOT gate by acting on the computational basis states with \nthe unitary matrix UNOT. Demonstrate that the transformation matrix is unitary by showing \nthat the norm of a general superposition state is unchanged by the transformation.\n 16.10 Show that the transformation matrix for a p rotation about the z-axis is the Pauli matrix sz. \n(Hint: as in Example 16.1, ignore an overall phase.)\n 16.11 Show that the transformation matrix for a p rotation about the y-axis is the Pauli matrix sy . \n(Hint: as in Example 16.1, ignore an overall phase.)\n 16.12 Show that the Hadamard gate for a spin-1/2 system can be made with a p rotation about the \nz-axis followed by a p>2 rotation about the y-axis.\n 16.13 Using the transformation equations of the CNOT gate in Eq. (16.40), derive the transforma-\ntion matrix in Eq. (16.41).\n 16.14 Show that the combination of a Hadamard gate and a CNOT gate (see Fig. 16.17) transforms \nthe computational basis states 0\n 009, 0\n 019, and 0 109 into Bell states.\n\n528 \nModern Applications of Quantum Mechanics\n 16.15 Show that the combination of a CNOT gate and a Hadamard gate (see Fig. 16.18) transforms a \nBell state into a computational basis state, for each of the possible Bell states.\n 16.16 Show that the complete state vector for Alice, Bob, and Carol’s qubits can be written as in\nEq. (16.52).\n 16.17 Show that the transformations that Alice asks Bob to do (see Table 16.2) produce the secret \nstate.\nRESOURCES\nFurther Reading\nThese references provide further details on laser cooling and magnetic traps for atoms:\nH. J. Metcalf and P. van der Straten, Laser Cooling and Trapping, New York: Springer \n University Press, 1999.\n \nC. J. Foot, Atomic Physics, Oxford: Oxford University Press, 2005.\n \nM. Fox, Quantum Optics: An Introduction, Oxford: Oxford University Press, 2006.\nB. H. Bransden and C. J. Joachain, Physics of Atoms and Molecules, 2nd ed., Harlow, England: \nPrentice Hall, 2003.\nC. Wieman, G. Flowers, and S. Gilbert, “Inexpensive laser cooling and trapping experiment for \nundergraduate laboratories,” Am. J. Phys. 63, 317–330 (1995).\n \nP. Gould, “Laser cooling of atoms to the Doppler limit,” Am. J. Phys. 65, 1120–1123 (1997).\nThe Nobel Prize in Physics 1997, Nobelprize.org: \n \nhttp://nobelprize.org/nobel_prizes/physics/laureates/1997/\nThese references provide further details on magnetic traps for macroscopic objects:\n \nA. Geim, “Everyone’s Magnetism,” Phys. Today 51(9), 36–39 (1998).\nM. D. Simon, L. O. Heﬂinger, and S. L. Ridgway, “Spin stabilized magnetic levitation,” Am. J. \nPhys. 65, 286–292 (1997).\n \nM. V. Berry and A. K. Geim, “Of ﬂying frogs and levitrons,” Eur. J. Phys. 18, 307–313 (1997).\nM. V. Berry, “The LevitronTM: an Adiabatic Trap for Spins,” Proc. R. Soc. Lond. A 452, \n 1207–1220 (1996).\nT. B. Jones, M. Washizu, and R. Gans, “Simple theory for the Levitron,” J. Appl. Phys. 82, \n883–888 (1997).\nThese references provide further details on quantum information processing:\n \nR. P. Feynman, “Simulating Physics with Computers,” Int. J. Theor. Phys. 21, 467–488 (1982).\n \nS. M. Barnett, Quantum Information, Oxford: Oxford University Press, 2009.\nP. Kaye, R. Laﬂamme, and M. Mosca, An Introduction to Quantum Computing, Oxford: Oxford\nUniversity Press, 2007.\nM. A. Nielsen and I. L. Chuang, Quantum Computation and Quantum Information, Cambridge: \nCambridge University Press, 2000.\n\n \n529\nA P P E N D I X\nA\n \nProbability\nQuantum mechanics is inherently a probabilistic theory, so we present here a brief review of some \nimportant concepts in probability theory. We distinguish between discrete probabilities, encountered \nin spin measurements, and continuous probabilities, encountered in position measurements.\nA.1 \u0002 DISCRETE PROBABILITY DISTRIBUTION\nImagine collecting together all the grades that students received in your English class last term. You \nﬁnd that the students received 8 A’s, 14 B’s, 7 C’s and 1 D. Though these are not random events, you \ncould still ask, what is the probability of receiving an A? A classmate received an A grade nA = 8 \ntimes out of the 30 total students, so the probability is the ratio\n \nPA =\nnA\nnA + nB + nC + nD\n= 8\n30. \n(A.1)\nYou calculate all four probabilities and represent them in a histogram, such as shown in Fig. A.1. \nThis set of probabilities is a discrete probability distribution. In this case, the distribution has been \ndetermined by experiment. In some cases, such as throwing dice, the probability distribution can be \ncalculated theoretically and compared to experiment.\nIn the general case, we label the possible results (e.g., grades) xi, and if there are N possible \nresults that can occur, then the probability of any one result is\n \nPxi =\nnxi\na\nN\ni=1\nnxi\n. \n(A.2)\nA\nB\nC\nD\nGrade\n0.5\n1.0\nPxi\nPA\nPB\nPC\nPD\nFIGURE A.1 The histogram of grades received in an English class.\n\n530 \nProbability\nThe sum of the individual probabilities must be one because you are certain to get some result. The \ngeneral statement of this condition is\n \na\nN\ni=1\nPxi = 1. \n(A.3)\nOf course, the most obvious use of the grade probability distribution is for calculating a grade \npoint average (GPA). Given the standard assignment of grade points A = 4, etc., the class GPA is\n \n GPA = 4nA + 3nB + 2nC + 1nD\nnA + nB + nC + nD\n \n \n = 4PA + P3PB + 2PC + 1PD. \n(A.4)\nIn the general case, we calculate the average or mean of the possible results using\n \n8x9 = a\nN\ni=1\nxi Pxi  , \n(A.5)\nwhere we use the angled brackets 89 to denote the average. In quantum mechanics, the average is \nreferred to as the expectation value, which is a bit misleading because it is not the value you expect to \nget. In fact, the expectation value is in general not one of the possible results. The class GPA may be \n3.14, but no student received that value as a grade.\nWe also quantify probability distributions by the spread of the distribution. The most used measure \nof the spread is the standard deviation s, deﬁned as the square root of the average of the squares of the \ndeviations from the average! Once more, slowly: (1) ﬁnd the deviation of each result xi from the average \nvalue 8x9; (2) square the deviations (to avoid negative values); (3) average all possible squared devia-\ntions, weighted by the probabilities of each result, as in Eq. (A.5); and (4) take the square root. This is also \ncalled the root-mean-square deviation, or rms deviation. Mathematically, the standard deviation is\n \ns = 281x - 8x9229 = B a\nN\ni=1\n1xi - 8x92\n2\n Pxi. \n(A.6)\nThe variance s2 is the square of the standard deviation.\nThere is a useful shortcut for evaluating the standard deviation of a probability distribution. Con-\nsider the variance:\n \ns2 = H1x - 8x92\n2I = a\nN\ni=1\n1xi - 8x92\n2 Pxi. \n(A.7)\nExpand the square\n \n s2 = a\nN\ni=1\nAx2\ni - 2 xi8x9 + 8x9\n2BPxi\n \n \n = a\nN\ni=1\nx2\ni  Pxi - a\nN\ni=1\n2xi8x9Pxi + a\nN\ni=1\n8x9\n2Pxi \n(A.8)\n \n = a\nN\ni=1\nx2\ni  Pxi - 28x9 a\nN\ni=1\nxi Pxi + 8x92 a\nN\ni=1\nPxi \n\nand use the deﬁnition of the average in Eq. (A.5) and the normalization condition in Eq. (A.3) to get\n \n s2 = 8x29 - 28x98x9 + 8x9\n2 \n \n = 8x29 - 8x9\n2.\n \n(A.9)\nSo the variance is also the difference between the average of the squares and the square of the average, \nwhere the average of the squares of the possible results is\n \n8x29 = a\nN\ni=1\nx 2\ni  Pxi. \n(A.10)\nNote that the square 8x9\n2 of the average and the average 8x29 of the squares are not generally equal. In \nfact, Eq. (A.9) implies that the 8x9\n2 = 8x29 only if the variance is zero, which happens only if there \nis no spread in the distribution, that is, there is only one possible result. Using Eq. (A.9), we write the \nstandard deviation as\n \ns = 28x29 - 8x92  . \n(A.11)\nIn quantum mechanics, we use the standard deviation for the uncertainty, and we use the symbol \u0006x \nor \u0006p or \u0006Sz instead of s.\nA.2 \u0002 CONTINUOUS PROBABILITY DISTRIBUTION\nIf the possible results of an experiment form a continuum rather than a discrete set, then we must \nmodify some of the deﬁnitions from the last section. Rather than speaking of a probability for a spe-\nciﬁc result, we must speak of the probability for a range of results within some interval. For example, \nif you were a product tester and were charged with specifying how long the battery lasts on a laptop \ncomputer, then you might make a series of measurements of the time it takes for the laptop to drain the \nbattery. Time is a continuum, but if you made measurements to the nearest minute, then the histogram \nof probability results would have bins of 1 minute on the time axis, as indicated in Fig. A.2(a) where \nwe use the nontraditional convention of labeling time with x to follow our notation in the last section. \nFor small enough intervals, you would expect that the probability Pxi6x6xi+\u0006x of obtaining a result \nx (min)\n3550\n3600\n3650\n0.00\n0.01\n0.02\n(a)\nPxi\nFIGURE A.2 (a) The discrete probability distribution of battery lifetimes expressed as a histogram, \nand (b) the continuous probability distribution of lifetimes expressed as a function.\nx\n3550\n3600\n3650\nx (min)\n0.00\n0.01\n0.02\n(b)\nP\nA.2 Continuous Probability Distribution \n531\n\n532 \nProbability\nwithin the time interval xi 6 x 6 xi + \u0006x is proportional to the width of the interval and to a factor \ntelling you the likelihood of results in that interval. We express this as\n \nPxi6x6xi+\u0006x = P1xi2\u0006x, \n(A.12)\nwhere P1xi2 is the likelihood factor. Because the probability Pxi6x6xi+\u0006x is a dimensionless number \nand \u0006x has dimensions (x could be time, height, velocity, etc.), the likelihood factor P1xi2 must have \ndimensions of 1>x. We call this the probability density P1x2 because it is the probability per unit \ntime (or height or velocity, etc.). We distinguish the probability density from a probability by denot-\ning it as a function rather a subscripted value. The probability density is a continuous probability \n distribution, in contrast to the discrete probability distribution in the previous section. For the battery \nexperiment, the continuous probability distribution is shown in Fig. A.2(b).\nFor a continuous probability distribution, the condition that the sum of the individual probabili-\nties must be one becomes an integral\n \nL\n\u0005\n- \u0005\nP1x2  dx = 1. \n(A.13)\nThe average or expectation value is\n \n8x9 =\nL\n\u0005\n- \u0005\nx P1x2  dx \n(A.14)\nand the expectation value of any other function of the measurement variable is\n \n8 f 1x29 =\nL\n\u0005\n- \u0005\nf  1x2P1x2  dx. \n(A.15)\nThe standard deviation is still deﬁned by Eq. (A.11)\n \ns = 28x29 - 8x92, \n(A.16)\nwith the new deﬁnition of the average in Eq. (A.14).\n\n \n533\nA P P E N D I X \nB\nComplex Numbers\nComplex numbers are a critical component of the mathematics of quantum mechanics, so we provide \na brief review here. Complex numbers are an extension of the real numbers to include an additional \nimaginary part. The imaginary number i is the square root of –1:\n \ni = 2-1. \n(B.1)\nA complex number has a real part and imaginary part and is written as\n \nz = a + ib, \n(B.2)\nwhere this form assumes that a and b are real values. We refer to a as the real part of z and b as the \nimaginary part of z, and denote them as\n \n a = Re 1z2  \n \n b = Im 1z2. \n \n(B.3)\nWhen we add two complex numbers together, we must keep the real and imaginary parts separate:\n \nz1 + z2 = 1a1 + ib12 + 1a2 + ib22 = 1a1 + a22 + i1b1 + b22. \n(B.4)\nThis makes it clear that the real and imaginary parts are the “apples and oranges” that you are often \ntold not to mix together. In fact, a complex number contains two independent pieces of information, \nmuch like the components of a vector. We even represent a complex number in a similar way, as \nshown in Fig. B.1.\nVisualization of complex numbers in this “complex plane” can be very powerful. The horizontal \naxis in Fig. B.1 corresponds to the real part of a complex number and the vertical axis corresponds to \nthe imaginary part. Expressing the complex number as z = a + ib corresponds to using the Cartesian \nrepresentation. Figure B.1 also suggests that a polar representation is useful and that the radius r and \nthe angle u could also characterize a complex number.\nHow do we connect the Cartesian and polar representations mathematically? Consider the expo-\nnential of a complex number. The Taylor series expansion of a complex exponential is\n \neiu = 1 + 1iu2 + 1\n2!\n 1iu2\n2 + 1\n3!\n 1iu2\n3 + 1\n4!\n 1iu2\n4 + 1\n5!\n 1iu2\n5 + ... . \n(B.5)\nEvaluating the powers of the imaginary number i results in half of the terms of the expansion being \nreal (the even powers) and half being imaginary (the odd powers). Moreover, alternating signs arise \nfrom i 2 = -1 and i 4 = +1, yielding\n \neiu = a1 - 1\n2!\n u2 + 1\n4!\n u4 - ...b + i au - 1\n3!\n u3 + 1\n5!\n u5 + ...b . \n(B.6)\n\n534 \nComplex Numbers\nThe bracketed terms are the Taylor series expansion for the cosine and sine functions, giving the \nfamous Euler’s formula:\n \neiu = cos u + i sin u  . \n(B.7)\nUsing Euler’s formula, the polar representation of a point in the complex plane is\n \nz = reiu = r cos u + ir sin u, \n(B.8)\nas depicted in Fig. B.1. We refer to r as the modulus or magnitude and u as the phase or argument. \nWe connect the Cartesian and polar viewpoints by equating the real and imaginary parts of Eqs. (B.2) \nand (B.8), giving\n \n a = r cos u \n \n b = r sin u, \n(B.9)\nwhich agrees with trigonometry. The inverse relations are\n \n r = 2a2 + b2 \n \n u = tan-1ab\nab . \n(B.10)\nCare must be exercised when ﬁnding the polar angle because the inverse tangent function is multi-\nvalued. However, the real and imaginary parts of a complex number separately determine the cosine \nand sine of the polar angle, so the correct quadrant is determined by using Eq. (B.10) in conjunction \nwith Eq. (B.9). You should practice converting numbers between Cartesian and polar forms, and use \nwhichever form is most convenient. For example, the complex number i is one unit along the imagi-\nnary axis in the Cartesian form. In the polar form, i is the number 1 rotated by p>2 from the real axis, \nReal Axis\nImaginary Axis\nr\nΘ\na\nb\nFIGURE B.1 Complex plane.\n\nComplex Numbers \n535\nthus i = eip>2. Likewise, the complex number \u00111 is one unit along the negative real axis in Cartesian \nform. In polar form, \u00111 has magnitude 1 and is rotated (has phase) p from the real axis, thus -1 = eip.\nThe polar representation of complex numbers makes multiplication and division easy:\n \n z1z 2 = r1eiu1r2eiu2 = r1r2e i(u1+u2) \n \n z1\nz2\n= r1eiu1\nr2eiu2 = r1\nr2\n e i (u1-u2). \n \n(B.11)\nAddition and subtraction are easier in the Cartesian representation [Eq. (B.4)].\nComplex numbers have a unique operation known as complex conjugation, which is deﬁned by \nchanging i S -i. We say that z* is the complex conjugate of z:\n \nz* = a - ib = re -iu. \n(B.12)\nIn the complex plane, this operator corresponds to reﬂection through the real axis. A complex number \nmultiplied by its own complex conjugate yields the square of its modulus or magnitude:\n \n \u0004 z \u0004\n2 = z z* = 1a + ib21a - ib2 = a2 + b2 \n \n = 1reiu21re -iu2 = r 2,\n \n \n(B.13)\nsometimes called the complex square. The resultant modulus from Eq. (B.13) agrees with the geo-\nmetric result in Eq. (B.10). The complex square is also a handy device to express a complex fraction \nin standard Cartesian form. For example, multiplying the numerator and denominator by the complex \nconjugate of the denominator places all the imaginary numbers in the numerator:\n \n w =\n1\na + ib = a\n1\na + ibb aa - ib\na - ibb = a - ib\na2 + b2 \n \n =\na\na2 + b2 - i \nb\na2 + b2 .\n \n(B.14)\nThis is standard form with\n \n Re 1w2 =\na\na2 + b2  \n \n Im 1w2 =\nb\na2 + b2 . \n(B.15)\nA particularly useful case is a = 0, b = 1, which gives 1>i = -i.\nComplex notation can also be used to make trigonometric manipulations much easier, even when \ncomplex numbers are not really needed. Euler’s formula can be inverted to express trigonometric \nfunctions in terms of complex exponentials\n \n  cos u = eiu + e -iu\n2\n \n \n  sin u = eiu - e -iu\n2i\n \n(B.16)\nthat are very handy. For example, consider the trigonometric identity\n \nsin 1a + b2 = sin a cos b + cos a sin b. \n(B.17)\n\n536 \nComplex Numbers\nIt can be derived using Eq. (B.16):\n \n sin 1a + b2 = ei(a+b) - e -i(a+b)\n2i\n \n \n = eiaeib - e -iae -ib\n2i\n \n \n = 1\n2i\n 51cos a + i sin a21cos b + i sin b2 - 1cos a - i sin a21cos b - i sin b26 (B.18)\n \n = 1\n2i\n 52i sin a cos b + 2i cos a sin b6\n \n \n =  sin a cos b + cos a sin b . \nThat is a useful trick when you can’t ﬁnd your trigonometry book!\n\n \n537\nA P P E N D I X\nC\n \nMatrices\nWe present some of the basic deﬁnitions and properties of matrices necessary to implement the matrix \nformulation of quantum mechanics. We adopt the Dirac bra-ket notation used throughout the text. We \nadopt the quantum mechanical viewpoint that matrices are representations of operators or states and so \nwe use the \u0003 notation where appropriate to mean “is represented by.”\nA matrix is an ordered array of numbers:\n \nA \u0003 •\nA11\nA12\nA13\ng\nA21\nA22\nA23\ng\nA31\nA32\nA33\ng\nf\nf\nf\nf\nμ , \n \n(C.1)\nwhere the subscript labels the rows and columns:\n \nAij =  Matrix element in the ith row and jth column. \n \n(C.2)\nA vector is a special case of a matrix with only one column or row. A column vector\n \n0 a9 \u0003 •\na1\na2\na3\nf\nμ \n \n(C.3)\nrequires only one subscript to label its elements. A row vector has its elements arranged in a row\n \n8b0 \u0003 1b1\nb2\nb3\n g\n 2. \n \n(C.4)\nTo add matrices, we add the corresponding elements:\n \nCi j = Ai j + Bi j. \n \n(C.5)\nFor example, given the two matrices\n \nA \u0003 ¢a\nb\nc\nd≤,   B \u0003 ¢e\nf\ng\nh≤, \n(C.6)\n\n538 \nMatrices\ntheir sum is\n \nA + B \u0003 ¢a\nb\nc\nd≤+ ¢e\nf\ng\nh≤= ¢a + e\nb + f\nc + g\nd + h≤ . \n \n(C.7)\nFor addition, the two matrices must have the same size and shape and the result is the same size and shape.\nMatrix multiplication is more complicated. If we multiply two matrices A and B to form a third \nmatrix C, then the elements of the matrix C are\n \nCi j = a\nn\nk=1\nAi k Bk j . \n \n(C.8)\nFor example, given the two matrices\n \nA \u0003 ¢a\nb\nc\nd≤ ,   B \u0003 ¢e\nf\ng\nh≤ , \n(C.9)\ntheir product is\n \nAB \u0003 ¢a\nb\nc\nd≤¢e\nf\ng\nh≤= ¢ae + bg\na f + bh\nce + dg\nc f + dh≤ . \n \n(C.10)\nIn general, Eq. (C.8) tells us that to ﬁnd the matrix element Cij in the ith row and jth column of C, take \nthe ith row of the matrix A and overlay it on top of the jth column of the matrix B. Multiply each pair of \noverlaid numbers and sum the products. For this to make any sense, the number of elements in a row \nof A must equal the number of elements in a column of B, which means that the number of columns in \nA must equal the number of rows in B. Thus, if A is an / * n matrix and B is an n * m matrix, then the \nproduct C = AB is an / * m matrix. Matrix multiplication is not commutative, that is\n \nAB \u0002 BA \n \n(C.11)\nin general.\nThe rules of matrix multiplication make it clear that multiplication of a column vector by a matrix \nyields another column vector\n \nA0 a9 = 0 b9 \n \n(C.12)\nand multiplication of a row vector and a matrix yields another row vector\n \n8c0 A = 8d0 , \n \n(C.13)\nbut each must occur in the order shown. The product of a row vector and a column vector in the \n“proper” bra-ket order is an inner product\n \n18b0 21 0 a92 = 8b0 a9 \n \n(C.14)\nor a scalar product because the result is a scalar. For example,\n \n8b 0\n a9 = 1r\ns\nt2°\nu\nv\nw\n¢ = r u + s v + t w. \n \n(C.15)\nThe product of a row vector and a column vector in the “wrong” ket-bra order is an outer product\n \n1 0 a9218b0 2 = 0 a98b0 , \n \n(C.16)\n\nwhich is a matrix. For example,\n \n@ a98b @ \u0003 °\nu\nv\nw\n¢ 1r\ns\nt2 = °\nur\nus\nut\nvr\nvs\nvt\nwr\nws\nwt\n¢ . \n \n(C.17)\nThe transpose of a matrix is obtained by interchanging rows and columns. In component nota-\ntion this means that\n \n1AT2ij = A ji. \n \n(C.18)\nFor example, if the matrix A is\n \nA \u0003 ¢a\nb\nc\nd≤ , \n \n(C.19)\nthen the transpose AT is\n \nAT \u0003 ¢a\nc\nb\nd≤ . \n(C.20)\nA matrix is called symmetric if it is equal to its transpose, A = AT. The transpose of a column vector \nis a row vector.\nThe Hermitian conjugate (or adjoint) of a matrix is obtained by transposing the matrix and \ncomplex conjugating each element. We denote the Hermitian conjugate with a dagger -. In compo-\nnent notation, the Hermitian conjugate is\n \n1A-2i j = A* \nj i . \n(C.21)\nFor example, if the matrix A is\n \nA \u0003 ¢a\nb\nc\nd≤ , \n(C.22)\nthen the Hermitian conjugate A- is\n \nA- \u0003 ¢a*\nc*\nb*\nd*≤ . \n(C.23)\nA matrix is called Hermitian (or self-adjoint) if it is equal to its Hermitian conjugate, A = A-. In \nquantum mechanics, all operators that correspond to physical observables are Hermitian operators.\nThe determinant of a matrix is deﬁned as the sum of the products of the elements of any row (or \ncolumn) with the cofactors of those elements. The cofactor of an element Aij of a matrix is the product \nof the factor (-1)i+j and the determinant of the submatrix obtained by striking out the row and column \ncontaining Aij. For example, the determinant of the 2 * 2 matrix A in Eq. (C.22) is\n \n det (A) = ` a\nb\nc\nd ` = ad - bc. \n(C.24)\nThe determinant of a 3 * 3 matrix is\n \n  det (A) = †\na\nb\nc\nd\ne\nf\ng\nh\ni\n† = a1-121+1 ` e\nf\nh\ni\n ` + b1-121+2 ` d\nf\ng\ni\n ` + c1-121+3 ` d\ne\ng\nh `  \n \n = a 1ei - f  h2 - b 1di - fg2 + c 1dh - eg2.\n \n(C.25)\nMatrices \n539\n\n540 \nMatrices\nThe eigenvalues and eigenvectors of a matrix are found by solving the eigenvalue problem:\n \nA0 a9 = l0 a9, \n(C.26)\nwhere l are the eigenvalues and 0 a9 are the eigenvectors. The eigenvalue equation has a solution \nwhen the determinant of the coefﬁcients of the homogeneous equations is zero:\n \n det 1A - l12 = 0 , \n(C.27)\nthat is\n \n det •   \nA11 - l\nA12\nA13\ng\nA21\nA22 - l\nA23\ng\nA31\nA32\nA33 - l\ng\nf\nf\nf\nf\nμ = 0 . \n(C.28)\nThe resulting equation is the characteristic equation and its solution yields the eigenvalues of the \nmatrix. For an n * n matrix, the characteristic equation is an nth order equation and yields n solutions, \nthough some may be degenerate or equal. To ﬁnd the eigenvectors of the matrix, we substitute each \neigenvalue in turn into the eigenvalue equation (C.26) and solve for the corresponding eigenvector.\nRESOURCES\nActivities\nMath Primer Course: A weeklong course that reviews matrix algebra and frames the discussion of \nmatrices in the context of vectors spaces and linear transformations. The course includes several stu-\ndent activities.\nwww.physics.oregonstate.edu/portfolioswiki/courses:home:prhome\n\n \n541\nA P P E N D I X\nD\nWaves and Fourier Analysis\nD.1 \u0002 CLASSICAL WAVES\nA classical wave in one dimension is represented by a function f 1x, t2 that is a solution of the classical \nwave equation\n \n0 2 f 1x, t2\n0 x 2\n= 1\nv2 \n0 2 f 1x, t2\n0 t2\n, \n(D.1)\nwhere v is the wave speed. This equation is applicable to water waves, waves on a string, electro-\nmagnetic waves, and other types of classical waves. Any function of the form f 1x { vt2 satisﬁes \nthis equation and represents a wave moving in the positive (for x - vt argument) or negative (for \nx + vt argument) x direction. The wave equation obeys the linear superposition principle, so any two \nsolutions can be added to form another valid solution. Because of this, we typically focus on the har-\nmonic or sinusoidal solutions and then use the Fourier principle to construct any general solution when \nneeded.\nA sinusoidal wave is periodic in space and in time, as shown in Fig. D.1, and is characterized by \nthe spatial period, or wavelength, l, and by the temporal period T. We write the sinusoidal wave as\n \nf 1x, t2 = A sin c 2p a x\nl - t\nTb + dd , \n(D.2)\nwhere A and d are the amplitude and phase constant, respectively, required to produce a general solu-\ntion to the second-order differential wave equation. It is standard practice to write the sinusoidal wave \nin a simpler form by using the wave vector k and the angular frequency v, given by\n k = 2p\nl\n \n v = 2p\nT . \n(D.3)\nThus we get\n \nf 1x, t2 = A sin1kx - vt + d2. \n(D.4)\nThe velocity of a point of ﬁxed phase on this harmonic wave is found by the condition\n \n d1phase2 = 0\n \n d1kx - vt + d2 = 0\n \n kdx - vdt = 0 , \n(D.5)\n\n542 \nWaves and Fourier Analysis\nyielding the phase velocity\n \nvphase = dx\ndt\n2\nfixed phase\n= v\nk = l\nT\n . \n(D.6)\nIn many cases, the phase velocity is referred to simply as the velocity of the wave.\nThe relation between the wave vector k and the angular frequency v\n \nv = v1k2 \n(D.7)\nis called the dispersion relation. We typically treat the wave vector k as the independent variable. If \nthe (phase) velocity is constant, independent of the wave vector, then we say that there is no dispersion \nin the system. If the velocity is not constant, then waves with different wave vectors (i.e., different \nwavelengths) move at different speeds and a general wave composed of different harmonic solutions \nwill disperse as it propagates. In that case, the motion of the superposition or wave packet is character-\nized by the group velocity\n \nvg =\ndv1k2\ndk\n2\nk0\n, \n(D.8)\nwhere k0 is the peak of the wave-vector distribution comprising the wave packet. The group velocity is \nthe same as the phase velocity if there is no dispersion.\nFor mathematical convenience, we often use the complex form of the sinusoidal wave\n \nei(kx-vt) = cos1kx - vt2 + i sin1kx - vt2, \n(D.9)\nnoting that we must take the real part at the end of the classical calculation, because we measure only \nreal quantities. Quantum mechanics uses complex numbers, so we focus on the complex form of the \nclassical wave.\nx\nf(x,t)\nf(x,t)\nΛ\nt\nT\n(a)\n(b)\nFIGURE D.1 A classical wave, showing (a) the wavelength \nin space and (b) the period in time, for the choice of the phase \nconstant d = 0.\n\nD.2 \u0002 FOURIER ANALYSIS\nFourier analysis is the decomposition of a general wave or oscillation into harmonic components. \nBecause we treat the wave vector as the independent variable of a wave, the Fourier decomposition \nis typically done in terms of wave vectors. A Fourier series is a sum of sinusoidal functions, each of \nwhich is a harmonic of some fundamental wave vector or spatial frequency. A Fourier transform is an \nintegral over a continuous distribution of sinusoidal functions.\nA Fourier series is appropriate when the system has boundary conditions that limit the allowed \nwave vectors to a discrete set. For a system where the spatial periodicity is 2L, the Fourier decomposi-\ntion of a general periodic function is the series\n \nf 1x2 =\na\n\u0005\nn= - \u0005\ncneikn\n x, \n(D.10)\nwhere the allowed wave vectors are\n \nkn = np\nL\n . \n(D.11)\nThe expansion coefﬁcients cn in Eq. (D.10) are complex. The real version of the Fourier expansion is\n \nf 1x2 = a0\n2 + a\n\u0005\nn=1\nc an cosanpx\nL b + bn sinanpx\nL b d . \n(D.12)\nThe expansion coefﬁcients an\n , bn\n , cn are obtained by calculating the overlap integrals (i.e., projections \nor inner products) of the desired function with the harmonic basis functions\n \n an = 1\nL L\n2L\n0\nf 1x2 cosanpx\nL b  dx \n \n bn = 1\nL L\n2L\n0\nf 1x2 sinanpx\nL b  dx \n(D.13)\n \n cn = 1\n2L L\n2L\n0\nf 1x2e-ikn\n x dx . \nA Fourier transform is appropriate when the system has no boundary conditions that limit the allowed \nwave vectors. In this case, the Fourier decomposition is an integral over a continuum of wave vectors:\n \nf 1x2 =\n1\n12p L\n\u0005\n- \u0005\na1k2eik x dk, \n(D.14)\nwhere the expansion function a1k2 is complex. To obtain the expansion function a1k2 for a given\nspatial function f 1x2 requires the inverse Fourier transform\n \na1k2 =\n1\n12p L\n\u0005\n- \u0005\nf 1x2e-ikx\n dx, \n(D.15)\nwhich is a projection of the spatial function f 1x2 onto the harmonic basis functions eikx> 12p. The \nbasis functions are orthogonal and normalized in the Dirac sense, which means their projections onto \neach other are Dirac delta functions\n \n1\n2p L\n\u0005\n- \u0005\neik\u0004xe-ikx\n dx = d1k - k\u00042 \n \n1\n2p L\n\u0005\n- \u0005\neikx\u0004e-ikx\n dk = d1x - x\u00042, \n(D.16)\nwhether viewed in the position representation or the wave-vector representation.\nD.2 Fourier Analysis \n543\n\n544 \nWaves and Fourier Analysis\nSome typical Fourier transform pairs are shown in Fig. D.2 and are listed here (without proper \nscale factors):\n \n f 1x2 = eik0\n x 3 a1k2 = d1k - k02\n \nsinusoid \ndelta function\n \n f 1x2 = eik0\n xe-x 2>2s2 3 a1k2 = e-s2(k-k0)2>2\n \nGaussian  \nGaussian\n \n f 1x2 = eik0\n xe-0  x 0>s 3 a1k2 =\n1\n1 + s21k - k02\n2  \n(D.17)\n \nexponential\n \nLorentzian\n \n  f 1x2 = eik0\n x;0 x0 6s 3 a1k2 =\nsin5s1k - k026\ns1k - k02\n. \n \nsquare pulse\n \nsinc \n(a)\n(b)\n(c)\n(d)\nk0\nk0\nk0\nk0\nk\nx\nInfinite wave\nDelta function\n\u000e\nGaussian\nGaussian\n\u000e\nExponential\nLorentzian\n\u000e\nSquare pulse\nSinc function\n\u000e\nFIGURE D.2 Fourier transform pairs: (a) Inﬁnite wave 3  delta \nfunction, (b) Gaussian 3  Gaussian, (c) exponential 3  Lorentzian, \n(d) square pulse 3  sinc function.\n\nIn each case, a1k2 and f 1x2 are Fourier transforms of each other following Eqs. (D.14) and (D.15). \nIn Fig. D.2, only the real part of the function f 1x2 is plotted and each wave has a central wavelength \nl0 = 2p>k0.\nThe spatial extent \u0006x of a function f 1x2 and the width \u0006k of the Fourier transform a1k2 in wave-\nvector space are inversely related through the uncertainty relation\n \n\u0006k\u0006x Ú 1. \n(D.18)\nThis relation tells us that if want to make a wave that is conﬁned to a small region of space, we need to \nuse a wide range of wave vectors. In quantum mechanics, this concept is the Heisenberg uncertainty \nrelation. To describe a wave f 1x, t2, we replace x with x - vt in the Fourier decomposition of the func-\ntion f 1x2, as long as there is no dispersion. This means that the wave retains its initial shape as it moves. \nWhen a system has dispersion, this replacement is no longer valid. The different speeds of the different \nwave-vector components of the superposition f 1x, t2 cause the shape of the wave to change as it propa-\ngates. The expansion function a1k2 remains the same and the time dependent wave is represented by\n \nf 1x, t2 =\n1\n22p L\n\u0005\n- \u0005\na1k2ei(kx-v(k)t) dk . \n(D.19)\nWe must recalculate the integral at each time to learn how the wave shape evolves.\nParseval’s theorem says that the power is the same whether calculated in position space or wave-\nvector space:\n \nL\n\u0005\n- \u0005\n0  f 1x2 0\n2dx =\nL\n\u0005\n- \u0005\n0 a 1k2 0\n2\n dk . \n(D.20)\nD.3 \u0002 QUANTUM MECHANICS\nIn quantum mechanics, we describe systems using momentum as the variable rather than wave vector, \nbut the Fourier ideas are similar. Converting from wave-vector space to momentum space requires \nsome care with the units:\n \n  f 1x2 =\n1\n12p L\n\u0005\n- \u0005\na1k2eik x  dk =\n1\n12p L\n\u0005\n- \u0005\na1 p>U2ei( p>U)x d1 p>U2 \n \n =\n1\n12ph L\n\u0005\n- \u0005\n1\n1U\n a1 p>U2ei(p>U)x dp . \n(D.21)\nThis tells us that the amplitudes in wave-vector space and momentum space are related by\n \nf1 p2 =\n1\n1U\n a1 k = p>U2. \n(D.22)\nHence, we arrive at the quantum mechanical version of the Fourier transform that connects wave func-\ntions in position space and momentum space:\n \n c1x2 =\n1\n22p U L\n\u0005\n- \u0005\nf\n  1 p2  eipx>U dp, \n(D.23)\n \n f1 p2 =\n1\n22p U L\n\u0005\n- \u0005\nc\n 1x2e-ipx>U dx. \n(D.24)\nD.3 Quantum Mechanics \n545\n\n546 \nWaves and Fourier Analysis\nParseval’s theorem applied to quantum mechanics says that the probability normalization condi-\ntion is the same whether calculated in position space or momentum space:\n \n1 =\nL\n\u0005\n- \u0005\n0 c1x2 0\n2\n dx =\nL\n\u0005\n- \u0005\n0 f\n 1 p2 0\n2\n  dp. \n(D.25)\nThe Heisenberg uncertainty relation relates the spatial extent \u0006x of a probability density 0 c1x2 0\n2 \nand the width \u0006p of the momentum space probability distribution 0 f1p2 0 2 in a manner analogous to \nEq. (D.18):\n \n\u0006p\u0006x Ú U\n2. \n(D.26)\nThis relation tells us that if want to make a wave function that is conﬁned to a small region of space, \nthen we must use a wide range of momenta. Hence, we cannot speak of a quantum mechanical system \nwith a well-deﬁned position and a well-deﬁned momentum.\n\n \n547\nA P P E N D I X \nE\nSeparation of Variables\nThe separation of variables procedure permits us to simplify a partial differential equation by \nseparating out the dependence on the different independent variables and creating multiple ordinary \ndifferential equations. To illustrate the method, we apply a six-step process to the classical wave equa-\ntion to show how the time dependence of the wave function can be found through a separate ordinary \ndifferential equation. The scalar wave equation is:\n \n\u00022u1r, t2 - 1\nv2 \n02u1r, t2\n0t 2\n= 0, \n(E.1)\nwhere v is the wave speed.\nTo separate the time dependence from the spatial dependence, the six steps are:\nStep 1: Write the partial differential equation in an appropriate coordinate system. For the wave \nequation, we choose Cartesian coordinates (this is not crucial in this example because we are separat-\ning only the time dependence):\n \n0 2u\n0x 2 + 0 2u\n0y2 + 0 2u\n0z 2 - 1\nv2 0 2u\n0t 2 = 0. \n(E.2)\nStep 2:  Assume that the solution u 1x, y, z, t2 can be written as the product of functions, at least \none of which depends on only one variable, in this case t. The other function(s) must not depend at all \non this variable, that is, assume\n \nu1x, y, z, t2 = S1x, y, z2T 1t2. \n(E.3)\nPlug this assumed solution into the partial differential equation Eq. (E.2). Because of the special \nform for u1x, y, z, t2, the partial derivatives each act on only one of the functions in u1x, y, z, t2.\n \nT1t20 2S1x, y, z2\n0 x 2\n+ T 1t20 2S1x, y, z2\n0 y2\n+ T 1t20 2 S1x, y, z2\n0 z 2\n- 1\nv 2 S1x, y, z2d 2T 1t2\ndt 2\n= 0. \n(E.4)\nAny partial derivatives that act only on a function of a single variable may be rewritten as total \nderivatives.\nStep 3: Divide by u1x, y, z, t2 in the form of Eq. (E.3):\n \n1\nS1x, y, z2 e\n0 2S1x, y, z2\n0x 2\n+\n0 2S1x, y, z2\n0y2\n+\n0 2S1x, y, z2\n0z 2\nf - 1\nv2 1\nT(t) d 2T1t2\ndt 2\n= 0. \n(E.5)\n\n548 \nSeparation of Variables\nStep 4: Isolate all of the dependence on the chosen separation variable (t) on one side of the\nequation. Do as much algebra as you need to do to achieve this. In our example, this is straightforward:\n \n1\nS1x, y, z2\n e 0 2 S1x, y, z2\n0 x 2\n+ 0 2 S1x, y, z2\n0 y2\n+ 0 2 S1x, y, z2\n0z 2\nf = 1\nv2 1\nT 1t2 d 2T 1t2\nd t 2\n. \n(E.6)\n \n(1111111111111111111111111)111111111111111111111111* \n(11111)11111*\n \nfunction of space only \nfunction of time only\nStep 5: Now imagine changing the isolated variable t by a small amount. In principle, the right-\nhand side of Eq. (E.6) could change as t changes, but nothing on the left-hand side would because there \nis no time dependence. Therefore, if the equation is to be true for all values of t, the particular combi-\nnation of t dependence on the right-hand side must be constant. We call this constant -k 2 (because we \nalready know what the answer is):\n \n1\nS1x, y, z2\n e\n0 2S1x, y, z2\n0 x 2\n+\n0 2S1x, y, z2\n0 y2\n+\n0 2S1x, y, z2\n0 z 2\nf = 1\nv2 1\nT 1t2 \nd 2T 1t2\ndt2\n = -k 2. \n(E.7)\nIn this way we have broken our original partial differential equation up into a pair of equations, one of \nwhich is an ordinary differential equation involving only t, the other is a partial differential equation \ninvolving only the three spatial variables:\n \n \n1\nS1x, y, z2\n e 0 2S1x, y, z2\n0 x 2\n+\n0 2S1x, y, z2\n0 y2\n+\n0 2S1x, y, z2\n0 z 2\nf = -k 2, \n(E.8)\n \n1\nv2 1\nT 1t2 \nd 2T 1t2\ndt 2\n = -k 2. \n(E.9)\nThe separation constant -k 2 appears in both equations.\nStep 6: Write each equation in standard form by multiplying each equation by its unknown \nfunction to clear it from the denominator:\n \n \n0 2S1x, y, z2\n0 x 2\n+\n0 2S1x, y, z2\n0 y2\n+\n0 2S1x, y, z2\n0 z2\n = -k2S1x, y, z2, \n(E.10)\n \n1\nv2 \nd 2T1t2\ndt 2  = -k 2 T 1t2. \n(E.11)\nWe have now separated the time dependence from the spatial dependence. Equation (E.11) is \nan ordinary differential equation for the time dependent part T1t2 of the complete wave function \nu1x, y, z, t2 = S1x, y, z2T 1t2. Equation (E.10) is still a partial differential equation for the space depen-\ndent part S1x, y, z2 of the complete wave function u1x, y, z, t2 = S1x, y, z2T 1t2. The six steps of this \nprocedure can be applied again to separate the different spatial parts of S1x, y, z2 into three separate \nordinary differential equations.\n\n \n549\nA P P E N D I X\nF\nIntegrals\nA small collection of useful integrals is listed below. You may already be accustomed to using Maple \nor Mathematica to do integrals, which is not too different than looking up an integral in this table. But \nbe careful to not become too reliant on the computer. For example, if the computer tells you the answer \nis zero, then maybe that should have been obvious from examining the symmetry of the integrand.\n \n \nL\nsin mx sin  nx dx =\n sin 1m - n2x\n2 1m - n2\n-\n sin 1m + n2x\n2 1m + n2\n, 1m2 \u0002 n22 \n(F.1)\n \n \nL\ncos mx cos nx dx =\nsin 1m - n2x\n2 1m - n2\n+\nsin 1m + n2x\n2 1m + n2 , 1m2 \u0002 n22 \n(F.2)\n \n \nL\nsin mx cos nx dx = - \ncos 1m - n2x\n2 1m - n2\n-\ncos 1m + n2x\n2 1m + n2\n, 1m2 \u0002 n22 \n(F.3)\n \n \nL\nsin2 ax dx = 1\n2\n  x - 1\n2a\n  sin ax cos ax \n(F.4)\n \n \nL\ncos2 ax dx = 1\n2\n x + 1\n2a\n  sin ax cos ax \n(F.5)\n \n \nL\nsin ax cos ax dx = 1\n2a\n  sin2 ax \n(F.6)\n \nL\nsin ax cosm ax dx = - cos m +  1 ax\n1m + 12a \n(F.7)\n \nL\nsinm ax cos ax dx = sinm+1 ax\n1m + 12a \n(F.8)\n \nL\nx sin ax dx = 1\na2\n  sin ax - x\na\n  cos ax \n(F.9)\n \nL\nx cos ax dx = 1\na2\n  cos ax + x\na\n  sin ax \n(F.10)\n \nL\nx2 sin ax dx = 2x\na2\n  sin ax - a2x 2 - 2\na3\n  cos ax \n(F.11)\n\n550 \nIntegrals\n \nL\nx2 cos ax  dx = 2x\na2 cos ax + a2x 2 - 2\na3\n sin ax \n(F.12)\n \nL\nx sin2 ax  dx = x 2\n4 - x\n4a\n sin 2ax -\n1\n8a2 cos 2ax \n(F.13)\n \nL\nx cos2 \n ax dx = x 2\n4 + x\n4a\n sin 2ax +\n1\n8a2 cos 2 ax \n(F.14)\n \nL\nx2 sin2 ax dx = x3\n6 - a x2\n4a -\n1\n8a3bsin 2 ax -\nx\n4a2 cos 2 ax \n(F.15)\n \nL\nx2 cos2 ax dx = x3\n6 + a x2\n4a -\n1\n8a3bsin 2 ax +\nx\n4a2 cos 2 ax \n(F.16)\n \nL\nxe-x dx = -xe-  x - e-  xa \n(F.17)\n \nL\nx 2e-x dx = -x 2e-x - 2xe-x - 2e-x \n(F.18)\n \nL\nx3e-x dx = -x 3e-x - 3x 2e-x - 6xe-x - 6e-x \n(F.19)\n \nL\nx4e-x dx = -x4e-x - 4x 3e-x - 12x 2e-x - 24xe-x - 24e-x \n(F.20)\n \nL\n\u0005\n0\nx ne-ax  d x =\nn!\nan +1 \n(F.21)\n \nL\n\u0005\n0\ne-a2\n x 2 d x = 1\n2a1p \n(F.22)\n \nL\n\u0005\n- \u0005\ne-a2x2+bx  dx = 1p\na\n eb2>4a2 \n(F.23)\n \nL\n\u0005\n0\nxe-  x 2  dx = 1\n2 \n(F.24)\n \nL\n\u0005\n0\nx 2e-x 2  dx = 1p\n4  \n \n(F.25)\n \nL\n\u0005\n0\nx 2ne-x 2\n dx = 1p 12n2!\nn!\n  \n1\n22 n +1 \n(F.26)\n \nL\n\u0005\n0\nx 2n+1e-x 2\n dx = n!\n2  \n(F.27)\n\n \n551\nA P P E N D I X\nG\nPhysical Constants\nThese values are taken from: “CODATA recommended values of the fundamental physical constants: \n2006,” P. J. Mohr, B. N. Taylor, and D. B. Newell, Rev. Mod Phys. 80, 633–730 (2008). Experimental \nuncertainties are shown in parentheses.\nQuantity \nSymbol \nValue\nSpeed of light in vacuum   \nc \n299 792 458 m >s 1Exact2 \nPermeability of free space \nm0 \n4p * 10-7 N # s2>C2 1Exact2 \nPermittivity of free space \ne0 = 1>m0c2 \n8.854 187 817... * 10-12 C2>N # m2 1Exact2\nPlanck>s constant \nU \n6.582 118 99 1162 * 10-16 eV # s\n \n \n1.054 571 628 1532 * 10-34  J # s\n \nh = 2pU \n4.135 667 33 1102 * 10-15 eV # s\n \n \n6.626 068 96 1332 * 10-34 J # s\nElementary charge\ne\n1.602 176 487 1402 * 10-19 C\nElectron mass \nme\n0.510 998 910 1132 MeV>c2\n \n \n9.109 382 15 1452 * 10-31 kg\nProton mass \nmp \n938.272 013 1232 MeV>c2\n \n \n1.672 621 637 1832 * 10-27 kg\nFine structure constant \na =\ne2\n4pe0  Uc \n1\n137.035 999 679 1942 \nRydberg constant \nR \u0005 = a2me c\n2h   \n10 973 731.568 527 1732 m-1\n \nR \u0005c \n3.289 841 960 361 1222 * 1015 Hz\nRydberg energy \nRyd = R\u0005hc \n13.605 691 93 1342 eV\nBohr radius \na0 = 4pe0  U2\nme e2  \n0.529 177 208 59 1362 * 10-10 m\nBohr magneton \nmB =\neU\n2me\n \n9.274 009 15 1232 * 10-24 J>T\n \nmB>h \n1.399 624 604 1352 MHz>Gauss\nNuclear magneton \nmN =\neU\n2mp\n \n5.050 783 24 1132 * 10-27 J>T\nBoltzmann constant \nkB \n1.380 650 4 1242 * 10-23 J>K\n\n552 \nPhysical Constants\n Conversion factors \n1 J = 107 erg = 6.24151 * 1018 eV\n1 eV = 1.60218 * 10-19 J\n1 eV corresponds to 1E = hf = hc>l = hcn2\n2.41799 * 1014 Hz 1 f = E>h2\n1239.84 nm 1l = hc>E2\n8065.54 cm-1 1n = E>hc2\n1 cm-1 corresponds to\n29.9792458 GHz 1 f = c n 2\n107 nm 1l = 1>  n 2\n1.23984 * 10-4 eV 1E = hcn2\nhc = 1240 eV # nm\n1 amu = 931.494 MeV>c2 = 1.66054 * 10-27 kg\n\n \n553\nA\nAbsorption, 91, 451–454\nAbsorption spectrum, 108\nAddition of angular momenta, 355\nAdiabatic theorem, 450\nAlgebraic method, 277–284\nAlgorithms, quantum computing, 517\nAmplitude, 10, 15\nAnalyzer, Stern-Gerlach device as, 4\nAngular integral, 463\nAngular momentum, 210–215, 357–359\nin atoms and spectroscopic  \nnotation, 377\nclassical, 210\ngeneralized, 357–359\nintrinsic, 2–3\nmotion of a particle on a ring, 218–227\nmotion on a sphere, 227–244\norbital, 2, 3, 211–212, 357\nof photon, 507\nquantum mechanical, 210–215\nspherical coordinates, 215–218\nspin, 211–212, 357\nAngular momentum basis, 359\nAngular momentum ladder operators,  \n359–360, 362–363, 378\ncommutation relations for, 359–360\nAngular momentum operators. See also \nSpherical harmonics\nL2, 211–212, 214, 228, 239, 245, 263, 272\nLz, 211–213, 214, 220–221, 239, 245, 263, \n272, 397, 399\nSz, 397, 399\nAngular momentum quantum number\neffective potential for, 250–251\nfor hydrogen, 255–256, 261\nAngular momentum states, ladder of, 360\nAnnihilation operators, 284\nAnomalous Zeeman effect, 396, 405\nAntibonding orbital, 439, 442, 473\nAnti-Helmholtz coils, 504–505\nAntisymmetric states, 411–413\nfermions and, 419, 421, 442\nsinglet, 416\nApplications of quantum mechanics, 502–528\nlaser cooling, 506–514\nmagnetic trapping, 502–506\nmanipulating atoms, 502–514\nquantum bits (qubits), 515–517\nquantum gates, 518–524\nquantum information processing, 514–526\nquantum teleportation, 524–526\nApproximation, nearest-neighbor, 474\nAssociated Laguerre polynomials, 262–263\nAssociated Legendre equation, 229\nAssociated Legendre functions, 233–235\npolar plots, 234, 235\nproperties of, 234\nAsymmetric square well, 147–150\nperturbation and, 312\nAsymptotic solutions to radial eigenvalue  \nequation, 252–253\nAtomic beam, 508–509\nAtomic clocks, 514\nAtomic levels, multiple bands from multiple, \n478–480\nAtomic number, 434\nAtomic parity violation experiments, 348\nAtom interferometry, 192–196\nAtom-light interactions, Einstein model of, 456–460\nAtoms\nangular momentum in, 377\nblackbody radiation and, 455–460\nclassical, 202\nlaser cooling of, 506–514\nmagnetic traps and, 502–506\nmanipulating with quantum mechanical forces, \n502–514\nquantum, 202\nAuto-ionization, 431\nAzimuthal angle, spin component in general  \ndirection and, 41\nAzimuthal eigenvalue equation, 217, 218–222\nB\nBalmer series, 259\nBand diagrams, 492\nIndex\n\n554 \nIndex\nBand gaps, 479\ndirect, 496–497\nindirect, 496–497\nBand of allowed energies, 475\nBand structure, 500\nBand width, 476\nBarrier penetration, 133\nBarrier potential, 188\nBarriers, 182\ntunneling through, 188–192\nBasis states, 14, 16, 32, 44, 45, 63\nharmonic oscillator, 293, 311\nproperties of, 165\nsuperposition of, 166\nBell, John, 7, 99–101\nBell inequality, 101\nBell state, 516–517\nentangled, 523\nquantum teleportation and, 525–526\ntransformation to computational basis, 524\nBell-state measurement, 524, 525–526\nBeta decay problem, 84\nBethe, Hans, 393\nBinnig, Gerd, 192\nBiot-Savart law, 389\nBits, 515\nBlackbody radiation, Einstein model and, 455–460\nBloch’s theorem, 480–482, 488, 490\nBlue-shifted beam, 509\nBohm, David, 98\nBohr, Niels, 103\nBohr energies, 259, 383, 385\nBohr energy levels of hydrogen, 392–393, 394, 402, \n403, 407\nBohr frequency, 71, 302\ntime-dependent perturbation and, 448, 449, \n450–451\nBohr magneton, 355, 393\nZeeman effect and, 394\nBohr oscillation, 90\nBohr radius, 257, 272\nBoltzmann’s constant, 458\nBoltzmann thermal distribution law, 457\nBonding\ncovalent, 441\nionic, 441\nBonding orbital, 439, 442, 473\nBorn-Oppenheimer approximation, 439\nBose-Einstein condensation, 422–423, 502, 514\nBosons, 412–413\nexchange interaction and, 420–421\ninteracting in one-dimensional potential energy \nwell, 423–427\nin many-particle system, 421–422\nsymmetric states and, 419, 421, 442\nin two-particle excited state, 416–420\nin two-particle ground state, 415–420\nBoundary condition at inﬁnity, 130\nﬁnite square well, 130\nBoundary conditions, 476–478\non wave function, 122, 128, 156\nBound eigenstates, 469\nBound particles, free particles vs., 162\nBound states, 120\nin potential energy well, 155\nBound systems, energy states of, 107\nBra-ket formulae, translating to wave function \n formulae, 116, 154\nBras (bra vectors), 11–13, 17–18. See also Dirac \nnotation\nmatrix notation and, 23\nscalar product and, 12\nBrillouin zones, 478\nband gaps in semiconductors and, 497\ndensity of states and, 485\nBroadband excitation, 455, 456–460\nC\nCarbon\nangular momentum in, 377\nas graphite, 497–498\nlow-dimensional, 497–498\nCarbon nanotubes, 498\nCarrier wave, of wave packet, 170\nCavity QED, 460\nCenter-of-mass, separating relative motion and, \n204–208\nCentral potential, 204\nCentrifugal barrier, 250\nCesium, hyperﬁne transition in, 365\nChain of periodic wells. See Periodic chain \nof wells\nCharacteristic (secular) equation, 39–40\nChemical shift, 324\nChirped cooling, 512\nClassical angular momentum, 210\nClassical atom, 202\nClassical harmonic oscillator, 275–276\nClassically allowed region, 120\nClassically forbidden regions, 120\nClassical turning points, 120\nClebsch-Gordan coefﬁcients, 369, 373, 374–376, 378\n\n Index \n555\nselection rules and, 463–464\nZeeman effect and, 397\nClosure (completeness) relation, 45\nharmonic oscillator eigenstates and, 291, 292\nCoefﬁcients\nClebsch-Gordan, 369, 373, 374–376, 378, 397, \n463–464\nEinstein A, 457–459, 466\nEinstein B, 457, 458, 461, 466\nreﬂection, 186, 190–191\nof spherical harmonic expansion, 243\ntransmission, 185–186, 191\nCoherent state, of harmonic oscillator, 303–304\nCoherent superposition, 20, 50\nCollapse (reduction, projection), of quantum state \nvector, 46\nColumn vector, 22–23\nCommutation relations\nfor angular momentum ladder operators, 359–360\nquantum mechanical angular momentum and, \n210–211\nCommutators, 54–56, 63\nCommute, 54–55\nCompatible observables, 55–56\nCompleteness, 11, 12, 26, 29, 137\nof basis states, 165\nenergy eigenstates and, 156\nspherical harmonics and, 238\nCompleteness relation (closure), 45, 291, 292\nComplete vectors, 11\nComplex numbers, 10, 11, 278\nComputational basis, 517\nConduction band, 493\nConstants, normalization, 14\nContinuous, discrete vs., 113, 114\nContinuous basis representation, 113\nContinuous superposition, 171–176\nContinuum states, 469\nControlled-NOT gate (CNOT gate), 522–524\nCopenhagen interpretation of quantum  \nmechanics, 103\nCoulomb interaction, 272, 432\nbetween identical charged particles, 426\nCoulomb potential energy\nof diatomic molecule, 305\nfor hydrogen atom, 251\nCoulomb wells, 470\nCoupled angular momentum quantum number, 378\nCoupled basis, 355, 361, 365–370, 378\nClebsch-Gordan coefﬁcients and, 372–376\neigenstates, 366–369\nidentical particles and, 410–412, 415–416\nspin-orbit coupling and, 390\nZeeman effect and, 398, 401, 405–406, 409\nCoupled basis operators, 390\nCoupled basis vectors, 372, 378\nCoupled magnetic quantum number, 378\nCovalent bonding, 441\nCreation operators, 284\nCross section, 461–462\nCrystal momentum, 478\nD\nDarwin term, 392–393\nde Broglie relation, 163, 197\nde Broglie wavelength, 163\natom interferometer and, 194, 196\nDecoherence, 105\nDegeneracy\nof particle-on-a-ring system, 222\nparticle on a sphere and, 239\nDegenerate energy state, 164\nDegenerate perturbation theory, 336–343\nhydrogen and, 346–351\nhyperﬁne interaction of hydrogen and, 361–365\nDegenerate subspace, 339\nDelta-function potential, 135\nDensity of states, 460, 469–470, 484–486\nDetailed balance, principle of, 457\nDeuterium, as fermion, 413\nDiagonalization\nof hyperﬁne perturbation, 361–365\nof operators, 38–41\nof perturbation Hamiltonian, 339–343,  \n348–350, 351\nDiagonal matrix, 36\nDiamagnetic material, magnetic traps and, 506\nDiatomic molecule, as example of rigid rotor, 237\nDifferential volume element, 209\nDirac, Paul A. M., 4\nDirac bra-ket notation, for ﬁrst-order energy  \ncorrection, 327\nDirac delta function, 165, 166, 453\nDirac normalization, 166\nDirac notation, 114, 115\nharmonic oscillator problem and, 289–293\nDirect band gaps, 496–497\nDirect integral, 424–425, 426\nin helium, 432–433\nDiscrete, continuous vs., 113, 114\nDiscrete basis representation, 113\nDiscrete superposition, 168–171\n\n556 \nIndex\nDispersion relation, 475, 478–479, 480, 484–485, \n488, 489\nDisplaced Gaussian superposition, time dependence \nof spatial probability density for, 303, 304\nDoppler cooling limit, 514\nDoppler effect, laser cooling and, 509–510, 512\nDot product term, 463\nDot (scalar) product, 11–12\nDouble ionization level, 429\nDouble-slit interference experiment, 10\natom interferomery and, 193–196\nE\nEffective mass, 494–496\nEffective potential energy (Veff), 250–251\nEhrenfest’s theorem, 77, 146, 303\nEigenstates. See also Energy eigenstates\ncoupled basis, 366–369\nenergy, 161–163\nﬁrst-order correction, 324–329\nfree particle, 161–167\nmass, 85\nmolecular, 471\nmomentum, 163–167, 180, 197\nfor particle on a ring, 224\nof perturbation Hamiltonian, 314\nposition, 113–114, 180\nprojection operators and, 45–46\nsimultaneous sets of, 55\nuncoupled basis, 366–369\nEigenvalue equations, 35. See also Energy \neigenvalue equations\nangular momenta and, 358, 377\nhydrogenic atom, 263\nin matrix form, 35\nin matrix notation, 38, 39–40\nfor spin-1/2 operator Sz, 63\nEigenvalues, 34–36. See also Energy eigenvalues\ndiagonalization of matrix and, 38–41\nof inﬁnite square well, 121, 125–126\npostulate 3 and, 34, 35\nEigenvectors, 34–36\ndiagonalization of matrix and, 38–41\nof Hermitian matrix, 44\nof time-independent Hamiltonian, 69\nas unit vectors in their own basis, 36\nEinstein, Albert, 97, 98\nbroadband excitation and, 455–460\nEinstein A coefﬁcient, 457–459, 466\nEinstein B coefﬁcient, 457, 458, 461, 466\nEinstein-Podolsky-Rosen paradox. See EPR \n (Einstein-Podolsky-Rosen) paradox\nEinstein’s locality principle, 99\nElectrical conductivity, 492–494\nElectric dipole approximation, 455\nElectric dipole Hamiltonian, 454\nElectric dipole interaction, 454–462\nElectric dipole moment\noscillating, 270–271\nStark effect in hydrogen and, 347–351\nstatic, 271\nElectric dipole transitions, selection rules and, \n462–465, 466\nElectric ﬁeld, application and effect on k values, \n493–494\nElectromagnetic coupling constant, 257, 385\nElectromagnetic ﬁeld, energy density of, 456\nElectron\nhyperﬁne interaction between proton and, 365–366\nin spin-orbit coupling, 388–389\nElectron diffraction experiments, 192\nElectron distribution of hydrogen, 265, 266\nElectronic conﬁgurations, 434, 435, 436\nElectron magnetic moment, 355–356, 357\nElectron mass (me), 256\nElectron rest mass energy, of hydrogen, 384\nElectron velocity, relativistic correction caused by, \n386–388\nElements of reality, 97\nEmission, 91, 451, 452\nspontaneous, 457–458\nstimulated, 452\nEmission spectrum, 108\nEncryption, factorization algorithm and, 517\nEnergy. See also Quantized energies\nCoulomb interaction, 432\nFermi, 423\nEnergy band diagram, 469–470\nEnergy bands, multiple, from multiple atomic levels, \n478–480\nEnergy basis, 69\nharmonic oscillator, 294\nEnergy conserving delta function, 453–454\nEnergy density\nblackbody, 458\nof electromagnetic ﬁeld, 456\nEnergy eigenstates, 70, 161–164\nasymmetric square well, 149–150\nenergy basis and, 137\nharmonic oscillator, 284–288\n\n Index \n557\nin harmonic oscillator well, 278\nin hydrogen atom, 278\ninﬁnite and ﬁnite wells, 134–135\ninﬁnite square well, 156\nin inﬁnite square well, 278\nof inﬁnite square well, 121, 123–125, 127, 143–145\nnumerical solutions, 151–153\nof periodic chain of wells, 471–476\nproperties of, 156\nqualitative (eyeball) solutions, 150–151\ntime evolution of, 162–163\nEnergy eigenstate wave functions\nﬁnite square well, 131–133\nof hydrogenic atom, 263–269\nEnergy eigenvalue equations, 68, 110–112, 156\nﬁnite square well, 128–133, 159\nharmonic oscillator, 277, 281, 283, 284\ninﬁnite square well, 119, 121–128\nKronig-Penney model and, 489–490\norbital angular momentum and, 214\nquantum mechanical tunneling, 189–190\nsolving numerically, 152–153\nin spherical coordinates, 208–209\nfor two-body system, 207\nunbound states and, 161\nusing LCAO method to solve, 471–473\nzeroth-order problem, 319\nEnergy eigenvalues, 203\nof hydrogen atom, 202–204\nof hydrogenic atom, 256\nof inﬁnite square well, 142\nof periodic chain of wells, 471–476\nof rigid rotor, 236–237\nEnergy estimation, using uncertainty principle, \n180–181\nEnergy ﬁngerprint, of microscopic systems, 107\nEnergy levels\nin bound systems, 107–108\nin GaAs quantum well, 147\nhydrogen, 256–259, 382–386\nhydrogenic atom, 263–264\nperiodic table and, 434–435\nperturbation and, 312\nin second-order perturbation theory, 332\nspin-1/2 particle in uniform magnetic ﬁeld, 313\nEnergy measurements\nspectroscopy, 107–109\nspherical harmonics, 242–244\nEnergy spectrum, 108\nof harmonic oscillator, 283\nof helium, 433\nhydrogen, 258–259\nof inﬁnite square well, 123\nfor Kronig-Penney model, 491\nfor particle on a ring, 222\nof rigid rotor, 236\nof scattering states, 184\nEntangled states, 98–102\nCNOT gate and, 522–523\nquantum bits and, 516–517\nSchrödinger cat paradox and, 104\nEnvelope, of wave packet, 170\nEPR (Einstein-Podolsky-Rosen) paradox, 97–102\nentangled quantum states and, 516\nEuler relation, 475\nEvanescent wave, 188\nEven parity, 136\nExchange force/exchange interaction, 420–421\nExchange integral, 425, 426\nin helium, 432–433\nExchange operator, 411\nExchange symmetry, 438\nhydrogen molecule, 442\nExcited state band, 479\nExcited states, 108\nexponential time decay of population of, 459\nhelium atom, 431–433\ntwo-particle, 416–420\nExpectation values, 51–52\nof harmonic oscillator, 295–296, 301–302\nof hydrogen molecule, 438\nof inﬁnite square well, 125, 126, 142–143, 146\nof the perturbation, 322–323\nof radial position, 267–269\nof wave function, 117–118\nEyeball solutions, to energy eigenstates, 150–151\nF\nFactorization algorithm, 517\nFermi, Enrico, 84\nFermi contact interaction, 356\nFermi energy, 423\nFermions, 412–413\nantisymmetric states and, 419, 421, 442\nexchange interaction, 420–421\ninteracting, 426–427\nin many-particle system, 422, 423\nin two-particle excited state, 416–420\nin two-particle ground state, 415–420\nFermi’s golden rule, 454, 460\n\n558 \nIndex\nFeynman, Richard, 8, 104, 514\nFine structure, 382\nof hydrogen, 382–393\nFine-structure constant, 257\nof hydrogen, 384, 385\nFine structure of hydrogen\nrelativistic correction, 386–388\nspin-orbit coupling, 388–393\nFinite square barrier, 188\nFinite square well, 128–133, 155, 469\nbarrier penetration, 135–136\nbound states and, 182, 183\ncompleteness, 137\nenergy eigenvalue equation, 159\ninversion symmetry and parity, 136\nnodes, 135\northonormality, 136\nschematic diagram of, 132\ntransmission and reﬂection coefﬁcients for, 186\ntransmission coefﬁcient for, 185–186\nwave function curvature, 133–135\nwaves incident upon, reﬂected from and \n transmitted through, 187\nFirst Brillouin zone, 478\nFirst-order energy correction, 320–324\nFirst-order state vector correction, 324–329,  \n334, 335\nForbidden transitions, 93, 136, 462\nFormulas\nhydrogen energy, 388\nPlanck blackbody radiation, 458\nRabi’s, 81–84, 90, 449\nRodrigues’, 231\nFourier energy-time uncertainty relation, 453\nFourier frequency-time uncertainty relation,  \n452–453\nFourier integral, 449\nFourier transform, 167, 197, 448\nof delta function, 178–179\nof emitted electromagnetic ﬁeld, 460\nof Gaussian function, 177–178\ninverse, 197\nmomentum space wave function and, 297\ntime-dependent generalization of, 172\nFourier wave packet, uncertainty principle and, \n177–178\nFree particle eigenstates, 161–167\nenergy eigenstates, 161–164\nmomentum eigenstates, 163–167\nFree particles, bound particles vs., 162\nFrequency\ngeneralized Rabi, 91\nLarmor, 333\nRabi, 91\nFull width at half maximum (FWHM), 91\nFunctions, activities, 159\nFWHM. See Full width at half maximum (FWHM)\nG\nGaAs\nband gap of, 496–497\nas direct-gap semiconductor, 497\nGaAs quantum well, 146–147\nGates\nlogic, 518\nquantum, 518–524\nGaussian function, probability analysis and, 298\nGaussian integral, 173\nGaussian momentum distribution, 172\nGaussian momentum space wave function,  \n172–174\nGaussian perturbation, 449–450\nGaussian wave packet, 172–176, 197, 201\ntime evolution of, 201\nuncertainty principle and, 177–178\nGedanken experiments, 97, 196\nEPR paradox, 97–102\nSchrödinger cat paradox, 102–105\nGeneralized angular momentum (J), 357–359\naddition of, 370–376\nmathematical rules for, 358\nGeneralized Rabi frequency, 91\nGeneral quantum state (c), 11\nGeneral quantum systems, 25–27, 62–63\nGerade state, 438–440, 442\nGerlach, Walther, 1\nGraphene, 497–498\nGraphite, 497–498\nGravity measurement, 514\nGrayscale density plots, for hydrogen energy  \neigenstates, 265, 267\nGround state, 108\nhelium atom, 428–430\ntwo-particle, 415–420\nGround state band, 479\nGroup velocity, 170, 493\nGrover’s search algorithm, 517\nGyromagnetic ratio, 72, 77, 356\nZeeman effect and, 394–395, 398, 401\nGyroscopic ratio, 3\n\n Index \n559\nH\nHadamard gate, 521, 523–524\nHamiltonian\ncenter-of-mass, 205–206\nchanges in energy eigenstates and, 445\nenergy eigenvalue equation and, 110\nharmonic oscillator, 277, 307\nperturbation, 313–319\nfor three-dimensional system of two  \nparticles, 204\ntime-dependent perturbation theory and, 445–462\nHamiltonian operators\nlight-matter interactions and, 92–93\ntime-dependent, 87–93\ntime-independent, 68–71\nHamiltonians. See also individual types\nHänsch, Theodor, 382\nHarmonic oscillator, 155, 275–311\nbasis states, 293, 311\nclassical, 275–276\nDirac notation, 289–293\nmanifestations of quantum mechanical  \npostulates, 308\nmatrix representations, 293–296\nmolecular vibrations, 305–307\nmomentum space wave function, 296–298\nperturbation theory and, 343–346\nquantum mechanical, 277–284\ntime dependence and, 300–304\nuncertainty principle and, 298–299\nwave functions, 284–289\nHarmonic oscillator Hamiltonian, 278–284\nHarmonic perturbation, 450–454\nHeisenberg, Werner, 103\nHeisenberg uncertainty principle, 56–57, 63\nharmonic oscillator and, 298–299\nmotion of electron’s wave packet and, 493\nposition and momentum and, 176–181\nHelicity, 465\nHelium atom\nexcited states, 431–433\nground state, 428–430\nsymmetrization postulate and, 427–433\nHelium Hamiltonian, 427\nHermite polynomials, 287–288, 297\nHermitian adjoint, 44\nHermitian matrices, 44\nHermitian operators, 44\nHamiltonian as, 68\nHidden variables, 99–100\nHilbert space, 10–11, 320, 368\nquantum information processing and, 514, 517\nHoles, 495–496\nHooke’s law, 275\nHopping matrix elements, 472, 474\nHybrids, of angular momentum eigenstates, 241\nHydrogen atom\nangular momentum in, 377, 378–379\nasymptotic solutions to radial equation, 252–253\nas boson, 413\nenergy eigenstates, 278\nenergy levels, 107–108, 382–386\nﬁne structure, 382–393\nfull hydrogen wave functions, 263–269\nground state energy, 384\nhydrogen energies and spectrum, 256–260\nhyperﬁne Hamiltonian for ground state of, 356–357\nhyperﬁne interaction effect on, 361–365\nhyperﬁne structure, 384–385\norbital angular momentum and, 207, 208, 209, 214\nperturbation of, 346–351\nprobability densities, 265–267, 274\n2p A 1s transition of, 465\nradial eigenvalue equation, 250–252\nradial probability integrand for 1s ground state, \n267–269\nradial wave functions, 261–263, 266–269\nrelative motion Hamiltonian and, 206–207, 208\nseparation of center-of-mass motion from relative \nmotion, 204–208\nseparation of variables, 215–218\nseries solution to radial equation, 253–256\nsolving energy eigenvalue problem for, 202–204\nspectroscopy of, 382\nStark effect in, 346–351\nsuperposition states, 270–271\ntransitions between states in, 260\ntransition wavelengths of, 109\nZeeman effect, 393–406\nZeeman structure of ground state, 406\nHydrogen chloride molecule\nenergy eigenvalues, 237\nmolecular vibration in, 306–307\nHydrogen energy eigenstates, radial wave  \nfunctions for, 262\nHydrogen energy formula, 388\nHydrogenic atoms, 251\nenergy eigenstate wave functions of, 263–269\nenergy eigenvalues of, 256\nradial wave functions of, 262\n\n560 \nIndex\nHydrogen molecule, 437–442\nhydrogen molecule H2, 440–442\nhydrogen molecule ion H2\n+, 438–440\nHydrogen wave functions, full, 263–269\nHyperﬁne interaction Hamiltonian, 356–357\nHyperﬁne perturbation Hamiltonian, 369–370\ndiagonalization of, 362–365\nHyperﬁne structure\naddition of generalized angular momenta, 355, \n370–376\ncoupled basis, 355, 365–370\ndiagonalization of hyperﬁne perturbation, 361–365\nof hydrogen, 355, 364–365, 378–379, 384–385\nhyperﬁne interaction, 355–357\nZeeman perturbation of 1s, 405–406\nI\nIdentical particles, 410–444\nhelium atom, 427–433\nhydrogen molecule, 437–442\ninteracting particles, 423–427\nin one dimension, 414–423\nperiodic table and, 434–437\ntwo spin-1/2 particles, 410–413\nIdentity matrix, 39\nIdentity operator 1, 45\nIncompatible observables, 8, 56\nIndependent electron approximation, 469\nIndirect band gaps, 496–497\nInﬁnite square well, 119–128, 155\nallowed energies, 156\nbarrier penetration, 135–136\ncompleteness, 137\nenergy eigenstates in, 278\nenergy spectrum of, 123\nﬁrst-order correction to perturbed,  \n328–329\ninversion symmetry and parity, 136\nnodes, 135\northonormality, 136\nschematic diagram of problem and solution, 127\nsuperposition states and time dependence  \nof, 140–145\ntime evolution of, 160\ntwo particles bound in, 417–421\nwave function curvature, 133–135\nInner product, 12, 13, 28\nmatrix notation and, 23\nprobability amplitude and, 15\nInput state, normalized, 21–22\nInstruction sets, 99–100\nInsulators, 491–494\nInterference, example of, 49–50\nInterference terms, 49\nIntrinsic angular momentum (S), 2–3\nInverse Fourier transform, 197\ntime-dependent generalization of, 172\nInversion symmetry, 136\nIonic bonding, 441\nIonization level, 429–430\ndouble, 429\nsingle, 430\nIonization limit, 256\nK\nKets (ket vectors), 4, 10, 11–14, 17–19, 28, 29. See \nalso Dirac notation; Eigenvectors\nenergy eigenvalue equation and, 110–111\nmatrix notation and, 22–25\nscalar product and, 12\nKinetic energy, 386\nKronecker delta, 26, 138, 139, 165, 166, 291, 294\nKronig-Penney model, 489–491\nk space, 478, 484\nallowed values of, 476–478\nL\nLadder operators, 281–284, 302, 307, 344\naddition of generalized angular momenta and, \n371–373\nangular momentum, 359–360, 362–363, 378\nmatrices for, 294–295\nLadder termination condition, 282\nLaguerre polynomials, 263\nassociated, 262–263\nLamb, Willis, 393\nLamb shift, 284, 384–385, 393, 464\nLandé g factor, 401\nmagnetic trapping and, 505\nLaplace series, 238\nLarmor frequency, 76–77, 79, 82–83, 333\nperturbing magnetic ﬁelds and, 313–314\ntime-dependent Hamiltonians and, 88, 91\nLarmor precession, 76–77, 82–83, 519\nLaser cooling, 506–514\nLaser excitation, 460–462\nLattice-matched growth, 146–147\nLCAO. See Linear combination of atomic orbitals \n(LCAO)\nLectures on Physics (Feynman), 8\n\n Index \n561\nLEDs. See Light-emitting diodes (LEDs)\nLegendre functions, 228, 233\nassociated, 233–235\nLegendre polynomials, 231–232, 233\nLegendre’s equation\nassociated, 229\nseries solution of, 228–233\nLeptons, 84–85\nLevitron, 506\nLight-emitting diodes (LEDs), 496\nLight-matter interactions, 92–93\nLight sensors, 496\nLinear combination of atomic orbitals (LCAO),  \n438, 469\nband structure of graphene and, 498\nenergy eigenvalues and eigenstates of N-well \nchain, 473–476\nenergy eigenvalues and eigenstates of two-well \nchain and, 471–473\nKronig-Penney method and, 489, 491\nsummary, 488–489\nLinear potential, 155\nLiquid helium, 422–423\nLocal hidden variable theory, 7, 99\nLocality principle, 99\nLogic gates, 518\nLorentzian curve, 91\nLorentzian frequency dependence, 461\nLorentzian function, 460\nLow-dimensional carbon, 497–498\nLowering operators, 278, 280–284\nLyman-a, 259\nLyman series, 259\nM\nMagnetic ﬁeld\nin a general direction, 78–84\nin magnetic trap, 504–505\nspin-orbit coupling and, 388–389\nin the z-direction, 72–78\nZeeman effect and, 394, 396–405\nZeeman effect with intermediate, 403–405\nZeeman effect with strong, 402–403\nZeeman effect with weak, 396–401\nMagnetic ﬁeld gradient, 3\nMagnetic moment (μ), 355–356, 357\nin Stern-Gerlach experiment, 1–3\nMagnetic quantum number (m), 62, 358, 378\nClebsch-Gordan coefﬁcient and, 464\nZeeman effect and, 394–395\nMagnetic resonance, 87–92\nMagnetic resonance imaging (MRI), 87\nMagnetic trapping, 502–506\nManifold of magnetic quantum number states, 358\nMaple software, 151, 243, 488\nMass eigenstates, 85\nMathematica software, 151, 243, 488\nMatLab, 151, 243\nMatrices\ncommutation relations and, 56\nrepresenting spin-1/2 operators, 63\nMatrix elements, 37–38\nhopping, 472, 474\nMatrix notation, 22–25, 29\neigenvalue equation in, 38, 39–40\noperators and, 35–36\nMatrix representations\nharmonic oscillator problem and, 293–296\nof operators, 37–38\nMaxwellian velocity distribution, in laser cooling, \n510–511\nMBE. See Molecular beam epitaxy (MBE)\nMean, 51\nMeasurement, 50–54\nBell-state, 524, 525–526\nenergy, 107–109, 242–244\nMermin, N. David, 98\nMesoscopic system, 104\nMetal-organic chemical vapor deposition (MOCVD), \n146\nMetals, 491–494\nMicroscopic systems, energy ﬁngerprint of, 107\nMinimum uncertainty state, 177–178\nMixed states, 19–20\nstatistical, 50\nin Stern-Gerlach experiment, 49–50\nsuperposition state vs., 19–20\nMixing angle, 85\nMOCVD. See Metal-organic chemical vapor deposi-\ntion (MOCVD)\nModulation envelope, of wave packet, 170\nMolecular beam epitaxy (MBE), 146\nMolecular eigenstate, 471\nMolecular orbitals, 478\nMolecular states, 473, 475, 480–482\nMolecular vibrations, harmonic oscillator and, \n305–307\nMolecular wave functions, 482–484\nMomentum, 110\nof center of mass, 204\n\n562 \nIndex\nMomentum, continued\ncomplementary to position, 180\ncrystal, 478\nof superposition state of harmonic oscillator, \n302–303\nuncertainty principle and, 176–181\nMomentum eigenstates, 163–167, 169, 180, 197\nMomentum eigenvalue equation, 163\nMomentum operators, 156\nmatrix representation, 295\nMomentum space wave function, 167, 171\nharmonic oscillator, 296–298\nMonochromatic excitation, 455\nMorse oscillator, 306\nMorse potential, 305–306\nMotion\nof a particle on a ring, 218–227\non a sphere, 227–244\nMRI. See Magnetic resonance imaging (MRI)\nMuon, decay to electron, 84\nN\nNearest-neighbor approximation, 474\nNeutrino mixing, 85\nNeutrino oscillations, 84–86\nNeutrinos\ndiscovery of, 84\nsolar neutrino problem, 84\nNewton’s second law, 151–152\nclassical harmonic oscillator and, 276\nNMR. See Nuclear magnetic resonance (NMR)\nNodes, 135\nNoether’s theorem, 210\nNondegenerate perturbation theory, 319–329, 351\nﬁrst-order energy correction, 320–324\nﬁrst-order state vector correction, 324–329\nsecond-order, 329–335\nNormalization, 11, 12–14\nof basis states, 165\nDirac, 166\nenergy eigenstates and, 156\nfor full hydrogen wave function, 264\nharmonic oscillator and, 291–292\nof input state, 21–22\nof perturbed state, 317\nof spin-1/2 basis vectors, 12\nof state vector, 26\nNormalization constant, 14\nNormalized vectors, 11\nNormalizing wave function, 114–115, 116\nNormal mode solutions, 475\nNuclear magnetic moments, 406\nNuclear magnetic resonance (NMR), 87\nNumber operator, 283–284\nNumerical solutions, to energy eigenstates, 151–153\nN-well chain, energy eigenvalues and eigenstates \nof, 473–476\nO\nObservable physical quantity, 4\nObservables\ncommuting, 54–56\ncompatible, 55–56\nincompatible, 8, 56\nsimultaneous, 55\nOdd parity, 136\nOperator F2, 366, 367, 368\nOperator J2, 358, 360, 390\nOperator L2, 390\nOperator method, 277–284\nOperators, 34–36. See also Angular momentum \noperators; Ladder operators\nactivities, 159\ncommuting, 54–56\ndiagonal in their own basis, 36\ndiagonalization of, 38–41\nHamiltonian, 68–71, 87–93\nHermitian, 44\nmatrix representation of, 37–38\nnew, 41–50\nprojection, 44–47\nspin component in general direction, 41–43\nOperator S2, 57–59, 390\nOperator S · I, 362–363, 369\nOperator Sn, 41\nOperator Sx, 41, 56, 57\nOperator Sy, 39–40, 41, 56, 57\nOperator Sz, 41, 46, 47, 50, 53–54, 59–61, 63\nidealized measurement of, 53–54\nmagnetic ﬁeld in the z-direction and, 72–73, \n75–76\nmatrix representation of, 36\nOptical molasses, 512–514\nOptical spectrum, 108\nOptical transitions, in helium, 433\nOptics interference analogy, 187\nOrbital angular momentum (L), 2, 3, 210–215, 357\nOrbital angular momentum quantum number, 212\nOrbital magnetic ﬁeld, spin-orbit coupling and, 388\nOrbital magnetic quantum number, 212\n\n Index \n563\nOrbitals\nantibonding, 473\nbonding, 473\nmolecular, 478\nOrthogonality, 11, 12, 136\naddition of generalized angular momenta and, \n371–372\nof basis states, 165\nenergy eigenstates and, 156\nharmonic oscillator and, 290–291\nOrthogonal vectors, 11\nOrthohelium, 433\nOrthonormality, 11, 26, 28, 136\nfor basis set of momentum states, 165–166\nharmonic oscillator and, 291\nspherical harmonics and, 238\nOscillating electric dipole moment, 270–271\nOscillator frequency, 277\nOuter product, 45\nOverall phase, 225\nP\nParahelium, 433\nParameters\na and b, 472–479, 486–489\ncalculation of, 486–489\nParity, 136\neven, 136\nof full hydrogen wave function, 267\nodd, 136\nrestriction on matrix elements and, 464\nspherical harmonics and, 239\nParity violation, 348\nParticle in a box, 120, 155\nParticle on a ring\nenergy spectrum for, 222\nmotion of, 218–227\nquantum measurements on, 223–224\nin superposition state, 223–227\nParticles. See Bound particles; Free particles; \n Identical particles\nPaschen series, 259\nPauli, Wolfgang, 84\nPauli exclusion principle, 410, 412, 418, 434, 442\nPauli matrices, 519\nPeriodic boundary conditions, 476–478\nPeriodic chain of wells, 469, 470\nenergy eigenvalues and eigenstates of, 471–476\nN-well chain, 473–476\ntwo-well chain, 471–473\nPeriodic systems, 469–501\napplications, 491–494\nBloch’s theorem, 480–482\nboundary conditions and allowed values of k, \n476–478\nBrillouin zones, 478\ncalculation of the model parameters, 486–489\ndensity of states, 484–486\ndirect and indirect band gaps, 496–497\neffective mass, 494–496\nenergy eigenvalues and eigenstates of periodic \nchain of wells, 471–476\nKronig-Penney model, 489–491\nlow-dimensional carbon, 497–498\nmolecular states, 480–482\nmolecular wave functions, 482–484\nmultiple bands from multiple atomic levels, \n478–480\nPeriodic table, 434–437\nPerturbation Hamiltonian, 313–319, 333\ndiagonalizing, 339–343, 348–350, 351\nPerturbations, 147–150\nPerturbation theory, 312–354\ndegenerate, 336–343\nharmonic oscillator and, 343–346\nhydrogen and, 346–351, 382–409\ninteracting particles and, 423–426\nnondegenerate. See Nondegenerate perturbation \ntheory\nspin-1/2 example, 313–317\nStark effect and, 346–351\ntime-dependent. See Time-dependent perturbation \ntheory\ntwo-level example, 317–319\nZeeman, of 1s hyperﬁne structure, 405–406\nPhase velocity, 162–163, 493\nPhonon, 496, 497\nPhotons, 194–195, 284\nband gaps in semiconductors and, 496–497\nEinstein model and, 455–460\nhelicity and, 465\nlaser cooling and, 507\nPhysical observables, 34–35\nPion, decay to muon, 84\n\u000b-pulse, 91\nPlanck blackbody radiation formula, 458\nPlanck’s constant, 3, 279\nPodolsky, Boris, 97\nPolar angle, spin component in general direction \nand, 41\n\n564 \nIndex\nPolar angle eigenvalue equation, 217, 218, 227–235\nPolar plots\nof associated Legendre functions, 234, 235\nof spherical harmonics, 240, 241, 242\nPolynomials\nassociated Laguerre, 262–263\nHermite, 287–288, 297\nLaguerre, 263\nLegendre, 231–232, 233\nPosition, 110\ncomplementary to momentum, 180\nuncertainty principle and, 176–181\nwave function and, 113–115\nPosition eigenstate, 180\nPosition operators, 156\nmatrix representation, 295\nPosition representation, 111\nPostulates of quantum mechanics, 27–28\n1 (one), 4–5, 27\nharmonic oscillator and, 289, 308\nhydrogen atom problem and, 308\nspin-1/2 system and, 308\n2 (two), 27, 34\nharmonic oscillator and, 289, 308\nhydrogen atom problem and, 308\nspin-1/2 system and, 308\n3 (three), 27\nharmonic oscillator and, 289, 308\nhydrogen atom problem and, 308\nspin-1/2 system and, 308\n4 (four), 14–15, 28, 29\nharmonic oscillator and, 293, 308\nhydrogen atom problem and, 308\nspin-1/2 system and, 308\n5 (ﬁve), 28, 46–47, 63\nharmonic oscillator and, 293, 308\nhydrogen atom problem and, 308\nspin-1/2 system and, 308\n6 (six), 28, 68\nharmonic oscillator and, 293, 308\nhydrogen atom problem and, 308\nspin-1/2 system and, 308\nmanifestations of, 308\nsymmetrization. See Symmetrization postulate\nPotential energy, 110. See also Finite square well; \nInﬁnite square well\ndouble-slit atom interferometer for measuring, 196\neffective, 250\nof electric dipole, 347\nof GaAs quantum well, 146–147\nharmonic oscillator, 277, 305\nmagnetic trapping and, 503, 505\nPotential energy diagram, 119\nPotential energy function, general, 276\nPotential energy shelf, 328\nPotential wells, 119–120\nbound and unbound states in, 182\ngeneral, 154, 155\nwaves incident upon, reﬂected from and  \ntransmitted through, 184\nPrecession\nLarmor, 76–77\nspin, 72–84\nPrincipal quantum number, hydrogen, 255, 261\nPrinciple of detailed balance, 457\nProbability\nprojection operators and, 46\nquantum mechanical, 14–15\nProbability amplitude, 15, 138\nwave function and, 116–118\nProbability cloud, 202\nProbability density, 154\nfor detecting particle on screen, 194\nof energy eigenstates of inﬁnite square  \nwell, 125–126\nof full hydrogen wave function, 265–267\nof Gaussian wave packet, 174–175\nof harmonic oscillator, 288–289, 301, 302, 304\nof hydrogen, 274\nof inﬁnite square well, 145\nof many-particle system, 421–422\nof momentum eigenstate, 165\nof momentum space, 298\none-particle, 421\nof position of particle on the ring, 224–226\ntwo-particle, 417–419, 421\nwave function and, 114, 118\nProbability function\nwave function and, 114\nProbability postulate (postulate 4), 14–15, 28, 29\nharmonic oscillator and, 293, 308\nhydrogen atom problem and, 308\nspin-1/2 system and, 308\nProduct\ndot (scalar), 11–12\ninner, 12, 13, 28\nouter, 45\nProduct state, 517\nProjection, 12\nof quantum state vector, 46\n\n Index \n565\nProjection operators, 44–47\nStern-Gerlach experiments and, 47–50\nProjection postulate (postulate 5), 28, 46–47, 63\nharmonic oscillator and, 293, 308\nhydrogen atom problem and, 308\nspin-1/2 system and, 308\nProjection theorem, 399–400\nProton\nhyperﬁne interaction between electron and, \n365–366\nin spin-orbit coupling, 388–389\nProton magnetic moment, 355–356, 357\nQ\nQED. See Quantum electrodynamics (QED)\nQualitative solutions, to energy eigenstates, 150–151\nQuantization, of spin angular momentum, 3–4\nQuantization condition, 122–123, 124\nQuantized energies, 107–160\nasymmetric square well, 147–150\nenergy eigenvalue equation, 110–112\nﬁnite square well, 128–137\nﬁtting energy eigenstates by computer, 151–154\nﬁtting energy eigenstates by eye, 150–151\ngeneral potential wells, 154\ninﬁnite square well, 119–128, 133–137\nquantum wells and dots, 146–147\nspectroscopy, 107–109\nsuperposition states and time dependence, 137–146\nwave function, 112–119\nQuantum atom, 202\nQuantum bits (qubits), 515–517\nentangled states, 516–517\nquantum teleportation and, 524–526\nsuperposition states, 515–516\nQuantum bound states, 160\nQuantum computer, 514\nQuantum computing algorithms, 517\nQuantum crystal, 500\nQuantum dots, 147\nQuantum electrodynamics (QED), 393, 459–460\ncavity, 460\nQuantum entanglements, 516–517\nQuantum fuzziness, 59\nQuantum gates, 518–524\ncontrolled-NOT gate, 522–524\nHadamard gate, 521, 523–524\nquantum NOT gate, 519–520\nQuantum information processing, 514–526\nquantum bits (qubits), 515–517\nquantum gates, 518–524\nquantum teleportation, 524–526\nQuantum measurements, on particle conﬁned to ring, \n223–224\nQuantum mechanical angular momentum, 210–215\nQuantum mechanical harmonic oscillator, 277–284\nQuantum mechanical tunneling, 188–192, 197, 201\nQuantum mechanics\nmodern applications. See Applications of quantum \nmechanics\nas nonlocal theory, 517\npostulates of. See Postulates of quantum mechanics\nQuantum NOT gate, 519–520\nQuantum number, 122–123\norbital angular momentum, 212\norbital magnetic, 212\nprincipal, 255\nQuantum parallelism, 516\nQuantum particles, behavior of, 1\nQuantum spookiness, 97–106\nEinstein-Podolsky-Rosen paradox, 97–102\nSchrödinger cat paradox, 102–105\nQuantum state vectors, 10–22, 154\nseparation of, 206–207\nspatial vectors and, 325\nsuperposition states, 19–22\nsymmetrization postulate and, 413\nwave function and, 112–113\nQuantum teleportation, 524–526\nQuantum wells, 146–147\nQuantum wires, 147\nQuasimomentum, 478\nR\nRabi, I. I., 87, 406\nRabi ﬂopping, 90–91, 92\nRabi frequency, 91\nRabi’s formula, 81–84, 90, 449\nRadial eigenvalue equation, 217, 218, 250–252\nasymptotic solutions to, 252–253\nseries solution to, 253–256\nRadial integrals, 463\nin relativistic energy correction, 388\nRadial position\nexpectation values of, 267–269\nRadial wave functions, 274\nhydrogen, 261–263, 266–269, 272\nRadiation pressure, 507\nRaising operators, 278, 280–284, 285, 286\nRamsauer-Townsend effect, 188\n\n566 \nIndex\nRate equation, 458\nReality, elements of, 97\nReciprocal space, 476\nRecoil velocity, 507\nRecurrence relation, 230–231, 254\nRed-shifted beam, 509\nReduced mass, 205\nReduction, of quantum state vector, 46\nReﬂection coefﬁcient, 186\nfor scattering from square barrier, 190–191\nRelative motion, separating center-of-mass and, \n204–208\nRelative phase, 225\nRelativistic energy, 386\nRelativistic energy correction, 386–388, 393\nRepresentation, 23\nResonance condition, 188\nRest mass energy, 386\nRetherford, Robert, 393\nRigid rotor, 227\nenergy eigenvalues of, 236–237\nRodrigues’ formula, 231\nRohrer, Heinrich, 192\nRoot-mean-square deviation (r.m.s. deviation), 52\nRosen, Nathan, 97\nRotational constant, 237\nRotation-vibration coupling, 306–307\nRow vector, 23\nRubidium laser cooling, 507–512\nRussell-Saunders notation, 377\nRydberg constant (R'), 259–260\nRydberg energy, 365, 383–384\nRydberg (Ryd), 260\nS\nScalar (dot) product, 11–12\nScanning tunneling microscope, 192\nScattering, 508–514\nunbound states and, 181–188\nScattering force, 508–514\nScattering states, 182–186, 197\nSchawlow, Arthur, 382\nSchrödinger, Erwin, 68\nSchrödinger cat paradox, 102–105\nSchrödinger equation, 28, 68–71\ntime-dependent Hamiltonians and, 88–90\ntime-dependent perturbation and, 446\ntime-dependent solution to, 137\nSchrödinger time evolution, 68–96\nharmonic oscillator and, 300\nmagnetic ﬁeld in general direction, 78–84\nmagnetic ﬁeld in the z-direction, 72–78\nneutrino oscillations, 84–86\nSchrödinger equation, 68–71\nspin precession, 72–84\nsuperposition states and, 225\ntime-dependent Hamiltonians, 87–93\ntime evolution of hydrogen atom and, 270–271\ntime-independent Hamiltonians, 68–71\nSearch algorithm, 517\nSecond-order energy correction, 344\nSecond-order nondegenerate perturbation theory, \n329–335\nSecond-order perturbation equation, 329\nSecular (characteristic) equation, 39–40\nSelection rules, 93, 462–465, 466\nSemiclassical method, atom-light interaction, 459\nSemiconductor quantum wells, 146–147\nSemiconductors, 491–494. See also GaAs; \n Silicon (Si)\nband gaps and, 496–497\nband structure and density of states of Si, 469–470\ngraphene as, 498\nSeparation constant, 217\nSeries solution of Legendre’s equation, 228–233\nSeries solution to radial eigenvalue equation, 253–256\nShell, 434\nShell number, 256\nShooting method model, 160\nShor, Peter, 517\nShor’s factorization algorithm, 517\nSilicon (Si), 492–493\nband gap of, 496–497\nband structure and density of states of, 469–470\ndensity of states for, 470\nas indirect-gap semiconductor, 497\nSilver atom, magnetic moment of, 3\nSimultaneous observables, 55\nSimultaneous sets of eigenstates, 55\nSingle ionization level, 430\nSingle-particle problem, 469\nSingle-slit diffraction, 10\nSinglet state, 368, 369\nantisymmetric, 416\nSodium, band states of, 492\nSodium nucleus\nﬁrst-order energy shifts due to perturbation of, \n323–324\nsecond-order energy shifts due to perturbation of, \n332–335\n\n Index \n567\nSoftware\nMaple, 151, 243, 488\nMathematica, 151, 243, 488\nMatLab, 151, 243\nSPINS. See SPINS software\nSolar cells, 496\nSolar neutrino problem, 84\nSpatial integrals, in relativistic energy correction, 388\nSpatial symmetry, 438\nSpatial vectors, 11\nproperties of, 11\nquantum state vectors and, 325\nSpectroscopic experiments, 445\nSpectroscopic notation, 377\nSpectroscopy, 93, 107–109\nof hydrogen atom, 382\nSphere, motion on, 227–244\nSpherical coordinates, 215–218\nenergy eigenvalue equation in, 208–209\nSpherical harmonic expansion, 243–244\nSpherical harmonics, 227, 237–240, 463\ncoefﬁcients of spherical harmonic  \nexpansion, 243\ncompleteness and, 238\northonormality and, 238\nparity and, 239\nproperties of, 238–239\nvisualization of, 240–244\nsp3 hybrid orbitals, angular dependence of, 214, 215\nSpin, 2\nin Stern-Gerlach experiment, 2–3\nZeeman effect with, 396\nZeeman effect without, 394–396\nSpin angular momentum quantum number (spin \nquantum number) (s), 62\nSpin angular momentum (S), 211–212, 357\nSpin component in general direction, 41–43\nSpin component measurement, 6–9, 10–11, 15–17, \n22, 24\nafter state preparation in new direction, 42–43\nSpin component quantum number (magnetic  \nquantum number) (m), 62\nSpin down, 4\nSpin ﬂip, 80–82, 90–92\nSpin-1/2 operators, matrices representing, 63\nSpin-1/2 particles, system of two, 381,  \n410–413\nClebsch-Gordon coefﬁcients for, 369\ncoupled basis in terms of uncoupled basis, \n365–370, 379\nSpin-1/2 systems, 4\nexpectation values and, 51–52\nket as basis, 11\nmeasurement in, 50–51\nperturbation and, 313–317\npostulate 4, 14–15\nSchrödinger time evolution and, 72–84\nspin angular momentum, 211–212\nvector model of, 58\nSpin-1 system, 59–62\nSpin-orbit coupling, 388–393\nSpin-orbit energy correction, 391, 392–393\nSpin-orbit Hamiltonian, 390\nSpin precession, 72–84\nexperiments, 77–78, 80–81\nmagnetic ﬁeld in general direction, 78–84\nmagnetic ﬁeld in the z-direction, 72–78\ntime-dependent Hamiltonians and, 90\nSpin-singlet fermions, interacting, 426\nSPINS software, 32–33, 521\nmagnetic ﬁeld in the z-direction, 74\nmatrix calculations in, 23\nmixed state and, 21\nStern-Gerlach experiment and, 5, 8\nWhich Path experiments and, 50\nSpin-statistics theorem, 412\nSpin-triplet fermions interacting, 426–427\nSpin up, 4\nSpin up state, eigenvalue equation for, 35\nSpin vector, S2 operator and magnitude of, \n57–59\nSpontaneous emission, 457–458\nSpontaneous emission rate, 458\ns-p superposition, 271\nSquare potential energy barrier, 189\nSquare wells, 470\nasymmetric, 147–150\nﬁnite. See Finite square well\ninﬁnite. See Inﬁnite square well\ns-states, Darwin term and, 392\nStandard deviation, 51, 52–54\ndeﬁned, 52\nStark effect, 271\nin hydrogen, 346–351\nlaser cooling and, 512\nState density, 484–486\nState vectors\nﬁrst-order correction to, 324–329\nnormalization of, 12–14, 15, 26\nStatic electric dipole moment, 271\n\n568 \nIndex\nStationary states, 70\nStatistical mixed state, 50\nStern, Otto, 1\nStern-Gerlach device, 2, 4\nStern-Gerlach experiment(s), 1–33\nwith applied uniform magnetic ﬁeld, 74\nexperiment 1, 5–6\nexperiment 2, 6–7, 60\nexperiment 3, 7–8, 47–50\nexperiment 4, 8–10, 47–50\ngeneral quantum systems, 25–27\nmagnetic trapping and, 502–503\nmatrix notation, 22–25\npostulates, 27–28\nprojection operator and, 47\nquantum state vectors, 10–22\nschematic of, 4\nsimulation, 33\nspin-1 system, 59–62\nZeeman effect and, 395–396, 406\nStern-Gerlach spin precession experiment,  \nquantum computing and, 520, 521\nStimulated emission, 452\nStong-ﬁeld seeking states, 504\nStretched state, 371\nSubshell, 434–437\nSuperﬂuid, 422–423\nSuperposition states, 19–22, 137–146\ncoherent, 50\ncoherent superposition and, 20\ncontinuous, 171–176\ndiscrete, 168–171\nof harmonic oscillator, 300–304\nhydrogen atom, 270–271\nmixed state vs., 19–20\nof momentum eigenstates, 165\nparticle on a ring in, 223–227\nprobability distribution of, 145\nquantum bits and, 515–516\nin Schrödinger cat experiment, 103\nin Stern-Gerlach experiment, 49\nSymmetric states, 411–413\nbosons and, 419, 421, 442\nSymmetric triplet states, 416\nSymmetrization postulate, 412–413, 418\nconsequences for many-particle system, 421–423\nhelium atom and, 427–433, 434\nperiodic table and, 434\nSz basis, 11\ndiagonalization of matrix and, 40–41\nSz representation, 23\nT\nTau, 84\nTaylor series expansion, 275\nTerm notation, 377\nThomas precession, 390\nTime dependence, 137–146\nin harmonic oscillator, 300–304\nof momentum eigenstate, 164\nTime-dependent Hamiltonians, 87–93\nlight-matter interactions, 92–93\nmagnetic resonance, 87–92\nTime-dependent perturbation theory, 445–468\nconstant perturbation, 449\nelectric dipole interaction, 454–462\nGaussian perturbation, 449–450\nharmonic perturbation, 450–454\nselection rules, 462–465\ntransition probability, 445–450\nTime-dependent problems, solving, 93\nTime evolution\nof energy eigenstates, 162–163\nof Gaussian wave packet, 201\nof harmonic oscillator, 311\nof inﬁnite square well solutions, 160\nwave packet and, 168–169\nTotal angular momentum, 503\naddition of angular momenta and, 372\ncoupled basis and, 366–367, 369, 371\nspectroscopic notation of, 377, 378\nZeeman effect and, 390, 391, 399\nTranscendental equations\nasymmetric square well, 150\nﬁnite square well, 130–131, 132\nTransition probability, 445–450\nas function of time, 452\nTransition rate, 453\nTransitions, 87, 445\nforbidden, 462\nharmonic oscillator and, 302, 306, 307\nin helium excited states, 433\nin hydrogen, 260\nTransmission coefﬁcient, 185–186\nfor scattering from square barrier, 191\nTransmission probability, for quantum mechanical \ntunneling, 190\nTriplet state, 368, 369\nsymmetric, 416\nTunneling, quantum mechanical, 188–192, 197, 201\n\n Index \n569\nTwo-body system, center-of-mass for, 204–205\nTwo-level system, perturbation and, 317–319\nTwo-particle Hamiltonian, 415\nTwo-particle probability density, 414\nTwo particles in one dimension, 414–423\nexchange interaction, 420–421\nsymmetrization postulate, consequences of, 421–423\ntwo-particle excited state, 416–417\ntwo-particle ground state, 415–416\nvisualization of states, 417–420\nTwo-particle wave function, 414\nTwo-state spin-1/2 quantum system, properties of \nnormalization, orthogonality and completeness \nin, 1\nTwo-well chain, energy eigenvalues and eigenstates \nof, 471–473\nU\nUnbound eigenstates, 469\nUnbound states, 120, 161–201\natom interferometry, 192–196\nfree particle eigenstates, 161–167\nscattering and, 181–188\ntunneling through barriers and, 188–192\nuncertainty principle, 176–181\nwave packets, 168–176\nUncertainty principle. See Heisenberg uncertainty \nprinciple\nUncoupled basis, 355, 361, 366, 378\nClebsch-Gordon coefﬁcients and, 374, 375\neigenstates, 366–369\nidentical particles and, 410–411, 415\nperturbation of hydrogen and, 390–391\nquantum bits and, 516, 517\nZeeman effect and, 397, 402–403, 404–405, 406\nUngerade state, 438–440, 442\nUnit vectors i, j, k ( n over letters), 11\nUpdate equations, 152\nV\nValence band, 493\nValence bond method, 441–442\nVector model, 58\nVectors. See also Bras (bra vectors); Eigenvectors; \nKets (ket vectors)\ncolumn, 22–23\ncomplete, 11\ncoupled basis, 378\nnormalized, 11\northogonal, 11\nquantum state. See Quantum state vectors\nrow, 23\nspatial, 11, 325\nspin, 57–59, 121, 123, 129, 484\nstate, 12–14, 15, 26, 324–329\nunit, 11\nwave, 121, 123, 129, 484\nVector space, Hilbert space, 10–11\nVelocity\nelectron, 386–388\ngroup, 170, 493\nphase, 162–163, 493\nrecoil, 507\nVelocity Verlet algorithm, 152\nVisualization of spherical harmonics, 240–244\nW\nWave function curvature, 133–135\nWave function formulae, translating bra-ket formulae \nto, 116, 154\nWave functions, 111, 112–119\nantisymmetric, 418\nboundary conditions on, 122, 156\nDirac expression for, 166\nof energy eigenstates of inﬁnite square well, 124\nGaussian momentum space, 172–176\nof harmonic oscillator, 284–289\nhydrogen, 263–269\nmolecular, 482–484\nmomentum space, 167, 171, 296–298\nnormalizing, 114–115, 116\nfor particle conﬁned to sphere, 227–228\nfor particle on a ring, 224–225\nof particle tunneling through barrier, 191\nradial, 261–263\nspherical harmonics, 244\nWave interference, 10\nWave packets, 165, 168–176, 197\ncontinuous superposition, 171–176\ndiscrete superposition, 168–171\nenvelope of, 170\nquantum tunneling and, 201\nuncertainty principle and, 176–181\nWave-particle duality, 125\nWave vector, 121, 484\nﬁnite square well, 129\ninﬁnite square well, 123, 129\nWave vector quantization condition, 123\nWeak-ﬁeld seeking states, 504\nWeak force/weak interaction, 84\n\n570 \nIndex\n“Welcher Weg” experiment, 50\n“Which Path” experiment, 50\nWigner-Eckhart theorem, 399\nX\nx-axis\nmagnetic ﬁeld component along, 78–79\nprobability for measuring spin component along, 75\nin Stern-Gerlach experiment, 6–7\nx-z plane, density plots in, 265, 267\nY\nYoung, Thomas, 10, 194\nZ\nz-axis\nprobability for measuring spin component  \nalong, 75\nin Stern-Gerlach experiment, 2–4, 5–8\nz-component, in Stern-Gerlach experiment, 3\nz-direction\nmagnetic ﬁeld in, 72–78\nmagnetic resonance and, 87–88\nZeeman effect, 382, 388, 393–406\nanomalous, 396, 405\nﬁrst-order energy correction, 400\nintermediate magnetic ﬁeld, 403–406, 407\nlaser cooling and, 512\nwith spin, 396\nstrong magnetic ﬁeld, 402–403, 405–406, 407\nweak magnetic ﬁeld, 396–401, 405–406, 407\nwithout spin, 394–396\nZeeman perturbation of 1s hyperﬁne structure, \n405–406\nZeeman energy levels, magnetic trapping and, 504\nZeeman Hamiltonian, 398\nZeeman perturbation Hamiltonian, 394\nZeroes, in Clebsch-Gordon tables, 375\nZero-point energy, 282\nZitterbewegung, 392\n\nUseful Definitions and Equations\nState vector, wave function:\t\n0 c9 \u001f c(x) = 8x\u001fc9\nNormalization:\t\n8c \u001f c9 =\nL\n\u001f\n-\u001f\n0\n c(x)02 dx = 1 \nMeasurement probability:\t\nan = 08an\u001fc902 = `\nL\n\u001f\n-\u001f\nw*\nan(x)c(x)dx `\n2\nExpectation value:\t\n8A9 = 8c0 A 0c9 = a\nn\nanan\nProbability density:\t\n (x) = 0 c(x)0 2\nPosition probability:\t\na  6  x 6  b =\nL\nb\na\n0c(x)0 2 dx\nPosition representation:\t\nxn \u001f  x,   pn \u001f -iU d\ndx\nEnergy eigenvalue equation:\t\nH0En9 = En0En9,  Hwn(x) = Enwn(x)\nOrthogonality:\t\n8En\u001fEm9 =\nL\n\u001f\n-\u001f\nw*\nn(x)wm(x)dx = dnm\nCompleteness:\t\n0c9 = a\nn\ncn0En9,  c(x) = a\nn\ncnwn(x)\n\nUseful Definitions and Equations\nSchrödinger equation:\t\niU d\ndt 0 c(t)9 = H(t)0 c(t)9\nSchrödinger time evolution:\t\n0 c(t)9 = a\nn\ncne-iEnt>U0En9\nPosition-momentum commutator:\t\n3xn ,  pn4 = iU\nMomentum space wave function:\t\nf(p) = 8p@ c9 =\n1\n22pU L\n\u001f\n-\u001f\n c(x)e-i px>Udx\nMomentum eigenstate:\t\n0 p9 \u001f wp(x) =\n1\n22pU\n eipx>U\nde Broglie wavelength:\t\nldeBroglie = h\np\nHeisenberg uncertainty relation: \t\n\u001ex\u001ep Ú U\n2\nPerturbation corrections:\t\nE(1)\nn\n= H\u001dnn = 8n(0)0H\u001d0n(0)9\n\t\nE(2)\nn\n= a\nm\u001fn\n08n(0)0H\u001d0m(0)902\n1E(0)\nn\n- E(0)\nm 2\nTransition probability:\t\ni S f(t) = 1\nU2 `\nL\nt\n0 8f\u001f H\u001d(t\u001d)\u001f i9ei1Ef -Ei2t\u001d>Udt\u001d `\n2\n\nSpin and Angular Momentum Relations\nSpin eigenvalue equations:\t\nSz 0+9 = U\n2 0+9, Sz0 -9 = - U\n2 0 -9\nSpin-1/2 eigenstates:\t\n\u001f+9 \u001f a1\n0b  \u001f+9x \u001f\n1\n22\n a1\n1b  \u001f+9y \u001f\n1\n22\n a1\ni b\n\t\n\u001f-9 \u001f a0\n1b  \u001f-9x \u001f\n1\n22\n a 1\n-1b \u001f-9y \u001f\n1\n22\n a 1\n-ib\nSpin-1/2 matrices:\n\t\nSx \u001f U\n2 a0\n1\n1\n0b\t\nSy \u001f U\n2 a0\n-i\ni\n0 b\n\t\nSz \u001f U\n2 a1\n0\n0\n-1b    S2 \u001f 3U2\n4  a1\n0\n0\n1b\nSpin-1 matrices:\n\t\nSx \u001f\nU\n12 °\n0\n1\n0\n1\n0\n1\n0\n1\n0\n¢    Sy \u001f\nU\n12 °\n0\n-i\n0\ni\n0\n-i\n0\ni\n0\n¢\n\t\nSz \u001f U °\n1\n0\n0\n0\n0\n0\n0\n0\n-1\n¢\t\nS2 \u001f 2U2 °\n1\n0\n0\n0\n1\n0\n0\n0\n1\n¢\nAngular momentum:\n\t\nJ 20 jmj9 =  j(j + 1)U20 jmj9\n\t\nJz0 jmj9 = mjU0 jmj9\n\t\nJ{0  jmj9 = U 3j( j + 1) - mj(mj { 1)4\n1>20\n  j, mj { 19\nOrbital angular momentum:\n L2Y/\nm(u, f) = /(/ + 1) U2Y/ m(u, f),  / = 0,1,2,3, ...\n\t\nLzY/ m(u, f) = m  UY / m(u, f),       m = -/,...,/\nAngular momentum commutators:\t 3Jx,Jy4 = iUJz,  3Jy,Jz4 = iUJx,  3Jz, Jx4 = iU Jy\n\t\n3J 2, Jx4 = 3J 2, Jy4 = 3J 2,Jz4 = 0 \n\nBound State Systems\nInfinite square well:\t\n En = n2p2U2\n2mL2 , n = 1,2,3, ...\n\t\n wn(x) = A\n2\nL\n sin npx\nL\nHydrogen atom:\t\nEn = -  1\nn2 m\n2U2 a e2\n4pe0\nb\n2\n= -  1\nn2 1\n2\n a2mc2 = -  1\nn2 13.6 eV\nHarmonic oscillator:\t\n En = Uv an + 1\n2b, n = 0,1,2,3, ...\n\t\na = A\nmv\n2U axn + i pn\nmvb\n\t\na† = A\nmv\n2U axn - i pn\nmvb\n\t\na0 n9 = 2n0 n - 19\n\t\na†0 n9 = 2n + 10 n + 19\nFundamental Constants \nPlanck’s constant:\t\nU = 6.582 * 10-16 eVs\nSpeed of light:\t\nc = 299  792  458 m>s\nElectron mass:\t\nme c2 = 511 keV\nProton mass:\t\nmp c2 = 938 MeV\nFine-structure constant:\t\na =\ne2\n4pe0 Uc \u001e\n1\n137\nBohr radius:\t\na0 = 0.0529 nm\nBohr magneton:\t\nmB\nh = 1.40 MHz>Gauss\n",
  "first_page": "",
  "extracted_at": "2026-02-08T00:47:06.261004+00:00"
}